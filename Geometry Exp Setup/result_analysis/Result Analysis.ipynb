{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_state(val, theta):\n",
    "    if val >= theta: return 1\n",
    "    else: return 0\n",
    "\n",
    "\n",
    "def compare_result(result, truth):\n",
    "    if truth == 1:\n",
    "        if result == 1: return 1\n",
    "        elif result == 0: return 2\n",
    "    elif truth == 0:\n",
    "        if result == 1: return 3\n",
    "        elif result == 0: return 4\n",
    "\n",
    "\n",
    "def estimation_measures(TP, FN, FP, TN):\n",
    "    if TP == 0 and FP == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = TP/(TP + FP)\n",
    "\n",
    "    if TP == 0 and FN == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = TP/(TP + FN)\n",
    "\n",
    "    if precision == 0 and recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2*precision*recall/(precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "\n",
    "def check_estimation(y_estimate, y_truth, theta):\n",
    "    TP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    for i in range(len(y_estimate)):\n",
    "        estimated = y_estimate[i]\n",
    "        truth = y_truth[i]\n",
    "        result = check_state(estimated, theta)\n",
    "        score_state = compare_result(result, truth)\n",
    "        if score_state == 1: TP += 1\n",
    "        elif score_state == 2: FN += 1\n",
    "        elif score_state == 3: FP += 1\n",
    "        elif score_state == 4: TN += 1\n",
    "    data = estimation_measures(TP, FN, FP, TN)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_theta(est_val, ground_val):\n",
    "    theta_values = [float(\"{0:.2f}\".format((0.0 + 0.02*i))) for i in range(50)]\n",
    "    f1_score = []\n",
    "    for theta in theta_values:\n",
    "        f1 = check_estimation(est_val, ground_val, theta)\n",
    "        f1_score.append(f1)\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(1, len(theta_values)):\n",
    "        if f1_score[index] < f1_score[i]:\n",
    "            index = i\n",
    "#     print(theta_values[index], f1_score[index])\n",
    "#     plt.plot(theta_values, f1_score)\n",
    "#     plt.show()\n",
    "    return theta_values[index]\n",
    "\n",
    "\n",
    "def get_predicted_value(X, theta):\n",
    "    pred = []\n",
    "    for i in range(len(X)):\n",
    "        if X[i] < theta:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(result):\n",
    "    print(\"Precision: %.1f\" % result[0])\n",
    "    print(\"Recall: %.1f\" % result[1])\n",
    "    print(\"F1 Score: %.1f\" % result[2])\n",
    "    print(\"Area Under Curve: %.1f\" % result[3])\n",
    "    \n",
    "\n",
    "def evaluation_results(y_test, y_predict):\n",
    "    recall = recall_score(y_test, y_predict)*100\n",
    "    precision = precision_score(y_test, y_predict)*100\n",
    "    f1 = f1_score(y_test, y_predict)*100\n",
    "    auc = roc_auc_score(y_test, y_predict)*100\n",
    "    return [precision, recall, f1, auc]\n",
    "\n",
    "\n",
    "def k_fold_testing(X, Y):\n",
    "    results = []\n",
    "    kf = KFold(n_splits = 5)\n",
    "    kf.get_n_splits(X)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "        opt_theta = get_optimal_theta(x_train, y_train)\n",
    "        y_predict = get_predicted_value(x_test, opt_theta)\n",
    "        result = evaluation_results(y_test, y_predict)\n",
    "        results.append(result)\n",
    "    results = np.array(results)\n",
    "    final_result = np.mean(results, axis = 0)\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - Proposed Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_propose = pd.read_csv(\"predicted_prereq.csv\")\n",
    "X_propose1 = df_propose[[\"tfidf_score\"]].to_numpy().ravel()\n",
    "X_propose2 = df_propose[[\"wiki_tfidf_score\"]].to_numpy().ravel()\n",
    "y_propose = df_propose[[\"relation\"]].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 62.2\n",
      "Recall: 49.3\n",
      "F1 Score: 54.9\n",
      "Area Under Curve: 67.9\n"
     ]
    }
   ],
   "source": [
    "# Proposed book tfidf\n",
    "result = k_fold_testing(X_propose1, y_propose)\n",
    "print_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 31.2\n",
      "Recall: 100.0\n",
      "F1 Score: 47.5\n",
      "Area Under Curve: 50.0\n"
     ]
    }
   ],
   "source": [
    "# Proposed book + wikipedia tfidf\n",
    "result = k_fold_testing(X_propose2, y_propose)\n",
    "print_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - RefD Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_propose = pd.read_csv(\"refd_estimated_results.csv\")\n",
    "X_propose1 = df_propose[[\"refd_equal\"]].to_numpy().ravel()\n",
    "X_propose2 = df_propose[[\"refd_tfidf\"]].to_numpy().ravel()\n",
    "y_propose = df_propose[[\"relation\"]].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 50.6\n",
      "Recall: 71.5\n",
      "F1 Score: 59.1\n",
      "Area Under Curve: 69.8\n"
     ]
    }
   ],
   "source": [
    "# RefD with Equal Method\n",
    "result = k_fold_testing(X_propose1, y_propose)\n",
    "print_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 39.3\n",
      "Recall: 80.1\n",
      "F1 Score: 52.7\n",
      "Area Under Curve: 62.1\n"
     ]
    }
   ],
   "source": [
    "# RefD with TFIDF Method\n",
    "result = k_fold_testing(X_propose2, y_propose)\n",
    "print_results(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results - RefD Given Vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_propose = pd.read_csv(\"refd_given_vals.csv\")\n",
    "X_propose1 = df_propose[[\"refd_val\"]].to_numpy().ravel()\n",
    "# X_propose2 = df_propose[[\"refd_tfidf\"]].to_numpy().ravel()\n",
    "y_propose = df_propose[[\"output\"]].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 51.0\n",
      "Recall: 74.3\n",
      "F1 Score: 60.1\n",
      "Area Under Curve: 70.7\n"
     ]
    }
   ],
   "source": [
    "# RefD with Equal Method\n",
    "result = k_fold_testing(X_propose1, y_propose)\n",
    "print_results(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
