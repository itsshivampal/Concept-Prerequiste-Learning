So far, we have discussed decision theory in the context of classification prob-
lems. We now turn to the case of regression problems, such as the curve fitting
example discussed earlier. The decision stage consists of choosing a specific esti-
mate y(x) of the value of t for each input x. Suppose that in doing so, we incur a
loss L(t, y(x)). The average, or expected, loss is then given by
E[L] =
L(t, y(x))p(x, t) dx dt.
(1.86)
A common choice of loss function in regression problems is the squared loss given
by L(t, y(x)) = {y(x) − t}2. In this case, the expected loss can be written
E[L] =
{y(x) − t}2p(x, t) dx dt.
Our goal is to choose y(x) so as to minimize E[L].
If we assume a completely
flexible function y(x), we can do this formally using the calculus of variations to
give
δE[L]
δy(x)
= 2
{y(x) − t}p(x, t) dt = 0.
Solving for y(x), and using the sum and product rules of probability, we obtain
(1.87)
(1.88)
tp(x, t) dt
y(x) =
p(x)
tp(t|x) dt = Et[t|x]
(1.89)
46
1. INTRODUCTION
Section 8.2
Section 8.2.2
Section 1.1
Appendix D
Figure 1.28 The regression function y(x),
which minimizes the expected
squared loss, is given by the
mean of the conditional distri-
bution p(t|x).
t
y(x0)
1.5. Decision Theory
47
y(x)
p(t|x0)
x0
x
Exercise 1.25
which is the conditional average of t conditioned on x and is known as the regression
function. This result is illustrated in Figure 1.28. It can readily be extended to mul-
tiple target variables represented by the vector t, in which case the optimal solution
is the conditional average y(x) = Et[t|x].
We can also derive this result in a slightly different way, which will also shed
light on the nature of the regression problem. Armed with the knowledge that the
optimal solution is the conditional expectation, we can expand the square term as
follows
{y(x) − t}2 = {y(x) − E[t|x] + E[t|x] − t}2
= {y(x) − E[t|x]}2 + 2{y(x) − E[t|x]}{E[t|x] − t} + {E[t|x] − t}2
where, to keep the notation uncluttered, we use E[t|x] to denote Et[t|x]. Substituting
into the loss function and performing the integral over t, we see that the cross-term
vanishes and we obtain an expression for the loss function in the form
E[L] =
{y(x) − E[t|x]}2 p(x) dx +
{E[t|x] − t}2p(x) dx.
(1.90)
The function y(x) we seek to determine enters only in the first term, which will be
minimized when y(x) is equal to E[t|x], in which case this term will vanish. This
is simply the result that we derived previously and that shows that the optimal least
squares predictor is given by the conditional mean. The second term is the variance
of the distribution of t, averaged over x.
It represents the intrinsic variability of
the target data and can be regarded as noise. Because it is independent of y(x), it
represents the irreducible minimum value of the loss function.
As with the classification problem, we can either determine the appropriate prob-
abilities and then use these to make optimal decisions, or we can build models that
make decisions directly. Indeed, we can identify three distinct approaches to solving
regression problems given, in order of decreasing complexity, by:
(a) First solve the inference problem of determining the joint density p(x, t). Then
normalize to find the conditional density p(t|x), and finally marginalize to find
the conditional mean given by (1.89).
Section 5.6
Exercise 1.27
E[Lq] =
|y(x) − t|qp(x, t) dx dt
(1.91)
which reduces to the expected squared loss for q = 2. The function |y − t|q is
plotted against y − t for various values of q in Figure 1.29. The minimum of E[Lq]
is given by the conditional mean for q = 2, the conditional median for q = 1, and
the conditional mode for q → 0.
1.6.
Information Theory
In this chapter, we have discussed a variety of concepts from probability theory and
decision theory that will form the foundations for much of the subsequent discussion
in this book. We close this chapter by introducing some additional concepts from
the field of information theory, which will also prove useful in our development of
pattern recognition and machine learning techniques. Again, we shall focus only on
the key concepts, and we refer the reader elsewhere for more detailed discussions
(Viterbi and Omura, 1979; Cover and Thomas, 1991; MacKay, 2003) .
We begin by considering a discrete random variable x and we ask how much
information is received when we observe a specific value for this variable. The
amount of information can be viewed as the ‘degree of surprise’ on learning the
value of x. If we are told that a highly improbable event has just occurred, we will
have received more information than if we were told that some very likely event
has just occurred, and if we knew that the event was certain to happen we would
receive no information. Our measure of information content will therefore depend
on the probability distribution p(x), and we therefore look for a quantity h(x) that
is a monotonic function of the probability p(x) and that expresses the information
content. The form of h(·) can be found by noting that if we have two events x
and y that are unrelated, then the information gain from observing both of them
should be the sum of the information gained from each of them separately, so that
h(x, y) = h(x) + h(y). Two unrelated events will be statistically independent and
so p(x, y) = p(x)p(y). From these two relationships, it is easily shown that h(x)
must be given by the logarithm of p(x) and so we have
Exercise 1.28
48
1. INTRODUCTION
(b) First solve the inference problem of determining the conditional density p(t|x),
and then subsequently marginalize to find the conditional mean given by (1.89).
(c) Find a regression function y(x) directly from the training data.
The relative merits of these three approaches follow the same lines as for classifica-
tion problems above.
The squared loss is not the only possible choice of loss function for regression.
Indeed, there are situations in which squared loss can lead to very poor results and
where we need to develop more sophisticated approaches. An important example
concerns situations in which the conditional distribution p(t|x) is multimodal, as
often arises in the solution of inverse problems. Here we consider briefly one simple
generalization of the squared loss, called the Minkowski loss, whose expectation is
given by
2
q
t
1
y
0
−2
2
q
t
1
y
q = 0.3
−1
0
y − t
1
2
q = 2
0
−2
−1
1
2
0
y − t
