The posterior mean solution (3.53) for the linear basis function model has an in-
teresting interpretation that will set the stage for kernel methods, including Gaussian
processes. If we substitute (3.53) into the expression (3.3), we see that the predictive
mean can be written in the form
Chapter 6
y(x, mN ) = mT
N φ(x) = βφ(x)TSN ΦTt =
βφ(x)TSN φ(xn)tn
(3.60)
n=1
where SN is defined by (3.51). Thus the mean of the predictive distribution at a point
x is given by a linear combination of the training set target variables tn, so that we
can write
where the function
y(x, mN ) =
k(x, xn)tn
n=1
k(x, x) = βφ(x)TSN φ(x)
(3.61)
(3.62)
is known as the smoother matrix or the equivalent kernel. Regression functions, such
as this, which make predictions by taking linear combinations of the training set
target values are known as linear smoothers. Note that the equivalent kernel depends
on the input values xn from the data set because these appear in the definition of
SN . The equivalent kernel is illustrated for the case of Gaussian basis functions in
Figure 3.10 in which the kernel functions k(x, x) have been plotted as a function of
x for three different values of x. We see that they are localized around x, and so the
mean of the predictive distribution at x, given by y(x, mN ), is obtained by forming
a weighted combination of the target values in which data points close to x are given
higher weight than points further removed from x. Intuitively, it seems reasonable
that we should weight local evidence more strongly than distant evidence. Note that
this localization property holds not only for the localized Gaussian basis functions
but also for the nonlocal polynomial and sigmoidal basis functions, as illustrated in
Figure 3.11.
160
3. LINEAR MODELS FOR REGRESSION
Figure 3.11 Examples of equiva-
lent kernels k(x, x) for x = 0
plotted as a function of x, corre-
sponding (left) to the polynomial ba-
sis functions and (right) to the sig-
moidal basis functions shown in Fig-
ure 3.1. Note that these are local-
ized functions of x even though the
corresponding basis functions are
nonlocal.
0.04
0.02
0
−1
0
N
1
0.04
0.02
0
−1
1
0
Further insight into the role of the equivalent kernel can be obtained by consid-
ering the covariance between y(x) and y(x), which is given by
cov[y(x), y(x)] = cov[φ(x)Tw, wTφ(x)]
= φ(x)TSN φ(x) = β−1k(x, x)
(3.63)
where we have made use of (3.49) and (3.62). From the form of the equivalent
kernel, we see that the predictive mean at nearby points will be highly correlated,
whereas for more distant pairs of points the correlation will be smaller.
The predictive distribution shown in Figure 3.8 allows us to visualize the point-
wise uncertainty in the predictions, governed by (3.59). However, by drawing sam-
ples from the posterior distribution over w, and plotting the corresponding model
functions y(x, w) as in Figure 3.9, we are visualizing the joint uncertainty in the
posterior distribution between the y values at two (or more) x values, as governed by
the equivalent kernel.
The formulation of linear regression in terms of a kernel function suggests an
alternative approach to regression as follows. Instead of introducing a set of basis
functions, which implicitly determines an equivalent kernel, we can instead define
a localized kernel directly and use this to make predictions for new input vectors x,
given the observed training set. This leads to a practical framework for regression
(and classification) called Gaussian processes, which will be discussed in detail in
Section 6.4.
We have seen that the effective kernel defines the weights by which the training
set target values are combined in order to make a prediction at a new value of x, and
it can be shown that these weights sum to one, in other words
k(x, xn) = 1
n=1
(3.64)
Exercise 3.14
for all values of x. This intuitively pleasing result can easily be proven informally
y(x)
by noting that the summation is equivalent to considering the predictive mean
for a set of target data in which tn = 1 for all n. Provided the basis functions are
linearly independent, that there are more data points than basis functions, and that
one of the basis functions is constant (corresponding to the bias parameter), then it is
clear that we can fit the training data exactly and hence that the predictive mean will
