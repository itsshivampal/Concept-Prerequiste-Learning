If we define a joint distribution over observed and latent variables, the correspond-
ing distribution of the observed variables alone is obtained by marginalization. This
allows relatively complex marginal distributions over observed variables to be ex-
pressed in terms of more tractable joint distributions over the expanded space of
observed and latent variables. The introduction of latent variables thereby allows
complicated distributions to be formed from simpler components. In this chapter,
we shall see that mixture distributions, such as the Gaussian mixture discussed in
Section 2.3.9, can be interpreted in terms of discrete latent variables. Continuous
latent variables will form the subject of Chapter 12.
As well as providing a framework for building more complex probability dis-
tributions, mixture models can also be used to cluster data. We therefore begin our
discussion of mixture distributions by considering the problem of finding clusters
in a set of data points, which we approach first using a nonprobabilistic technique
called the K-means algorithm (Lloyd, 1982). Then we introduce the latent variable
423
Section 9.1
424
9. MIXTURE MODELS AND EM
Section 9.2
Section 9.3
Section 9.4
view of mixture distributions in which the discrete latent variables can be interpreted
as defining assignments of data points to specific components of the mixture. A gen-
eral technique for finding maximum likelihood estimators in latent variable models
is the expectation-maximization (EM) algorithm. We first of all use the Gaussian
mixture distribution to motivate the EM algorithm in a fairly informal way, and then
we give a more careful treatment based on the latent variable viewpoint. We shall
see that the K-means algorithm corresponds to a particular nonprobabilistic limit of
EM applied to mixtures of Gaussians. Finally, we discuss EM in some generality.
Gaussian mixture models are widely used in data mining, pattern recognition,
machine learning, and statistical analysis. In many applications, their parameters are
determined by maximum likelihood, typically using the EM algorithm. However, as
we shall see there are some significant limitations to the maximum likelihood ap-
proach, and in Chapter 10 we shall show that an elegant Bayesian treatment can be
given using the framework of variational inference. This requires little additional
computation compared with EM, and it resolves the principal difficulties of maxi-
mum likelihood while also allowing the number of components in the mixture to be
inferred automatically from the data.
N
K
