N
As a second illustration of variational inference, we return to the Bayesian linear
regression model of Section 3.3. In the evidence framework, we approximated the
integration over α and β by making point estimates obtained by maximizing the log
marginal likelihood. A fully Bayesian approach would integrate over the hyperpa-
rameters as well as over the parameters. Although exact integration is intractable,
we can use variational methods to find a tractable approximation. In order to sim-
plify the discussion, we shall suppose that the noise precision parameter β is known,
and is fixed to its true value, although the framework is easily extended to include
the distribution over β. For the linear regression model, the variational treatment
will turn out to be equivalent to the evidence framework. Nevertheless, it provides a
good exercise in the use of variational methods and will also lay the foundation for
variational treatment of Bayesian logistic regression in Section 10.6.
Recall that the likelihood function for w, and the prior over w, are given by
Exercise 10.26
N (tn|wTφn, β−1)
p(t|w) =
p(w|α) = N (w|0, α−1I)
n=1
(10.87)
(10.88)
where φn = φ(xn). We now introduce a prior distribution over α. From our dis-
cussion in Section 2.3.6, we know that the conjugate prior for the precision of a
Gaussian is given by a gamma distribution, and so we choose
(10.89)
where Gam(·|·,·) is defined by (B.26). Thus the joint distribution of all the variables
is given by
(10.90)
p(α) = Gam(α|a0, b0)
p(t, w, α) = p(t|w)p(w|α)p(α).
This can be represented as a directed graphical model as shown in Figure 10.8.
