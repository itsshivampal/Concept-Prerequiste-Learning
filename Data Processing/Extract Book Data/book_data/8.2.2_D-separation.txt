We now give a general statement of the d-separation property (Pearl, 1988) for
directed graphs. Consider a general directed graph in which A, B, and C are arbi-
trary nonintersecting sets of nodes (whose union may be smaller than the complete
set of nodes in the graph). We wish to ascertain whether a particular conditional
independence statement A ⊥⊥ B | C is implied by a given directed acyclic graph. To
do so, we consider all possible paths from any node in A to any node in B. Any such
path is said to be blocked if it includes a node such that either
(a) the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the
node is in the set C, or
(b) the arrows meet head-to-head at the node, and neither the node, nor any of its
descendants, is in the set C.
If all paths are blocked, then A is said to be d-separated from B by C, and the joint
distribution over all of the variables in the graph will satisfy A ⊥⊥ B | C.
The concept of d-separation is illustrated in Figure 8.22. In graph (a), the path
from a to b is not blocked by node f because it is a tail-to-tail node for this path
and is not observed, nor is it blocked by node e because, although the latter is a
head-to-head node, it has a descendant c because is in the conditioning set. Thus
the conditional independence statement a ⊥⊥ b | c does not follow from this graph.
In graph (b), the path from a to b is blocked by node f because this is a tail-to-tail
node that is observed, and so the conditional independence property a ⊥⊥ b | f will
8.2. Conditional Independence
379
Figure 8.22 Illustration of the con-
cept of d-separation. See the text for
details.
a
f
(a)
e
c
a
f
b
e
c
(b)
b
N
Section 2.3
be satisfied by any distribution that factorizes according to this graph. Note that this
path is also blocked by node e because e is a head-to-head node and neither it nor its
descendant are in the conditioning set.
For the purposes of d-separation, parameters such as α and σ2 in Figure 8.5,
indicated by small filled circles, behave in the same was as observed nodes. How-
ever, there are no marginal distributions associated with such nodes. Consequently
parameter nodes never themselves have parents and so all paths through these nodes
will always be tail-to-tail and hence blocked. Consequently they play no role in
d-separation.
Another example of conditional independence and d-separation is provided by
the concept of i.i.d. (independent identically distributed) data introduced in Sec-
tion 1.2.4. Consider the problem of finding the posterior distribution for the mean
of a univariate Gaussian distribution. This can be represented by the directed graph
shown in Figure 8.23 in which the joint distribution is defined by a prior p(µ) to-
gether with a set of conditional distributions p(xn|µ) for n = 1, . . . , N. In practice,
we observe D = {x1, . . . , xN} and our goal is to infer µ. Suppose, for a moment,
that we condition on µ and consider the joint distribution of the observations. Using
d-separation, we note that there is a unique path from any xi to any other xj=i and
that this path is tail-to-tail with respect to the observed node µ. Every such path is
blocked and so the observations D = {x1, . . . , xN} are independent given µ, so that
Figure 8.23 (a) Directed graph corre-
sponding to the problem
of inferring the mean µ of
a univariate Gaussian dis-
tribution from observations
x1, . . . , xN .
(b) The same
graph drawn using the plate
notation.
p(D|µ) =
p(xn|µ).
n=1
(8.34)
x1
xN
xn
(a)
N
(b)
N
N
380
8. GRAPHICAL MODELS
Figure 8.24 A graphical representation of the ‘naive Bayes’
model
Conditioned on the
class label z,
the components of the observed
vector x = (x1, . . . , xD)T are assumed to be
independent.
for classification.
x1
z
xD
Section 3.3
However, if we integrate over µ, the observations are in general no longer indepen-
dent
p(D) =
0
p(D|µ)p(µ) dµ =
p(xn).
n=1
(8.35)
Here µ is a latent variable, because its value is not observed.
Another example of a model representing i.i.d. data is the graph in Figure 8.7
corresponding to Bayesian polynomial regression. Here the stochastic nodes corre-
t. We see that the node for w is tail-to-tail with respect to
spond to {tn}, w and
the path from
t to any one of the nodes tn and so we have the following conditional
independence property
t ⊥⊥ tn | w.
(8.36)
Thus, conditioned on the polynomial coefficients w, the predictive distribution for
t is independent of the training data {t1, . . . , tN}. We can therefore first use the
training data to determine the posterior distribution over the coefficients w and then
we can discard the training data and use the posterior distribution for w to make
predictions of
t for new input observations
x.
A related graphical structure arises in an approach to classification called the
naive Bayes model, in which we use conditional independence assumptions to sim-
plify the model structure. Suppose our observed variable consists of a D-dimensional
vector x = (x1, . . . , xD)T, and we wish to assign observed values of x to one of K
classes. Using the 1-of-K encoding scheme, we can represent these classes by a K-
dimensional binary vector z. We can then define a generative model by introducing
a multinomial prior p(z|µ) over the class labels, where the kth component µk of µ
is the prior probability of class Ck, together with a conditional distribution p(x|z)
for the observed vector x. The key assumption of the naive Bayes model is that,
conditioned on the class z, the distributions of the input variables x1, . . . , xD are in-
dependent. The graphical representation of this model is shown in Figure 8.24. We
see that observation of z blocks the path between xi and xj for j = i (because such
paths are tail-to-tail at the node z) and so xi and xj are conditionally independent
given z. If, however, we marginalize out z (so that z is unobserved) the tail-to-tail
path from xi to xj is no longer blocked. This tells us that in general the marginal
density p(x) will not factorize with respect to the components of x. We encountered
a simple application of the naive Bayes model in the context of fusing data from
different sources for medical diagnosis in Section 1.5.
If we are given a labelled training set, comprising inputs {x1, . . . , xN} together
with their class labels, then we can fit the naive Bayes model to the training data
8.2. Conditional Independence
381
using maximum likelihood assuming that the data are drawn independently from
the model. The solution is obtained by fitting the model for each class separately
using the correspondingly labelled data. As an example, suppose that the probability
density within each class is chosen to be Gaussian. In this case, the naive Bayes
assumption then implies that the covariance matrix for each Gaussian is diagonal,
and the contours of constant density within each class will be axis-aligned ellipsoids.
The marginal density, however, is given by a superposition of diagonal Gaussians
(with weighting coefficients given by the class priors) and so will no longer factorize
with respect to its components.
The naive Bayes assumption is helpful when the dimensionality D of the input
space is high, making density estimation in the full D-dimensional space more chal-
lenging. It is also useful if the input vector contains both discrete and continuous
variables, since each can be represented separately using appropriate models (e.g.,
Bernoulli distributions for binary observations or Gaussians for real-valued vari-
ables). The conditional independence assumption of this model is clearly a strong
one that may lead to rather poor representations of the class-conditional densities.
Nevertheless, even if this assumption is not precisely satisfied, the model may still
give good classification performance in practice because the decision boundaries can
be insensitive to some of the details in the class-conditional densities, as illustrated
in Figure 1.27.
We have seen that a particular directed graph represents a specific decomposition
of a joint probability distribution into a product of conditional probabilities. The
graph also expresses a set of conditional independence statements obtained through
the d-separation criterion, and the d-separation theorem is really an expression of the
equivalence of these two properties. In order to make this clear, it is helpful to think
of a directed graph as a filter. Suppose we consider a particular joint probability
distribution p(x) over the variables x corresponding to the (nonobserved) nodes of
the graph. The filter will allow this distribution to pass through if, and only if, it can
be expressed in terms of the factorization (8.5) implied by the graph. If we present to
the filter the set of all possible distributions p(x) over the set of variables x, then the
subset of distributions that are passed by the filter will be denoted DF, for directed
factorization. This is illustrated in Figure 8.25. Alternatively, we can use the graph as
a different kind of filter by first listing all of the conditional independence properties
obtained by applying the d-separation criterion to the graph, and then allowing a
distribution to pass only if it satisfies all of these properties. If we present all possible
distributions p(x) to this second kind of filter, then the d-separation theorem tells us
that the set of distributions that will be allowed through is precisely the set DF.
It should be emphasized that the conditional independence properties obtained
from d-separation apply to any probabilistic model described by that particular di-
rected graph. This will be true, for instance, whether the variables are discrete or
continuous or a combination of these. Again, we see that a particular graph is de-
scribing a whole family of probability distributions.
At one extreme we have a fully connected graph that exhibits no conditional in-
dependence properties at all, and which can represent any possible joint probability
distribution over the given variables. The set DF will contain all possible distribu-
k
p(xi|x{j=i}) =
p(x1, . . . , xD)
p(x1, . . . , xD) dxi
p(xk|pak)
p(xk|pak) dxi
k
in which the integral is replaced by a summation in the case of discrete variables. We
now observe that any factor p(xk|pak) that does not have any functional dependence
on xi can be taken outside the integral over xi, and will therefore cancel between
numerator and denominator. The only factors that remain will be the conditional
distribution p(xi|pai) for node xi itself, together with the conditional distributions
for any nodes xk such that node xi is in the conditioning set of p(xk|pak), in other
words for which xi is a parent of xk. The conditional p(xi|pai) will depend on the
parents of node xi, whereas the conditionals p(xk|pak) will depend on the children
382
8. GRAPHICAL MODELS
p(x)
DF
Figure 8.25 We can view a graphical model (in this case a directed graph) as a filter in which a prob-
ability distribution p(x) is allowed through the filter if, and only if, it satisfies the directed
factorization property (8.5). The set of all possible probability distributions p(x) that pass
through the filter is denoted DF. We can alternatively use the graph to filter distributions
according to whether they respect all of the conditional independencies implied by the
d-separation properties of the graph. The d-separation theorem says that it is the same
set of distributions DF that will be allowed through this second kind of filter.
tions p(x). At the other extreme, we have the fully disconnected graph, i.e., one
having no links at all. This corresponds to joint distributions which factorize into the
product of the marginal distributions over the variables comprising the nodes of the
graph.
Note that for any given graph, the set of distributions DF will include any dis-
tributions that have additional independence properties beyond those described by
the graph. For instance, a fully factorized distribution will always be passed through
the filter implied by any graph over the corresponding set of variables.
We end our discussion of conditional independence properties by exploring the
concept of a Markov blanket or Markov boundary. Consider a joint distribution
p(x1, . . . , xD) represented by a directed graph having D nodes, and consider the
conditional distribution of a particular node with variables xi conditioned on all of
the remaining variables xj=i. Using the factorization property (8.5), we can express
this conditional distribution in the form
