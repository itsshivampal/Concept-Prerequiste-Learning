3. Invariance is built into the pre-processing by extracting features that are invari-
ant under the required transformations. Any subsequent regression or classi-
fication system that uses such features as inputs will necessarily also respect
these invariances.
4. The final option is to build the invariance properties into the structure of a neu-
ral network (or into the definition of a kernel function in the case of techniques
such as the relevance vector machine). One way to achieve this is through the
use of local receptive fields and shared weights, as discussed in the context of
convolutional neural networks in Section 5.5.6.
Approach 1 is often relatively easy to implement and can be used to encourage com-
plex invariances such as those illustrated in Figure 5.14. For sequential training
algorithms, this can be done by transforming each input pattern before it is presented
to the model so that, if the patterns are being recycled, a different transformation
(drawn from an appropriate distribution) is added each time. For batch methods, a
similar effect can be achieved by replicating each data point a number of times and
transforming each copy independently. The use of such augmented data can lead to
significant improvements in generalization (Simard et al., 2003), although it can also
be computationally costly.
Approach 2 leaves the data set unchanged but modifies the error function through
the addition of a regularizer. In Section 5.5.5, we shall show that this approach is
closely related to approach 2.
5.5. Regularization in Neural Networks
263
Figure 5.14 Illustration of the synthetic warping of a handwritten digit. The original image is shown on the
left. On the right, the top row shows three examples of warped digits, with the corresponding displacement
fields shown on the bottom row. These displacement fields are generated by sampling random displacements
∆x, ∆y ∈ (0, 1) at each pixel and then smoothing by convolution with Gaussians of width 0.01, 30 and 60
respectively.
One advantage of approach 3 is that it can correctly extrapolate well beyond the
range of transformations included in the training set. However, it can be difficult
to find hand-crafted features with the required invariances that do not also discard
information that can be useful for discrimination.
5.5.4 Tangent propagation
We can use regularization to encourage models to be invariant to transformations
of the input through the technique of tangent propagation (Simard et al., 1992).
Consider the effect of a transformation on a particular input vector xn. Provided the
transformation is continuous (such as translation or rotation, but not mirror reflection
for instance), then the transformed pattern will sweep out a manifold M within the
D-dimensional input space. This is illustrated in Figure 5.15, for the case of D =
2 for simplicity. Suppose the transformation is governed by a single parameter ξ
(which might be rotation angle for instance). Then the subspace M swept out by xn
Figure 5.15 Illustration of a two-dimensional
input space
showing the effect of a continuous transforma-
tion on a particular input vector xn. A one-
dimensional transformation, parameterized by
the continuous variable ξ, applied to xn causes
it to sweep out a one-dimensional manifold M.
Locally, the effect of the transformation can be
approximated by the tangent vector τ n.
x2
τ n
xn
M
x1
i=1
D
D
i=1
D
264
5. NEURAL NETWORKS
will be one-dimensional, and will be parameterized by ξ. Let the vector that results
from acting on xn by this transformation be denoted by s(xn, ξ), which is defined
so that s(x, 0) = x. Then the tangent to the curve M is given by the directional
derivative τ = ∂s/∂ξ, and the tangent vector at the point xn is given by
τ n = ∂s(xn, ξ)
ξ=0
(5.125)
Under a transformation of the input vector, the network output vector will, in general,
change. The derivative of output k with respect to ξ is given by
∂yk
ξ=0
∂yk
∂xi
∂xi
ξ=0
Jkiτi
(5.126)
where Jki is the (k, i) element of the Jacobian matrix J, as discussed in Section 5.3.4.
The result (5.126) can be used to modify the standard error function, so as to encour-
age local invariance in the neighbourhood of the data points, by the addition to the
original error function E of a regularization function Ω to give a total error function
of the form
E = E + λΩ
(5.127)
where λ is a regularization coefficient and
1
2
n
k
∂ynk
2
1
2
ξ=0
n
k
i=1
2
Jnkiτni
(5.128)
Exercise 5.26
The regularization function will be zero when the network mapping function is in-
variant under the transformation in the neighbourhood of each pattern vector, and
the value of the parameter λ determines the balance between fitting the training data
and learning the invariance property.
In a practical implementation, the tangent vector τ n can be approximated us-
ing finite differences, by subtracting the original vector xn from the corresponding
vector after transformation using a small value of ξ, and then dividing by ξ. This is
illustrated in Figure 5.16.
The regularization function depends on the network weights through the Jaco-
bian J. A backpropagation formalism for computing the derivatives of the regu-
larizer with respect to the network weights is easily obtained by extension of the
techniques introduced in Section 5.3.
If the transformation is governed by L parameters (e.g., L = 3 for the case of
translations combined with in-plane rotations in a two-dimensional image), then the
manifold M will have dimensionality L, and the corresponding regularizer is given
by the sum of terms of the form (5.128), one for each transformation. If several
transformations are considered at the same time, and the network mapping is made
invariant to each separately, then it will be (locally) invariant to combinations of the
transformations (Simard et al., 1992).
5.5. Regularization in Neural Networks
265
Figure 5.16 Illustration
showing
(a) the original image x of a hand-
written digit, (b) the tangent vector
τ corresponding to an infinitesimal
clockwise rotation, (c) the result of
adding a small contribution from the
tangent vector to the original image
giving x + τ with  = 15 degrees,
and (d) the true image rotated for
comparison.
(a)
(c)
(b)
(d)
A related technique, called tangent distance, can be used to build invariance
properties into distance-based methods such as nearest-neighbour classifiers (Simard
et al., 1993).
