We have noted earlier that the mechanism of automatic relevance determination
causes a subset of parameters to be driven to zero. We now examine in more detail
350
7. SPARSE KERNEL MACHINES
t2
C
t
t1
t2
C
t
t1
Figure 7.10 Illustration of the mechanism for sparsity in a Bayesian linear regression model, showing a training
set vector of target values given by t = (t1, t2)T, indicated by the cross, for a model with one basis vector
ϕ = (φ(x1), φ(x2))T, which is poorly aligned with the target data vector t. On the left we see a model having
only isotropic noise, so that C = β−1I, corresponding to α = ∞, with β set to its most probable value. On
the right we see the same model but with a finite value of α. In each case the red ellipse corresponds to unit
Mahalanobis distance, with |C| taking the same value for both plots, while the dashed green circle shows the
contrition arising from the noise term β−1. We see that any finite value of α reduces the probability of the
observed data, and so for the most probable solution the basis vector is removed.
the mechanism of sparsity in the context of the relevance vector machine. In the
process, we will arrive at a significantly faster procedure for optimizing the hyper-
parameters compared to the direct techniques given above.
Before proceeding with a mathematical analysis, we first give some informal
insight into the origin of sparsity in Bayesian linear models. Consider a data set
comprising N = 2 observations t1 and t2, together with a model having a single
basis function φ(x), with hyperparameter α, along with isotropic noise having pre-
cision β. From (7.85), the marginal likelihood is given by p(t|α, β) = N (t|0, C) in
which the covariance matrix takes the form
C =
1
I +
1
ϕϕT
(7.92)
where ϕ denotes the N-dimensional vector (φ(x1), φ(x2))T, and similarly t =
(t1, t2)T. Notice that this is just a zero-mean Gaussian process model over t with
covariance C. Given a particular observation for t, our goal is to find α and β by
maximizing the marginal likelihood. We see from Figure 7.10 that, if there is a poor
alignment between the direction of ϕ and that of the training data vector t, then the
corresponding hyperparameter α will be driven to ∞, and the basis vector will be
pruned from the model. This arises because any finite value for α will always assign
a lower probability to the data, thereby decreasing the value of the density at t, pro-
vided that β is set to its optimal value. We see that any finite value for α would cause
the distribution to be elongated in a direction away from the data, thereby increasing
the probability mass in regions away from the observed data and hence reducing the
value of the density at the target data vector itself. For the more general case of M
7.2. Relevance Vector Machines
351
basis vectors ϕ1, . . . , ϕM a similar intuition holds, namely that if a particular basis
vector is poorly aligned with the data vector t, then it is likely to be pruned from the
model.
We now investigate the mechanism for sparsity from a more mathematical per-
spective, for a general case involving M basis functions. To motivate this analysis
we first note that, in the result (7.87) for re-estimating the parameter αi, the terms on
the right-hand side are themselves also functions of αi. These results therefore rep-
resent implicit solutions, and iteration would be required even to determine a single
αi with all other αj for j = i fixed.
This suggests a different approach to solving the optimization problem for the
RVM, in which we make explicit all of the dependence of the marginal likelihood
(7.85) on a particular αi and then determine its stationary points explicitly (Faul and
Tipping, 2002; Tipping and Faul, 2003). To do this, we first pull out the contribution
from αi in the matrix C defined by (7.86) to give
C = β−1I +
α−1
j ϕjϕT
j + α−1
i ϕiϕT
i
j=i
= C−i + α−1
i ϕiϕT
i
where ϕi denotes the ith column of Φ, in other words the N-dimensional vector with
elements (φi(x1), . . . , φi(xN )), in contrast to φn, which denotes the nth row of Φ.
The matrix C−i represents the matrix C with the contribution from basis function i
removed. Using the matrix identities (C.7) and (C.15), the determinant and inverse
of C can then be written
(7.93)
(7.94)
(7.95)
Exercise 7.15
|C| = |C−i||1 + α−1
i ϕT
C−1
−i ϕiϕT
C−1 = C−1
αi + ϕT
−i −
i C−1
−i ϕi|
i C−1
−i
−i ϕi
i C−1
Using these results, we can then write the log marginal likelihood function (7.85) in
the form
(7.96)
where L(α−i) is simply the log marginal likelihood with basis function ϕi omitted,
and the quantity λ(αi) is defined by
L(α) = L(α−i) + λ(αi)
λ(αi) =
1
2
ln αi − ln (αi + si) + q2
αi + si
i
(7.97)
and contains all of the dependence on αi. Here we have introduced the two quantities
si = ϕT
qi = ϕT
i C−1
−i ϕi
i C−1
−i t.
(7.98)
(7.99)
Here si is called the sparsity and qi is known as the quality of ϕi, and as we shall
see, a large value of si relative to the value of qi means that the basis function ϕi
352
7. SPARSE KERNEL MACHINES
of
the
Figure 7.11 Plots
log
likelihood λ(αi) versus
marginal
ln αi showing on the left, the single
maximum at a finite αi for q2
i = 4
and si = 1 (so that q2
i > si) and on
the right, the maximum at αi = ∞
i = 1 and si = 2 (so that
for q2
i < si).
q2
2
0
−2
−4
2
0
−2
−4
−5
0
5
−5
0
5
is more likely to be pruned from the model. The ‘sparsity’ measures the extent to
which basis function ϕi overlaps with the other basis vectors in the model, and the
‘quality’ represents a measure of the alignment of the basis vector ϕn with the error
between the training set values t = (t1, . . . , tN )T and the vector y
−i of predictions
that would result from the model with the vector ϕi excluded (Tipping and Faul,
2003).
The stationary points of the marginal likelihood with respect to αi occur when
the derivative
dλ(αi)
dαi
= α−1
i s2
i − (q2
2(αi + si)2
i − si)
(7.100)
is equal to zero. There are two possible forms for the solution. Recalling that αi � 0,
we see that if q2
i > si, we
can solve for αi to obtain
i < si, then αi → ∞ provides a solution. Conversely, if q2
Exercise 7.16
αi = s2
i
q2
i − si
(7.101)
These two solutions are illustrated in Figure 7.11. We see that the relative size of
the quality and sparsity terms determines whether a particular basis vector will be
pruned from the model or not. A more complete analysis (Faul and Tipping, 2002),
based on the second derivatives of the marginal likelihood, confirms these solutions
are indeed the unique maxima of λ(αi).
Note that this approach has yielded a closed-form solution for αi, for given
values of the other hyperparameters. As well as providing insight into the origin of
sparsity in the RVM, this analysis also leads to a practical algorithm for optimizing
the hyperparameters that has significant speed advantages. This uses a fixed set
of candidate basis vectors, and then cycles through them in turn to decide whether
each vector should be included in the model or not. The resulting sequential sparse
Bayesian learning algorithm is described below.
Sequential Sparse Bayesian Learning Algorithm
1. If solving a regression problem, initialize β.
2. Initialize using one basis function ϕ1, with hyperparameter α1 set using
(7.101), with the remaining hyperparameters αj for j = i initialized to
infinity, so that only ϕ1 is included in the model.
7.2. Relevance Vector Machines
353
3. Evaluate Σ and m, along with qi and si for all basis functions.
4. Select a candidate basis function ϕi.
5. If q2
the model, then update αi using (7.101).
i > si, and αi < ∞, so that the basis vector ϕi is already included in
i > si, and αi = ∞, then add ϕi to the model, and evaluate hyperpa-
i � si, and αi < ∞ then remove basis function ϕi from the model,
rameter αi using (7.101).
7. If q2
6. If q2
and set αi = ∞.
8. If solving a regression problem, update β.
9. If converged terminate, otherwise go to 3.
Note that if q2
from the model and no action is required.
i � si and αi = ∞, then the basis function ϕi is already excluded
In practice, it is convenient to evaluate the quantities
The quality and sparseness variables can then be expressed in the form
Qi = ϕT
Si = ϕT
i C−1t
i C−1ϕi.
qi =
si =
αiQi
αi − Si
αiSi
αi − Si
(7.102)
(7.103)
(7.104)
(7.105)
Exercise 7.17
Note that when αi = ∞, we have qi = Qi and si = Si. Using (C.7), we can write
(7.106)
(7.107)
Qi = βϕT
Si = βϕT
i ΦΣΦTt
i ΦΣΦTϕi
i t − β2ϕT
i ϕi − β2ϕT
where Φ and Σ involve only those basis vectors that correspond to finite hyperpa-
rameters αi. At each stage the required computations therefore scale like O(M 3),
where M is the number of active basis vectors in the model and is typically much
smaller than the number N of training patterns.
