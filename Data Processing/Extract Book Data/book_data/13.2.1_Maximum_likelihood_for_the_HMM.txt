If we have observed a data set X = {x1, . . . , xN}, we can determine the param-
eters of an HMM using maximum likelihood. The likelihood function is obtained
from the joint distribution (13.10) by marginalizing over the latent variables
p(X|θ) =
p(X, Z|θ).
Z
(13.11)
Because the joint distribution p(X, Z|θ) does not factorize over n (in contrast to the
mixture distribution considered in Chapter 9), we cannot simply treat each of the
summations over zn independently. Nor can we perform the summations explicitly
because there are N variables to be summed over, each of which has K states, re-
sulting in a total of K N terms. Thus the number of terms in the summation grows
616
13. SEQUENTIAL DATA
Section 9.2
exponentially with the length of the chain. In fact, the summation in (13.11) cor-
responds to summing over exponentially many paths through the lattice diagram in
Figure 13.7.
We have already encountered a similar difficulty when we considered the infer-
ence problem for the simple chain of variables in Figure 8.32. There we were able
to make use of the conditional independence properties of the graph to re-order the
summations in order to obtain an algorithm whose cost scales linearly, instead of
exponentially, with the length of the chain. We shall apply a similar technique to the
hidden Markov model.
A further difficulty with the expression (13.11) for the likelihood function is that,
because it corresponds to a generalization of a mixture distribution, it represents a
summation over the emission models for different settings of the latent variables.
Direct maximization of the likelihood function will therefore lead to complex ex-
pressions with no closed-form solutions, as was the case for simple mixture models
(recall that a mixture model for i.i.d. data is a special case of the HMM).
We therefore turn to the expectation maximization algorithm to find an efficient
framework for maximizing the likelihood function in hidden Markov models. The
EM algorithm starts with some initial selection for the model parameters, which we
denote by θold. In the E step, we take these parameter values and find the posterior
distribution of the latent variables p(Z|X, θold). We then use this posterior distri-
bution to evaluate the expectation of the logarithm of the complete-data likelihood
function, as a function of the parameters θ, to give the function Q(θ, θold) defined
by
Q(θ, θold) =
Z
p(Z|X, θold) ln p(X, Z|θ).
At this point, it is convenient to introduce some notation. We shall use γ(zn) to
denote the marginal posterior distribution of a latent variable zn, and ξ(zn−1, zn) to
denote the joint posterior distribution of two successive latent variables, so that
γ(zn) = p(zn|X, θold)
ξ(zn−1, zn) = p(zn−1, zn|X, θold).
For each value of n, we can store γ(zn) using a set of K nonnegative numbers
that sum to unity, and similarly we can store ξ(zn−1, zn) using a K × K matrix of
nonnegative numbers that again sum to unity. We shall also use γ(znk) to denote the
conditional probability of znk = 1, with a similar use of notation for ξ(zn−1,j, znk)
and for other probabilistic variables introduced later. Because the expectation of a
binary random variable is just the probability that it takes the value 1, we have
γ(znk) = E[znk] =
z
γ(z)znk
ξ(zn−1,j, znk) = E[zn−1,jznk] =
γ(z)zn−1,jznk.
z
If we substitute the joint distribution p(X, Z|θ) given by (13.10) into (13.12),
(13.12)
(13.13)
(13.14)
(13.15)
(13.16)
k=1
K
N
K
K
K
γ(z1k)
K
γ(z1j)
n=2
n=2
j=1
N
N
K
N
13.2. Hidden Markov Models
617
and make use of the definitions of γ and ξ , we obtain
Q(θ, θold) =
γ(z1k) ln πk +
ξ(zn−1,j, znk) ln Ajk
j=1
k=1
n=1
k=1
γ(znk) ln p(xn|φk).
(13.17)
The goal of the E step will be to evaluate the quantities γ(zn) and ξ(zn−1, zn) effi-
ciently, and we shall discuss this in detail shortly.
In the M step, we maximize Q(θ, θold) with respect to the parameters θ =
{π, A, φ} in which we treat γ(zn) and ξ(zn−1, zn) as constant. Maximization with
respect to π and A is easily achieved using appropriate Lagrange multipliers with
the results
πk =
Ajk =
(13.18)
(13.19)
ξ(zn−1,j, znk)
ξ(zn−1,j, znl)
l=1
n=2
The EM algorithm must be initialized by choosing starting values for π and A, which
should of course respect the summation constraints associated with their probabilis-
tic interpretation. Note that any elements of π or A that are set to zero initially will
remain zero in subsequent EM updates. A typical initialization procedure would
involve selecting random starting values for these parameters subject to the summa-
tion and non-negativity constraints. Note that no particular modification to the EM
results are required for the case of left-to-right models beyond choosing initial values
for the elements Ajk in which the appropriate elements are set to zero, because these
will remain zero throughout.
To maximize Q(θ, θold) with respect to φk, we notice that only the final term
in (13.17) depends on φk, and furthermore this term has exactly the same form as
the data-dependent term in the corresponding function for a standard mixture dis-
tribution for i.i.d. data, as can be seen by comparison with (9.40) for the case of a
Gaussian mixture. Here the quantities γ(znk) are playing the role of the responsibil-
ities. If the parameters φk are independent for the different components, then this
term decouples into a sum of terms one for each value of k, each of which can be
maximized independently. We are then simply maximizing the weighted log likeli-
hood function for the emission density p(x|φk) with weights γ(znk). Here we shall
suppose that this maximization can be done efficiently. For instance, in the case of
Exercise 13.5
Exercise 13.6
n=1
N
n=1
N
N
n=1
i=1
K
D
N
k=1
µk =
γ(znk)xn
γ(znk)
γ(znk)(xn − µk)(xn − µk)T
Σk =
n=1
γ(znk)
(13.20)
(13.21)
For the case of discrete multinomial observed variables, the conditional distribution
of the observations takes the form
p(x|z) =
µxizk
ik
(13.22)
and the corresponding M-step equations are given by
N
γ(znk)xni
µik =
n=1
N
(13.23)
γ(znk)
n=1
An analogous result holds for Bernoulli observed variables.
The EM algorithm requires initial values for the parameters of the emission dis-
tribution. One way to set these is first to treat the data initially as i.i.d. and fit the
emission density by maximum likelihood, and then use the resulting values to ini-
tialize the parameters for EM.
