The simplest representation of a linear discriminant function is obtained by tak-
ing a linear function of the input vector so that
y(x) = wTx + w0
(4.4)
where w is called a weight vector, and w0 is a bias (not to be confused with bias in
the statistical sense). The negative of the bias is sometimes called a threshold. An
input vector x is assigned to class C1 if y(x) � 0 and to class C2 otherwise. The cor-
responding decision boundary is therefore defined by the relation y(x) = 0, which
corresponds to a (D − 1)-dimensional hyperplane within the D-dimensional input
space. Consider two points xA and xB both of which lie on the decision surface.
Because y(xA) = y(xB) = 0, we have wT(xA − xB) = 0 and hence the vector w is
orthogonal to every vector lying within the decision surface, and so w determines the
orientation of the decision surface. Similarly, if x is a point on the decision surface,
then y(x) = 0, and so the normal distance from the origin to the decision surface is
given by
We therefore see that the bias parameter w0 determines the location of the decision
surface. These properties are illustrated for the case of D = 2 in Figure 4.1.
Furthermore, we note that the value of y(x) gives a signed measure of the per-
pendicular distance r of the point x from the decision surface. To see this, consider
wTx
w
(4.5)
w0
w
182
4. LINEAR MODELS FOR CLASSIFICATION
Figure 4.1 Illustration of the geometry of a
linear discriminant function in two dimensions.
The decision surface, shown in red, is perpen-
dicular to w, and its displacement from the
origin is controlled by the bias parameter w0.
Also, the signed orthogonal distance of a gen-
eral point x from the decision surface is given
by y(x)/w.
y > 0
y = 0
y < 0
x2
R1
R2
x
y(x)
w
x1
x⊥
−w0
w
w
r = y(x)
w
y(x) =
wT
x.
(4.7)
(4.8)
an arbitrary point x and let x⊥ be its orthogonal projection onto the decision surface,
so that
(4.6)
w
w
Multiplying both sides of this result by wT and adding w0, and making use of y(x) =
wTx + w0 and y(x⊥) = wTx⊥ + w0 = 0, we have
x = x⊥ + r
This result is illustrated in Figure 4.1.
As with the linear regression models in Chapter 3, it is sometimes convenient
to use a more compact notation in which we introduce an additional dummy ‘input’
value x0 = 1 and then define
x = (x0, x) so that
w = (w0, w) and
In this case, the decision surfaces are D-dimensional hyperplanes passing through
the origin of the D + 1-dimensional expanded input space.
