In some applications of probabilistic inference, we may have prior knowledge
that can be conveniently expressed through the prior distribution. For example, if
the prior assigns zero probability to some value of variable, then the posterior dis-
tribution will necessarily also assign zero probability to that value, irrespective of
 dλ
dη
118
2. PROBABILITY DISTRIBUTIONS
any subsequent observations of data. In many cases, however, we may have little
idea of what form the distribution should take. We may then seek a form of prior
distribution, called a noninformative prior, which is intended to have as little influ-
ence on the posterior distribution as possible (Jeffries, 1946; Box and Tao, 1973;
Bernardo and Smith, 1994). This is sometimes referred to as ‘letting the data speak
for themselves’.
If we have a distribution p(x|λ) governed by a parameter λ, we might be tempted
to propose a prior distribution p(λ) = const as a suitable prior. If λ is a discrete
variable with K states, this simply amounts to setting the prior probability of each
state to 1/K. In the case of continuous parameters, however, there are two potential
difficulties with this approach. The first is that, if the domain of λ is unbounded,
this prior distribution cannot be correctly normalized because the integral over λ
diverges. Such priors are called improper. In practice, improper priors can often
be used provided the corresponding posterior distribution is proper, i.e., that it can
be correctly normalized. For instance, if we put a uniform prior distribution over
the mean of a Gaussian, then the posterior distribution for the mean, once we have
observed at least one data point, will be proper.
A second difficulty arises from the transformation behaviour of a probability
density under a nonlinear change of variables, given by (1.27). If a function h(λ)
is constant, and we change variables to λ = η2, then
h(η) = h(η2) will also be
constant. However, if we choose the density pλ(λ) to be constant, then the density
of η will be given, from (1.27), by
pη(η) = pλ(λ)
= pλ(η2)2η ∝ η
(2.231)
and so the density over η will not be constant. This issue does not arise when we use
maximum likelihood, because the likelihood function p(x|λ) is a simple function of
λ and so we are free to use any convenient parameterization. If, however, we are to
choose a prior distribution that is constant, we must take care to use an appropriate
representation for the parameters.
Here we consider two simple examples of noninformative priors (Berger, 1985).
First of all, if a density takes the form
p(x|µ) = f(x − µ)
(2.232)
then the parameter µ is known as a location parameter. This family of densities
x = x + c,
exhibits translation invariance because if we shift x by a constant to give
then
µ) = f(
(2.233)
µ = µ + c. Thus the density takes the same form in the
where we have defined
new variable as in the original one, and so the density is independent of the choice
of origin. We would like to choose a prior distribution that reflects this translation
invariance property, and so we choose a prior that assigns equal probability mass to
x −
p(
x|
x
x
1
2.4. The Exponential Family
119
an interval A � µ � B as to the shifted interval A − c � µ � B − c. This implies
B
A
p(µ) dµ =
p(µ) dµ =
B−c
A−c
B
A
p(µ − c) dµ
and because this must hold for all choices of A and B, we have
p(µ − c) = p(µ)
which implies that p(µ) is constant. An example of a location parameter would be
the mean µ of a Gaussian distribution. As we have seen, the conjugate prior distri-
bution for µ in this case is a Gaussian p(µ|µ0, σ2
0), and we obtain a
noninformative prior by taking the limit σ2
0 → ∞. Indeed, from (2.141) and (2.142)
we see that this gives a posterior distribution over µ in which the contributions from
the prior vanish.
0) = N (µ|µ0, σ2
As a second example, consider a density of the form
Exercise 2.59
where σ > 0. Note that this will be a normalized density provided f(x) is correctly
normalized. The parameter σ is known as a scale parameter, and the density exhibits
scale invariance because if we scale x by a constant to give
x = cx, then
p(x|σ) =
1
f
p(
x|
f
(2.234)
(2.235)
(2.236)
(2.237)
σ = cσ. This transformation corresponds to a change of
where we have defined
scale, for example from meters to kilometers if x is a length, and we would like
to choose a prior distribution that reflects this scale invariance. If we consider an
interval A � σ � B, and a scaled interval A/c � σ � B/c, then the prior should
assign equal probability mass to these two intervals. Thus we have
B
A
p(σ) dσ =
B/c
A/c
p(σ) dσ =
B
A
p
1
c
1
c
dσ
(2.238)
and because this must hold for choices of A and B, we have
p(σ) = p
1
c
1
c
(2.239)
and hence p(σ) ∝ 1/σ. Note that again this is an improper prior because the integral
of the distribution over 0 � σ � ∞ is divergent. It is sometimes also convenient
to think of the prior distribution for a scale parameter in terms of the density of the
log of the parameter. Using the transformation rule (1.27) for densities we see that
p(ln σ) = const. Thus, for this prior there is the same probability mass in the range
1 � σ � 10 as in the range 10 � σ � 100 and in 100 � σ � 1000.
120
Section 2.3
2. PROBABILITY DISTRIBUTIONS
An example of a scale parameter would be the standard deviation σ of a Gaussian
distribution, after we have taken account of the location parameter µ, because
x/σ)2
N (x|µ, σ2) ∝ σ−1 exp
(2.240)
x = x − µ. As discussed earlier, it is often more convenient to work in terms
where
of the precision λ = 1/σ2 rather than σ itself. Using the transformation rule for
densities, we see that a distribution p(σ) ∝ 1/σ corresponds to a distribution over λ
of the form p(λ) ∝ 1/λ. We have seen that the conjugate prior for λ was the gamma
distribution Gam(λ|a0, b0) given by (2.146). The noninformative prior is obtained
as the special case a0 = b0 = 0. Again, if we examine the results (2.150) and (2.151)
for the posterior distribution of λ, we see that for a0 = b0 = 0, the posterior depends
only on terms arising from the data and not from the prior.
