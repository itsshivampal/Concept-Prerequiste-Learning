We now derive the backpropagation algorithm for a general network having ar-
bitrary feed-forward topology, arbitrary differentiable nonlinear activation functions,
and a broad class of error function. The resulting formulae will then be illustrated
using a simple layered network structure having a single layer of sigmoidal hidden
units together with a sum-of-squares error.
Many error functions of practical interest, for instance those defined by maxi-
mum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data
point in the training set, so that
E(w) =
En(w).
(5.44)
Here we shall consider the problem of evaluating ∇En(w) for one such term in the
error function. This may be used directly for sequential optimization, or the results
can be accumulated over the training set in the case of batch methods.
Consider first a simple linear model in which the outputs yk are linear combina-
tions of the input variables xi so that
together with an error function that, for a particular input pattern n, takes the form
5.3. Error Backpropagation
243
where zi is the activation of a unit, or input, that sends a connection to unit j, and wji
is the weight associated with that connection. In Section 5.1, we saw that biases can
be included in this sum by introducing an extra unit, or input, with activation fixed
at +1. We therefore do not need to deal with biases explicitly. The sum in (5.48) is
transformed by a nonlinear activation function h(·) to give the activation zj of unit j
in the form
(5.49)
Note that one or more of the variables zi in the sum in (5.48) could be an input, and
similarly, the unit j in (5.49) could be an output.
zj = h(aj).
For each pattern in the training set, we shall suppose that we have supplied the
corresponding input vector to the network and calculated the activations of all of
the hidden and output units in the network by successive application of (5.48) and
(5.49). This process is often called forward propagation because it can be regarded
as a forward flow of information through the network.
Now consider the evaluation of the derivative of En with respect to a weight
wji. The outputs of the various units will depend on the particular input pattern n.
However, in order to keep the notation uncluttered, we shall omit the subscript n
from the network variables. First we note that En depends on the weight wji only
via the summed input aj to unit j. We can therefore apply the chain rule for partial
derivatives to give
∂En
∂wji
= ∂En
∂aj
∂aj
∂wji
We now introduce a useful notation
δj ≡
∂En
∂aj
(5.50)
(5.51)
where the δ’s are often referred to as errors for reasons we shall see shortly. Using
(5.48), we can write
(5.52)
(5.53)
∂aj
∂wji
= zi.
Substituting (5.51) and (5.52) into (5.50), we then obtain
∂En
∂wji
= δjzi.
Equation (5.53) tells us that the required derivative is obtained simply by multiplying
the value of δ for the unit at the output end of the weight by the value of z for the unit
at the input end of the weight (where z = 1 in the case of a bias). Note that this takes
the same form as for the simple linear model considered at the start of this section.
Thus, in order to evaluate the derivatives, we need only to calculate the value of δj
for each hidden and output unit in the network, and then apply (5.53).
As we have seen already, for the output units, we have
δk = yk − tk
(5.54)
k
δj ≡
∂En
∂ak
∂ak
∂aj
∂En
∂aj
(5.55)
where the sum runs over all units k to which unit j sends connections. The arrange-
ment of units and weights is illustrated in Figure 5.7. Note that the units labelled k
could include other hidden units and/or output units. In writing down (5.55), we are
making use of the fact that variations in aj give rise to variations in the error func-
tion only through variations in the variables ak. If we now substitute the definition
of δ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the
following backpropagation formula
δj = h(aj)
wkjδk
k
(5.56)
which tells us that the value of δ for a particular hidden unit can be obtained by
propagating the δ’s backwards from units higher up in the network, as illustrated
in Figure 5.7. Note that the summation in (5.56) is taken over the first index on
wkj (corresponding to backward propagation of information through the network),
whereas in the forward propagation equation (5.10) it is taken over the second index.
Because we already know the values of the δ’s for the output units, it follows that
by recursively applying (5.56) we can evaluate the δ’s for all of the hidden units in a
feed-forward network, regardless of its topology.
The backpropagation procedure can therefore be summarized as follows.
Error Backpropagation
1. Apply an input vector xn to the network and forward propagate through
the network using (5.48) and (5.49) to find the activations of all the hidden
and output units.
2. Evaluate the δk for all the output units using (5.54).
3. Backpropagate the δ’s using (5.56) to obtain δj for each hidden unit in the
network.
4. Use (5.53) to evaluate the required derivatives.
244
5. NEURAL NETWORKS
Figure 5.7 Illustration of the calculation of δj for hidden unit j by
backpropagation of the δ’s from those units k to which
unit j sends connections. The blue arrow denotes the
direction of information flow during forward propagation,
and the red arrows indicate the backward propagation
of error information.
zi
wji
δj
zj
wkj
δk
δ1
provided we are using the canonical link as the output-unit activation function. To
evaluate the δ’s for hidden units, we again make use of the chain rule for partial
derivatives,
k=1
i=0
K
n
D
M
5.3. Error Backpropagation
245
For batch methods, the derivative of the total error E can then be obtained by
repeating the above steps for each pattern in the training set and then summing over
all patterns:
∂E
∂wji
∂En
∂wji
(5.57)
In the above derivation we have implicitly assumed that each hidden or output unit in
the network has the same activation function h(·). The derivation is easily general-
ized, however, to allow different units to have individual activation functions, simply
by keeping track of which form of h(·) goes with which unit.
