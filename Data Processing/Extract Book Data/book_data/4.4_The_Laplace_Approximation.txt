213
Thus y and η must related, and we denote this relation through η = ψ(y).
Following Nelder and Wedderburn (1972), we define a generalized linear model
to be one for which y is a nonlinear function of a linear combination of the input (or
feature) variables so that
y = f(wTφ)
(4.120)
where f(·) is known as the activation function in the machine learning literature, and
f−1(·) is known as the link function in statistics.
Now consider the log likelihood function for this model, which, as a function of
η, is given by
ln p(t|η, s) =
n=1
ln p(tn|η, s) =
ln g(ηn) + ηntn
s
+ const
(4.121)
where we are assuming that all observations share a common scale parameter (which
corresponds to the noise variance for a Gaussian distribution for instance) and so s
is independent of n. The derivative of the log likelihood with respect to the model
parameters w is then given by
∇w ln p(t|η, s) =
ln g(ηn) + tn
s
dηn
dyn
dyn
dan∇an
n=1
1
s {tn − yn} ψ(yn)f(an)φn
(4.122)
where an = wTφn, and we have used yn = f(an) together with the result (4.119)
for E[t|η]. We now see that there is a considerable simplification if we choose a
particular form for the link function f−1(y) given by
f−1(y) = ψ(y)
(4.123)
which gives f(ψ(y)) = y and hence f(ψ)ψ(y) = 1. Also, because a = f−1(y),
we have a = ψ and hence f(a)ψ(y) = 1. In this case, the gradient of the error
function reduces to
∇ ln E(w) =
1
s
{yn − tn}φn.
n=1
(4.124)
For the Gaussian s = β−1, whereas for the logistic model s = 1.
4.4. The Laplace Approximation
In Section 4.5 we shall discuss the Bayesian treatment of logistic regression. As
we shall see, this is more complex than the Bayesian treatment of linear regression
models, discussed in Sections 3.3 and 3.5. In particular, we cannot integrate exactly
214
Chapter 10
Chapter 11
4. LINEAR MODELS FOR CLASSIFICATION
over the parameter vector w since the posterior distribution is no longer Gaussian.
It is therefore necessary to introduce some form of approximation. Later in the
book we shall consider a range of techniques based on analytical approximations
and numerical sampling.
Here we introduce a simple, but widely used, framework called the Laplace ap-
proximation, that aims to find a Gaussian approximation to a probability density
defined over a set of continuous variables. Consider first the case of a single contin-
uous variable z, and suppose the distribution p(z) is defined by
f(z) dz is the normalization coefficient. We shall suppose that the
where Z =
value of Z is unknown. In the Laplace method the goal is to find a Gaussian approx-
imation q(z) which is centred on a mode of the distribution p(z). The first step is to
find a mode of p(z), in other words a point z0 such that p(z0) = 0, or equivalently
p(z) =
f(z)
1
Z
df(z)
dz
= 0.
z=z0
A Gaussian distribution has the property that its logarithm is a quadratic function
of the variables. We therefore consider a Taylor expansion of ln f(z) centred on the
mode z0 so that
where
Note that the first-order term in the Taylor expansion does not appear since z0 is a
local maximum of the distribution. Taking the exponential we obtain
ln f(z)  ln f(z0) −
1
2 A(z − z0)2
A = −
d2
dz2 ln f(z)
z=z0
f(z)  f(z0) exp
A
2
(z − z0)2
We can then obtain a normalized distribution q(z) by making use of the standard
result for the normalization of a Gaussian, so that
q(z) =
1/2
exp
A
2π
A
2
(z − z0)2
(4.130)
The Laplace approximation is illustrated in Figure 4.14. Note that the Gaussian
approximation will only be well defined if its precision A > 0, in other words the
stationary point z0 must be a local maximum, so that the second derivative of f(z)
at the point z0 is negative.
(4.125)
(4.126)
(4.127)
(4.128)
(4.129)
4.4. The Laplace Approximation
215
40
30
20
10
0
−2
4
0.8
0.6
0.4
0.2
0
−2
−1
0
1
2
3
Figure 4.14 Illustration of the Laplace approximation applied to the distribution p(z) ∝ exp(−z2/2)σ(20z + 4)
where σ(z) is the logistic sigmoid function defined by σ(z) = (1 + e−z)−1. The left plot shows the normalized
distribution p(z) in yellow, together with the Laplace approximation centred on the mode z0 of p(z) in red. The
right plot shows the negative logarithms of the corresponding curves.
−1
0
1
2
3
4
We can extend the Laplace method to approximate a distribution p(z) = f(z)/Z
defined over an M-dimensional space z. At a stationary point z0 the gradient ∇f(z)
will vanish. Expanding around this stationary point we have
ln f(z)  ln f(z0) −
(z − z0)TA(z − z0)
1
2
where the M × M Hessian matrix A is defined by
A = − ∇∇ ln f(z)|z=z0
(4.131)
(4.132)
and ∇ is the gradient operator. Taking the exponential of both sides we obtain
f(z)  f(z0) exp
(z − z0)TA(z − z0)
(4.133)
1
2
The distribution q(z) is proportional to f(z) and the appropriate normalization coef-
ficient can be found by inspection, using the standard result (2.43) for a normalized
multivariate Gaussian, giving
q(z) = |A|1/2
(2π)M/2
exp
1
2
(z − z0)TA(z − z0)
= N (z|z0, A−1)
(4.134)
where |A| denotes the determinant of A. This Gaussian distribution will be well
defined provided its precision matrix, given by A, is positive definite, which implies
that the stationary point z0 must be a local maximum, not a minimum or a saddle
point.
In order to apply the Laplace approximation we first need to find the mode z0,
and then evaluate the Hessian matrix at that mode. In practice a mode will typi-
cally be found by running some form of numerical optimization algorithm (Bishop
216
4. LINEAR MODELS FOR CLASSIFICATION
and Nabney, 2008). Many of the distributions encountered in practice will be mul-
timodal and so there will be different Laplace approximations according to which
mode is being considered. Note that the normalization constant Z of the true distri-
bution does not need to be known in order to apply the Laplace method. As a result
of the central limit theorem, the posterior distribution for a model is expected to
become increasingly better approximated by a Gaussian as the number of observed
data points is increased, and so we would expect the Laplace approximation to be
most useful in situations where the number of data points is relatively large.
One major weakness of the Laplace approximation is that, since it is based on a
Gaussian distribution, it is only directly applicable to real variables. In other cases
it may be possible to apply the Laplace approximation to a transformation of the
variable. For instance if 0 � τ < ∞ then we can consider a Laplace approximation
of ln τ . The most serious limitation of the Laplace framework, however, is that
it is based purely on the aspects of the true distribution at a specific value of the
variable, and so can fail to capture important global properties. In Chapter 10 we
shall consider alternative approaches which adopt a more global perspective.
