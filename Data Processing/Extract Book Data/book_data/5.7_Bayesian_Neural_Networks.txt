where we have used (5.148). Because a standard network trained by least squares
is approximating the conditional mean, we see that a mixture density network can
reproduce the conventional least-squares result as a special case. Of course, as we
have already noted, for a multimodal distribution the conditional mean is of limited
value.
We can similarly evaluate the variance of the density function about the condi-
Exercise 5.37
tional average, to give
s2(x) = E
K
t − E[t|x]2 |x
k(x) +
πk(x)
πl(x)µl(x)
l=1
where we have used (5.148) and (5.158). This is more general than the corresponding
least-squares result because the variance is a function of x.
We have seen that for multimodal distributions, the conditional mean can give
a poor representation of the data. For instance, in controlling the simple robot arm
shown in Figure 5.18, we need to pick one of the two possible joint angle settings
in order to achieve the desired end-effector location, whereas the average of the two
solutions is not itself a solution.
In such cases, the conditional mode may be of
more value. Because the conditional mode for the mixture density network does not
have a simple analytical solution, this would require numerical iteration. A simple
alternative is to take the mean of the most probable component (i.e., the one with the
largest mixing coefficient) at each value of x. This is shown for the toy data set in
Figure 5.21(d).
5.7. Bayesian Neural Networks
So far, our discussion of neural networks has focussed on the use of maximum like-
lihood to determine the network parameters (weights and biases). Regularized max-
imum likelihood can be interpreted as a MAP (maximum posterior) approach in
which the regularizer can be viewed as the logarithm of a prior parameter distribu-
tion. However, in a Bayesian treatment we need to marginalize over the distribution
of parameters in order to make predictions.
In Section 3.3, we developed a Bayesian solution for a simple linear regression
model under the assumption of Gaussian noise. We saw that the posterior distribu-
tion, which is Gaussian, could be evaluated exactly and that the predictive distribu-
tion could also be found in closed form. In the case of a multilayered network, the
highly nonlinear dependence of the network function on the parameter values means
that an exact Bayesian treatment can no longer be found. In fact, the log of the pos-
terior distribution will be nonconvex, corresponding to the multiple local minima in
the error function.
The technique of variational inference, to be discussed in Chapter 10, has been
applied to Bayesian neural networks using a factorized Gaussian approximation
N
278
5. NEURAL NETWORKS
to the posterior distribution (Hinton and van Camp, 1993) and also using a full-
covariance Gaussian (Barber and Bishop, 1998a; Barber and Bishop, 1998b). The
most complete treatment, however, has been based on the Laplace approximation
(MacKay, 1992c; MacKay, 1992b) and forms the basis for the discussion given here.
We will approximate the posterior distribution by a Gaussian, centred at a mode of
the true posterior. Furthermore, we shall assume that the covariance of this Gaus-
sian is small so that the network function is approximately linear with respect to the
parameters over the region of parameter space for which the posterior probability is
significantly nonzero. With these two approximations, we will obtain models that
are analogous to the linear regression and classification models discussed in earlier
chapters and so we can exploit the results obtained there. We can then make use of
the evidence framework to provide point estimates for the hyperparameters and to
compare alternative models (for example, networks having different numbers of hid-
den units). To start with, we shall discuss the regression case and then later consider
the modifications needed for solving classification tasks.
