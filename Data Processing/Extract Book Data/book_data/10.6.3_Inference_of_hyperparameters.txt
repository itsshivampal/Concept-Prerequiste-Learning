So far, we have treated the hyperparameter α in the prior distribution as a known
constant. We now extend the Bayesian logistic regression model to allow the value of
this parameter to be inferred from the data set. This can be achieved by combining
the global and local variational approximations into a single framework, so as to
maintain a lower bound on the marginal likelihood at each stage. Such a combined
approach was adopted by Bishop and Svens´en (2003) in the context of a Bayesian
treatment of the hierarchical mixture of experts model.
(10.166)
(10.167)
(10.168)
10.6. Variational Logistic Regression
503
Specifically, we consider once again a simple isotropic Gaussian prior distribu-
tion of the form
p(w|α) = N (w|0, α−1I).
(10.165)
Our analysis is readily extended to more general Gaussian priors, for instance if we
wish to associate a different hyperparameter with different subsets of the parame-
ters wj. As usual, we consider a conjugate hyperprior over α given by a gamma
distribution
governed by the constants a0 and b0.
The marginal likelihood for this model now takes the form
p(α) = Gam(α|a0, b0)
p(t) =
p(w, α, t) dw dα
where the joint distribution is given by
p(w, α, t) = p(t|w)p(w|α)p(α).
We are now faced with an analytically intractable integration over w and α, which
we shall tackle by using both the local and global variational approaches in the same
model
To begin with, we introduce a variational distribution q(w, α), and then apply
the decomposition (10.2), which in this instance takes the form
(10.169)
where the lower bound L(q) and the Kullback-Leibler divergence KL(qp) are de-
fined by
ln p(t) = L(q) + KL(qp)
L(q) =
q(w, α) ln
dw dα
KL(qp) = −
q(w, α) ln
dw dα.
p(w, α, t)
q(w, α)
p(w, α|t))
q(w, α)
(10.170)
(10.171)
At this point, the lower bound L(q) is still intractable due to the form of the
likelihood factor p(t|w). We therefore apply the local variational bound to each of
the logistic sigmoid factors as before. This allows us to use the inequality (10.152)
and place a lower bound on L(q), which will therefore also be a lower bound on the
log marginal likelihood
ln p(t) � L(q) �
L(q, ξ)
q(w, α) ln
h(w, ξ)p(w|α)p(α)
q(w, α)
dw dα.
(10.172)
Next we assume that the variational distribution factorizes between parameters and
hyperparameters so that
q(w, α) = q(w)q(α).
(10.173)
N
n=1
n=1
N
n=1
N
504
10. APPROXIMATE INFERENCE
With this factorization we can appeal to the general result (10.9) to find expressions
for the optimal factors. Consider first the distribution q(w). Discarding terms that
are independent of w, we have
ln q(w) = Eα [ln{h(w, ξ)p(w|α)p(α)}] + const
= ln h(w, ξ) + Eα [ln p(w|α)] + const.
We now substitute for ln h(w, ξ) using (10.153), and for ln p(w|α) using (10.165),
giving
ln q(w) = −
E[α]
2
wTw +
(tn − 1/2)wTφn − λ(ξn)wTφnφT
nw
+ const.
We see that this is a quadratic function of w and so the solution for q(w) will be
Gaussian. Completing the square in the usual way, we obtain
q(w) = N (w|µN , ΣN )
where we have defined
Σ−1
N µN =
(tn − 1/2)φn
Σ−1
N = E[α]I + 2
λ(ξn)φnφT
n.
Similarly, the optimal solution for the factor q(α) is obtained from
ln q(α) = Ew [ln p(w|α)] + ln p(α) + const.
Substituting for ln p(w|α) using (10.165), and for ln p(α) using (10.166), we obtain
ln q(α) = M
2
ln α −
2 E
wTw
+ (a0 − 1) ln α − b0α + const.
We recognize this as the log of a gamma distribution, and so we obtain
q(α) = Gam(α|aN , bN ) =
where
1
Γ(a0) ab0
0 αa0−1e−b0α
aN = a0 + M
2
1
2Ew
bN = b0 +
wTw
(10.174)
(10.175)
(10.176)
(10.177)
(10.178)
(10.179)
