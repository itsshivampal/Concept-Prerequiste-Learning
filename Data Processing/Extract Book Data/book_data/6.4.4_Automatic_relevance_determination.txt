In the previous section, we saw how maximum likelihood could be used to de-
termine a value for the correlation length-scale parameter in a Gaussian process.
This technique can usefully be extended by incorporating a separate parameter for
each input variable (Rasmussen and Williams, 2006). The result, as we shall see, is
that the optimization of these parameters by maximum likelihood allows the relative
importance of different inputs to be inferred from the data. This represents an exam-
ple in the Gaussian process context of automatic relevance determination, or ARD,
which was originally formulated in the framework of neural networks (MacKay,
1994; Neal, 1996). The mechanism by which appropriate inputs are preferred is
discussed in Section 7.2.2.
Consider a Gaussian process with a two-dimensional input space x = (x1, x2),
having a kernel function of the form
k(x, x) = θ0 exp
1
2
ηi(xi − xi)2
i=1
(6.71)
Samples from the resulting prior over functions y(x) are shown for two different
settings of the precision parameters ηi in Figure 6.9. We see that, as a particu-
lar parameter ηi becomes small, the function becomes relatively insensitive to the
corresponding input variable xi. By adapting these parameters to a data set using
maximum likelihood, it becomes possible to detect input variables that have little
effect on the predictive distribution, because the corresponding values of ηi will be
small. This can be useful in practice because it allows such inputs to be discarded.
ARD is illustrated using a simple synthetic data set having three inputs x1, x2 and x3
(Nabney, 2002) in Figure 6.10. The target variable t, is generated by sampling 100
values of x1 from a Gaussian, evaluating the function sin(2πx1), and then adding
D
D
6.4. Gaussian Processes
313
Figure 6.10 Illustration of automatic rele-
vance determination in a Gaus-
sian process for a synthetic prob-
lem having three inputs x1, x2,
and x3,
for which the curves
show the corresponding values of
the hyperparameters η1 (red), η2
(green), and η3 (blue) as a func-
tion of the number of iterations
when optimizing the marginal
likelihood. Details are given in
the text. Note the logarithmic
scale on the vertical axis.
102
100
10−2
10−4
0
20
40
60
80
100
Gaussian noise. Values of x2 are given by copying the corresponding values of x1
and adding noise, and values of x3 are sampled from an independent Gaussian dis-
tribution. Thus x1 is a good predictor of t, x2 is a more noisy predictor of t, and x3
has only chance correlations with t. The marginal likelihood for a Gaussian process
with ARD parameters η1, η2, η3 is optimized using the scaled conjugate gradients
algorithm. We see from Figure 6.10 that η1 converges to a relatively large value, η2
converges to a much smaller value, and η3 becomes very small indicating that x3 is
irrelevant for predicting t.
The ARD framework is easily incorporated into the exponential-quadratic kernel
(6.63) to give the following form of kernel function, which has been found useful for
applications of Gaussian processes to a range of regression problems
k(xn, xm) = θ0 exp
1
2
ηi(xni − xmi)2
i=1
+ θ2 + θ3
xnixmi
(6.72)
i=1
where D is the dimensionality of the input space.
