In order to exploit kernel substitution, we need to be able to construct valid kernel
functions. One approach is to choose a feature space mapping φ(x) and then use
this to find the corresponding kernel, as is illustrated in Figure 6.1. Here the kernel
function is defined for a one-dimensional input space by
k(x, x) = φ(x)Tφ(x) =
φi(x)φi(x)
(6.10)
where φi(x) are the basis functions.
An alternative approach is to construct kernel functions directly. In this case,
we must ensure that the function we choose is a valid kernel, in other words that it
corresponds to a scalar product in some (perhaps infinite dimensional) feature space.
As a simple example, consider a kernel function given by
k(x, z) =
xTz
2
(6.11)
1
0.5
0
−0.5
−1
−1
0.04
0.02
0
−1
1
0.75
0.5
0.25
0
−1
0.04
0.02
0
−1
0
1
0
1
6.2. Constructing Kernels
295
1
0.75
0.5
0.25
0
−1
0.04
0.02
0
−1
1
1
0
0
1
1
0
0
Figure 6.1 Illustration of the construction of kernel functions starting from a corresponding set of basis func-
tions. In each column the lower plot shows the kernel function k(x, x) defined by (6.10) plotted as a function of
x for x = 0, while the upper plot shows the corresponding basis functions given by polynomials (left column),
‘Gaussians’ (centre column), and logistic sigmoids (right column).
If we take the particular case of a two-dimensional input space x = (x1, x2) we
can expand out the terms and thereby identify the corresponding nonlinear feature
mapping
k(x, z) =
2 = (x1z1 + x2z2)2
2z2
2
xTz
= x2
1 + 2x1z1x2z2 + x2
1z2
1,√2x1x2, x2
= (x2
= φ(x)Tφ(z).
1,√2z1z2, z2
2)T
2)(z2
(6.12)
2)T and
We see that the feature mapping takes the form φ(x) = (x2
therefore comprises all possible second order terms, with a specific weighting be-
tween them.
1,√2x1x2, x2
More generally, however, we need a simple way to test whether a function con-
stitutes a valid kernel without having to construct the function φ(x) explicitly. A
necessary and sufficient condition for a function k(x, x) to be a valid kernel (Shawe-
Taylor and Cristianini, 2004) is that the Gram matrix K, whose elements are given by
k(xn, xm), should be positive semidefinite for all possible choices of the set {xn}.
Note that a positive semidefinite matrix is not the same thing as a matrix whose
elements are nonnegative.
One powerful technique for constructing new kernels is to build them out of
simpler kernels as building blocks. This can be done using the following properties:
Appendix C
296
6. KERNEL METHODS
Techniques for Constructing New Kernels.
Given valid kernels k1(x, x) and k2(x, x), the following new kernels will also
be valid:
k(x, x) = ck1(x, x)
k(x, x) = f(x)k1(x, x)f(x)
k(x, x) = q (k1(x, x))
k(x, x) = exp (k1(x, x))
k(x, x) = k1(x, x) + k2(x, x)
k(x, x) = k1(x, x)k2(x, x)
k(x, x) = k3 (φ(x), φ(x))
k(x, x) = xTAx
k(x, x) = ka(xa, xa) + kb(xb, xb)
k(x, x) = ka(xa, xa)kb(xb, xb)
(6.13)
(6.14)
(6.15)
(6.16)
(6.17)
(6.18)
(6.19)
(6.20)
(6.21)
(6.22)
where c > 0 is a constant, f(·) is any function, q(·) is a polynomial with nonneg-
ative coefficients, φ(x) is a function from x to RM , k3(·,·) is a valid kernel in
RM , A is a symmetric positive semidefinite matrix, xa and xb are variables (not
necessarily disjoint) with x = (xa, xb), and ka and kb are valid kernel functions
over their respective spaces.
Equipped with these properties, we can now embark on the construction of more
complex kernels appropriate to specific applications. We require that the kernel
k(x, x) be symmetric and positive semidefinite and that it expresses the appropriate
form of similarity between x and x according to the intended application. Here we
consider a few common examples of kernel functions. For a more extensive discus-
sion of ‘kernel engineering’, see Shawe-Taylor and Cristianini (2004).
We saw that the simple polynomial kernel k(x, x) =
xTx
2 contains only
If we consider the slightly generalized kernel k(x, x) =
2 with c > 0, then the corresponding feature mapping φ(x) contains con-
M
terms of degree two.
xTx + c
stant and linear terms as well as terms of order two. Similarly, k(x, x) =
contains all monomials of order M. For instance, if x and x are two images, then
the kernel represents a particular weighted sum of all possible products of M pixels
in the first image with M pixels in the second image. This can similarly be gener-
M
alized to include all terms up to degree M by considering k(x, x) =
with c > 0. Using the results (6.17) and (6.18) for combining kernels we see that
these will all be valid kernel functions.
xTx + c
xTx
Another commonly used kernel takes the form
k(x, x) = exp
−x − x2/2σ2
(6.23)
and is often called a ‘Gaussian’ kernel. Note, however, that in this context it is
not interpreted as a probability density, and hence the normalization coefficient is
Exercise 6.11
Exercise 6.12
6.2. Constructing Kernels
297
(6.24)
(6.25)
omitted. We can see that this is a valid kernel by expanding the square
x − x2 = xTx + (x)Tx − 2xTx
to give
k(x, x) = exp
−xTx/2σ2
exp
xTx/σ2
exp
−(x)Tx/2σ2
and then making use of (6.14) and (6.16), together with the validity of the linear
kernel k(x, x) = xTx. Note that the feature vector that corresponds to the Gaussian
kernel has infinite dimensionality.
The Gaussian kernel is not restricted to the use of Euclidean distance. If we use
kernel substitution in (6.24) to replace xTx with a nonlinear kernel κ(x, x), we
obtain
k(x, x) = exp
1
2σ2 (κ(x, x) + κ(x, x) − 2κ(x, x))
(6.26)
An important contribution to arise from the kernel viewpoint has been the exten-
sion to inputs that are symbolic, rather than simply vectors of real numbers. Kernel
functions can be defined over objects as diverse as graphs, sets, strings, and text doc-
uments. Consider, for instance, a fixed set and define a nonvectorial space consisting
of all possible subsets of this set. If A1 and A2 are two such subsets then one simple
choice of kernel would be
k(A1, A2) = 2|A1∩A2|
(6.27)
where A1 ∩ A2 denotes the intersection of sets A1 and A2, and |A| denotes the
number of subsets in A. This is a valid kernel function because it can be shown to
correspond to an inner product in a feature space.
One powerful approach to the construction of kernels starts from a probabilistic
generative model (Haussler, 1999), which allows us to apply generative models in a
discriminative setting. Generative models can deal naturally with missing data and
in the case of hidden Markov models can handle sequences of varying length. By
contrast, discriminative models generally give better performance on discriminative
tasks than generative models. It is therefore of some interest to combine these two
approaches (Lasserre et al., 2006). One way to combine them is to use a generative
model to define a kernel, and then use this kernel in a discriminative approach.
Given a generative model p(x) we can define a kernel by
k(x, x) = p(x)p(x).
(6.28)
This is clearly a valid kernel function because we can interpret it as an inner product
in the one-dimensional feature space defined by the mapping p(x). It says that two
inputs x and x are similar if they both have high probabilities. We can use (6.13) and
(6.17) to extend this class of kernels by considering sums over products of different
probability distributions, with positive weighting coefficients p(i), of the form
k(x, x) =
p(x|i)p(x|i)p(i).
i
(6.29)
Z
N
298
6. KERNEL METHODS
This is equivalent, up to an overall multiplicative constant, to a mixture distribution
in which the components factorize, with the index i playing the role of a ‘latent’
variable. Two inputs x and x will give a large value for the kernel function, and
hence appear similar, if they have significant probability under a range of different
components. Taking the limit of an infinite sum, we can also consider kernels of the
form
k(x, x) =
p(x|z)p(x|z)p(z) dz
(6.30)
where z is a continuous latent variable.
Now suppose that our data consists of ordered sequences of length L so that
an observation is given by X = {x1, . . . , xL}. A popular generative model for
sequences is the hidden Markov model, which expresses the distribution p(X) as a
marginalization over a corresponding sequence of hidden states Z = {z1, . . . , zL}.
We can use this approach to define a kernel function measuring the similarity of two
sequences X and X by extending the mixture representation (6.29) to give
k(X, X) =
p(X|Z)p(X|Z)p(Z)
(6.31)
so that both observed sequences are generated by the same hidden sequence Z. This
model can easily be extended to allow sequences of differing length to be compared.
An alternative technique for using generative models to define kernel functions
is known as the Fisher kernel (Jaakkola and Haussler, 1999). Consider a parametric
generative model p(x|θ) where θ denotes the vector of parameters. The goal is to
find a kernel that measures the similarity of two input vectors x and x induced by the
generative model. Jaakkola and Haussler (1999) consider the gradient with respect
to θ, which defines a vector in a ‘feature’ space having the same dimensionality as
θ. In particular, they consider the Fisher score
Section 9.2
Section 13.2
Exercise 6.13
from which the Fisher kernel is defined by
g(θ, x) = ∇θ ln p(x|θ)
Here F is the Fisher information matrix, given by
k(x, x) = g(θ, x)TF−1g(θ, x).
(6.32)
(6.33)
F = Ex
g(θ, x)g(θ, x)T
(6.34)
where the expectation is with respect to x under the distribution p(x|θ). This can
be motivated from the perspective of information geometry (Amari, 1998), which
considers the differential geometry of the space of model parameters. Here we sim-
ply note that the presence of the Fisher information matrix causes this kernel to be
invariant under a nonlinear re-parameterization of the density model θ → ψ(θ).
In practice, it is often infeasible to evaluate the Fisher information matrix. One
approach is simply to replace the expectation in the definition of the Fisher informa-
tion with the sample average, giving
1
N
F 
g(θ, xn)g(θ, xn)T.
(6.35)
n=1
N
