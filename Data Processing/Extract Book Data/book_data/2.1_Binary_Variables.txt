We begin by considering a single binary random variable x ∈ {0, 1}. For example,
x might describe the outcome of flipping a coin, with x = 1 representing ‘heads’,
and x = 0 representing ‘tails’. We can imagine that this is a damaged coin so that
the probability of landing heads is not necessarily the same as that of landing tails.
The probability of x = 1 will be denoted by the parameter µ so that
p(x = 1|µ) = µ
(2.1)
N
N
N
N
n=1
N
2.1. Binary Variables
69
where 0 � µ � 1, from which it follows that p(x = 0|µ) = 1 − µ. The probability
distribution over x can therefore be written in the form
Bern(x|µ) = µx(1 − µ)1−x
(2.2)
which is known as the Bernoulli distribution. It is easily verified that this distribution
is normalized and that it has mean and variance given by
E[x] = µ
var[x] = µ(1 − µ).
(2.3)
(2.4)
Now suppose we have a data set D = {x1, . . . , xN} of observed values of x.
We can construct the likelihood function, which is a function of µ, on the assumption
that the observations are drawn independently from p(x|µ), so that
p(D|µ) =
p(xn|µ) =
n=1
n=1
µxn(1 − µ)1−xn.
(2.5)
In a frequentist setting, we can estimate a value for µ by maximizing the likelihood
function, or equivalently by maximizing the logarithm of the likelihood. In the case
of the Bernoulli distribution, the log likelihood function is given by
ln p(D|µ) =
ln p(xn|µ) =
n=1
{xn ln µ + (1 − xn) ln(1 − µ)} .
(2.6)
At this point, it is worth noting that the log likelihood function depends on the N
observations xn only through their sum
n xn. This sum provides an example of a
sufficient statistic for the data under this distribution, and we shall study the impor-
tant role of sufficient statistics in some detail. If we set the derivative of ln p(D|µ)
with respect to µ equal to zero, we obtain the maximum likelihood estimator
µML =
1
N
xn
n=1
(2.7)
Exercise 2.1
Section 2.4
Jacob Bernoulli
1654–1705
Jacob Bernoulli, also known as
Jacques or James Bernoulli, was a
Swiss mathematician and was the
first of many in the Bernoulli family
to pursue a career in science and
mathematics. Although compelled
to study philosophy and theology against his will by
his parents, he travelled extensively after graduating
in order to meet with many of the leading scientists of
his time, including Boyle and Hooke in England. When
he returned to Switzerland, he taught mechanics and
became Professor of Mathematics at Basel in 1687.
Unfortunately, rivalry between Jacob and his younger
brother Johann turned an initially productive collabora-
tion into a bitter and public dispute. Jacob’s most sig-
nificant contributions to mathematics appeared in The
ArtofConjecture published in 1713, eight years after
his death, which deals with topics in probability the-
ory including what has become known as the Bernoulli
distribution.
1
0
0.2
0.1
0
2
3
4
5
m
6
7
8
9
10
which is also known as the sample mean. If we denote the number of observations
of x = 1 (heads) within this data set by m, then we can write (2.7) in the form
µML = m
N
(2.8)
so that the probability of landing heads is given, in this maximum likelihood frame-
work, by the fraction of observations of heads in the data set.
Now suppose we flip a coin, say, 3 times and happen to observe 3 heads. Then
N = m = 3 and µML = 1. In this case, the maximum likelihood result would
predict that all future observations should give heads. Common sense tells us that
this is unreasonable, and in fact this is an extreme example of the over-fitting associ-
ated with maximum likelihood. We shall see shortly how to arrive at more sensible
conclusions through the introduction of a prior distribution over µ.
We can also work out the distribution of the number m of observations of x = 1,
given that the data set has size N. This is called the binomial distribution, and
from (2.5) we see that it is proportional to µm(1 − µ)N−m. In order to obtain the
normalization coefficient we note that out of N coin flips, we have to add up all
of the possible ways of obtaining m heads, so that the binomial distribution can be
written
Bin(m|N, µ) =
N
m
µm(1 − µ)N−m
N!
N
m
(N − m)!m!
(2.9)
(2.10)
is the number of ways of choosing m objects out of a total of N identical objects.
Figure 2.1 shows a plot of the binomial distribution for N = 10 and µ = 0.25.
The mean and variance of the binomial distribution can be found by using the
result of Exercise 1.10, which shows that for independent events the mean of the
sum is the sum of the means, and the variance of the sum is the sum of the variances.
Because m = x1 + . . . + xN , and for each observation the mean and variance are
70
2. PROBABILITY DISTRIBUTIONS
Figure 2.1 Histogram plot of the binomial dis-
tribution (2.9) as a function of m for
N = 10 and µ = 0.25.
0.3
where
Exercise 2.3
N
N
given by (2.3) and (2.4), respectively, we have
2.1. Binary Variables
71
E[m] ≡
mBin(m|N, µ) = N µ
m=0
(2.11)
(2.12)
var[m] ≡
m=0
(m − E[m])2 Bin(m|N, µ) = N µ(1 − µ).
These results can also be proved directly using calculus.
