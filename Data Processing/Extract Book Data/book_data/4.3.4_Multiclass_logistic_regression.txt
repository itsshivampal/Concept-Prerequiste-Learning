In our discussion of generative models for multiclass classification, we have
seen that for a large class of distributions, the posterior probabilities are given by a
softmax transformation of linear functions of the feature variables, so that
p(Ck|φ) = yk(φ) =
exp(ak)
j exp(aj)
where the ‘activations’ ak are given by
ak = wT
k φ.
(4.104)
(4.105)
(4.106)
There we used maximum likelihood to determine separately the class-conditional
densities and the class priors and then found the corresponding posterior probabilities
using Bayes’ theorem, thereby implicitly determining the parameters {wk}. Here we
consider the use of maximum likelihood to determine the parameters {wk} of this
model directly. To do this, we will require the derivatives of yk with respect to all of
the activations aj. These are given by
∂yk
∂aj
= yk(Ikj − yj)
where Ikj are the elements of the identity matrix.
Next we write down the likelihood function. This is most easily done using
the 1-of-K coding scheme in which the target vector tn for a feature vector φn
belonging to class Ck is a binary vector with all elements zero except for element k,
which equals one. The likelihood function is then given by
p(T|w1, . . . , wK) =
n=1
k=1
p(Ck|φn)tnk =
ytnk
nk
(4.107)
where ynk = yk(φn), and T is an N × K matrix of target variables with elements
tnk. Taking the negative logarithm then gives
E(w1, . . . , wK) = − ln p(T|w1, . . . , wK) = −
tnk ln ynk
(4.108)
n=1
k=1
which is known as the cross-entropy error function for the multiclass classification
problem.
We now take the gradient of the error function with respect to one of the param-
eter vectors wj. Making use of the result (4.106) for the derivatives of the softmax
function, we obtain
∇wj E(w1, . . . , wK) =
(ynj − tnj) φn
n=1
(4.109)
Section 4.2
Exercise 4.17
Exercise 4.18
N
210
4. LINEAR MODELS FOR CLASSIFICATION
k tnk = 1. Once again, we see the same form arising
where we have made use of
for the gradient as was found for the sum-of-squares error function with the linear
model and the cross-entropy error for the logistic regression model, namely the prod-
uct of the error (ynj − tnj) times the basis function φn. Again, we could use this
to formulate a sequential algorithm in which patterns are presented one at a time, in
which each of the weight vectors is updated using (3.22).
We have seen that the derivative of the log likelihood function for a linear regres-
sion model with respect to the parameter vector w for a data point n took the form
of the ‘error’ yn − tn times the feature vector φn. Similarly, for the combination
of logistic sigmoid activation function and cross-entropy error function (4.90), and
for the softmax activation function with the multiclass cross-entropy error function
(4.108), we again obtain this same simple form. This is an example of a more general
result, as we shall see in Section 4.3.6.
To find a batch algorithm, we again appeal to the Newton-Raphson update to
obtain the corresponding IRLS algorithm for the multiclass problem. This requires
evaluation of the Hessian matrix that comprises blocks of size M × M in which
block j, k is given by
Exercise 4.20
∇wk∇wj E(w1, . . . , wK) = −
ynk(Ikj − ynj)φnφT
n.
n=1
(4.110)
As with the two-class problem, the Hessian matrix for the multiclass logistic regres-
sion model is positive definite and so the error function again has a unique minimum.
Practical details of IRLS for the multiclass case can be found in Bishop and Nabney
(2008).
