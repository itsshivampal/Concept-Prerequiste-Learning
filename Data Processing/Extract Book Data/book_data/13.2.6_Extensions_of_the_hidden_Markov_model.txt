The basic hidden Markov model, along with the standard training algorithm
based on maximum likelihood, has been extended in numerous ways to meet the
requirements of particular applications. Here we discuss a few of the more important
examples.
We see from the digits example in Figure 13.11 that hidden Markov models can
be quite poor generative models for the data, because many of the synthetic digits
look quite unrepresentative of the training data. If the goal is sequence classifica-
tion, there can be significant benefit in determining the parameters of hidden Markov
models using discriminative rather than maximum likelihood techniques. Suppose
we have a training set of R observation sequences Xr, where r = 1, . . . , R, each of
which is labelled according to its class m, where m = 1, . . . , M. For each class, we
have a separate hidden Markov model with its own parameters θm, and we treat the
problem of determining the parameter values as a standard classification problem in
which we optimize the cross-entropy
where p(m) is the prior probability of class m. Optimization of this cost function
is more complex than for maximum likelihood (Kapadia, 1998), and in particular
Using Bayes’ theorem this can be expressed in terms of the sequence probabilities
associated with the hidden Markov models
ln p(mr|Xr).
ln
r=1
p(Xr|θr)p(mr)
l=1 p(Xr|θl)p(lr)
M
(13.72)
(13.73)
632
13. SEQUENTIAL DATA
Figure 13.17 Section of an autoregressive hidden
Markov model, in which the distribution
of the observation xn depends on a
subset of the previous observations as
well as on the hidden state zn.
In this
example, the distribution of xn depends
on the two previous observations xn−1
and xn−2.
zn−1
xn−1
zn
xn
zn+1
xn+1
requires that every training sequence be evaluated under each of the models in or-
der to compute the denominator in (13.73). Hidden Markov models, coupled with
discriminative training methods, are widely used in speech recognition (Kapadia,
1998).
A significant weakness of the hidden Markov model is the way in which it rep-
resents the distribution of times for which the system remains in a given state. To see
the problem, note that the probability that a sequence sampled from a given hidden
Markov model will spend precisely T steps in state k and then make a transition to a
different state is given by
p(T ) = (Akk)T (1 − Akk) ∝ exp (−T ln Akk)
(13.74)
T
and so is an exponentially decaying function of T . For many applications, this will
be a very unrealistic model of state duration. The problem can be resolved by mod-
elling state duration directly in which the diagonal coefficients Akk are all set to zero,
and each state k is explicitly associated with a probability distribution p(T|k) of pos-
sible duration times. From a generative point of view, when a state k is entered, a
value T representing the number of time steps that the system will remain in state k
is then drawn from p(T|k). The model then emits T values of the observed variable
xt, which are generally assumed to be independent so that the corresponding emis-
t=1 p(xt|k). This approach requires some straightforward
sion density is simply
modifications to the EM optimization procedure (Rabiner, 1989).
Another limitation of the standard HMM is that it is poor at capturing long-
range correlations between the observed variables (i.e., between variables that are
separated by many time steps) because these must be mediated via the first-order
Markov chain of hidden states. Longer-range effects could in principle be included
by adding extra links to the graphical model of Figure 13.5. One way to address this
is to generalize the HMM to give the autoregressive hidden Markov model (Ephraim
et al., 1989), an example of which is shown in Figure 13.17. For discrete observa-
tions, this corresponds to expanded tables of conditional probabilities for the emis-
sion distributions. In the case of a Gaussian emission density, we can use the linear-
Gaussian framework in which the conditional distribution for xn given the values
of the previous observations, and the value of zn, is a Gaussian whose mean is a
linear combination of the values of the conditioning variables. Clearly the number
of additional links in the graph must be limited to avoid an excessive the number of
free parameters. In the example shown in Figure 13.17, each observation depends on
Figure 13.18 Example of an input-output hidden
Markov model.
In this case, both the
emission probabilities and the transition
probabilities depend on the values of a
sequence of observations u1, . . . , uN .
13.2. Hidden Markov Models
633
un−1
zn−1
xn−1
un
zn
xn
un+1
zn+1
xn+1
the two preceding observed variables as well as on the hidden state. Although this
graph looks messy, we can again appeal to d-separation to see that in fact it still has
a simple probabilistic structure. In particular, if we imagine conditioning on zn we
see that, as with the standard HMM, the values of zn−1 and zn+1 are independent,
corresponding to the conditional independence property (13.5). This is easily veri-
fied by noting that every path from node zn−1 to node zn+1 passes through at least
one observed node that is head-to-tail with respect to that path. As a consequence,
we can again use a forward-backward recursion in the E step of the EM algorithm to
determine the posterior distributions of the latent variables in a computational time
that is linear in the length of the chain. Similarly, the M step involves only a minor
modification of the standard M-step equations. In the case of Gaussian emission
densities this involves estimating the parameters using the standard linear regression
equations, discussed in Chapter 3.
We have seen that the autoregressive HMM appears as a natural extension of the
standard HMM when viewed as a graphical model. In fact the probabilistic graphical
modelling viewpoint motivates a plethora of different graphical structures based on
the HMM. Another example is the input-output hidden Markov model (Bengio and
Frasconi, 1995), in which we have a sequence of observed variables u1, . . . , uN , in
addition to the output variables x1, . . . , xN , whose values influence either the dis-
tribution of latent variables or output variables, or both. An example is shown in
Figure 13.18. This extends the HMM framework to the domain of supervised learn-
ing for sequential data. It is again easy to show, through the use of the d-separation
criterion, that the Markov property (13.5) for the chain of latent variables still holds.
To verify this, simply note that there is only one path from node zn−1 to node zn+1
and this is head-to-tail with respect to the observed node zn. This conditional inde-
pendence property again allows the formulation of a computationally efficient learn-
ing algorithm. In particular, we can determine the parameters θ of the model by
maximizing the likelihood function L(θ) = p(X|U, θ) where U is a matrix whose
rows are given by uT
n. As a consequence of the conditional independence property
(13.5) this likelihood function can be maximized efficiently using an EM algorithm
in which the E step involves forward and backward recursions.
Another variant of the HMM worthy of mention is the factorial hidden Markov
model (Ghahramani and Jordan, 1997), in which there are multiple independent
Exercise 13.18
634
13. SEQUENTIAL DATA
Figure 13.19 A factorial hidden Markov model com-
prising two Markov chains of latent vari-
ables. For continuous observed variables
x, one possible choice of emission model
is a linear-Gaussian density in which the
mean of the Gaussian is a linear combi-
nation of the states of the corresponding
latent variables.
z(2)
n−1
z(1)
n−1
xn−1
z(2)
n
z(1)
n
z(2)
n+1
z(1)
n+1
xn
xn+1
Markov chains of latent variables, and the distribution of the observed variable at
a given time step is conditional on the states of all of the corresponding latent vari-
ables at that same time step. Figure 13.19 shows the corresponding graphical model.
The motivation for considering factorial HMM can be seen by noting that in order to
represent, say, 10 bits of information at a given time step, a standard HMM would
need K = 210 = 1024 latent states, whereas a factorial HMM could make use of 10
binary latent chains. The primary disadvantage of factorial HMMs, however, lies in
the additional complexity of training them. The M step for the factorial HMM model
is straightforward. However, observation of the x variables introduces dependencies
between the latent chains, leading to difficulties with the E step. This can be seen
by noting that in Figure 13.19, the variables z(1)
n are connected by a path
which is head-to-head at node xn and hence they are not d-separated. The exact E
step for this model does not correspond to running forward and backward recursions
along the M Markov chains independently. This is confirmed by noting that the key
conditional independence property (13.5) is not satisfied for the individual Markov
chains in the factorial HMM model, as is shown using d-separation in Figure 13.20.
Now suppose that there are M chains of hidden nodes and for simplicity suppose
that all latent variables have the same number K of states. Then one approach would
be to note that there are KM combinations of latent variables at a given time step
n and z(2)
Figure 13.20 Example of a path, highlighted in green,
which is head-to-head at the observed
nodes xn−1 and xn+1, and head-to-tail
at the unobserved nodes z(2)
n and
z(2)
n+1. Thus the path is not blocked and
so the conditional independence property
(13.5) does not hold for the individual la-
tent chains of the factorial HMM model.
As a consequence, there is no efficient
exact E step for this model.
n−1, z(2)
z(2)
n−1
z(1)
n−1
xn−1
z(2)
n
z(1)
n
z(2)
n+1
z(1)
n+1
xn
xn+1
Section 10.1
