In deriving these variational update equations for the Gaussian mixture model,
we assumed a particular factorization of the variational posterior distribution given
by (10.42). However, the optimal solutions for the various factors exhibit additional
factorizations. In particular, the solution for q(µ, Λ) is given by the product of an
independent distribution q(µk, Λk) over each of the components k of the mixture,
whereas the variational posterior distribution q(Z) over the latent variables, given
by (10.48), factorizes into an independent distribution q(zn) for each observation n
(note that it does not further factorize with respect to k because, for each value of n,
the znk are constrained to sum to one over k). These additional factorizations are a
consequence of the interaction between the assumed factorization and the conditional
independence properties of the true distribution, as characterized by the directed
graph in Figure 10.5.
We shall refer to these additional factorizations as induced factorizations be-
cause they arise from an interaction between the factorization assumed in the varia-
tional posterior distribution and the conditional independence properties of the true
joint distribution. In a numerical implementation of the variational approach it is
important to take account of such additional factorizations. For instance, it would
be very inefficient to maintain a full precision matrix for the Gaussian distribution
over a set of variables if the optimal form for that distribution always had a diago-
nal precision matrix (corresponding to a factorization with respect to the individual
variables described by that Gaussian).
Such induced factorizations can easily be detected using a simple graphical test
based on d-separation as follows. We partition the latent variables into three disjoint
groups A, B, C and then let us suppose that we are assuming a factorization between
C and the remaining latent variables, so that
q(A, B, C) = q(A, B)q(C).
(10.84)
Using the general result (10.9), together with the product rule for probabilities, we
see that the optimal solution for q(A, B) is given by
ln q(A, B) = EC[ln p(X, A, B, C)] + const
= EC[ln p(A, B|X, C)] + const.
(10.85)
We now ask whether this resulting solution will factorize between A and B, in
other words whether q(A, B) = q(A)q(B). This will happen if, and only if,
ln p(A, B|X, C) = ln p(A|X, C) + ln p(B|X, C), that is, if the conditional inde-
pendence relation
(10.86)
A ⊥⊥ B | X, C
486
10. APPROXIMATE INFERENCE
is satisfied. We can test to see if this relation does hold, for any choice of A and B
by making use of the d-separation criterion.
To illustrate this, consider again the Bayesian mixture of Gaussians represented
by the directed graph in Figure 10.5, in which we are assuming a variational fac-
torization given by (10.42). We can see immediately that the variational posterior
distribution over the parameters must factorize between π and the remaining param-
eters µ and Λ because all paths connecting π to either µ or Λ must pass through
one of the nodes zn all of which are in the conditioning set for our conditional inde-
pendence test and all of which are head-to-tail with respect to such paths.
