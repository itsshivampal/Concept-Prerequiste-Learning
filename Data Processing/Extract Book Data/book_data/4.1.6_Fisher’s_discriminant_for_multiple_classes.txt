We now consider the generalization of the Fisher discriminant to K > 2 classes,
and we shall assume that the dimensionality D of the input space is greater than the
number K of classes. Next, we introduce D > 1 linear ‘features’ yk = wT
k x, where
k = 1, . . . , D. These feature values can conveniently be grouped together to form
a vector y. Similarly, the weight vectors {wk} can be considered to be the columns
of a matrix W, so that
(4.39)
Note that again we are not including any bias parameters in the definition of y. The
generalization of the within-class covariance matrix to the case of K classes follows
from (4.28) to give
y = WTx.
SW =
Sk
where
(xn − mk)(xn − mk)T
Sk =
mk =
xn
n∈Ck
and Nk is the number of patterns in class Ck. In order to find a generalization of the
between-class covariance matrix, we follow Duda and Hart (1973) and consider first
the total covariance matrix
where m is the mean of the total data set
ST =
(xn − m)(xn − m)T
m =
xn =
1
N
Nkmk
k=1
and N =
k Nk is the total number of data points. The total covariance matrix can
be decomposed into the sum of the within-class covariance matrix, given by (4.40)
and (4.41), plus an additional matrix SB, which we identify as a measure of the
between-class covariance
ST = SW + SB
where
SB =
k=1
Nk(mk − m)(mk − m)T.
(4.40)
(4.41)
(4.42)
(4.43)
(4.44)
(4.45)
(4.46)
n∈Ck
k=1
k=1
K
K
n∈Ck
K
k=1
192
4. LINEAR MODELS FOR CLASSIFICATION
These covariance matrices have been defined in the original x-space. We can now
define similar matrices in the projected D-dimensional y-space
and
where
sW =
(yn − µk)(yn − µk)T
sB =
Nk(µk − µ)(µk − µ)T
µk =
1
Nk
yn,
1
N
Nkµk.
(4.47)
(4.48)
(4.49)
Again we wish to construct a scalar that is large when the between-class covariance
is large and when the within-class covariance is small. There are now many possible
choices of criterion (Fukunaga, 1990). One example is given by
J(W) = Tr
s−1
W sB
(4.50)
This criterion can then be rewritten as an explicit function of the projection matrix
W in the form
J(w) = Tr
(WSWWT)−1(WSBWT)
(4.51)
Maximization of such criteria is straightforward, though somewhat involved, and is
discussed at length in Fukunaga (1990). The weight values are determined by those
eigenvectors of S−1
W SB that correspond to the D largest eigenvalues.
There is one important result that is common to all such criteria, which is worth
emphasizing. We first note from (4.46) that SB is composed of the sum of K ma-
trices, each of which is an outer product of two vectors and therefore of rank 1. In
addition, only (K − 1) of these matrices are independent as a result of the constraint
(4.44). Thus, SB has rank at most equal to (K − 1) and so there are at most (K − 1)
nonzero eigenvalues. This shows that the projection onto the (K − 1)-dimensional
subspace spanned by the eigenvectors of SB does not alter the value of J(w), and
so we are therefore unable to find more than (K − 1) linear ‘features’ by this means
(Fukunaga, 1990).
