So far in this section, we have introduced a number of concepts from information
theory, including the key notion of entropy. We now start to relate these ideas to
pattern recognition. Consider some unknown distribution p(x), and suppose that
we have modelled this using an approximating distribution q(x). If we use q(x) to
construct a coding scheme for the purpose of transmitting values of x to a receiver,
then the average additional amount of information (in nats) required to specify the
value of x (assuming we choose an efficient coding scheme) as a result of using q(x)
instead of the true distribution p(x) is given by
KL(pq) = −
p(x) ln q(x) dx −
p(x) ln
q(x)
p(x)
dx.
p(x) ln p(x) dx
(1.113)
This is known as the relative entropy or Kullback-Leibler divergence, or KL diver-
gence (Kullback and Leibler, 1951), between the distributions p(x) and q(x). Note
that it is not a symmetrical quantity, that is to say KL(pq) ≡ KL(qp).
We now show that the Kullback-Leibler divergence satisfies KL(pq) � 0 with
equality if, and only if, p(x) = q(x). To do this we first introduce the concept of
convex functions. A function f(x) is said to be convex if it has the property that
every chord lies on or above the function, as shown in Figure 1.31. Any value of x
in the interval from x = a to x = b can be written in the form λa + (1 − λ)b where
0 � λ � 1. The corresponding point on the chord is given by λf(a) + (1 − λ)f(b),
Claude Shannon
1916–2001
After graduating from Michigan and
MIT, Shannon joined the AT&T Bell
Telephone laboratories in 1941. His
paper ‘A Mathematical Theory of
Communication’ published in the
Bell System Technical Journal in
1948 laid the foundations for modern information the-
ory. This paper introduced the word ‘bit’, and his con-
cept that information could be sent as a stream of 1s
and 0s paved the way for the communications revo-
lution.
It is said that von Neumann recommended to
Shannon that he use the term entropy, not only be-
cause of its similarity to the quantity used in physics,
but also because “nobody knows what entropy really
is, so in any discussion you will always have an advan-
tage”.
i=1
M
f
i=1
M
56
1. INTRODUCTION
Figure 1.31 A convex function f (x) is one for which ev-
ery chord (shown in blue) lies on or above
the function (shown in red).
f(x)
xλ
xλ
b
x
chord
a
and the corresponding value of the function is f (λa + (1 − λ)b). Convexity then
implies
(1.114)
f(λa + (1 − λ)b) � λf(a) + (1 − λ)f(b).
This is equivalent to the requirement that the second derivative of the function be
everywhere positive. Examples of convex functions are x ln x (for x > 0) and x2. A
function is called strictly convex if the equality is satisfied only for λ = 0 and λ = 1.
If a function has the opposite property, namely that every chord lies on or below the
function, it is called concave, with a corresponding definition for strictly concave. If
a function f(x) is convex, then −f(x) will be concave.
convex function f(x) satisfies
Using the technique of proof by induction, we can show from (1.114) that a
Exercise 1.36
Exercise 1.38
λixi
λif(xi)
(1.115)
where λi � 0 and
i λi = 1, for any set of points {xi}. The result (1.115) is
known as Jensen’s inequality. If we interpret the λi as the probability distribution
over a discrete variable x taking the values {xi}, then (1.115) can be written
f (E[x]) � E[f(x)]
(1.116)
where E[·] denotes the expectation. For continuous variables, Jensen’s inequality
takes the form
(1.117)
f(x)p(x) dx.
xp(x) dx
f
We can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler
divergence (1.113) to give
KL(pq) = −
p(x) ln
q(x)
p(x)
dx � − ln
q(x) dx = 0
(1.118)
n=1
N
1.6. Information Theory
57
where we have used the fact that − ln x is a convex function, together with the nor-
q(x) dx = 1. In fact, − ln x is a strictly convex function,
malization condition
so the equality will hold if, and only if, q(x) = p(x) for all x. Thus we can in-
terpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two
distributions p(x) and q(x).
We see that there is an intimate relationship between data compression and den-
sity estimation (i.e., the problem of modelling an unknown probability distribution)
because the most efficient compression is achieved when we know the true distri-
bution. If we use a distribution that is different from the true one, then we must
necessarily have a less efficient coding, and on average the additional information
that must be transmitted is (at least) equal to the Kullback-Leibler divergence be-
tween the two distributions.
Suppose that data is being generated from an unknown distribution p(x) that we
wish to model. We can try to approximate this distribution using some parametric
distribution q(x|θ), governed by a set of adjustable parameters θ, for example a
multivariate Gaussian. One way to determine θ is to minimize the Kullback-Leibler
divergence between p(x) and q(x|θ) with respect to θ. We cannot do this directly
because we don’t know p(x). Suppose, however, that we have observed a finite set
of training points xn, for n = 1, . . . , N, drawn from p(x). Then the expectation
with respect to p(x) can be approximated by a finite sum over these points, using
(1.35), so that
KL(pq) 
{− ln q(xn|θ) + ln p(xn)} .
(1.119)
The second term on the right-hand side of (1.119) is independent of θ, and the first
term is the negative log likelihood function for θ under the distribution q(x|θ) eval-
uated using the training set. Thus we see that minimizing this Kullback-Leibler
divergence is equivalent to maximizing the likelihood function.
Now consider the joint distribution between two sets of variables x and y given
by p(x, y). If the sets of variables are independent, then their joint distribution will
factorize into the product of their marginals p(x, y) = p(x)p(y). If the variables are
not independent, we can gain some idea of whether they are ‘close’ to being indepen-
dent by considering the Kullback-Leibler divergence between the joint distribution
and the product of the marginals, given by
I[x, y] ≡ KL(p(x, y)p(x)p(y))
p(x, y) ln
p(x)p(y)
p(x, y)
dx dy
(1.120)
Exercise 1.41
which is called the mutual information between the variables x and y. From the
properties of the Kullback-Leibler divergence, we see that I(x, y) � 0 with equal-
ity if, and only if, x and y are independent. Using the sum and product rules of
probability, we see that the mutual information is related to the conditional entropy
through
I[x, y] = H[x] − H[x|y] = H[y] − H[y|x].
(1.121)
M
N
N
58
1. INTRODUCTION
Thus we can view the mutual information as the reduction in the uncertainty about x
by virtue of being told the value of y (or vice versa). From a Bayesian perspective,
we can view p(x) as the prior distribution for x and p(x|y) as the posterior distribu-
tion after we have observed new data y. The mutual information therefore represents
the reduction in uncertainty about x as a consequence of the new observation y.
