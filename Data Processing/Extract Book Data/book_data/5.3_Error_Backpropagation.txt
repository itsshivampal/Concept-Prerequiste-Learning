241
This update is repeated by cycling through the data either in sequence or by selecting
points at random with replacement. There are of course intermediate scenarios in
which the updates are based on batches of data points.
One advantage of on-line methods compared to batch methods is that the former
handle redundancy in the data much more efficiently. To see, this consider an ex-
treme example in which we take a data set and double its size by duplicating every
data point. Note that this simply multiplies the error function by a factor of 2 and so
is equivalent to using the original error function. Batch methods will require double
the computational effort to evaluate the batch error function gradient, whereas on-
line methods will be unaffected. Another property of on-line gradient descent is the
possibility of escaping from local minima, since a stationary point with respect to
the error function for the whole data set will generally not be a stationary point for
each data point individually.
Nonlinear optimization algorithms, and their practical application to neural net-
work training, are discussed in detail in Bishop and Nabney (2008).
5.3. Error Backpropagation
Our goal in this section is to find an efficient technique for evaluating the gradient
of an error function E(w) for a feed-forward neural network. We shall see that
this can be achieved using a local message passing scheme in which information is
sent alternately forwards and backwards through the network and is known as error
backpropagation, or sometimes simply as backprop.
It should be noted that the term backpropagation is used in the neural com-
puting literature to mean a variety of different things. For instance, the multilayer
perceptron architecture is sometimes called a backpropagation network. The term
backpropagation is also used to describe the training of a multilayer perceptron us-
ing gradient descent applied to a sum-of-squares error function. In order to clarify
the terminology, it is useful to consider the nature of the training process more care-
fully. Most training algorithms involve an iterative procedure for minimization of an
error function, with adjustments to the weights being made in a sequence of steps. At
each such step, we can distinguish between two distinct stages. In the first stage, the
derivatives of the error function with respect to the weights must be evaluated. As
we shall see, the important contribution of the backpropagation technique is in pro-
viding a computationally efficient method for evaluating such derivatives. Because
it is at this stage that errors are propagated backwards through the network, we shall
use the term backpropagation specifically to describe the evaluation of derivatives.
In the second stage, the derivatives are then used to compute the adjustments to be
made to the weights. The simplest such technique, and the one originally considered
by Rumelhart et al. (1986), involves gradient descent. It is important to recognize
that the two stages are distinct. Thus, the first stage, namely the propagation of er-
rors backwards through the network in order to evaluate derivatives, can be applied
to many other kinds of network and not just the multilayer perceptron. It can also be
applied to error functions other that just the simple sum-of-squares, and to the eval-
n=1
N
k
i
yk =
wkixi
En =
1
2
(ynk − tnk)2
∂En
∂wji
= (ynj − tnj)xni
(5.45)
(5.46)
(5.47)
where ynk = yk(xn, w). The gradient of this error function with respect to a weight
wji is given by
which can be interpreted as a ‘local’ computation involving the product of an ‘error
signal’ ynj − tnj associated with the output end of the link wji and the variable xni
associated with the input end of the link. In Section 4.3.2, we saw how a similar
formula arises with the logistic sigmoid activation function together with the cross
entropy error function, and similarly for the softmax activation function together
with its matching cross-entropy error function. We shall now see how this simple
result extends to the more complex setting of multilayer feed-forward networks.
In a general feed-forward network, each unit computes a weighted sum of its
inputs of the form
aj =
wjizi
i
(5.48)
242
5. NEURAL NETWORKS
uation of other derivatives such as the Jacobian and Hessian matrices, as we shall
see later in this chapter. Similarly, the second stage of weight adjustment using the
calculated derivatives can be tackled using a variety of optimization schemes, many
of which are substantially more powerful than simple gradient descent.
