Let us suppose that observations are being drawn from some unknown probabil-
ity density p(x) in some D-dimensional space, which we shall take to be Euclidean,
and we wish to estimate the value of p(x). From our earlier discussion of locality,
let us consider some small region R containing x. The probability mass associated
with this region is given by
Section 2.1
P =
p(x) dx.
(2.242)
R
Now suppose that we have collected a data set comprising N observations drawn
from p(x). Because each data point has a probability P of falling within R, the total
number K of points that lie inside R will be distributed according to the binomial
distribution
(2.243)
N!
Bin(K|N, P ) =
K!(N − K)! P K(1 − P )1−K.
Using (2.11), we see that the mean fraction of points falling inside the region is
E[K/N] = P , and similarly using (2.12) we see that the variance around this mean
is var[K/N] = P (1 − P )/N. For large N, this distribution will be sharply peaked
around the mean and so
(2.244)
If, however, we also assume that the region R is sufficiently small that the probability
density p(x) is roughly constant over the region, then we have
K  N P.
P  p(x)V
(2.245)
where V is the volume of R. Combining (2.244) and (2.245), we obtain our density
estimate in the form
(2.246)
p(x) = K
N V
Note that the validity of (2.246) depends on two contradictory assumptions, namely
that the region R be sufficiently small that the density is approximately constant over
the region and yet sufficiently large (in relation to the value of that density) that the
number K of points falling inside the region is sufficient for the binomial distribution
to be sharply peaked.
n=1
N
N
n=1
N
2.5. Nonparametric Methods
123
We can exploit the result (2.246) in two different ways. Either we can fix K and
determine the value of V from the data, which gives rise to the K-nearest-neighbour
technique discussed shortly, or we can fix V and determine K from the data, giv-
ing rise to the kernel approach. It can be shown that both the K-nearest-neighbour
density estimator and the kernel density estimator converge to the true probability
density in the limit N → ∞ provided V shrinks suitably with N, and K grows with
N (Duda and Hart, 1973).
We begin by discussing the kernel method in detail, and to start with we take
the region R to be a small hypercube centred on the point x at which we wish to
determine the probability density. In order to count the number K of points falling
within this region, it is convenient to define the following function
k(u) =
|ui| � 1/2,
1,
0, otherwise
i = 1, . . . , D,
(2.247)
which represents a unit cube centred on the origin. The function k(u) is an example
of a kernel function, and in this context is also called a Parzen window. From (2.247),
the quantity k((x − xn)/h) will be one if the data point xn lies inside a cube of side
h centred on x, and zero otherwise. The total number of data points lying inside this
cube will therefore be
Substituting this expression into (2.246) then gives the following result for the esti-
mated density at x
K =
k
x − xn
h
p(x) =
1
N
1
hD k
x − xn
h
(2.248)
(2.249)
where we have used V = hD for the volume of a hypercube of side h in D di-
mensions. Using the symmetry of the function k(u), we can now re-interpret this
equation, not as a single cube centred on x but as the sum over N cubes centred on
the N data points xn.
As it stands, the kernel density estimator (2.249) will suffer from one of the same
problems that the histogram method suffered from, namely the presence of artificial
discontinuities, in this case at the boundaries of the cubes. We can obtain a smoother
density model if we choose a smoother kernel function, and a common choice is the
Gaussian, which gives rise to the following kernel density model
p(x) =
1
N
1
exp
−x − xn2
2h2
(2.250)
(2πh2)1/2
n=1
where h represents the standard deviation of the Gaussian components. Thus our
density model is obtained by placing a Gaussian over each data point and then adding
up the contributions over the whole data set, and then dividing by N so that the den-
sity is correctly normalized. In Figure 2.25, we apply the model (2.250) to the data
124
2. PROBABILITY DISTRIBUTIONS
Figure 2.25 Illustration of
the kernel density model
(2.250) applied to the same data set used
to demonstrate the histogram approach in
Figure 2.24. We see that h acts as a
smoothing parameter and that if it is set
too small (top panel), the result is a very
noisy density model, whereas if it is set
too large (bottom panel), then the bimodal
nature of the underlying distribution from
which the data is generated (shown by the
green curve) is washed out. The best den-
sity model is obtained for some intermedi-
ate value of h (middle panel).
h = 0.005
h = 0.07
h = 0.2
5
0
5
0
0
5
0
0
0
0.5
0.5
0.5
1
1
1
set used earlier to demonstrate the histogram technique. We see that, as expected,
the parameter h plays the role of a smoothing parameter, and there is a trade-off
between sensitivity to noise at small h and over-smoothing at large h. Again, the
optimization of h is a problem in model complexity, analogous to the choice of bin
width in histogram density estimation, or the degree of the polynomial used in curve
fitting.
We can choose any other kernel function k(u) in (2.249) subject to the condi-
tions
k(u) � 0,
k(u) du = 1
(2.251)
(2.252)
which ensure that the resulting probability distribution is nonnegative everywhere
and integrates to one. The class of density model given by (2.249) is called a kernel
density estimator, or Parzen estimator. It has a great merit that there is no compu-
tation involved in the ‘training’ phase because this simply requires storage of the
training set. However, this is also one of its great weaknesses because the computa-
tional cost of evaluating the density grows linearly with the size of the data set.
