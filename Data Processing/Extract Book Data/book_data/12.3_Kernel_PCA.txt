In Chapter 6, we saw how the technique of kernel substitution allows us to take an
algorithm expressed in terms of scalar products of the form xTx(cid:4)
and generalize
that algorithm by replacing the scalar products with a nonlinear kernel. Here we
apply this technique of kernel substitution to principal component analysis, thereby
obtaining a nonlinear generalization called kernel PCA (Sch¨olkopf et al., 1998).
Consider a data set {xn} of observations, where n = 1, . . . , N, in a space of
(cid:5)
dimensionality D. In order to keep the notation uncluttered, we shall assume that
we have already subtracted the sample mean from each of the vectors xn, so that
n xn = 0. The ﬁrst step is to express conventional PCA in such a form that the
data vectors {xn} appear only in the form of the scalar products xT
nxm. Recall that
the principal components are deﬁned by the eigenvectors ui of the covariance matrix
where i = 1, . . . , D. Here the D × D sample covariance matrix S is deﬁned by
Sui = λiui
(12.71)
N(cid:2)
n=1
S =
1
N
xnxT
n,
(12.72)
and the eigenvectors are normalized such that uT
i ui = 1.
Now consider a nonlinear transformation φ(x) into an M-dimensional feature
space, so that each data point xn is thereby projected onto a point φ(xn). We can
x2
12.3. Kernel PCA
587
φ1
v1
x1
φ2
Figure 12.16 Schematic illustration of kernel PCA. A data set in the original data space (left-hand plot) is
projected by a nonlinear transformation φ(x) into a feature space (right-hand plot). By performing PCA in the
feature space, we obtain the principal components, of which the ﬁrst is shown in blue and is denoted by the
vector v1. The green lines in feature space indicate the linear projections onto the ﬁrst principal component,
which correspond to nonlinear projections in the original data space. Note that in general it is not possible to
represent the nonlinear principal component by a vector in x space.
(cid:5)
now perform standard PCA in the feature space, which implicitly deﬁnes a nonlinear
principal component model in the original data space, as illustrated in Figure 12.16.
For the moment, let us assume that the projected data set also has zero mean,
n φ(xn) = 0. We shall return to this point shortly. The M × M sample
so that
covariance matrix in feature space is given by
φ(xn)φ(xn)T
(12.73)
N(cid:2)
n=1
C =
1
N
and its eigenvector expansion is deﬁned by
(12.74)
i = 1, . . . , M. Our goal is to solve this eigenvalue problem without having to work
explicitly in the feature space. From the deﬁnition of C, the eigenvector equations
tells us that vi satisﬁes
Cvi = λivi
φ(xn)
φ(xn)Tvi
= λivi
(12.75)
and so we see that (provided λi > 0) the vector vi is given by a linear combination
of the φ(xn) and so can be written in the form
N(cid:2)
n=1
1
N
(cid:27)
(cid:26)
N(cid:2)
n=1
vi =
ainφ(xn).
(12.76)
588
12. CONTINUOUS LATENT VARIABLES
Exercise 12.26
Substituting this expansion back into the eigenvector equation, we obtain
φ(xn)φ(xn)T
aimφ(xm) = λi
ainφ(xn).
(12.77)
n=1
m=1
n=1
The key step is now to express this in terms of the kernel function k(xn, xm) =
φ(xn)Tφ(xm), which we do by multiplying both sides by φ(xl)T to give
N(cid:2)
N(cid:2)
1
N
1
N
N(cid:2)
m(cid:2)
N(cid:2)
N(cid:2)
k(xl, xn)
aimk(xn, xm) = λi
aink(xl, xn).
(12.78)
n=1
m=1
n=1
This can be written in matrix notation as
K2ai = λiNKai
(12.79)
where ai is an N-dimensional column vector with elements ani for n = 1, . . . , N.
We can ﬁnd solutions for ai by solving the following eigenvalue problem
Kai = λiNai
(12.80)
in which we have removed a factor of K from both sides of (12.79). Note that
the solutions of (12.79) and (12.80) differ only by eigenvectors of K having zero
eigenvalues that do not affect the principal components projection.
The normalization condition for the coefﬁcients ai is obtained by requiring that
the eigenvectors in feature space be normalized. Using (12.76) and (12.80), we have
1 = vT
i vi =
ainaimφ(xn)Tφ(xm) = aT
i Kai = λiNaT
i ai.
(12.81)
N(cid:2)
N(cid:2)
n=1
m=1
Having solved the eigenvector problem, the resulting principal component pro-
jections can then also be cast in terms of the kernel function so that, using (12.76),
the projection of a point x onto eigenvector i is given by
N(cid:2)
N(cid:2)
yi(x) = φ(x)Tvi =
ainφ(x)Tφ(xn) =
aink(x, xn)
(12.82)
n=1
n=1
and so again is expressed in terms of the kernel function.
In the original D-dimensional x space there are D orthogonal eigenvectors and
hence we can ﬁnd at most D linear principal components. The dimensionality M
of the feature space, however, can be much larger than D (even inﬁnite), and thus
we can ﬁnd a number of nonlinear principal components that can exceed D. Note,
however, that the number of nonzero eigenvalues cannot exceed the number N of
data points, because (even if M > N) the covariance matrix in feature space has
rank at most equal to N. This is reﬂected in the fact that kernel PCA involves the
eigenvector expansion of the N × N matrix K.
12.3. Kernel PCA
589
So far we have assumed that the projected data set given by φ(xn) has zero
mean, which in general will not be the case. We cannot simply compute and then
subtract off the mean, since we wish to avoid working directly in feature space, and
so again, we formulate the algorithm purely in terms of the kernel function. The
projected data points after centralizing, denoted(cid:4)φ(xn), are given by
φ(xl)
(12.83)
and the corresponding elements of the Gram matrix are given by
(cid:4)φ(xn) = φ(xn) − 1
N(cid:2)
N
l=1
(cid:4)Knm = (cid:4)φ(xn)T(cid:4)φ(xm)
= φ(xn)Tφ(xm) − 1
N
− 1
N
N(cid:2)
l=1
N(cid:2)
l=1
= k(xn, xm) − 1
N
N(cid:2)
l=1
φ(xn)Tφ(xl)
N(cid:2)
N(cid:2)
1
N 2
j=1
l=1
N(cid:2)
l=1
k(xl, xm)
N(cid:2)
N(cid:2)
φ(xl)Tφ(xm) +
φ(xj)Tφ(xl)
This can be expressed in matrix notation as
− 1
N
k(xn, xl) +
1
N 2
j=1
l=1
(cid:4)K = K − 1N K − K1N + 1N K1N
k(xj, xl).
(12.84)
(12.85)
where 1N denotes the N × N matrix in which every element takes the value 1/N.
Thus we can evaluate (cid:4)K using only the kernel function and then use (cid:4)K to determine
Exercise 12.27
the eigenvalues and eigenvectors. Note that the standard PCA algorithm is recovered
as a special case if we use a linear kernel k(x, x(cid:4)) = xTx(cid:4)
. Figure 12.17 shows an
example of kernel PCA applied to a synthetic data set (Sch¨olkopf et al., 1998). Here
a ‘Gaussian’ kernel of the form
k(x, x(cid:4)) = exp(−(cid:5)x − x(cid:4)(cid:5)2/0.1)
(12.86)
is applied to a synthetic data set. The lines correspond to contours along which the
projection onto the corresponding principal component, deﬁned by
N(cid:2)
φ(x)Tvi =
aink(x, xn)
(12.87)
is constant.
n=1
590
12. CONTINUOUS LATENT VARIABLES
Figure 12.17 Example of kernel PCA, with a Gaussian kernel applied to a synthetic data set in two dimensions,
showing the ﬁrst eight eigenfunctions along with their eigenvalues. The contours are lines along which the
projection onto the corresponding principal component is constant. Note how the ﬁrst two eigenvectors separate
the three clusters, the next three eigenvectors split each of the cluster into halves, and the following three
eigenvectors again split the clusters into halves along directions orthogonal to the previous splits.
One obvious disadvantage of kernel PCA is that it involves ﬁnding the eigenvec-
tors of the N × N matrix (cid:4)K rather than the D × D matrix S of conventional linear
(cid:1)xn onto the L-dimensional principal subspace, deﬁned by
Finally, we note that in standard linear PCA, we often retain some reduced num-
ber L < D of eigenvectors and then approximate a data vector xn by its projection
PCA, and so in practice for large data sets approximations are often used.
xT
nui
ui.
(12.88)
i=1
In kernel PCA, this will in general not be possible. To see this, note that the map-
ping φ(x) maps the D-dimensional x space into a D-dimensional manifold in the
M-dimensional feature space φ. The vector x is known as the pre-image of the
corresponding point φ(x). However, the projection of points in feature space onto
the linear PCA subspace in that space will typically not lie on the nonlinear D-
dimensional manifold and so will not have a corresponding pre-image in data space.
Techniques have therefore been proposed for ﬁnding approximate pre-images (Bakir
et al., 2004).
L(cid:2)
(cid:10)
(cid:1)xn =
(cid:11)
