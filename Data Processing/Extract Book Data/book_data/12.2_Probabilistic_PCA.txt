The formulation of PCA discussed in the previous section was based on a linear
projection of the data onto a subspace of lower dimensionality than the original data
space. We now show that PCA can also be expressed as the maximum likelihood
solution of a probabilistic latent variable model. This reformulation of PCA, known
as probabilistic PCA, brings several advantages compared with conventional PCA:
• Probabilistic PCA represents a constrained form of the Gaussian distribution
in which the number of free parameters can be restricted while still allowing
the model to capture the dominant correlations in a data set.
Section 12.2.2
Section 12.2.3
Section 8.1.4
Section 8.2.2
12.2. Probabilistic PCA
571
• We can derive an EM algorithm for PCA that is computationally efﬁcient in
situations where only a few leading eigenvectors are required and that avoids
having to evaluate the data covariance matrix as an intermediate step.
• The combination of a probabilistic model and EM allows us to deal with miss-
ing values in the data set.
• Mixtures of probabilistic PCA models can be formulated in a principled way
and trained using the EM algorithm.
• Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which
the dimensionality of the principal subspace can be found automatically from
the data.
• The existence of a likelihood function allows direct comparison with other
probabilistic density models. By contrast, conventional PCA will assign a low
reconstruction cost to data points that are close to the principal subspace even
if they lie arbitrarily far from the training data.
• Probabilistic PCA can be used to model class-conditional densities and hence
be applied to classiﬁcation problems.
• The probabilistic PCA model can be run generatively to provide samples from
the distribution.
This formulation of PCA as a probabilistic model was proposed independently by
Tipping and Bishop (1997, 1999b) and by Roweis (1998). As we shall see later, it is
closely related to factor analysis (Basilevsky, 1994).
Probabilistic PCA is a simple example of the linear-Gaussian framework, in
which all of the marginal and conditional distributions are Gaussian. We can formu-
late probabilistic PCA by ﬁrst introducing an explicit latent variable z corresponding
to the principal-component subspace. Next we deﬁne a Gaussian prior distribution
p(z) over the latent variable, together with a Gaussian conditional distribution p(x|z)
for the observed variable x conditioned on the value of the latent variable. Speciﬁ-
cally, the prior distribution over z is given by a zero-mean unit-covariance Gaussian
p(z) = N (z|0, I).
(12.31)
Similarly, the conditional distribution of the observed variable x, conditioned on the
value of the latent variable z, is again Gaussian, of the form
p(x|z) = N (x|Wz + µ, σ2I)
(12.32)
in which the mean of x is a general linear function of z governed by the D × M
matrix W and the D-dimensional vector µ. Note that this factorizes with respect to
the elements of x, in other words this is an example of the naive Bayes model. As
we shall see shortly, the columns of W span a linear subspace within the data space
that corresponds to the principal subspace. The other parameter in this model is the
scalar σ2 governing the variance of the conditional distribution. Note that there is no
572
12. CONTINUOUS LATENT VARIABLES
x2
w
x2
p(x|ˆz)
ˆz|w|
p(z)
p(x)
ˆz
z
x1
x1
Figure 12.9 An illustration of the generative view of the probabilistic PCA model for a two-dimensional data
space and a one-dimensional latent space. An observed data point x is generated by ﬁrst drawing a value bz
for the latent variable from its prior distribution p(z) and then drawing a value for x from an isotropic Gaussian
distribution (illustrated by the red circles) having mean wbz + µ and covariance σ2I. The green ellipses show the
density contours for the marginal distribution p(x).
Exercise 12.4
loss of generality in assuming a zero mean, unit covariance Gaussian for the latent
distribution p(z) because a more general Gaussian distribution would give rise to an
equivalent probabilistic model.
We can view the probabilistic PCA model from a generative viewpoint in which
a sampled value of the observed variable is obtained by ﬁrst choosing a value for
the latent variable and then sampling the observed variable conditioned on this la-
tent value. Speciﬁcally, the D-dimensional observed variable x is deﬁned by a lin-
ear transformation of the M-dimensional latent variable z plus additive Gaussian
‘noise’, so that
x = Wz + µ + 
(12.33)
where z is an M-dimensional Gaussian latent variable, and  is a D-dimensional
zero-mean Gaussian-distributed noise variable with covariance σ2I. This generative
process is illustrated in Figure 12.9. Note that this framework is based on a mapping
from latent space to data space, in contrast to the more conventional view of PCA
discussed above. The reverse mapping, from data space to the latent space, will be
obtained shortly using Bayes’ theorem.
Suppose we wish to determine the values of the parameters W, µ and σ2 using
maximum likelihood. To write down the likelihood function, we need an expression
for the marginal distribution p(x) of the observed variable. This is expressed, from
the sum and product rules of probability, in the form
(cid:6)
p(x) =
p(x|z)p(z) dz.
(12.34)
Exercise 12.7
Because this corresponds to a linear-Gaussian model, this marginal distribution is
again Gaussian, and is given by
p(x) = N (x|µ, C)
(12.35)
12.2. Probabilistic PCA
573
(cid:9)
(12.37)
where the D × D covariance matrix C is deﬁned by
C = WWT + σ2I.
(12.36)
This result can also be derived more directly by noting that the predictive distribution
will be Gaussian and then evaluating its mean and covariance using (12.33). This
gives
(cid:8)
(cid:8)
E[x] = E[Wz + µ + ] = µ
cov[x] = E
= E
(Wz + )(Wz + )T
WzzTWT
(cid:9)
(12.38)
where we have used the fact that z and  are independent random variables and hence
are uncorrelated.
+ E[T] = WWT + σ2I
Intuitively, we can think of the distribution p(x) as being deﬁned by taking an
isotropic Gaussian ‘spray can’ and moving it across the principal subspace spraying
Gaussian ink with density determined by σ2 and weighted by the prior distribution.
The accumulated ink density gives rise to a ‘pancake’ shaped distribution represent-
ing the marginal density p(x).
an orthogonal matrix. Using the orthogonality property RRT = I, we see that the
The predictive distribution p(x) is governed by the parameters µ, W, and σ2.
However, there is redundancy in this parameterization corresponding to rotations of
the latent space coordinates. To see this, consider a matrix,W = WR where R is
quantity,W,WT that appears in the covariance matrix C takes the form
and hence is independent of R. Thus there is a whole family of matrices,W all of
,W,WT = WRRTWT = WWT
(12.39)
which give rise to the same predictive distribution. This invariance can be understood
in terms of rotations within the latent space. We shall return to a discussion of the
number of independent parameters in this model later.
When we evaluate the predictive distribution, we require C−1, which involves
the inversion of a D× D matrix. The computation required to do this can be reduced
by making use of the matrix inversion identity (C.7) to give
−2WM−1WT
−1I − σ
where the M × M matrix M is deﬁned by
C−1 = σ
(12.40)
Exercise 12.8
M = WTW + σ2I.
(12.41)
Because we invert M rather than inverting C directly, the cost of evaluating C−1 is
reduced from O(D3) to O(M 3).
As well as the predictive distribution p(x), we will also require the posterior
distribution p(z|x), which can again be written down directly using the result (2.116)
for linear-Gaussian models to give
(12.42)
Note that the posterior mean depends on x, whereas the posterior covariance is in-
dependent of x.
z|M−1WT(x − µ), σ
−2M
p(z|x) = N(cid:10)
(cid:11)
574
12. CONTINUOUS LATENT VARIABLES
Figure 12.10 The probabilistic PCA model for a data set of N obser-
vations of x can be expressed as a directed graph in
which each observation xn is associated with a value
zn of the latent variable.
σ2
zn
W
xn
N
