We have seen how the derivatives of an error function with respect to the weights
can be obtained by the propagation of errors backwards through the network. The
technique of backpropagation can also be applied to the calculation of other deriva-
tives. Here we consider the evaluation of the Jacobian matrix, whose elements are
given by the derivatives of the network outputs with respect to the inputs
Jki ≡
∂yk
∂xi
(5.70)
where each such derivative is evaluated with all other inputs held fixed. Jacobian
matrices play a useful role in systems built from a number of distinct modules, as
illustrated in Figure 5.8. Each module can comprise a fixed or adaptive function,
which can be linear or nonlinear, so long as it is differentiable. Suppose we wish
to minimize an error function E with respect to the parameter w in Figure 5.8. The
derivative of the error function is given by
∂E
∂w
k,j
∂E
∂yk
∂yk
∂zj
∂zj
∂w
(5.71)
in which the Jacobian matrix for the red module in Figure 5.8 appears in the middle
term.
Because the Jacobian matrix provides a measure of the local sensitivity of the
outputs to changes in each of the input variables, it also allows any known errors ∆xi
i
j
j
associated with the inputs to be propagated through the trained network in order to
estimate their contribution ∆yk to the errors at the outputs, through the relation
∆yk 
∂yk
∂xi
∆xi
(5.72)
which is valid provided the |∆xi| are small. In general, the network mapping rep-
resented by a trained neural network will be nonlinear, and so the elements of the
Jacobian matrix will not be constants but will depend on the particular input vector
used. Thus (5.72) is valid only for small perturbations of the inputs, and the Jacobian
itself must be re-evaluated for each new input vector.
The Jacobian matrix can be evaluated using a backpropagation procedure that is
similar to the one derived earlier for evaluating the derivatives of an error function
with respect to the weights. We start by writing the element Jki in the form
Jki = ∂yk
∂xi
∂yk
∂aj
∂aj
∂xi
wji
∂yk
∂aj
(5.73)
where we have made use of (5.48). The sum in (5.73) runs over all units j to which
the input unit i sends connections (for example, over all units in the first hidden
layer in the layered topology considered earlier). We now write down a recursive
backpropagation formula to determine the derivatives ∂yk/∂aj
248
5. NEURAL NETWORKS
∂yk
∂aj
l
∂yk
∂al
∂al
∂aj
= h(aj)
wlj
∂yk
∂al
l
(5.74)
where the sum runs over all units l to which unit j sends connections (corresponding
to the first index of wlj). Again, we have made use of (5.48) and (5.49). This
backpropagation starts at the output units for which the required derivatives can be
found directly from the functional form of the output-unit activation function. For
instance, if we have individual sigmoidal activation functions at each output unit,
then
∂yk
∂aj
whereas for softmax outputs we have
= δkjσ(aj)
(5.75)
(5.76)
∂yk
∂aj
= δkjyk − ykyj.
We can summarize the procedure for evaluating the Jacobian matrix as follows.
Apply the input vector corresponding to the point in input space at which the Ja-
cobian matrix is to be found, and forward propagate in the usual way to obtain the
