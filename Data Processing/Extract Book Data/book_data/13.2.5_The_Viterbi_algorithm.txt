In many applications of hidden Markov models, the latent variables have some
meaningful interpretation, and so it is often of interest to find the most probable
sequence of hidden states for a given observation sequence. For instance in speech
recognition, we might wish to find the most probable phoneme sequence for a given
series of acoustic observations. Because the graph for the hidden Markov model is
a directed tree, this problem can be solved exactly using the max-sum algorithm.
We recall from our discussion in Section 8.4.5 that the problem of finding the most
probable sequence of latent states is not the same as that of finding the set of states
that are individually the most probable. The latter problem can be solved by first
running the forward-backward (sum-product) algorithm to find the latent variable
marginals γ(zn) and then maximizing each of these individually (Duda et al., 2001).
However, the set of such states will not, in general, correspond to the most probable
sequence of states. In fact, this set of states might even represent a sequence having
zero probability, if it so happens that two successive states, which in isolation are
individually the most probable, are such that the transition matrix element connecting
them is zero.
In practice, we are usually interested in finding the most probable sequence of
states, and this can be solved efficiently using the max-sum algorithm, which in the
context of hidden Markov models is known as the Viterbi algorithm (Viterbi, 1967).
Note that the max-sum algorithm works with log probabilities and so there is no
need to use re-scaled variables as was done with the forward-backward algorithm.
Figure 13.16 shows a fragment of the hidden Markov model expanded as lattice
diagram. As we have already noted, the number of possible paths through the lattice
grows exponentially with the length of the chain. The Viterbi algorithm searches this
space of paths efficiently to find the most probable path with a computational cost
that grows only linearly with the length of the chain.
As with the sum-product algorithm, we first represent the hidden Markov model
as a factor graph, as shown in Figure 13.15. Again, we treat the variable node zN
as the root, and pass messages to the root starting with the leaf nodes. Using the
results (8.93) and (8.94), we see that the messages passed in the max-sum algorithm
are given by
µzn→fn+1(zn) = µfn→zn(zn)
µfn+1→zn+1(zn+1) = max
zn
ln fn+1(zn, zn+1) + µzn→fn+1(zn)
(13.66)
. (13.67)
630
13. SEQUENTIAL DATA
Figure 13.16 A fragment of
the HMM lattice
showing two possible paths. The Viterbi algorithm
efficiently determines the most probable path from
amongst the exponentially many possibilities. For
any given path, the corresponding probability is
given by the product of the elements of the tran-
sition matrix Ajk, corresponding to the probabil-
ities p(zn+1|zn) for each segment of the path,
along with the emission densities p(xn|k) asso-
ciated with each node on the path.
k = 1
k = 2
k = 3
n − 2
n − 1
n
n + 1
If we eliminate µzn→fn+1(zn) between these two equations, and make use of (13.46),
we obtain a recursion for the f → z messages of the form
ω(zn+1) = ln p(xn+1|zn+1) + max
zn {ln p(x+1|zn) + ω(zn)}
(13.68)
where we have introduced the notation ω(zn) ≡ µfn→zn(zn).
From (8.95) and (8.96), these messages are initialized using
ω(z1) = ln p(z1) + ln p(x1|z1).
(13.69)
where we have used (13.45). Note that to keep the notation uncluttered, we omit
the dependence on the model parameters θ that are held fixed when finding the most
probable sequence.
Exercise 13.16
The Viterbi algorithm can also be derived directly from the definition (13.6) of
the joint distribution by taking the logarithm and then exchanging maximizations
and summations. It is easily seen that the quantities ω(zn) have the probabilistic
interpretation
ω(zn) = max
p(x1, . . . , xn, z1, . . . , zn).
(13.70)
z1,...,zn−1
Once we have completed the final maximization over zN , we will obtain the
value of the joint distribution p(X, Z) corresponding to the most probable path. We
also wish to find the sequence of latent variable values that corresponds to this path.
To do this, we simply make use of the back-tracking procedure discussed in Sec-
tion 8.4.5. Specifically, we note that the maximization over zn must be performed
for each of the K possible values of zn+1. Suppose we keep a record of the values
of zn that correspond to the maxima for each value of the K values of zn+1. Let us
denote this function by ψ(kn) where k ∈ {1, . . . , K}. Once we have passed mes-
sages to the end of the chain and found the most probable state of zN , we can then
use this function to backtrack along the chain by applying it recursively
n = ψ(kmax
kmax
n+1).
(13.71)
r=1
R
R
13.2. Hidden Markov Models
631
Intuitively, we can understand the Viterbi algorithm as follows. Naively, we
could consider explicitly all of the exponentially many paths through the lattice,
evaluate the probability for each, and then select the path having the highest proba-
bility. However, we notice that we can make a dramatic saving in computational cost
as follows. Suppose that for each path we evaluate its probability by summing up
products of transition and emission probabilities as we work our way forward along
each path through the lattice. Consider a particular time step n and a particular state
k at that time step. There will be many possible paths converging on the correspond-
ing node in the lattice diagram. However, we need only retain that particular path
that so far has the highest probability. Because there are K states at time step n, we
need to keep track of K such paths. At time step n + 1, there will be K 2 possible
paths to consider, comprising K possible paths leading out of each of the K current
states, but again we need only retain K of these corresponding to the best path for
each state at time n+1. When we reach the final time step N we will discover which
state corresponds to the overall most probable path. Because there is a unique path
coming into that state we can trace the path back to step N − 1 to see what state it
occupied at that time, and so on back through the lattice to the state n = 1.
