As we have seen, the probabilistic PCA model can be expressed in terms of a
marginalization over a continuous latent space z in which for each data point xn,
there is a corresponding latent variable zn. We can therefore make use of the EM
algorithm to ﬁnd maximum likelihood estimates of the model parameters. This may
seem rather pointless because we have already obtained an exact closed-form so-
lution for the maximum likelihood parameter values. However, in spaces of high
dimensionality, there may be computational advantages in using an iterative EM
procedure rather than working directly with the sample covariance matrix. This EM
procedure can also be extended to the factor analysis model, for which there is no
closed-form solution. Finally, it allows missing data to be handled in a principled
way.
We can derive the EM algorithm for probabilistic PCA by following the general
framework for EM. Thus we write down the complete-data log likelihood and take
its expectation with respect to the posterior distribution of the latent distribution
evaluated using ‘old’ parameter values. Maximization of this expected complete-
data log likelihood then yields the ‘new’ parameter values. Because the data points
Exercise 12.14
Section 12.2.4
Section 9.4
578
12. CONTINUOUS LATENT VARIABLES
are assumed independent, the complete-data log likelihood function takes the form
ln p
X, Z|µ, W, σ2
{ln p(xn|zn) + ln p(zn)}
(12.52)
(cid:10)
(cid:11)
N(cid:2)
n=1
where the nth row of the matrix Z is given by zn. We already know that the exact
maximum likelihood solution for µ is given by the sample mean x deﬁned by (12.1),
and it is convenient to substitute for µ at this stage. Making use of the expressions
(12.31) and (12.32) for the latent and conditional distributions, respectively, and tak-
ing the expectation with respect to the posterior distribution over the latent variables,
we obtain
(cid:10)
(cid:11)
D
2
ln(2πσ2) +
1
2 Tr
E[znzT
n]
E[ln p
X, Z|µ, W, σ2
(cid:10)
(cid:10)
1
2σ2
1
2σ2 Tr
(cid:12)
(cid:11)
] = − N(cid:2)
(cid:11)(cid:13)
n=1
(cid:5)xn − µ(cid:5)2 − 1
σ2
E[zn]TWT(xn − µ)
E[znzT
n]WTW
(12.53)
Exercise 12.15
Note that this depends on the posterior distribution only through the sufﬁcient statis-
tics of the Gaussian. Thus in the E step, we use the old parameter values to evaluate
E[zn] = M−1WT(xn − x)
n] = σ2M−1 + E[zn]E[zn]T
E[znzT
(12.54)
(12.55)
which follow directly from the posterior distribution (12.42) together with the stan-
dard result E[znzT
n] = cov[zn] + E[zn]E[zn]T. Here M is deﬁned by (12.41).
In the M step, we maximize with respect to W and σ2, keeping the posterior
statistics ﬁxed. Maximization with respect to σ2 is straightforward. For the maxi-
mization with respect to W we make use of (C.24), and obtain the M-step equations
(xn − x)E[zn]T
(cid:31)
N(cid:2)
−1
(cid:26)(cid:5)xn − x(cid:5)2 − 2E[zn]TWT
E[znzT
n]
n=1
(cid:11)(cid:27)
(cid:31)
N(cid:2)
n=1
1
N D
+Tr
N(cid:2)
(cid:10)
n=1
E[znzT
new(xn − x)
n]WT
newWnew
(12.57)
(12.56)
Wnew =
σ2
new =
The EM algorithm for probabilistic PCA proceeds by initializing the parameters
and then alternately computing the sufﬁcient statistics of the latent space posterior
distribution using (12.54) and (12.55) in the E step and revising the parameter values
using (12.56) and (12.57) in the M step.
One of the beneﬁts of the EM algorithm for PCA is computational efﬁciency
for large-scale applications (Roweis, 1998). Unlike conventional PCA based on an
12.2. Probabilistic PCA
579
eigenvector decomposition of the sample covariance matrix, the EM approach is
iterative and so might appear to be less attractive. However, each cycle of the EM
algorithm can be computationally much more efﬁcient than conventional PCA in
spaces of high dimensionality. To see this, we note that the eigendecomposition of
the covariance matrix requires O(D3) computation. Often we are interested only
in the ﬁrst M eigenvectors and their corresponding eigenvalues, in which case we
can use algorithms that are O(M D2). However, the evaluation of the covariance
matrix itself takes O(N D2) computations, where N is the number of data points.
Algorithms such as the snapshot method (Sirovich, 1987), which assume that the
eigenvectors are linear combinations of the data vectors, avoid direct evaluation of
the covariance matrix but are O(N 3) and hence unsuited to large data sets. The EM
algorithm described here also does not construct the covariance matrix explicitly.
Instead, the most computationally demanding steps are those involving sums over
the data set that are O(N DM). For large D, and M (cid:13) D, this can be a signiﬁcant
saving compared to O(N D2) and can offset the iterative nature of the EM algorithm.
Note that this EM algorithm can be implemented in an on-line form in which
each D-dimensional data point is read in and processed and then discarded before
the next data point is considered. To see this, note that the quantities evaluated in
the E step (an M-dimensional vector and an M × M matrix) can be computed for
each data point separately, and in the M step we need to accumulate sums over data
points, which we can do incrementally. This approach can be advantageous if both
N and D are large.
Because we now have a fully probabilistic model for PCA, we can deal with
missing data, provided that it is missing at random, by marginalizing over the dis-
tribution of the unobserved variables. Again these missing values can be treated
using the EM algorithm. We give an example of the use of this approach for data
visualization in Figure 12.11.
Another elegant feature of the EM approach is that we can take the limit σ2 → 0,
corresponding to standard PCA, and still obtain a valid EM-like algorithm (Roweis,
1998). From (12.55), we see that the only quantity we need to compute in the E step
is E[zn]. Furthermore, the M step is simpliﬁed because M = WTW. To emphasize
nth row is given by the vector xn − x and similarly deﬁne Ω to be a matrix of size
D × M whose nth row is given by the vector E[zn]. The E step (12.54) of the EM
algorithm for PCA then becomes
the simplicity of the algorithm, let us deﬁne (cid:4)X to be a matrix of size N × D whose
and the M step (12.56) takes the form
old
Ω = (WT
oldWold)−1WT
(cid:4)X
Wnew = (cid:4)XTΩT(ΩΩT)−1.
(12.58)
(12.59)
Again these can be implemented in an on-line form. These equations have a simple
interpretation as follows. From our earlier discussion, we see that the E step involves
an orthogonal projection of the data points onto the current estimate for the principal
subspace. Correspondingly, the M step represents a re-estimation of the principal
580
12. CONTINUOUS LATENT VARIABLES
Figure 12.11 Probabilistic PCA visualization of a portion of the oil ﬂow data set for the ﬁrst 100 data points. The
left-hand plot shows the posterior mean projections of the data points on the principal subspace. The right-hand
plot is obtained by ﬁrst randomly omitting 30% of the variable values and then using EM to handle the missing
values. Note that each data point then has at least one missing measurement but that the plot is very similar to
the one obtained without missing values.
Exercise 12.17
subspace to minimize the squared reconstruction error in which the projections are
ﬁxed.
We can give a simple physical analogy for this EM algorithm, which is easily
visualized for D = 2 and M = 1. Consider a collection of data points in two
dimensions, and let the one-dimensional principal subspace be represented by a solid
rod. Now attach each data point to the rod via a spring obeying Hooke’s law (stored
energy is proportional to the square of the spring’s length). In the E step, we keep
the rod ﬁxed and allow the attachment points to slide up and down the rod so as to
minimize the energy. This causes each attachment point (independently) to position
itself at the orthogonal projection of the corresponding data point onto the rod. In
the M step, we keep the attachment points ﬁxed and then release the rod and allow it
to move to the minimum energy position. The E and M steps are then repeated until
a suitable convergence criterion is satisﬁed, as is illustrated in Figure 12.12.
