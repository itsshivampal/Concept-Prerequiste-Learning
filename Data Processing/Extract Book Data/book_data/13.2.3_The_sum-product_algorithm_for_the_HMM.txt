The directed graph that represents the hidden Markov model, shown in Fig-
ure 13.5, is a tree and so we can solve the problem of finding local marginals for the
hidden variables using the sum-product algorithm. Not surprisingly, this turns out to
be equivalent to the forward-backward algorithm considered in the previous section,
and so the sum-product algorithm therefore provides us with a simple way to derive
the alpha-beta recursion formulae.
We begin by transforming the directed graph of Figure 13.5 into a factor graph,
of which a representative fragment is shown in Figure 13.14. This form of the fac-
tor graph shows all variables, both latent and observed, explicitly. However, for
the purpose of solving the inference problem, we shall always be conditioning on
the variables x1, . . . , xN , and so we can simplify the factor graph by absorbing the
emission probabilities into the transition probability factors. This leads to the sim-
plified factor graph representation in Figure 13.15, in which the factors are given
by
h(z1) = p(z1)p(x1|z1)
fn(zn−1, zn) = p(zn|zn−1)p(xn|zn).
(13.45)
(13.46)
z1
zn−1
626
13. SEQUENTIAL DATA
Figure 13.15 A simplified form of fac-
tor graph to describe the hidden Markov
model.
h
fn
zn−1
zn
To derive the alpha-beta algorithm, we denote the final hidden variable zN as
the root node, and first pass messages from the leaf node h to the root. From the
general results (8.66) and (8.69) for message propagation, we see that the messages
which are propagated in the hidden Markov model take the form
µzn−1→fn(zn−1) = µfn−1→zn−1(zn−1)
µfn→zn(zn) =
fn(zn−1, zn)µzn−1→fn(zn−1)
zn−1
(13.47)
(13.48)
These equations represent the propagation of messages forward along the chain and
are equivalent to the alpha recursions derived in the previous section, as we shall
now show. Note that because the variable nodes zn have only two neighbours, they
perform no computation.
We can eliminate µzn−1→fn(zn−1) from (13.48) using (13.47) to give a recur-
sion for the f → z messages of the form
µfn→zn(zn) =
fn(zn−1, zn)µfn−1→zn−1(zn−1).
(13.49)
If we now recall the definition (13.46), and if we define
α(zn) = µfn→zn(zn)
(13.50)
then we obtain the alpha recursion given by (13.36). We also need to verify that
the quantities α(zn) are themselves equivalent to those defined previously. This
is easily done by using the initial condition (8.71) and noting that α(z1) is given
by h(z1) = p(z1)p(x1|z1) which is identical to (13.37). Because the initial α is
the same, and because they are iteratively computed using the same equation, all
subsequent α quantities must be the same.
Next we consider the messages that are propagated from the root node back to
the leaf node. These take the form
µfn+1→fn(zn) =
zn+1
fn+1(zn, zn+1)µfn+2→fn+1(zn+1)
(13.51)
where, as before, we have eliminated the messages of the type z → f since the
variable nodes perform no computation. Using the definition (13.46) to substitute
for fn+1(zn, zn+1), and defining
β(zn) = µfn+1→zn(zn)
(13.52)
13.2. Hidden Markov Models
627
we obtain the beta recursion given by (13.38). Again, we can verify that the beta
variables themselves are equivalent by noting that (8.70) implies that the initial mes-
sage send by the root variable node is µzN→fN (zN ) = 1, which is identical to the
initialization of β(zN ) given in Section 13.2.2.
The sum-product algorithm also specifies how to evaluate the marginals once all
the messages have been evaluated. In particular, the result (8.63) shows that the local
marginal at the node zn is given by the product of the incoming messages. Because
we have conditioned on the variables X = {x1, . . . , xN}, we are computing the
joint distribution
p(zn, X) = µfn→zn(zn)µfn+1→zn(zn) = α(zn)β(zn).
Dividing both sides by p(X), we then obtain
γ(zn) = p(zn, X)
p(X)
= α(zn)β(zn)
p(X)
(13.53)
(13.54)
Exercise 13.11
in agreement with (13.33). The result (13.43) can similarly be derived from (8.72).
