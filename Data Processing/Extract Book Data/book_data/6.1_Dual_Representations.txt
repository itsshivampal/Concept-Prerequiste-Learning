293
6.1. Dual Representations
Many linear models for regression and classification can be reformulated in terms of
a dual representation in which the kernel function arises naturally. This concept will
play an important role when we consider support vector machines in the next chapter.
Here we consider a linear regression model whose parameters are determined by
minimizing a regularized sum-of-squares error function given by
J(w) =
1
2
wTφ(xn) − tn
2 + λ
2
wTw
(6.2)
where λ � 0. If we set the gradient of J(w) with respect to w equal to zero, we see
that the solution for w takes the form of a linear combination of the vectors φ(xn),
with coefficients that are functions of w, of the form
w = −
1
wTφ(xn) − tn
φ(xn) =
anφ(xn) = ΦTa
(6.3)
where Φ is the design matrix, whose nth row is given by φ(xn)T. Here the vector
a = (a1, . . . , aN )T, and we have defined
an = −
1
wTφ(xn) − tn
(6.4)
Instead of working with the parameter vector w, we can now reformulate the least-
squares algorithm in terms of the parameter vector a, giving rise to a dual represen-
tation. If we substitute w = ΦTa into J(w), we obtain
J(a) =
1
2
aTΦΦTΦΦTa − aTΦΦTt +
1
2
tTt + λ
2
aTΦΦTa
(6.5)
where t = (t1, . . . , tN )T. We now define the Gram matrix K = ΦΦT, which is an
N × N symmetric matrix with elements
Knm = φ(xn)Tφ(xm) = k(xn, xm)
(6.6)
where we have introduced the kernel function k(x, x) defined by (6.1). In terms of
the Gram matrix, the sum-of-squares error function can be written as
J(a) =
1
2
aTKKa − aTKt +
1
2
tTt + λ
2
aTKa.
(6.7)
Setting the gradient of J(a) with respect to a to zero, we obtain the following solu-
tion
a = (K + λIN )−1 t.
(6.8)
If we substitute this back into the linear regression model, we obtain the following
prediction for a new input x
y(x) = wTφ(x) = aTΦφ(x) = k(x)T (K + λIN )−1 t
(6.9)
where we have defined the vector k(x) with elements kn(x) = k(xn, x). Thus we
see that the dual formulation allows the solution to the least-squares problem to be
expressed entirely in terms of the kernel function k(x, x). This is known as a dual
formulation because, by noting that the solution for a can be expressed as a linear
combination of the elements of φ(x), we recover the original formulation in terms of
the parameter vector w. Note that the prediction at x is given by a linear combination
of the target values from the training set. In fact, we have already obtained this result,
using a slightly different notation, in Section 3.3.3.
In the dual formulation, we determine the parameter vector a by inverting an
N × N matrix, whereas in the original parameter space formulation we had to invert
an M × M matrix in order to determine w. Because N is typically much larger
than M, the dual formulation does not seem to be particularly useful. However, the
advantage of the dual formulation, as we shall see, is that it is expressed entirely in
terms of the kernel function k(x, x). We can therefore work directly in terms of
kernels and avoid the explicit introduction of the feature vector φ(x), which allows
us implicitly to use feature spaces of high, even infinite, dimensionality.
The existence of a dual representation based on the Gram matrix is a property of
many linear models, including the perceptron. In Section 6.4, we will develop a dual-
ity between probabilistic linear models for regression and the technique of Gaussian
processes. Duality will also play an important role when we discuss support vector
machines in Chapter 7.
Exercise 6.1
Exercise 6.2
i=1
M
294
6. KERNEL METHODS
