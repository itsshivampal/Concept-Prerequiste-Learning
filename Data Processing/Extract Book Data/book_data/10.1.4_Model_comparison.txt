As well as performing inference over the hidden variables Z, we may also
wish to compare a set of candidate models, labelled by the index m, and having
prior probabilities p(m). Our goal is then to approximate the posterior probabilities
p(m|X), where X is the observed data. This is a slightly more complex situation
than that considered so far because different models may have different structure
and indeed different dimensionality for the hidden variables Z. We cannot there-
fore simply consider a factorized approximation q(Z)q(m), but must instead recog-
nize that the posterior over Z must be conditioned on m, and so we must consider
q(Z, m) = q(Z|m)q(m). We can readily verify the following decomposition based
on this variational distribution
ln p(X) = Lm −
q(Z|m)q(m) ln
p(Z, m|X)
q(Z|m)q(m)
where the Lm is a lower bound on ln p(X) and is given by
Lm =
m
Z
q(Z|m)q(m) ln
p(Z, X, m)
q(Z|m)q(m)
Here we are assuming discrete Z, but the same analysis applies to continuous latent
variables provided the summations are replaced with integrations. We can maximize
Lm with respect to the distribution q(m) using a Lagrange multiplier, with the result
(10.36)
However, if we maximize Lm with respect to the q(Z|m), we find that the solutions
for different m are coupled, as we expect because they are conditioned on m. We
proceed instead by first optimizing each of the q(Z|m) individually by optimization
q(m) ∝ p(m) exp{Lm}.
474
10. APPROXIMATE INFERENCE
N
K
K
N
K
10.2.
Section 10.4.1
of (10.35), and then subsequently determining the q(m) using (10.36). After nor-
malization the resulting values for q(m) can be used for model selection or model
averaging in the usual way.
Illustration: Variational Mixture of Gaussians
We now return to our discussion of the Gaussian mixture model and apply the vari-
ational inference machinery developed in the previous section. This will provide a
good illustration of the application of variational methods and will also demonstrate
how a Bayesian treatment elegantly resolves many of the difficulties associated with
the maximum likelihood approach (Attias, 1999b). The reader is encouraged to work
through this example in detail as it provides many insights into the practical appli-
cation of variational methods. Many Bayesian models, corresponding to much more
sophisticated distributions, can be solved by straightforward extensions and general-
izations of this analysis.
Our starting point is the likelihood function for the Gaussian mixture model, il-
lustrated by the graphical model in Figure 9.6. For each observation xn we have
a corresponding latent variable zn comprising a 1-of-K binary vector with ele-
ments znk for k = 1, . . . , K. As before we denote the observed data set by X =
{x1, . . . , xN}, and similarly we denote the latent variables by Z = {z1, . . . , zN}.
From (9.10) we can write down the conditional distribution of Z, given the mixing
coefficients π, in the form
p(Z|π) =
πznk
k
n=1
k=1
(10.37)
Similarly, from (9.11), we can write down the conditional distribution of the ob-
served data vectors, given the latent variables and the component parameters
p(X|Z, µ, Λ) =
N
n=1
k=1
xn|µk, Λ−1
k
znk
(10.38)
where µ = {µk} and Λ = {Λk}. Note that we are working in terms of precision
matrices rather than covariance matrices as this somewhat simplifies the mathemat-
ics.
Next we introduce priors over the parameters µ, Λ and π. The analysis is con-
siderably simplified if we use conjugate prior distributions. We therefore choose a
Dirichlet distribution over the mixing coefficients π
p(π) = Dir(π|α0) = C(α0)
πα0−1
k
k=1
(10.39)
where by symmetry we have chosen the same parameter α0 for each of the compo-
nents, and C(α0) is the normalization constant for the Dirichlet distribution defined
K
