We now seek a factorization rule for undirected graphs that will correspond to
the above conditional independence test. Again, this will involve expressing the joint
distribution p(x) as a product of functions defined over sets of variables that are local
to the graph. We therefore need to decide what is the appropriate notion of locality
in this case.
8.3. Markov Random Fields
385
Figure 8.28 For an undirected graph, the Markov blanket of a node
xi consists of the set of neighbouring nodes.
It has the
property that the conditional distribution of xi, conditioned
on all the remaining variables in the graph, is dependent
only on the variables in the Markov blanket.
If we consider two nodes xi and xj that are not connected by a link, then these
variables must be conditionally independent given all other nodes in the graph. This
follows from the fact that there is no direct path between the two nodes, and all other
paths pass through nodes that are observed, and hence those paths are blocked. This
conditional independence property can be expressed as
p(xi, xj|x\{i,j}) = p(xi|x\{i,j})p(xj|x\{i,j})
(8.38)
where x\{i,j} denotes the set x of all variables with xi and xj removed. The factor-
ization of the joint distribution must therefore be such that xi and xj do not appear
in the same factor in order for the conditional independence property to hold for all
possible distributions belonging to the graph.
This leads us to consider a graphical concept called a clique, which is defined
as a subset of the nodes in a graph such that there exists a link between all pairs of
nodes in the subset. In other words, the set of nodes in a clique is fully connected.
Furthermore, a maximal clique is a clique such that it is not possible to include any
other nodes from the graph in the set without it ceasing to be a clique. These concepts
are illustrated by the undirected graph over four variables shown in Figure 8.29. This
graph has five cliques of two nodes given by {x1, x2}, {x2, x3}, {x3, x4}, {x4, x2},
and {x1, x3}, as well as two maximal cliques given by {x1, x2, x3} and {x2, x3, x4}.
The set {x1, x2, x3, x4} is not a clique because of the missing link from x1 to x4.
We can therefore define the factors in the decomposition of the joint distribution
to be functions of the variables in the cliques. In fact, we can consider functions
of the maximal cliques, without loss of generality, because other cliques must be
subsets of maximal cliques. Thus, if {x1, x2, x3} is a maximal clique and we define
an arbitrary function over this clique, then including another factor defined over a
subset of these variables would be redundant.
Let us denote a clique by C and the set of variables in that clique by xC. Then
Figure 8.29 A four-node undirected graph showing a clique (outlined in
green) and a maximal clique (outlined in blue).
x1
x3
x2
x4
C
1
Z
the joint distribution is written as a product of potential functions ψC(xC) over the
maximal cliques of the graph
p(x) =
ψC(xC).
(8.39)
386
8. GRAPHICAL MODELS
Here the quantity Z, sometimes called the partition function, is a normalization con-
stant and is given by
Z =
ψC(xC)
x
C
(8.40)
which ensures that the distribution p(x) given by (8.39) is correctly normalized.
By considering only potential functions which satisfy ψC(xC) � 0 we ensure that
p(x) � 0. In (8.40) we have assumed that x comprises discrete variables, but the
framework is equally applicable to continuous variables, or a combination of the two,
in which the summation is replaced by the appropriate combination of summation
and integration.
Note that we do not restrict the choice of potential functions to those that have a
specific probabilistic interpretation as marginal or conditional distributions. This is
in contrast to directed graphs in which each factor represents the conditional distribu-
tion of the corresponding variable, conditioned on the state of its parents. However,
in special cases, for instance where the undirected graph is constructed by starting
with a directed graph, the potential functions may indeed have such an interpretation,
as we shall see shortly.
One consequence of the generality of the potential functions ψC(xC) is that
their product will in general not be correctly normalized. We therefore have to in-
troduce an explicit normalization factor given by (8.40). Recall that for directed
graphs, the joint distribution was automatically normalized as a consequence of the
normalization of each of the conditional distributions in the factorization.
The presence of this normalization constant is one of the major limitations of
undirected graphs. If we have a model with M discrete nodes each having K states,
then the evaluation of the normalization term involves summing over KM states and
so (in the worst case) is exponential in the size of the model. The partition function
is needed for parameter learning because it will be a function of any parameters that
govern the potential functions ψC(xC). However, for evaluation of local conditional
distributions, the partition function is not needed because a conditional is the ratio
of two marginals, and the partition function cancels between numerator and denom-
inator when evaluating this ratio. Similarly, for evaluating local marginal probabil-
ities we can work with the unnormalized joint distribution and then normalize the
marginals explicitly at the end. Provided the marginals only involves a small number
of variables, the evaluation of their normalization coefficient will be feasible.
So far, we have discussed the notion of conditional independence based on sim-
ple graph separation and we have proposed a factorization of the joint distribution
that is intended to correspond to this conditional independence structure. However,
we have not made any formal connection between conditional independence and
factorization for undirected graphs. To do so we need to restrict attention to poten-
tial functions ψC(xC) that are strictly positive (i.e., never zero or negative for any
8.3. Markov Random Fields
387
choice of xC). Given this restriction, we can make a precise relationship between
factorization and conditional independence.
To do this we again return to the concept of a graphical model as a filter, corre-
sponding to Figure 8.25. Consider the set of all possible distributions defined over
a fixed set of variables corresponding to the nodes of a particular undirected graph.
We can define UI to be the set of such distributions that are consistent with the set
of conditional independence statements that can be read from the graph using graph
separation. Similarly, we can define UF to be the set of such distributions that can
be expressed as a factorization of the form (8.39) with respect to the maximal cliques
of the graph. The Hammersley-Clifford theorem (Clifford, 1990) states that the sets
UI and UF are identical.
convenient to express them as exponentials, so that
Because we are restricted to potential functions which are strictly positive it is
ψC(xC) = exp{−E(xC)}
(8.41)
where E(xC) is called an energy function, and the exponential representation is
called the Boltzmann distribution. The joint distribution is defined as the product of
potentials, and so the total energy is obtained by adding the energies of each of the
maximal cliques.
In contrast to the factors in the joint distribution for a directed graph, the po-
tentials in an undirected graph do not have a specific probabilistic interpretation.
Although this gives greater flexibility in choosing the potential functions, because
there is no normalization constraint, it does raise the question of how to motivate a
choice of potential function for a particular application. This can be done by view-
ing the potential function as expressing which configurations of the local variables
are preferred to others. Global configurations that have a relatively high probability
are those that find a good balance in satisfying the (possibly conflicting) influences
of the clique potentials. We turn now to a specific example to illustrate the use of
undirected graphs.
