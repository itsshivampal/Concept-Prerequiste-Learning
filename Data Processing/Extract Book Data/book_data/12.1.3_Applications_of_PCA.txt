We can illustrate the use of PCA for data compression by considering the off-
line digits data set. Because each eigenvector of the covariance matrix is a vector
Exercise 12.2
Appendix A
566
12. CONTINUOUS LATENT VARIABLES
Mean
λ1 = 3.4 · 105
λ2 = 2.8 · 105
λ3 = 2.4 · 105
λ4 = 1.6 · 105
Figure 12.3 The mean vector x along with the ﬁrst four PCA eigenvectors u1, . . . , u4 for the off-line
digits data set, together with the corresponding eigenvalues.
in the original D-dimensional space, we can represent the eigenvectors as images of
the same size as the data points. The ﬁrst ﬁve eigenvectors, along with the corre-
sponding eigenvalues, are shown in Figure 12.3. A plot of the complete spectrum of
eigenvalues, sorted into decreasing order, is shown in Figure 12.4(a). The distortion
measure J associated with choosing a particular value of M is given by the sum
of the eigenvalues from M + 1 up to D and is plotted for different values of M in
Figure 12.4(b).
If we substitute (12.12) and (12.13) into (12.10), we can write the PCA approx-
imation to a data vector xn in the form
D(cid:2)
M(cid:2)
(cid:4)xn =
(xT
M(cid:2)
(cid:10)
i=1
= x +
nui)ui +
(xTui)ui
(cid:11)
i=M +1
nui − xTui
xT
ui
(12.19)
(12.20)
i=1
x 106
3
J
2
1
200
400
(a)
600
i
0
0
200
400
(b)
600
M
x 105
3
λi
2
1
0
0
Figure 12.4 (a) Plot of the eigenvalue spectrum for the off-line digits data set.
(b) Plot of the sum of the
discarded eigenvalues, which represents the sum-of-squares distortion J introduced by projecting the data onto
a principal component subspace of dimensionality M.
12.1. Principal Component Analysis
567
Original
M = 1
M = 10
M = 50
M = 250
Figure 12.5 An original example from the off-line digits data set together with its PCA reconstructions
obtained by retaining M principal components for various values of M. As M increases
the reconstruction becomes more accurate and would become perfect when M = D =
28 × 28 = 784.
(cid:10)
(cid:11)
where we have made use of the relation
D(cid:2)
i=1
x =
(cid:10)
(cid:11)
ui
xTui
(12.21)
which follows from the completeness of the {ui}. This represents a compression
of the data set, because for each data point we have replaced the D-dimensional
vector xn with an M-dimensional vector having components
. The
smaller the value of M, the greater the degree of compression. Examples of PCA
reconstructions of data points for the digits data set are shown in Figure 12.5.
nui − xTui
xT
Another application of principal component analysis is to data pre-processing.
In this case, the goal is not dimensionality reduction but rather the transformation of
a data set in order to standardize certain of its properties. This can be important in
allowing subsequent pattern recognition algorithms to be applied successfully to the
data set. Typically, it is done when the original variables are measured in various dif-
ferent units or have signiﬁcantly different variability. For instance in the Old Faithful
data set, the time between eruptions is typically an order of magnitude greater than
the duration of an eruption. When we applied the K-means algorithm to this data
set, we ﬁrst made a separate linear re-scaling of the individual variables such that
each variable had zero mean and unit variance. This is known as standardizing the
data, and the covariance matrix for the standardized data has components
N(cid:2)
ρij =
1
N
(xni − xi)
(xnj − xj)
n=1
σi
σj
(12.22)
where σi is the variance of xi. This is known as the correlation matrix of the original
data and has the property that if two components xi and xj of the data are perfectly
correlated, then ρij = 1, and if they are uncorrelated, then ρij = 0.
However, using PCA we can make a more substantial normalization of the data
to give it zero mean and unit covariance, so that different variables become decorre-
lated. To do this, we ﬁrst write the eigenvector equation (12.17) in the form
SU = UL
(12.23)
Appendix A
Section 9.1
568
12. CONTINUOUS LATENT VARIABLES
100
90
80
70
60
50
40
2
0
−2
2
0
−2
2
4
6
−2
0
2
−2
0
2
Figure 12.6 Illustration of the effects of linear pre-processing applied to the Old Faithful data set. The plot on
the left shows the original data. The centre plot shows the result of standardizing the individual variables to zero
mean and unit variance. Also shown are the principal axes of this normalized data set, plotted over the range
±λ1/2
. The plot on the right shows the result of whitening of the data to give it zero mean and unit covariance.
i
where L is a D × D diagonal matrix with elements λi, and U is a D × D orthog-
onal matrix with columns given by ui. Then we deﬁne, for each data point xn, a
transformed value given by
yn = L−1/2UT(xn − x)
(12.24)
where x is the sample mean deﬁned by (12.1). Clearly, the set {yn} has zero mean,
and its covariance is given by the identity matrix because
N(cid:2)
n=1
N(cid:2)
n=1
1
N
ynyT
n =
1
N
L−1/2UT(xn − x)(xn − x)TUL−1/2
= L−1/2UTSUL−1/2 = L−1/2LL−1/2 = I.
(12.25)
Appendix A
Appendix A
This operation is known as whitening or sphereing the data and is illustrated for the
Old Faithful data set in Figure 12.6.
It is interesting to compare PCA with the Fisher linear discriminant which was
discussed in Section 4.1.4. Both methods can be viewed as techniques for linear
dimensionality reduction. However, PCA is unsupervised and depends only on the
values xn whereas Fisher linear discriminant also uses class-label information. This
difference is highlighted by the example in Figure 12.7.
Another common application of principal component analysis is to data visual-
ization. Here each data point is projected onto a two-dimensional (M = 2) principal
subspace, so that a data point xn is plotted at Cartesian coordinates given by xT
nu1
and xT
nu2, where u1 and u2 are the eigenvectors corresponding to the largest and
second largest eigenvalues. An example of such a plot, for the oil ﬂow data set, is
shown in Figure 12.8.
12.1. Principal Component Analysis
569
Figure 12.7 A comparison of principal compo-
nent analysis with Fisher’s linear
discriminant for linear dimension-
ality reduction. Here the data in
two dimensions, belonging to two
classes shown in red and blue, is
to be projected onto a single di-
mension. PCA chooses the direc-
tion of maximum variance, shown
by the magenta curve, which leads
to strong class overlap, whereas
the Fisher linear discriminant takes
account of
the class labels and
leads to a projection onto the green
curve giving much better class
separation.
1.5
1
0.5
0
−0.5
−1
−1.5
−2
−5
0
5
Figure 12.8 Visualization of the oil ﬂow data set obtained
by projecting the data onto the ﬁrst two prin-
cipal components. The red, blue, and green
points correspond to the ‘laminar’, ‘homo-
geneous’, and ‘annular’ ﬂow conﬁgurations
respectively.
