Throughout this chapter, we have focussed on models comprising a linear combina-
tion of fixed, nonlinear basis functions. We have seen that the assumption of linearity
in the parameters led to a range of useful properties including closed-form solutions
to the least-squares problem, as well as a tractable Bayesian treatment. Furthermore,
for a suitable choice of basis functions, we can model arbitrary nonlinearities in the
j=1
M
M
Exercises
173
mapping from input variables to targets. In the next chapter, we shall study an anal-
ogous class of models for classification.
It might appear, therefore, that such linear models constitute a general purpose
framework for solving problems in pattern recognition. Unfortunately, there are
some significant shortcomings with linear models, which will cause us to turn in
later chapters to more complex models such as support vector machines and neural
networks.
The difficulty stems from the assumption that the basis functions φj(x) are fixed
before the training data set is observed and is a manifestation of the curse of dimen-
sionality discussed in Section 1.4. As a consequence, the number of basis functions
needs to grow rapidly, often exponentially, with the dimensionality D of the input
space.
Fortunately, there are two properties of real data sets that we can exploit to help
alleviate this problem. First of all, the data vectors {xn} typically lie close to a non-
linear manifold whose intrinsic dimensionality is smaller than that of the input space
as a result of strong correlations between the input variables. We will see an example
of this when we consider images of handwritten digits in Chapter 12. If we are using
localized basis functions, we can arrange that they are scattered in input space only
in regions containing data. This approach is used in radial basis function networks
and also in support vector and relevance vector machines. Neural network models,
which use adaptive basis functions having sigmoidal nonlinearities, can adapt the
parameters so that the regions of input space over which the basis functions vary
corresponds to the data manifold. The second property is that target variables may
have significant dependence on only a small number of possible directions within the
data manifold. Neural networks can exploit this property by choosing the directions
in input space to which the basis functions respond.
4 Linear Models for Classification
In the previous chapter, we explored a class of regression models having particularly
simple analytical and computational properties. We now discuss an analogous class
of models for solving classification problems. The goal in classification is to take an
input vector x and to assign it to one of K discrete classes Ck where k = 1, . . . , K.
In the most common scenario, the classes are taken to be disjoint, so that each input is
assigned to one and only one class. The input space is thereby divided into decision
regions whose boundaries are called decision boundaries or decision surfaces. In
this chapter, we consider linear models for classification, by which we mean that the
decision surfaces are linear functions of the input vector x and hence are defined
by (D − 1)-dimensional hyperplanes within the D-dimensional input space. Data
sets whose classes can be separated exactly by linear decision surfaces are said to be
linearly separable.
For regression problems, the target variable t was simply the vector of real num-
bers whose values we wish to predict. In the case of classification, there are various
179
180
