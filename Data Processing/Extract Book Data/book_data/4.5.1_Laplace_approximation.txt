Recall from Section 4.4 that the Laplace approximation is obtained by finding
the mode of the posterior distribution and then fitting a Gaussian centred at that
mode. This requires evaluation of the second derivatives of the log posterior, which
is equivalent to finding the Hessian matrix.
Because we seek a Gaussian representation for the posterior distribution, it is
natural to begin with a Gaussian prior, which we write in the general form
p(w) = N (w|m0, S0)
(4.140)
1
2
N
n=1
N
218
4. LINEAR MODELS FOR CLASSIFICATION
where m0 and S0 are fixed hyperparameters. The posterior distribution over w is
given by
(4.141)
where t = (t1, . . . , tN )T. Taking the log of both sides, and substituting for the prior
distribution using (4.140), and for the likelihood function using (4.89), we obtain
p(w|t) ∝ p(w)p(t|w)
ln p(w|t) = −
(w − m0)TS−1
0 (w − m0)
n=1
{tn ln yn + (1 − tn) ln(1 − yn)} + const
(4.142)
where yn = σ(wTφn). To obtain a Gaussian approximation to the posterior dis-
tribution, we first maximize the posterior distribution to give the MAP (maximum
posterior) solution wMAP, which defines the mean of the Gaussian. The covariance
is then given by the inverse of the matrix of second derivatives of the negative log
likelihood, which takes the form
SN = −∇∇ ln p(w|t) = S−1
0 +
yn(1 − yn)φnφT
n.
(4.143)
The Gaussian approximation to the posterior distribution therefore takes the form
q(w) = N (w|wMAP, SN ).
(4.144)
Having obtained a Gaussian approximation to the posterior distribution, there
remains the task of marginalizing with respect to this distribution in order to make
predictions.
