For most probabilistic models of practical interest, exact inference is intractable, and
so we have to resort to some form of approximation. In Chapter 10, we discussed
inference algorithms based on deterministic approximations, which include methods
such as variational Bayes and expectation propagation. Here we consider approxi-
mate inference methods based on numerical sampling, also known as Monte Carlo
techniques.
Although for some applications the posterior distribution over unobserved vari-
ables will be of direct interest in itself, for most situations the posterior distribution
is required primarily for the purpose of evaluating expectations, for example in order
to make predictions. The fundamental problem that we therefore wish to address in
this chapter involves finding the expectation of some function f(z) with respect to a
probability distribution p(z). Here, the components of z might comprise discrete or
continuous variables or some combination of the two. Thus in the case of continuous
523
l=1
L
f(z)
z
(11.1)
(11.2)
524
11. SAMPLING METHODS
Figure 11.1 Schematic illustration of a function f (z)
whose expectation is to be evaluated with
respect to a distribution p(z).
p(z)
variables, we wish to evaluate the expectation
E[f] =
f(z)p(z) dz
where the integral is replaced by summation in the case of discrete variables. This
is illustrated schematically for a single continuous variable in Figure 11.1. We shall
suppose that such expectations are too complex to be evaluated exactly using analyt-
ical techniques.
The general idea behind sampling methods is to obtain a set of samples z(l)
(where l = 1, . . . , L) drawn independently from the distribution p(z). This allows
the expectation (11.1) to be approximated by a finite sum
f =
1
L
f(z(l)).
Exercise 11.1
As long as the samples z(l) are drawn from the distribution p(z), then E[
and so the estimator
by
f] = E[f]
f has the correct mean. The variance of the estimator is given
var[
f] =
E
(f âˆ’ E[f])2
(11.3)
1
L
is the variance of the function f(z) under the distribution p(z). It is worth emphasiz-
ing that the accuracy of the estimator therefore does not depend on the dimension-
ality of z, and that, in principle, high accuracy may be achievable with a relatively
small number of samples z(l). In practice, ten or twenty independent samples may
suffice to estimate an expectation to sufficient accuracy.
The problem, however, is that the samples {z(l)} might not be independent, and
so the effective sample size might be much smaller than the apparent sample size.
Also, referring back to Figure 11.1, we note that if f(z) is small in regions where
p(z) is large, and vice versa, then the expectation may be dominated by regions
of small probability, implying that relatively large sample sizes will be required to
achieve sufficient accuracy.
For many models, the joint distribution p(z) is conveniently specified in terms
of a graphical model. In the case of a directed graph with no observed variables, it is
M
11. SAMPLING METHODS
525
straightforward to sample from the joint distribution (assuming that it is possible to
sample from the conditional distributions at each node) using the following ances-
tral sampling approach, discussed briefly in Section 8.1.2. The joint distribution is
specified by
p(z) =
p(zi|pai)
i=1
(11.4)
where zi are the set of variables associated with node i, and pai denotes the set of
variables associated with the parents of node i. To obtain a sample from the joint
distribution, we make one pass through the set of variables in the order z1, . . . , zM
sampling from the conditional distributions p(zi|pai). This is always possible be-
cause at each step all of the parent values will have been instantiated. After one pass
through the graph, we will have obtained a sample from the joint distribution.
Now consider the case of a directed graph in which some of the nodes are in-
stantiated with observed values. We can in principle extend the above procedure, at
least in the case of nodes representing discrete variables, to give the following logic
sampling approach (Henrion, 1988), which can be seen as a special case of impor-
tance sampling discussed in Section 11.1.4. At each step, when a sampled value is
obtained for a variable zi whose value is observed, the sampled value is compared
to the observed value, and if they agree then the sample value is retained and the al-
gorithm proceeds to the next variable in turn. However, if the sampled value and the
observed value disagree, then the whole sample so far is discarded and the algorithm
starts again with the first node in the graph. This algorithm samples correctly from
the posterior distribution because it corresponds simply to drawing samples from the
joint distribution of hidden variables and data variables and then discarding those
samples that disagree with the observed data (with the slight saving of not continu-
ing with the sampling from the joint distribution as soon as one contradictory value is
observed). However, the overall probability of accepting a sample from the posterior
decreases rapidly as the number of observed variables increases and as the number
of states that those variables can take increases, and so this approach is rarely used
in practice.
In the case of probability distributions defined by an undirected graph, there is
no one-pass sampling strategy that will sample even from the prior distribution with
no observed variables. Instead, computationally more expensive techniques must be
employed, such as Gibbs sampling, which is discussed in Section 11.3.
As well as sampling from conditional distributions, we may also require samples
from a marginal distribution. If we already have a strategy for sampling from a joint
distribution p(u, v), then it is straightforward to obtain samples from the marginal
distribution p(u) simply by ignoring the values for v in each sample.
There are numerous texts dealing with Monte Carlo methods. Those of partic-
ular interest from the statistical inference perspective include Chen et al. (2001),
Gamerman (1997), Gilks et al. (1996), Liu (2001), Neal (1996), and Robert and
Casella (1999). Also there are review articles by Besag et al. (1995), Brooks (1998),
Diaconis and Saloff-Coste (1998), Jerrum and Sinclair (1996), Neal (1993), Tierney
(1994), and Andrieu et al. (2003) that provide additional information on sampling
 dz
dy
y
