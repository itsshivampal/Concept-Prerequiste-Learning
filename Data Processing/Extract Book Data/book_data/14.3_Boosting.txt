657
ECOM =
1
M
EAV.
(14.14)
This apparently dramatic result suggests that the average error of a model can be
reduced by a factor of M simply by averaging M versions of the model. Unfortu-
nately, it depends on the key assumption that the errors due to the individual models
are uncorrelated. In practice, the errors are typically highly correlated, and the reduc-
tion in overall error is generally small. It can, however, be shown that the expected
committee error will not exceed the expected error of the constituent models, so that
ECOM � EAV. In order to achieve more significant improvements, we turn to a more
sophisticated technique for building committees, known as boosting.
Exercise 14.3
14.3. Boosting
Boosting is a powerful technique for combining multiple ‘base’ classifiers to produce
a form of committee whose performance can be significantly better than that of any
of the base classifiers. Here we describe the most widely used form of boosting
algorithm called AdaBoost, short for ‘adaptive boosting’, developed by Freund and
Schapire (1996). Boosting can give good results even if the base classifiers have a
performance that is only slightly better than random, and hence sometimes the base
classifiers are known as weak learners. Originally designed for solving classification
problems, boosting can also be extended to regression (Friedman, 2001).
The principal difference between boosting and the committee methods such as
bagging discussed above, is that the base classifiers are trained in sequence, and
each base classifier is trained using a weighted form of the data set in which the
weighting coefficient associated with each data point depends on the performance
of the previous classifiers. In particular, points that are misclassified by one of the
base classifiers are given greater weight when used to train the next classifier in
the sequence. Once all the classifiers have been trained, their predictions are then
combined through a weighted majority voting scheme, as illustrated schematically
in Figure 14.1.
Consider a two-class classification problem, in which the training data comprises
input vectors x1, . . . , xN along with corresponding binary target variables t1, . . . , tN
where tn ∈ {−1, 1}. Each data point is given an associated weighting parameter
wn, which is initially set 1/N for all data points. We shall suppose that we have
a procedure available for training a base classifier using weighted data to give a
function y(x) ∈ {−1, 1}. At each stage of the algorithm, AdaBoost trains a new
classifier using a data set in which the weighting coefficients are adjusted according
to the performance of the previously trained classifier so as to give greater weight
to the misclassified data points. Finally, when the desired number of base classifiers
have been trained, they are combined to form a committee using coefficients that
give different weight to different base classifiers. The precise form of the AdaBoost
algorithm is given below.
{w(M )
n
yM (x)
αmym(x)
M
m
n=1
N
N
n=1
N
658
14. COMBINING MODELS
Figure 14.1 Schematic illustration of
the
boosting framework.
Each
base classifier ym(x) is trained
on a weighted form of the train-
ing set (blue arrows) in which
the weights w(m)
depend on
the performance of
the pre-
vious base classifier ym−1(x)
(green arrows). Once all base
classifiers have been trained,
they are combined to give
the final classifier YM (x) (red
arrows).
n
{w(1)
n }
y1(x)
AdaBoost
{w(2)
n }
y2(x)
YM (x) = sign
1. Initialize the data weighting coefficients {wn} by setting w(1)
2. For m = 1, . . . , M:
n = 1, . . . , N.
n = 1/N for
(a) Fit a classifier ym(x) to the training data by minimizing the weighted
error function
Jm =
n I(ym(xn) = tn)
w(m)
(14.15)
where I(ym(xn) = tn) is the indicator function and equals 1 when
ym(xn) = tn and 0 otherwise.
(b) Evaluate the quantities
n I(ym(xn) = tn)
w(m)
m =
n=1
w(m)
n
and then use these to evaluate
αm = ln
1 − 	m
m
(c) Update the data weighting coefficients
(14.16)
(14.17)
w(m+1)
n
= w(m)
n
exp{αmI(ym(xn) = tn)}
(14.18)
M
m
N
14.3. Boosting
659
3. Make predictions using the final model, which is given by
YM (x) = sign
αmym(x)
(14.19)
m=1
n
We see that the first base classifier y1(x) is trained using weighting coeffi-
cients w(1)
n that are all equal, which therefore corresponds to the usual procedure
for training a single classifier. From (14.18), we see that in subsequent iterations
the weighting coefficients w(m)
are increased for data points that are misclassified
and decreased for data points that are correctly classified. Successive classifiers are
therefore forced to place greater emphasis on points that have been misclassified by
previous classifiers, and data points that continue to be misclassified by successive
classifiers receive ever greater weight. The quantities 	m represent weighted mea-
sures of the error rates of each of the base classifiers on the data set. We therefore
see that the weighting coefficients αm defined by (14.17) give greater weight to the
more accurate classifiers when computing the overall output given by (14.19).
The AdaBoost algorithm is illustrated in Figure 14.2, using a subset of 30 data
points taken from the toy classification data set shown in Figure A.7. Here each base
learners consists of a threshold on one of the input variables. This simple classifier
corresponds to a form of decision tree known as a ‘decision stumps’, i.e., a deci-
sion tree with a single node. Thus each base learner classifies an input according to
whether one of the input features exceeds some threshold and therefore simply parti-
tions the space into two regions separated by a linear decision surface that is parallel
to one of the axes.
