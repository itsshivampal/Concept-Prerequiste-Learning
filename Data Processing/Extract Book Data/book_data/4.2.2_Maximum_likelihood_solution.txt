Once we have specified a parametric functional form for the class-conditional
densities p(x|Ck), we can then determine the values of the parameters, together with
the prior class probabilities p(Ck), using maximum likelihood. This requires a data
set comprising observations of x along with their corresponding class labels.
Consider first the case of two classes, each having a Gaussian class-conditional
density with a shared covariance matrix, and suppose we have a data set {xn, tn}
where n = 1, . . . , N. Here tn = 1 denotes class C1 and tn = 0 denotes class C2. We
denote the prior class probability p(C1) = π, so that p(C2) = 1 − π. For a data point
xn from class C1, we have tn = 1 and hence
p(xn,C1) = p(C1)p(xn|C1) = πN (xn|µ1, Σ).
Similarly for class C2, we have tn = 0 and hence
p(xn,C2) = p(C2)p(xn|C2) = (1 − π)N (xn|µ2, Σ).
Thus the likelihood function is given by
p(t|π, µ1, µ2, Σ) =
n=1
[πN (xn|µ1, Σ)]tn [(1 − π)N (xn|µ2, Σ)]1−tn
(4.71)
where t = (t1, . . . , tN )T. As usual, it is convenient to maximize the log of the
likelihood function. Consider first the maximization with respect to π. The terms in
N
n=1
N
n=1
n=1
n=1
n=1
N
N
N
1
N1
n=1
n=1
n=1
N
N
N
n=1
N
N
Exercise 4.9
201
(4.72)
(4.74)
(4.75)
(4.76)
4.2. Probabilistic Generative Models
the log likelihood function that depend on π are
{tn ln π + (1 − tn) ln(1 − π)} .
Setting the derivative with respect to π equal to zero and rearranging, we obtain
1
N
tn = N1
N
N1
N1 + N2
(4.73)
where N1 denotes the total number of data points in class C1, and N2 denotes the total
number of data points in class C2. Thus the maximum likelihood estimate for π is
simply the fraction of points in class C1 as expected. This result is easily generalized
to the multiclass case where again the maximum likelihood estimate of the prior
probability associated with class Ck is given by the fraction of the training set points
assigned to that class.
Now consider the maximization with respect to µ1. Again we can pick out of
the log likelihood function those terms that depend on µ1 giving
tn lnN (xn|µ1, Σ) = −
tn(xn − µ1)TΣ−1(xn − µ1) + const.
1
2
Setting the derivative with respect to µ1 to zero and rearranging, we obtain
which is simply the mean of all the input vectors xn assigned to class C1. By a
similar argument, the corresponding result for µ2 is given by
µ1 =
tnxn
µ2 =
1
N2
(1 − tn)xn
which again is the mean of all the input vectors xn assigned to class C2.
Finally, consider the maximum likelihood solution for the shared covariance
matrix Σ. Picking out the terms in the log likelihood function that depend on Σ, we
have
1
2
1
2
n=1
N
2
tn ln|Σ| −
1
2
(1 − tn) ln|Σ| −
tn(xn − µ1)TΣ−1(xn − µ1)
1
2
(1 − tn)(xn − µ2)TΣ−1(xn − µ2)
ln|Σ| −
N
2
Tr
Σ−1S
(4.77)
D
n∈C1
n∈C2
D
202
4. LINEAR MODELS FOR CLASSIFICATION
where we have defined
S1 =
S = N1
N
1
N1
1
N2
S2 =
S2
S1 + N2
N
(xn − µ1)(xn − µ1)T
(xn − µ2)(xn − µ2)T.
(4.78)
(4.79)
(4.80)
Exercise 4.10
Section 2.3.7
Section 8.2.2
Exercise 4.11
Using the standard result for the maximum likelihood solution for a Gaussian distri-
bution, we see that Σ = S, which represents a weighted average of the covariance
matrices associated with each of the two classes separately.
This result is easily extended to the K class problem to obtain the corresponding
maximum likelihood solutions for the parameters in which each class-conditional
density is Gaussian with a shared covariance matrix. Note that the approach of fitting
Gaussian distributions to the classes is not robust to outliers, because the maximum
likelihood estimation of a Gaussian is not robust.
