The maximum likelihood framework gave point estimates for the parameters µ
and Σ. Now we develop a Bayesian treatment by introducing prior distributions
over these parameters. Let us begin with a simple example in which we consider a
single Gaussian random variable x. We shall suppose that the variance σ2 is known,
and we consider the task of inferring the mean µ given a set of N observations
X = {x1, . . . , xN}. The likelihood function, that is the probability of the observed
data given µ, viewed as a function of µ, is given by
p(X|µ) =
p(xn|µ) =
n=1
1
(2πσ2)N/2
exp
1
2σ2
(xn − µ)2
n=1
(2.137)
Again we emphasize that the likelihood function p(X|µ) is not a probability distri-
bution over µ and is not normalized.
We see that the likelihood function takes the form of the exponential of a quad-
ratic form in µ. Thus if we choose a prior p(µ) given by a Gaussian, it will be a
N
p(µ) = N
and the posterior distribution is given by
µ|µ0, σ2
0
p(µ|X) ∝ p(X|µ)p(µ).
where
p(µ|X) = N
µ|µN , σ2
N
µN =
1
σ2
N
0
σ2
0 + σ2 µ0 + N σ2
+ N
σ2
N σ2
N σ2
1
σ2
0
0 + σ2 µML
(2.138)
(2.139)
(2.140)
(2.141)
(2.142)
in which µML is the maximum likelihood solution for µ given by the sample mean
µML =
1
N
xn.
n=1
(2.143)
It is worth spending a moment studying the form of the posterior mean and
variance. First of all, we note that the mean of the posterior distribution given by
(2.141) is a compromise between the prior mean µ0 and the maximum likelihood
solution µML. If the number of observed data points N = 0, then (2.141) reduces
to the prior mean as expected. For N → ∞, the posterior mean is given by the
maximum likelihood solution. Similarly, consider the result (2.142) for the variance
of the posterior distribution. We see that this is most naturally expressed in terms
of the inverse variance, which is called the precision. Furthermore, the precisions
are additive, so that the precision of the posterior is given by the precision of the
prior plus one contribution of the data precision from each of the observed data
points. As we increase the number of observed data points, the precision steadily
increases, corresponding to a posterior distribution with steadily decreasing variance.
With no observed data points, we have the prior variance, whereas if the number of
N goes to zero and the posterior distribution
data points N → ∞, the variance σ2
becomes infinitely peaked around the maximum likelihood solution. We therefore
see that the maximum likelihood result of a point estimate for µ given by (2.143) is
recovered precisely from the Bayesian formalism in the limit of an infinite number
of observations. Note also that for finite N, if we take the limit σ2
0 → ∞ in which the
prior has infinite variance then the posterior mean (2.141) reduces to the maximum
N = σ2/N.
likelihood result, while from (2.142) the posterior variance is given by σ2
98
2. PROBABILITY DISTRIBUTIONS
conjugate distribution for this likelihood function because the corresponding poste-
rior will be a product of two exponentials of quadratic functions of µ and hence will
also be Gaussian. We therefore take our prior distribution to be
Exercise 2.38
Simple manipulation involving completing the square in the exponent shows that the
posterior distribution is given by
N−1
n=1
N
N
2.3. The Gaussian Distribution
Figure 2.12 Illustration of Bayesian inference for
the mean µ of a Gaussian distri-
bution, in which the variance is as-
sumed to be known. The curves
show the prior distribution over µ
(the curve labelled N = 0), which
in this case is itself Gaussian, along
with the posterior distribution given
by (2.140) for increasing numbers N
of data points. The data points are
generated from a Gaussian of mean
0.8 and variance 0.1, and the prior is
chosen to have mean 0. In both the
prior and the likelihood function, the
variance is set to the true value.
5
0
−1
N = 0
N = 10
N = 2
N = 1
0
99
1
Exercise 2.40
Section 2.3.5
We illustrate our analysis of Bayesian inference for the mean of a Gaussian
distribution in Figure 2.12. The generalization of this result to the case of a D-
dimensional Gaussian random variable x with known covariance and unknown mean
is straightforward.
We have already seen how the maximum likelihood expression for the mean of
a Gaussian can be re-cast as a sequential update formula in which the mean after
observing N data points was expressed in terms of the mean after observing N − 1
data points together with the contribution from data point xN . In fact, the Bayesian
paradigm leads very naturally to a sequential view of the inference problem. To see
this in the context of the inference of the mean of a Gaussian, we write the posterior
distribution with the contribution from the final data point xN separated out so that
p(µ|D) ∝
p(µ)
p(xn|µ)
p(xN|µ).
(2.144)
The term in square brackets is (up to a normalization coefficient) just the posterior
distribution after observing N − 1 data points. We see that this can be viewed as
a prior distribution, which is combined using Bayes’ theorem with the likelihood
function associated with data point xN to arrive at the posterior distribution after
observing N data points. This sequential view of Bayesian inference is very general
and applies to any problem in which the observed data are assumed to be independent
and identically distributed.
So far, we have assumed that the variance of the Gaussian distribution over the
data is known and our goal is to infer the mean. Now let us suppose that the mean
is known and we wish to infer the variance. Again, our calculations will be greatly
simplified if we choose a conjugate form for the prior distribution. It turns out to be
most convenient to work with the precision λ ≡ 1/σ2. The likelihood function for λ
takes the form
p(X|λ) =
n=1
N (xn|µ, λ−1) ∝ λN/2 exp
(xn − µ)2
n=1
(2.145)
2
2
1
0
N
N
n=1
100
2. PROBABILITY DISTRIBUTIONS
2
1
0
0
a = 0.1
b = 0.1
2
1
1
0
0
2
1
a = 1
b = 1
a = 4
b = 6
2
2
0
1
Figure 2.13 Plot of the gamma distribution Gam(λ|a, b) defined by (2.146) for various values of the parameters
a and b.
The corresponding conjugate prior should therefore be proportional to the product
of a power of λ and the exponential of a linear function of λ. This corresponds to
the gamma distribution which is defined by
Gam(λ|a, b) =
1
Γ(a) baλa−1 exp(−bλ).
(2.146)
Exercise 2.41
Exercise 2.42
Here Γ(a) is the gamma function that is defined by (1.141) and that ensures that
(2.146) is correctly normalized. The gamma distribution has a finite integral if a > 0,
and the distribution itself is finite if a � 1. It is plotted, for various values of a and
b, in Figure 2.13. The mean and variance of the gamma distribution are given by
E[λ] = a
b
var[λ] = a
b2 .
(2.147)
(2.148)
Consider a prior distribution Gam(λ|a0, b0). If we multiply by the likelihood
function (2.145), then we obtain a posterior distribution
p(λ|X) ∝ λa0−1λN/2 exp
−b0λ −
2
(xn − µ)2
n=1
(2.149)
which we recognize as a gamma distribution of the form Gam(λ|aN , bN ) where
aN = a0 + N
2
1
2
bN = b0 +
(xn − µ)2 = b0 + N
2 σ2
ML
(2.150)
(2.151)
ML is the maximum likelihood estimator of the variance. Note that in (2.149)
where σ2
there is no need to keep track of the normalization constants in the prior and the
likelihood function because, if required, the correct coefficient can be found at the
end using the normalized form (2.146) for the gamma distribution.
N
n=1
n=1
N
n=1
N
Section 2.2
2.3. The Gaussian Distribution
101
From (2.150), we see that the effect of observing N data points is to increase
the value of the coefficient a by N/2. Thus we can interpret the parameter a0 in
the prior in terms of 2a0 ‘effective’ prior observations. Similarly, from (2.151) we
see that the N data points contribute N σ2
ML is
the variance, and so we can interpret the parameter b0 in the prior as arising from
the 2a0 ‘effective’ prior observations having variance 2b0/(2a0) = b0/a0. Recall
that we made an analogous interpretation for the Dirichlet prior. These distributions
are examples of the exponential family, and we shall see that the interpretation of
a conjugate prior in terms of effective fictitious data points is a general one for the
exponential family of distributions.
ML/2 to the parameter b, where σ2
Instead of working with the precision, we can consider the variance itself. The
conjugate prior in this case is called the inverse gamma distribution, although we
shall not discuss this further because we will find it more convenient to work with
the precision.
Now suppose that both the mean and the precision are unknown. To find a
conjugate prior, we consider the dependence of the likelihood function on µ and λ
p(X|µ, λ) =
1/2
2π
exp
N
2
(xn − µ)2
λ1/2 exp
λµ2
2
exp
xn −
2
x2
n
(2.152)
We now wish to identify a prior distribution p(µ, λ) that has the same functional
dependence on µ and λ as the likelihood function and that should therefore take the
form
p(µ, λ) ∝
= exp
λ1/2 exp
λµ2
2
exp{cλµ − dλ}
2
(µ − c/β)2
λβ/2 exp
d −
c2
2β
(2.153)
where c, d, and β are constants. Since we can always write p(µ, λ) = p(µ|λ)p(λ),
we can find p(µ|λ) and p(λ) by inspection. In particular, we see that p(µ|λ) is a
Gaussian whose precision is a linear function of λ and that p(λ) is a gamma distri-
bution, so that the normalized prior takes the form
p(µ, λ) = N (µ|µ0, (βλ)−1)Gam(λ|a, b)
(2.154)
where we have defined new constants given by µ0 = c/β, a = 1 + β/2, b =
d−c2/2β. The distribution (2.154) is called the normal-gamma or Gaussian-gamma
distribution and is plotted in Figure 2.14. Note that this is not simply the product
of an independent Gaussian prior over µ and a gamma prior over λ, because the
precision of µ is a linear function of λ. Even if we chose a prior in which µ and λ
were independent, the posterior distribution would exhibit a coupling between the
precision of µ and the value of λ.
2
0
D
102
2. PROBABILITY DISTRIBUTIONS
Figure 2.14 Contour plot of the normal-gamma
distribution (2.154) for parameter
values µ0 = 0, β = 2, a = 5 and
b = 6.
2
1
0
−2
In the case of the multivariate Gaussian distribution N
for a D-
dimensional variable x, the conjugate prior distribution for the mean µ, assuming
the precision is known, is again a Gaussian. For known mean and unknown precision
matrix Λ, the conjugate prior is the Wishart distribution given by
x|µ, Λ−1
Exercise 2.45
W(Λ|W, ν) = B|Λ|(ν−D−1)/2 exp
1
2
Tr(W−1Λ)
(2.155)
where ν is called the number of degrees of freedom of the distribution, W is a D×D
scale matrix, and Tr(·) denotes the trace. The normalization constant B is given by
B(W, ν) = |W|−ν/2
2νD/2 πD(D−1)/4
i=1
ν + 1 − i
2
−1
(2.156)
Again, it is also possible to define a conjugate prior over the covariance matrix itself,
rather than over the precision matrix, which leads to the inverse Wishart distribu-
tion, although we shall not discuss this further. If both the mean and the precision
are unknown, then, following a similar line of reasoning to the univariate case, the
conjugate prior is given by
p(µ, Λ|µ0, β, W, ν) = N (µ|µ0, (βΛ)−1)W(Λ|W, ν)
(2.157)
which is known as the normal-Wishart or Gaussian-Wishart distribution.
