In order to apply Gaussian process models to the problem of regression, we need
to take account of the noise on the observed target values, which are given by
tn = yn + 	n
(6.57)
where yn = y(xn), and 	n is a random noise variable whose value is chosen inde-
pendently for each observation n. Here we shall consider noise processes that have
a Gaussian distribution, so that
p(tn|yn) = N (tn|yn, β−1)
(6.58)
where β is a hyperparameter representing the precision of the noise. Because the
noise is independent for each data point, the joint distribution of the target values
t = (t1, . . . , tN )T conditioned on the values of y = (y1, . . . , yN )T is given by an
isotropic Gaussian of the form
(6.59)
where IN denotes the N × N unit matrix. From the definition of a Gaussian process,
the marginal distribution p(y) is given by a Gaussian whose mean is zero and whose
covariance is defined by a Gram matrix K so that
p(t|y) = N (t|y, β−1IN )
p(y) = N (y|0, K).
(6.60)
The kernel function that determines K is typically chosen to express the property
that, for points xn and xm that are similar, the corresponding values y(xn) and
y(xm) will be more strongly correlated than for dissimilar points. Here the notion
of similarity will depend on the application.
In order to find the marginal distribution p(t), conditioned on the input values
x1, . . . , xN , we need to integrate over y. This can be done by making use of the
results from Section 2.3.3 for the linear-Gaussian model. Using (2.115), we see that
the marginal distribution of t is given by
p(t) =
p(t|y)p(y) dy = N (t|0, C)
(6.61)
6.4. Gaussian Processes
307
(6.62)
where the covariance matrix C has elements
C(xn, xm) = k(xn, xm) + β−1δnm.
This result reflects the fact that the two Gaussian sources of randomness, namely
that associated with y(x) and that associated with 	, are independent and so their
covariances simply add.
One widely used kernel function for Gaussian process regression is given by the
exponential of a quadratic form, with the addition of constant and linear terms to
give
k(xn, xm) = θ0 exp
θ1
2 xn − xm2
+ θ2 + θ3xT
nxm.
(6.63)
Note that the term involving θ3 corresponds to a parametric model that is a linear
function of the input variables. Samples from this prior are plotted for various values
of the parameters θ0, . . . , θ3 in Figure 6.5, and Figure 6.6 shows a set of points sam-
pled from the joint distribution (6.60) along with the corresponding values defined
by (6.61).
So far, we have used the Gaussian process viewpoint to build a model of the
joint distribution over sets of data points. Our goal in regression, however, is to
make predictions of the target variables for new inputs, given a set of training data.
Let us suppose that tN = (t1, . . . , tN )T, corresponding to input values x1, . . . , xN ,
comprise the observed training set, and our goal is to predict the target variable tN +1
for a new input vector xN +1. This requires that we evaluate the predictive distri-
bution p(tN +1|tN ). Note that this distribution is conditioned also on the variables
x1, . . . , xN and xN +1. However, to keep the notation simple we will not show these
conditioning variables explicitly.
To find the conditional distribution p(tN +1|t), we begin by writing down the
joint distribution p(tN +1), where tN +1 denotes the vector (t1, . . . , tN , tN +1)T. We
then apply the results from Section 2.3.1 to obtain the required conditional distribu-
tion, as illustrated in Figure 6.7.
From (6.61), the joint distribution over t1, . . . , tN +1 will be given by
p(tN +1) = N (tN +1|0, CN +1)
(6.64)
where CN +1 is an (N + 1) × (N + 1) covariance matrix with elements given by
(6.62). Because this joint distribution is Gaussian, we can apply the results from
Section 2.3.1 to find the conditional Gaussian distribution. To do this, we partition
the covariance matrix as follows
CN +1 =
CN k
kT
c
(6.65)
where CN is the N × N covariance matrix with elements given by (6.62) for n, m =
1, . . . , N, the vector k has elements k(xn, xN +1) for n = 1, . . . , N, and the scalar
308
6. KERNEL METHODS
(1.00, 4.00, 0.00, 0.00)
0
−0.5
0.5
(1.00, 0.25, 0.00, 0.00)
3
1.5
0
−1.5
−3
−1
3
1.5
0
−1.5
−3
−1
9
4.5
0
−4.5
1
−9
−1
9
4.5
0
−4.5
−9
−1
(9.00, 4.00, 0.00, 0.00)
−0.5
0
0.5
1
(1.00, 4.00, 10.00, 0.00)
−0.5
0
0.5
1
3
1.5
0
−1.5
−3
−1
4
2
0
−2
−4
−1
(1.00, 64.00, 0.00, 0.00)
0
−0.5
0.5
(1.00, 4.00, 0.00, 5.00)
1
−0.5
0
0.5
1
−0.5
0
0.5
1
Figure 6.5 Samples from a Gaussian process prior defined by the covariance function (6.63). The title above
each plot denotes (θ0, θ1, θ2, θ3).
c = k(xN +1, xN +1) + β−1. Using the results (2.81) and (2.82), we see that the con-
ditional distribution p(tN +1|t) is a Gaussian distribution with mean and covariance
given by
m(xN +1) = kTC−1
N t
σ2(xN +1) = c − kTC−1
N k.
(6.66)
(6.67)
These are the key results that define Gaussian process regression. Because the vector
k is a function of the test point input value xN +1, we see that the predictive distribu-
tion is a Gaussian whose mean and variance both depend on xN +1. An example of
Gaussian process regression is shown in Figure 6.8.
The only restriction on the kernel function is that the covariance matrix given by
(6.62) must be positive definite. If λi is an eigenvalue of K, then the corresponding
eigenvalue of C will be λi + β−1. It is therefore sufficient that the kernel matrix
k(xn, xm) be positive semidefinite for any pair of points xn and xm, so that λi � 0,
because any eigenvalue λi that is zero will still give rise to a positive eigenvalue
for C because β > 0. This is the same restriction on the kernel function discussed
earlier, and so we can again exploit all of the techniques in Section 6.2 to construct
−1
N
−3
Figure 6.6 Illustration of
the sampling of data
points {tn} from a Gaussian process.
The blue curve shows a sample func-
tion from the Gaussian process prior
over functions, and the red points
show the values of yn obtained by
evaluating the function at a set of in-
put values {xn}. The correspond-
ing values of {tn}, shown in green,
are obtained by adding independent
Gaussian noise to each of the {yn}.
t
3
0
6.4. Gaussian Processes
309
0
x
1
suitable kernels.
tion of xN +1, in the form
Note that the mean (6.66) of the predictive distribution can be written, as a func-
Exercise 6.21
m(xN +1) =
ank(xn, xN +1)
(6.68)
n=1
where an is the nth component of C−1
N t. Thus, if the kernel function k(xn, xm)
depends only on the distance xn − xm, then we obtain an expansion in radial
basis functions.
The results (6.66) and (6.67) define the predictive distribution for Gaussian pro-
cess regression with an arbitrary kernel function k(xn, xm). In the particular case in
which the kernel function k(x, x) is defined in terms of a finite set of basis functions,
we can derive the results obtained previously in Section 3.3.2 for linear regression
starting from the Gaussian process viewpoint.
For such models, we can therefore obtain the predictive distribution either by
taking a parameter space viewpoint and using the linear regression result or by taking
a function space viewpoint and using the Gaussian process result.
The central computational operation in using Gaussian processes will involve
the inversion of a matrix of size N × N, for which standard methods require O(N 3)
computations. By contrast, in the basis function model we have to invert a matrix
SN of size M × M, which has O(M 3) computational complexity. Note that for
both viewpoints, the matrix inversion must be performed once for the given training
set. For each new test point, both methods require a vector-matrix multiply, which
has cost O(N 2) in the Gaussian process case and O(M 2) for the linear basis func-
tion model. If the number M of basis functions is smaller than the number N of
data points, it will be computationally more efficient to work in the basis function
310
6. KERNEL METHODS
Figure 6.7 Illustration of
the mechanism of
Gaussian process regression for
the case of one training point and
one test point, in which the red el-
lipses show contours of the joint dis-
tribution p(t1, t2). Here t1 is the
training data point, and condition-
ing on the value of t1, correspond-
ing to the vertical blue line, we ob-
tain p(t2|t1) shown as a function of
t2 by the green curve.
1
0
−1
t2
m(x2)
t1
−1
0
1
framework. However, an advantage of a Gaussian processes viewpoint is that we
can consider covariance functions that can only be expressed in terms of an infinite
number of basis functions.
For large training data sets, however, the direct application of Gaussian process
methods can become infeasible, and so a range of approximation schemes have been
developed that have better scaling with training set size than the exact approach
(Gibbs, 1997; Tresp, 2001; Smola and Bartlett, 2001; Williams and Seeger, 2001;
Csat´o and Opper, 2002; Seeger et al., 2003). Practical issues in the application of
Gaussian processes are discussed in Bishop and Nabney (2008).
We have introduced Gaussian process regression for the case of a single tar-
get variable. The extension of this formalism to multiple target variables, known
as co-kriging (Cressie, 1993), is straightforward. Various other extensions of Gaus-
Exercise 6.23
Figure 6.8 Illustration of Gaussian process re-
gression applied to the sinusoidal
data set in Figure A.6 in which the
three right-most data points have
been omitted. The green curve
shows the sinusoidal function from
which the data points, shown in
blue, are obtained by sampling and
addition of Gaussian noise. The
red line shows the mean of
the
Gaussian process predictive distri-
bution, and the shaded region cor-
responds to plus and minus two
standard deviations. Notice how
the uncertainty increases in the re-
gion to the right of the data points.
1
0.5
0
−0.5
−1
0
0.2
0.4
0.6
0.8
1
1
2
1
2
6.4. Gaussian Processes
311
sian process regression have also been considered, for purposes such as modelling
the distribution over low-dimensional manifolds for unsupervised learning (Bishop
et al., 1998a) and the solution of stochastic differential equations (Graepel, 2003).
