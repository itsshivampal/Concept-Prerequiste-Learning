Figure 12.2 Principal component analysis seeks a space
of lower dimensionality, known as the princi-
pal subspace and denoted by the magenta
line, such that
the orthogonal projection of
the data points (red dots) onto this subspace
maximizes the variance of the projected points
(green dots). An alternative deﬁnition of PCA
is based on minimizing the sum-of-squares
of the projection errors, indicated by the blue
lines.
x2
xn
(cid:4)xn
561
u1
x1
Section 12.2
Section 12.4
a particular form of linear-Gaussian latent variable model. This probabilistic refor-
mulation brings many advantages, such as the use of EM for parameter estimation,
principled extensions to mixtures of PCA models, and Bayesian formulations that
allow the number of principal components to be determined automatically from the
data. Finally, we discuss brieﬂy several generalizations of the latent variable concept
that go beyond the linear-Gaussian assumption including non-Gaussian latent vari-
ables, which leads to the framework of independent component analysis, as well as
models having a nonlinear relationship between latent and observed variables.
12.1. Principal Component Analysis
Principal component analysis, or PCA, is a technique that is widely used for appli-
cations such as dimensionality reduction, lossy data compression, feature extraction,
and data visualization (Jolliffe, 2002). It is also known as the Karhunen-Lo`eve trans-
form.
There are two commonly used deﬁnitions of PCA that give rise to the same
algorithm. PCA can be deﬁned as the orthogonal projection of the data onto a lower
dimensional linear space, known as the principal subspace, such that the variance of
the projected data is maximized (Hotelling, 1933). Equivalently, it can be deﬁned as
the linear projection that minimizes the average projection cost, deﬁned as the mean
squared distance between the data points and their projections (Pearson, 1901). The
process of orthogonal projection is illustrated in Figure 12.2. We consider each of
these deﬁnitions in turn.
