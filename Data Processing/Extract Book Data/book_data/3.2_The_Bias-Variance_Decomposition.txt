As before, we can maximize this function with respect to W, giving
WML =
ΦTΦ
−1 ΦTT.
If we examine this result for each target variable tk, we have
wk =
ΦTΦ
−1 ΦTtk = Φ†tk
Exercise 3.6
where tk is an N-dimensional column vector with components tnk for n = 1, . . . N.
Thus the solution to the regression problem decouples between the different target
variables, and we need only compute a single pseudo-inverse matrix Φ†, which is
shared by all of the vectors wk.
The extension to general Gaussian noise distributions having arbitrary covari-
ance matrices is straightforward. Again, this leads to a decoupling into K inde-
pendent regression problems. This result is unsurprising because the parameters W
define only the mean of the Gaussian noise distribution, and we know from Sec-
tion 2.3.4 that the maximum likelihood solution for the mean of a multivariate Gaus-
sian is independent of the covariance. From now on, we shall therefore consider a
single target variable t for simplicity.
3.2. The Bias-Variance Decomposition
So far in our discussion of linear models for regression, we have assumed that the
form and number of basis functions are both fixed. As we have seen in Chapter 1,
the use of maximum likelihood, or equivalently least squares, can lead to severe
over-fitting if complex models are trained using data sets of limited size. However,
limiting the number of basis functions in order to avoid over-fitting has the side
effect of limiting the flexibility of the model to capture interesting and important
trends in the data. Although the introduction of regularization terms can control
over-fitting for models with many parameters, this raises the question of how to
determine a suitable value for the regularization coefficient λ. Seeking the solution
that minimizes the regularized error function with respect to both the weight vector
w and the regularization coefficient λ is clearly not the right approach since this
leads to the unregularized solution with λ = 0.
As we have seen in earlier chapters, the phenomenon of over-fitting is really an
unfortunate property of maximum likelihood and does not arise when we marginalize
over parameters in a Bayesian setting. In this chapter, we shall consider the Bayesian
view of model complexity in some depth. Before doing so, however, it is instructive
to consider a frequentist viewpoint of the model complexity issue, known as the bias-
variance trade-off. Although we shall introduce this concept in the context of linear
basis function models, where it is easy to illustrate the ideas using simple examples,
the discussion has more general applicability.
In Section 1.5.5, when we discussed decision theory for regression problems,
we considered various loss functions each of which leads to a corresponding optimal
prediction once we are given the conditional distribution p(t|x). A popular choice is
148
3. LINEAR MODELS FOR REGRESSION
the squared loss function, for which the optimal prediction is given by the conditional
expectation, which we denote by h(x) and which is given by
h(x) = E[t|x] =
tp(t|x) dt.
(3.36)
At this point, it is worth distinguishing between the squared loss function arising
from decision theory and the sum-of-squares error function that arose in the maxi-
mum likelihood estimation of model parameters. We might use more sophisticated
techniques than least squares, for example regularization or a fully Bayesian ap-
proach, to determine the conditional distribution p(t|x). These can all be combined
with the squared loss function for the purpose of making predictions.
We showed in Section 1.5.5 that the expected squared loss can be written in the
form
E[L] =
{y(x) − h(x)}2 p(x) dx +
{h(x) − t}2p(x, t) dx dt.
(3.37)
Recall that the second term, which is independent of y(x), arises from the intrinsic
noise on the data and represents the minimum achievable value of the expected loss.
The first term depends on our choice for the function y(x), and we will seek a so-
lution for y(x) which makes this term a minimum. Because it is nonnegative, the
smallest that we can hope to make this term is zero. If we had an unlimited supply of
data (and unlimited computational resources), we could in principle find the regres-
sion function h(x) to any desired degree of accuracy, and this would represent the
optimal choice for y(x). However, in practice we have a data set D containing only
a finite number N of data points, and consequently we do not know the regression
function h(x) exactly.
If we model the h(x) using a parametric function y(x, w) governed by a pa-
rameter vector w, then from a Bayesian perspective the uncertainty in our model is
expressed through a posterior distribution over w. A frequentist treatment, however,
involves making a point estimate of w based on the data set D, and tries instead
to interpret the uncertainty of this estimate through the following thought experi-
ment. Suppose we had a large number of data sets each of size N and each drawn
independently from the distribution p(t, x). For any given data set D, we can run
our learning algorithm and obtain a prediction function y(x;D). Different data sets
from the ensemble will give different functions and consequently different values of
the squared loss. The performance of a particular learning algorithm is then assessed
by taking the average over this ensemble of data sets.
Consider the integrand of the first term in (3.37), which for a particular data set
D takes the form
(3.38)
Because this quantity will be dependent on the particular data set D, we take its aver-
age over the ensemble of data sets. If we add and subtract the quantity ED[y(x;D)]
{y(x;D) − h(x)}2.
3.2. The Bias-Variance Decomposition
149
(3.39)
inside the braces, and then expand, we obtain
{y(x;D) − ED[y(x;D)] + ED[y(x;D)] − h(x)}2
= {y(x;D) − ED[y(x;D)]}2 + {ED[y(x;D)] − h(x)}2
+2{y(x;D) − ED[y(x;D)]}{ED[y(x;D)] − h(x)}.
We now take the expectation of this expression with respect to D and note that the
final term will vanish, giving
{y(x;D) − h(x)}2
ED
= {ED[y(x;D)] − h(x)}2
(bias)2
variance
+ ED
{y(x;D) − ED[y(x;D)]}2
(3.40)
We see that the expected squared difference between y(x;D) and the regression
function h(x) can be expressed as the sum of two terms. The first term, called the
squared bias, represents the extent to which the average prediction over all data sets
differs from the desired regression function. The second term, called the variance,
measures the extent to which the solutions for individual data sets vary around their
average, and hence this measures the extent to which the function y(x;D) is sensitive
to the particular choice of data set. We shall provide some intuition to support these
definitions shortly when we consider a simple example.
So far, we have considered a single input value x. If we substitute this expansion
back into (3.37), we obtain the following decomposition of the expected squared loss
expected loss = (bias)2 + variance + noise
where
(bias)2 =
variance =
noise =
{ED[y(x;D)] − h(x)}2p(x) dx
{y(x;D) − ED[y(x;D)]}2
ED
{h(x) − t}2p(x, t) dx dt
(3.41)
(3.42)
p(x) dx
(3.43)
(3.44)
and the bias and variance terms now refer to integrated quantities.
Our goal is to minimize the expected loss, which we have decomposed into the
sum of a (squared) bias, a variance, and a constant noise term. As we shall see, there
is a trade-off between bias and variance, with very flexible models having low bias
and high variance, and relatively rigid models having high bias and low variance.
The model with the optimal predictive capability is the one that leads to the best
balance between bias and variance. This is illustrated by considering the sinusoidal
data set from Chapter 1. Here we generate 100 data sets, each containing N = 25
data points, independently from the sinusoidal curve h(x) = sin(2πx). The data
sets are indexed by l = 1, . . . , L, where L = 100, and for each data set D(l) we
Appendix A
150
3. LINEAR MODELS FOR REGRESSION
t
1
0
−1
t
1
0
−1
t
1
0
−1
0
0
0
ln λ = 2.6
t
1
0
−1
1
x
0
1
x
ln λ = −0.31
t
1
0
−1
1
x
0
1
x
ln λ = −2.4
t
1
0
−1
1
x
0
1
x
Figure 3.5 Illustration of the dependence of bias and variance on model complexity, governed by a regulariza-
tion parameter λ, using the sinusoidal data set from Chapter 1. There are L = 100 data sets, each having N = 25
data points, and there are 24 Gaussian basis functions in the model so that the total number of parameters is
M = 25 including the bias parameter. The left column shows the result of fitting the model to the data sets for
various values of ln λ (for clarity, only 20 of the 100 fits are shown). The right column shows the corresponding
average of the 100 fits (red) along with the sinusoidal function from which the data sets were generated (green).
l=1
L
0.15
0.12
0.09
0.06
0.03
n=1
N
N
0
151
1
2
(3.45)
(3.46)
(3.47)
Figure 3.6 Plot of squared bias and variance,
together with their sum, correspond-
ing to the results shown in Fig-
ure 3.5. Also shown is the average
test set error for a test data set size
of 1000 points. The minimum value
of (bias)2 + variance occurs around
ln λ = −0.31, which is close to the
value that gives the minimum error
on the test data.
3.2. The Bias-Variance Decomposition
(bias)2
variance
(bias)2 + variance
test error
−1
ln λ
0
−3
−2
fit a model with 24 Gaussian basis functions by minimizing the regularized error
function (3.27) to give a prediction function y(l)(x) as shown in Figure 3.5. The
top row corresponds to a large value of the regularization coefficient λ that gives low
variance (because the red curves in the left plot look similar) but high bias (because
the two curves in the right plot are very different). Conversely on the bottom row, for
which λ is small, there is large variance (shown by the high variability between the
red curves in the left plot) but low bias (shown by the good fit between the average
model fit and the original sinusoidal function). Note that the result of averaging many
solutions for the complex model with M = 25 is a very good fit to the regression
function, which suggests that averaging may be a beneficial procedure. Indeed, a
weighted averaging of multiple solutions lies at the heart of a Bayesian approach,
although the averaging is with respect to the posterior distribution of parameters, not
with respect to multiple data sets.
We can also examine the bias-variance trade-off quantitatively for this example.
The average prediction is estimated from
y(x) =
y(l)(x)
1
L
and the integrated squared bias and integrated variance are then given by
(bias)2 =
variance =
1
N
1
N
{y(xn) − h(xn)}2
1
L
L
y(l)(xn) − y(xn)
2
n=1
l=1
where the integral over x weighted by the distribution p(x) is approximated by a
finite sum over data points drawn from that distribution. These quantities, along
with their sum, are plotted as a function of ln λ in Figure 3.6. We see that small
values of λ allow the model to become finely tuned to the noise on each individual
152
3. LINEAR MODELS FOR REGRESSION
data set leading to large variance. Conversely, a large value of λ pulls the weight
parameters towards zero leading to large bias.
Although the bias-variance decomposition may provide some interesting in-
sights into the model complexity issue from a frequentist perspective, it is of lim-
ited practical value, because the bias-variance decomposition is based on averages
with respect to ensembles of data sets, whereas in practice we have only the single
observed data set. If we had a large number of independent training sets of a given
size, we would be better off combining them into a single large training set, which
of course would reduce the level of over-fitting for a given model complexity.
Given these limitations, we turn in the next section to a Bayesian treatment of
linear basis function models, which not only provides powerful insights into the
issues of over-fitting but which also leads to practical techniques for addressing the
question model complexity.
