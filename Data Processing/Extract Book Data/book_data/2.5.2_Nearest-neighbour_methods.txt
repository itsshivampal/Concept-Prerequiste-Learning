One of the difficulties with the kernel approach to density estimation is that the
parameter h governing the kernel width is fixed for all kernels. In regions of high
data density, a large value of h may lead to over-smoothing and a washing out of
structure that might otherwise be extracted from the data. However, reducing h may
lead to noisy estimates elsewhere in data space where the density is smaller. Thus
the optimal choice for h may be dependent on location within the data space. This
issue is addressed by nearest-neighbour methods for density estimation.
We therefore return to our general result (2.246) for local density estimation,
and instead of fixing V and determining the value of K from the data, we consider
a fixed value of K and use the data to find an appropriate value for V . To do this,
we consider a small sphere centred on the point x at which we wish to estimate the
2.5. Nonparametric Methods
Figure 2.26 Illustration of K-nearest-neighbour den-
sity estimation using the same data set
as in Figures 2.25 and 2.24. We see
that the parameter K governs the degree
of smoothing, so that a small value of
K leads to a very noisy density model
(top panel), whereas a large value (bot-
tom panel) smoothes out the bimodal na-
ture of the true distribution (shown by the
green curve) from which the data set was
generated.
K = 1
K = 5
K = 30
5
0
5
0
0
5
0
0
0
125
1
1
1
0.5
0.5
0.5
Exercise 2.61
density p(x), and we allow the radius of the sphere to grow until it contains precisely
K data points. The estimate of the density p(x) is then given by (2.246) with V set to
the volume of the resulting sphere. This technique is known as K nearest neighbours
and is illustrated in Figure 2.26, for various choices of the parameter K, using the
same data set as used in Figure 2.24 and Figure 2.25. We see that the value of K
now governs the degree of smoothing and that again there is an optimum choice for
K that is neither too large nor too small. Note that the model produced by K nearest
neighbours is not a true density model because the integral over all space diverges.
We close this chapter by showing how the K-nearest-neighbour technique for
density estimation can be extended to the problem of classification. To do this, we
apply the K-nearest-neighbour density estimation technique to each class separately
and then make use of Bayes’ theorem. Let us suppose that we have a data set com-
k Nk = N. If we
prising Nk points in class Ck with N points in total, so that
wish to classify a new point x, we draw a sphere centred on x containing precisely
K points irrespective of their class. Suppose this sphere has volume V and contains
Kk points from class Ck. Then (2.246) provides an estimate of the density associated
with each class
(2.253)
p(x|Ck) = Kk
NkV
Similarly, the unconditional density is given by
p(x) = K
N V
while the class priors are given by
p(Ck) = Nk
N
(2.254)
(2.255)
We can now combine (2.253), (2.254), and (2.255) using Bayes’ theorem to obtain
the posterior probability of class membership
p(Ck|x) = p(x|Ck)p(Ck)
p(x)
= Kk
K
(2.256)
126
2. PROBABILITY DISTRIBUTIONS
x2
x2
Figure 2.27 (a) In the K-nearest-
neighbour classifier, a new point,
shown by the black diamond, is clas-
sified according to the majority class
membership of the K closest train-
ing data points, in this case K =
3.
In the nearest-neighbour
(K = 1) approach to classification,
the resulting decision boundary is
composed of hyperplanes that form
perpendicular bisectors of pairs of
points from different classes.
(b)
x1
x1
(a)
(b)
If we wish to minimize the probability of misclassification, this is done by assigning
the test point x to the class having the largest posterior probability, corresponding to
the largest value of Kk/K. Thus to classify a new point, we identify the K nearest
points from the training data set and then assign the new point to the class having the
largest number of representatives amongst this set. Ties can be broken at random.
The particular case of K = 1 is called the nearest-neighbour rule, because a test
point is simply assigned to the same class as the nearest point from the training set.
These concepts are illustrated in Figure 2.27.
In Figure 2.28, we show the results of applying the K-nearest-neighbour algo-
rithm to the oil flow data, introduced in Chapter 1, for various values of K. As
expected, we see that K controls the degree of smoothing, so that small K produces
many small regions of each class, whereas large K leads to fewer larger regions.
K = 1
x7
2
1
K = 3
x7
2
1
x7
2
1
K = 3 1
0
0
1
x6
2
0
0
1
x6
2
0
0
1
x6
2
Figure 2.28 Plot of 200 data points from the oil data set showing values of x6 plotted against x7, where the
red, green, and blue points correspond to the ‘laminar’, ‘annular’, and ‘homogeneous’ classes, respectively. Also
shown are the classifications of the input space given by the K-nearest-neighbour algorithm for various values
of K.
x=0
1
Exercises
127
An interesting property of the nearest-neighbour (K = 1) classifier is that, in the
limit N → ∞, the error rate is never more than twice the minimum achievable error
rate of an optimal classifier, i.e., one that uses the true class distributions (Cover and
Hart, 1967) .
As discussed so far, both the K-nearest-neighbour method, and the kernel den-
sity estimator, require the entire training data set to be stored, leading to expensive
computation if the data set is large. This effect can be offset, at the expense of some
additional one-off computation, by constructing tree-based search structures to allow
(approximate) near neighbours to be found efficiently without doing an exhaustive
search of the data set. Nevertheless, these nonparametric methods are still severely
limited. On the other hand, we have seen that simple parametric models are very
restricted in terms of the forms of distribution that they can represent. We therefore
need to find density models that are very flexible and yet for which the complexity
of the models can be controlled independently of the size of the training set, and we
shall see in subsequent chapters how to achieve this.
3 Linear Models for Regression
The focus so far in this book has been on unsupervised learning, including topics
such as density estimation and data clustering. We turn now to a discussion of super-
vised learning, starting with regression. The goal of regression is to predict the value
of one or more continuous target variables t given the value of a D-dimensional vec-
tor x of input variables. We have already encountered an example of a regression
problem when we considered polynomial curve fitting in Chapter 1. The polynomial
is a specific example of a broad class of functions called linear regression models,
which share the property of being linear functions of the adjustable parameters, and
which will form the focus of this chapter. The simplest form of linear regression
models are also linear functions of the input variables. However, we can obtain a
much more useful class of functions by taking linear combinations of a fixed set of
nonlinear functions of the input variables, known as basis functions. Such models
are linear functions of the parameters, which gives them simple analytical properties,
and yet can be nonlinear with respect to the input variables.
137
138
