When neural networks are applied to regression problems, it is common to use
a sum-of-squares error function of the form
1
2
(yn − tn)2
(5.82)
where we have considered the case of a single output in order to keep the notation
simple (the extension to several outputs is straightforward). We can then write the
Hessian matrix in the form
H = ∇∇E =
∇yn∇yn +
(yn − tn)∇∇yn.
n=1
(5.83)
If the network has been trained on the data set, and its outputs yn happen to be very
close to the target values tn, then the second term in (5.83) will be small and can
be neglected. More generally, however, it may be appropriate to neglect this term
by the following argument. Recall from Section 1.5.5 that the optimal function that
minimizes a sum-of-squares loss is the conditional average of the target data. The
quantity (yn − tn) is then a random variable with zero mean. If we assume that its
value is uncorrelated with the value of the second derivative term on the right-hand
side of (5.83), then the whole term will average to zero in the summation over n.
By neglecting the second term in (5.83), we arrive at the Levenberg–Marquardt
approximation or outer product approximation (because the Hessian matrix is built
up from a sum of outer products of vectors), given by
H 
bnbT
n
n=1
(5.84)
where bn = ∇yn = ∇an because the activation function for the output units is
simply the identity. Evaluation of the outer product approximation for the Hessian
is straightforward as it only involves first derivatives of the error function, which
can be evaluated efficiently in O(W ) steps using standard backpropagation. The
elements of the matrix can then be found in O(W 2) steps by simple multiplication.
It is important to emphasize that this approximation is only likely to be valid for a
network that has been trained appropriately, and that for a general network mapping
the second derivative terms on the right-hand side of (5.83) will typically not be
negligible.
In the case of the cross-entropy error function for a network with logistic sigmoid
output-unit activation functions, the corresponding approximation is given by
Exercise 5.16
Exercise 5.17
Exercise 5.19
Exercise 5.20
An analogous result can be obtained for multiclass networks having softmax output-
unit activation functions.
H 
n=1
yn(1 − yn)bnbT
n.
(5.85)
N
252
5. NEURAL NETWORKS
