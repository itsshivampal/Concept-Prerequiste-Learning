Some of the applications for the Hessian matrix discussed above require the
inverse of the Hessian, rather than the Hessian itself. For this reason, there has
been some interest in using a diagonal approximation to the Hessian, in other words
one that simply replaces the off-diagonal elements with zeros, because its inverse is
trivial to evaluate. Again, we shall consider an error function that consists of a sum
of terms, one for each pattern in the data set, so that E =
n En. The Hessian can
then be obtained by considering one pattern at a time, and then summing the results
over all patterns. From (5.48), the diagonal elements of the Hessian, for pattern n,
can be written
∂2En
∂w2
ji
= ∂2En
∂a2
j
z2
i .
(5.79)
Using (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can
be found recursively using the chain rule of differential calculus to give a backprop-
agation equation of the form
∂2En
∂a2
j
= h(aj)2
k
k
wkjwkj
∂2En
∂ak∂ak
+ h(aj)
wkj
∂En
∂ak
(5.80)
If we now neglect off-diagonal elements in the second-derivative terms, we obtain
(Becker and Le Cun, 1989; Le Cun et al., 1990)
∂2En
∂a2
j
= h(aj)2
w2
kj
∂2En
∂a2
k
k
+ h(aj)
wkj
∂En
∂ak
k
(5.81)
Note that the number of computational steps required to evaluate this approximation
is O(W ), where W is the total number of weight and bias parameters in the network,
compared with O(W 2) for the full Hessian.
Ricotti et al. (1988) also used the diagonal approximation to the Hessian, but
they retained all terms in the evaluation of ∂2En/∂a2
j and so obtained exact expres-
sions for the diagonal terms. Note that this no longer has O(W ) scaling. The major
problem with diagonal approximations, however, is that in practice the Hessian is
typically found to be strongly nondiagonal, and so these approximations, which are
driven mainly be computational convenience, must be treated with care.
N
n=1
N
N
E =
n=1
N
N
5.4. The Hessian Matrix
251
