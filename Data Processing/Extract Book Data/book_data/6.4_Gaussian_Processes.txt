303
0.2
0.4
0.6
0.8
1
In fact, this model defines not only a conditional expectation but also a full
conditional distribution given by
p(t|x) = p(t, x)
p(t, x) dt
f(x − xn, t − tn)
(6.48)
n
f(x − xm, t − tm) dt
m
Exercise 6.18
from which other expectations can be evaluated.
As an illustration we consider the case of a single input variable x in which
f(x, t) is given by a zero-mean isotropic Gaussian over the variable z = (x, t) with
variance σ2. The corresponding conditional distribution (6.48) is given by a Gaus-
sian mixture, and is shown, together with the conditional mean, for the sinusoidal
synthetic data set in Figure 6.3.
An obvious extension of this model is to allow for more flexible forms of Gaus-
sian components, for instance having different variance parameters for the input and
target variables. More generally, we could model the joint distribution p(t, x) using
a Gaussian mixture model, trained using techniques discussed in Chapter 9 (Ghahra-
mani and Jordan, 1994), and then find the corresponding conditional distribution
p(t|x). In this latter case we no longer have a representation in terms of kernel func-
tions evaluated at the training set data points. However, the number of components
in the mixture model can be smaller than the number of training set points, resulting
in a model that is faster to evaluate for test data points. We have thereby accepted an
increased computational cost during the training phase in order to have a model that
is faster at making predictions.
6.4. Gaussian Processes
In Section 6.1, we introduced kernels by applying the concept of duality to a non-
probabilistic model for regression. Here we extend the role of kernels to probabilis-
304
6. KERNEL METHODS
tic discriminative models, leading to the framework of Gaussian processes. We shall
thereby see how kernels arise naturally in a Bayesian setting.
In Chapter 3, we considered linear regression models of the form y(x, w) =
wTφ(x) in which w is a vector of parameters and φ(x) is a vector of fixed nonlinear
basis functions that depend on the input vector x. We showed that a prior distribution
over w induced a corresponding prior distribution over functions y(x, w). Given a
training data set, we then evaluated the posterior distribution over w and thereby
obtained the corresponding posterior distribution over regression functions, which
in turn (with the addition of noise) implies a predictive distribution p(t|x) for new
input vectors x.
In the Gaussian process viewpoint, we dispense with the parametric model and
instead define a prior probability distribution over functions directly. At first sight, it
might seem difficult to work with a distribution over the uncountably infinite space of
functions. However, as we shall see, for a finite training set we only need to consider
the values of the function at the discrete set of input values xn corresponding to the
training set and test set data points, and so in practice we can work in a finite space.
Models equivalent to Gaussian processes have been widely studied in many dif-
ferent fields. For instance, in the geostatistics literature Gaussian process regression
is known as kriging (Cressie, 1993). Similarly, ARMA (autoregressive moving aver-
age) models, Kalman filters, and radial basis function networks can all be viewed as
forms of Gaussian process models. Reviews of Gaussian processes from a machine
learning perspective can be found in MacKay (1998), Williams (1999), and MacKay
(2003), and a comparison of Gaussian process models with alternative approaches is
given in Rasmussen (1996). See also Rasmussen and Williams (2006) for a recent
textbook on Gaussian processes.
