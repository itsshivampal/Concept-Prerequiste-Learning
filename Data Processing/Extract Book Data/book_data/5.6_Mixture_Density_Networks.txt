The goal of supervised learning is to model a conditional distribution p(t|x), which
for many simple regression problems is chosen to be Gaussian. However, practical
machine learning problems can often have significantly non-Gaussian distributions.
These can arise, for example, with inverse problems in which the distribution can be
multimodal, in which case the Gaussian assumption can lead to very poor predic-
tions.
As a simple example of an inverse problem, consider the kinematics of a robot
arm, as illustrated in Figure 5.18. The forward problem involves finding the end ef-
fector position given the joint angles and has a unique solution. However, in practice
we wish to move the end effector of the robot to a specific position, and to do this we
must set appropriate joint angles. We therefore need to solve the inverse problem,
which has two solutions as seen in Figure 5.18.
Forward problems often corresponds to causality in a physical system and gen-
erally have a unique solution. For instance, a specific pattern of symptoms in the
human body may be caused by the presence of a particular disease. In pattern recog-
nition, however, we typically have to solve an inverse problem, such as trying to
predict the presence of a disease given a set of symptoms. If the forward problem
involves a many-to-one mapping, then the inverse problem will have multiple solu-
tions. For instance, several different diseases may result in the same symptoms.
In the robotics example, the kinematics is defined by geometrical equations, and
the multimodality is readily apparent. However, in many machine learning problems
the presence of multimodality, particularly in problems involving spaces of high di-
mensionality, can be less obvious. For tutorial purposes, however, we shall consider
a simple toy problem for which we can easily visualize the multimodality. Data for
this problem is generated by sampling a variable x uniformly over the interval (0, 1),
to give a set of values {xn}, and the corresponding target values tn are obtained
(x1, x2)
(x1, x2)
elbow
up
θ1
elbow
down
(5.147)
K
1
5.6. Mixture Density Networks
Figure 5.19 On the left is the data
set for a simple ‘forward problem’ in
which the red curve shows the result
of fitting a two-layer neural network
by minimizing the sum-of-squares
error function. The corresponding
inverse problem, shown on the right,
is obtained by exchanging the roles
of x and t. Here the same net-
work trained again by minimizing the
sum-of-squares error function gives
a very poor fit to the data due to the
multimodality of the data set.
1
0
0
1
0
0
273
1
by computing the function xn + 0.3 sin(2πxn) and then adding uniform noise over
the interval (−0.1, 0.1). The inverse problem is then obtained by keeping the same
data points but exchanging the roles of x and t. Figure 5.19 shows the data sets for
the forward and inverse problems, along with the results of fitting two-layer neural
networks having 6 hidden units and a single linear output unit by minimizing a sum-
of-squares error function. Least squares corresponds to maximum likelihood under
a Gaussian assumption. We see that this leads to a very poor model for the highly
non-Gaussian inverse problem.
We therefore seek a general framework for modelling conditional probability
distributions. This can be achieved by using a mixture model for p(t|x) in which
both the mixing coefficients as well as the component densities are flexible functions
of the input vector x, giving rise to the mixture density network. For any given value
of x, the mixture model provides a general formalism for modelling an arbitrary
conditional density function p(t|x). Provided we consider a sufficiently flexible
network, we then have a framework for approximating arbitrary conditional distri-
butions.
Here we shall develop the model explicitly for Gaussian components, so that
p(t|x) =
k=1
πk(x)N
t|µk(x), σ2
k(x)
(5.148)
This is an example of a heteroscedastic model since the noise variance on the data
is a function of the input vector x. Instead of Gaussians, we can use other distribu-
tions for the components, such as Bernoulli distributions if the target variables are
binary rather than continuous. We have also specialized to the case of isotropic co-
variances for the components, although the mixture density network can readily be
extended to allow for general covariance matrices by representing the covariances
using a Cholesky factorization (Williams, 1996). Even with isotropic components,
the conditional distribution p(t|x) does not assume factorization with respect to the
components of t (in contrast to the standard sum-of-squares regression model) as a
consequence of the mixture distribution.
We now take the various parameters of the mixture model, namely the mixing
k(x), to be governed by
coefficients πk(x), the means µk(x), and the variances σ2
K
274
5. NEURAL NETWORKS
xD
x1
θM
θ1
p(t|x)
t
Figure 5.20 The mixturedensitynetwork can represent general conditional probability densities p(t|x)
by considering a parametric mixture model for the distribution of t whose parameters are
determined by the outputs of a neural network that takes x as its input vector.
the outputs of a conventional neural network that takes x as its input. The structure
of this mixture density network is illustrated in Figure 5.20. The mixture density
network is closely related to the mixture of experts discussed in Section 14.5.3. The
principle difference is that in the mixture density network the same function is used
to predict the parameters of all of the component densities as well as the mixing co-
efficients, and so the nonlinear hidden units are shared amongst the input-dependent
functions.
The neural network in Figure 5.20 can, for example, be a two-layer network
having sigmoidal (‘tanh’) hidden units. If there are L components in the mixture
model (5.148), and if t has K components, then the network will have L output unit
k that determine the mixing coefficients πk(x), K outputs
activations denoted by aπ
k that determine the kernel widths σk(x), and L × K outputs denoted
denoted by aσ
by aµ
kj that determine the components µkj(x) of the kernel centres µk(x). The total
number of network outputs is given by (K + 2)L, as compared with the usual K
outputs for a network, which simply predicts the conditional means of the target
variables.
The mixing coefficients must satisfy the constraints
πk(x) = 1,
k=1
0 � πk(x) � 1
(5.149)
which can be achieved using a set of softmax outputs
πk(x) =
exp(aπ
k)
K
l=1 exp(aπ
l )
(5.150)
Similarly, the variances must satisfy σ2
of the exponentials of the corresponding network activations using
k(x) � 0 and so can be represented in terms
Finally, because the means µk(x) have real components, they can be represented
σk(x) = exp(aσ
k).
(5.151)
N
k
5.6. Mixture Density Networks
directly by the network output activations
µkj(x) = aµ
kj.
The adaptive parameters of the mixture density network comprise the vector w
of weights and biases in the neural network, that can be set by maximum likelihood,
or equivalently by minimizing an error function defined to be the negative logarithm
of the likelihood. For independent data, this error function takes the form
E(w) = −
ln
n=1
k=1
πk(xn, w)N
tn|µk(xn, w), σ2
k(xn, w)
(5.153)
where we have made the dependencies on w explicit.
In order to minimize the error function, we need to calculate the derivatives of
the error E(w) with respect to the components of w. These can be evaluated by
using the standard backpropagation procedure, provided we obtain suitable expres-
sions for the derivatives of the error with respect to the output-unit activations. These
represent error signals δ for each pattern and for each output unit, and can be back-
propagated to the hidden units and the error function derivatives evaluated in the
usual way. Because the error function (5.153) is composed of a sum of terms, one
for each training data point, we can consider the derivatives for a particular pattern
n and then find the derivatives of E by summing over all patterns.
Because we are dealing with mixture distributions, it is convenient to view the
mixing coefficients πk(x) as x-dependent prior probabilities and to introduce the
corresponding posterior probabilities given by
where Nnk denotes N (tn|µk(xn), σ2
ing coefficients are given by
k(xn)).
The derivatives with respect to the network output activations governing the mix-
Similarly, the derivatives with respect to the output activations controlling the com-
ponent means are given by
γk(t|x) = πkNnk
l=1 πlNnl
K
∂En
∂aπ
k
= πk − γk.
∂En
∂aµ
kl
= γk
µkl − tl
σ2
k
Finally, the derivatives with respect to the output activations controlling the compo-
nent variances are given by
∂En
∂aσ
k
= −γk
t − µk2
σ3
k
1
σk
(5.157)
Exercise 5.34
Exercise 5.35
Exercise 5.36
275
(5.152)
(5.154)
(5.155)
(5.156)
276
5. NEURAL NETWORKS
Figure 5.21 (a) Plot of the mixing
coefficients πk(x) as a function of
x for the three kernel functions in a
mixture density network trained on
the data shown in Figure 5.19. The
model has three Gaussian compo-
nents, and uses a two-layer multi-
layer perceptron with five ‘tanh’ sig-
moidal units in the hidden layer, and
nine outputs (corresponding to the 3
means and 3 variances of the Gaus-
sian components and the 3 mixing
coefficients). At both small and large
values of x, where the conditional
probability density of the target data
is unimodal, only one of
the ker-
nels has a high value for its prior
probability, while at intermediate val-
ues of x, where the conditional den-
sity is trimodal, the three mixing co-
efficients have comparable values.
(b) Plots of the means µk(x) using
the same colour coding as for the
mixing coefficients.
(c) Plot of the
contours of the corresponding con-
ditional probability density of the tar-
get data for the same mixture den-
sity network.
the ap-
proximate conditional mode, shown
by the red points, of the conditional
density.
(d) Plot of
1
0
1
0
0
0
1
0
1
0
0
0
1
1
(b)
(d)
1
1
(a)
(c)
K
We illustrate the use of a mixture density network by returning to the toy ex-
ample of an inverse problem shown in Figure 5.19. Plots of the mixing coeffi-
cients πk(x), the means µk(x), and the conditional density contours corresponding
to p(t|x), are shown in Figure 5.21. The outputs of the neural network, and hence the
parameters in the mixture model, are necessarily continuous single-valued functions
of the input variables. However, we see from Figure 5.21(c) that the model is able to
produce a conditional density that is unimodal for some values of x and trimodal for
other values by modulating the amplitudes of the mixing components πk(x).
Once a mixture density network has been trained, it can predict the conditional
density function of the target data for any given value of the input vector. This
conditional density represents a complete description of the generator of the data, so
far as the problem of predicting the value of the output vector is concerned. From
this density function we can calculate more specific quantities that may be of interest
in different applications. One of the simplest of these is the mean, corresponding to
the conditional average of the target data, and is given by
E [t|x] =
tp(t|x) dt =
k=1
πk(x)µk(x)
(5.158)
k=1
⎧⎨⎩σ2
'''''µk(x) −
K
'''''2
⎫⎬⎭ (5.160)
(5.159)
277
