Comparison of the K-means algorithm with the EM algorithm for Gaussian
mixtures shows that there is a close similarity. Whereas the K-means algorithm
performs a hard assignment of data points to clusters, in which each data point is
associated uniquely with one cluster, the EM algorithm makes a soft assignment
based on the posterior probabilities. In fact, we can derive the K-means algorithm
as a particular limit of EM for Gaussian mixtures as follows.
Consider a Gaussian mixture model in which the covariance matrices of the
mixture components are given by 	I, where 	 is a variance parameter that is shared
Exercise 9.5
Section 8.2
Exercise 9.8
(9.41)
K
N
444
9. MIXTURE MODELS AND EM
by all of the components, and I is the identity matrix, so that
p(x|µk, Σk) =
(2π	)1/2
1
exp
1
2	x − µk2
We now consider the EM algorithm for a mixture of K Gaussians of this form in
which we treat 	 as a fixed constant, instead of a parameter to be re-estimated. From
(9.13) the posterior probabilities, or responsibilities, for a particular data point xn,
are given by
γ(znk) = πk exp{−xn − µk2/2	}
−xn − µj2/2
j πj exp
(9.42)
If we consider the limit 	 → 0, we see that in the denominator the term for which
xn − µj2 is smallest will go to zero most slowly, and hence the responsibilities
γ(znk) for the data point xn all go to zero except for term j, for which the responsi-
bility γ(znj) will go to unity. Note that this holds independently of the values of the
πk so long as none of the πk is zero. Thus, in this limit, we obtain a hard assignment
of data points to clusters, just as in the K-means algorithm, so that γ(znk) → rnk
where rnk is defined by (9.2). Each data point is thereby assigned to the cluster
having the closest mean.
The EM re-estimation equation for the µk, given by (9.17), then reduces to the
K-means result (9.4). Note that the re-estimation formula for the mixing coefficients
(9.22) simply re-sets the value of πk to be equal to the fraction of data points assigned
to cluster k, although these parameters no longer play an active role in the algorithm.
Finally, in the limit 	 → 0 the expected complete-data log likelihood, given by
(9.40), becomes
EZ[ln p(X, Z|µ, Σ, π)] → −
1
2
rnkxn − µk2 + const.
(9.43)
n=1
k=1
Thus we see that in this limit, maximizing the expected complete-data log likelihood
is equivalent to minimizing the distortion measure J for the K-means algorithm
given by (9.1).
Note that the K-means algorithm does not estimate the covariances of the clus-
ters but only the cluster means. A hard-assignment version of the Gaussian mixture
model with general covariance matrices, known as the elliptical K-means algorithm,
has been considered by Sung and Poggio (1994).
