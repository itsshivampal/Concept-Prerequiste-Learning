As well as considering probabilities defined over discrete sets of events, we
also wish to consider probabilities with respect to continuous variables. We shall
limit ourselves to a relatively informal discussion. If the probability of a real-valued
variable x falling in the interval (x, x + δx) is given by p(x)δx for δx → 0, then
p(x) is called the probability density over x. This is illustrated in Figure 1.12. The
probability that x will lie in an interval (a, b) is then given by
p(x ∈ (a, b)) =
b
a
p(x) dx.
(1.24)
 dx
dy
18
1. INTRODUCTION
Figure 1.12 The concept of probability for
discrete variables can be ex-
tended to that of a probability
density p(x) over a continuous
variable x and is such that the
probability of x lying in the inter-
val (x, x + δx) is given by p(x)δx
for δx → 0. The probability
density can be expressed as the
derivative of a cumulative distri-
bution function P (x).
p(x)
P (x)
x
(1.25)
(1.26)
δx
p(x) � 0
p(x) dx = 1.
Because probabilities are nonnegative, and because the value of x must lie some-
where on the real axis, the probability density p(x) must satisfy the two conditions
Under a nonlinear change of variable, a probability density transforms differently
from a simple function, due to the Jacobian factor. For instance, if we consider
f(y) = f(g(y)).
a change of variables x = g(y), then a function f(x) becomes
Now consider a probability density px(x) that corresponds to a density py(y) with
respect to the new variable y, where the suffices denote the fact that px(x) and py(y)
are different densities. Observations falling in the range (x, x + δx) will, for small
values of δx, be transformed into the range (y, y + δy) where px(x)δx  py(y)δy,
and hence
py(y) = px(x)
= px(g(y))|g(y)| .
(1.27)
One consequence of this property is that the concept of the maximum of a probability
density is dependent on the choice of variable.
The probability that x lies in the interval (−∞, z) is given by the cumulative
distribution function defined by
P (z) =
p(x) dx
z
(1.28)
which satisfies P (x) = p(x), as shown in Figure 1.12.
If we have several continuous variables x1, . . . , xD, denoted collectively by the
vector x, then we can define a joint probability density p(x) = p(x1, . . . , xD) such
Exercise 1.4
x
(1.29)
(1.30)
(1.31)
(1.32)
1.2. Probability Theory
19
that the probability of x falling in an infinitesimal volume δx containing the point x
is given by p(x)δx. This multivariate probability density must satisfy
p(x) � 0
p(x) dx = 1
in which the integral is taken over the whole of x space. We can also consider joint
probability distributions over a combination of discrete and continuous variables.
Note that if x is a discrete variable, then p(x) is sometimes called a probability
mass function because it can be regarded as a set of ‘probability masses’ concentrated
at the allowed values of x.
The sum and product rules of probability, as well as Bayes’ theorem, apply
equally to the case of probability densities, or to combinations of discrete and con-
tinuous variables. For instance, if x and y are two real variables, then the sum and
product rules take the form
p(x) =
p(x, y) dy
p(x, y) = p(y|x)p(x).
A formal justification of the sum and product rules for continuous variables (Feller,
1966) requires a branch of mathematics called measure theory and lies outside the
scope of this book. Its validity can be seen informally, however, by dividing each
real variable into intervals of width ∆ and considering the discrete probability dis-
tribution over these intervals. Taking the limit ∆ → 0 then turns sums into integrals
and gives the desired result.
