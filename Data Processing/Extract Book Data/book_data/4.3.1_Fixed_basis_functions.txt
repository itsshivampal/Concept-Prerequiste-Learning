So far in this chapter, we have considered classification models that work di-
rectly with the original input vector x. However, all of the algorithms are equally
applicable if we first make a fixed nonlinear transformation of the inputs using a
vector of basis functions φ(x). The resulting decision boundaries will be linear in
the feature space φ, and these correspond to nonlinear decision boundaries in the
original x space, as illustrated in Figure 4.12. Classes that are linearly separable
in the feature space φ(x) need not be linearly separable in the original observation
space x. Note that as in our discussion of linear models for regression, one of the
4.3. Probabilistic Discriminative Models
205
basis functions is typically set to a constant, say φ0(x) = 1, so that the correspond-
ing parameter w0 plays the role of a bias. For the remainder of this chapter, we shall
include a fixed basis function transformation φ(x), as this will highlight some useful
similarities to the regression models discussed in Chapter 3.
For many problems of practical interest, there is significant overlap between
the class-conditional densities p(x|Ck). This corresponds to posterior probabilities
p(Ck|x), which, for at least some values of x, are not 0 or 1. In such cases, the opti-
mal solution is obtained by modelling the posterior probabilities accurately and then
applying standard decision theory, as discussed in Chapter 1. Note that nonlinear
transformations φ(x) cannot remove such class overlap. Indeed, they can increase
the level of overlap, or create overlap where none existed in the original observation
space. However, suitable choices of nonlinearity can make the process of modelling
the posterior probabilities easier.
Such fixed basis function models have important limitations, and these will be
resolved in later chapters by allowing the basis functions themselves to adapt to the
data. Notwithstanding these limitations, models with fixed nonlinear basis functions
play an important role in applications, and a discussion of such models will intro-
duce many of the key concepts needed for an understanding of their more complex
counterparts.
