We begin by considering models in which the observed variables are related
linearly to the latent variables, but for which the latent distribution is non-Gaussian.
An important class of such models, known as independent component analysis, or
ICA, arises when we consider a distribution over the latent variables that factorizes,
so that
M(cid:14)
p(z) =
p(zj).
(12.89)
j=1
To understand the role of such models, consider a situation in which two people
are talking at the same time, and we record their voices using two microphones.
If we ignore effects such as time delay and echoes, then the signals received by
the microphones at any point in time will be given by linear combinations of the
amplitudes of the two voices. The coefﬁcients of this linear combination will be
constant, and if we can infer their values from sample data, then we can invert the
mixing process (assuming it is nonsingular) and thereby obtain two clean signals
each of which contains the voice of just one person. This is an example of a problem
called blind source separation in which ‘blind’ refers to the fact that we are given
only the mixed data, and neither the original sources nor the mixing coefﬁcients are
observed (Cardoso, 1998).
This type of problem is sometimes addressed using the following approach
(MacKay, 2003) in which we ignore the temporal nature of the signals and treat the
successive samples as i.i.d. We consider a generative model in which there are two
latent variables corresponding to the unobserved speech signal amplitudes, and there
are two observed variables given by the signal values at the microphones. The latent
variables have a joint distribution that factorizes as above, and the observed variables
are given by a linear combination of the latent variables. There is no need to include
a noise distribution because the number of latent variables equals the number of ob-
served variables, and therefore the marginal distribution of the observed variables
will not in general be singular, so the observed variables are simply deterministic
linear combinations of the latent variables. Given a data set of observations, the
592
12. CONTINUOUS LATENT VARIABLES
Exercise 12.29
likelihood function for this model is a function of the coefﬁcients in the linear com-
bination. The log likelihood can be maximized using gradient-based optimization
giving rise to a particular version of independent component analysis.
The success of this approach requires that the latent variables have non-Gaussian
distributions. To see this, recall that in probabilistic PCA (and in factor analysis) the
latent-space distribution is given by a zero-mean isotropic Gaussian. The model
therefore cannot distinguish between two different choices for the latent variables
where these differ simply by a rotation in latent space. This can be veriﬁed directly
by noting that the marginal density (12.35), and hence the likelihood function, is
unchanged if we make the transformation W → WR where R is an orthogonal
matrix satisfying RRT = I, because the matrix C given by (12.36) is itself invariant.
Extending the model to allow more general Gaussian latent distributions does not
change this conclusion because, as we have seen, such a model is equivalent to the
zero-mean isotropic Gaussian latent variable model.
Another way to see why a Gaussian latent variable distribution in a linear model
is insufﬁcient to ﬁnd independent components is to note that the principal compo-
nents represent a rotation of the coordinate system in data space such as to diagonal-
ize the covariance matrix, so that the data distribution in the new coordinates is then
uncorrelated. Although zero correlation is a necessary condition for independence
it is not, however, sufﬁcient. In practice, a common choice for the latent-variable
distribution is given by
p(zj) =
1
π cosh(zj)
1
π(ezj + e−zj )
(12.90)
which has heavy tails compared to a Gaussian, reﬂecting the observation that many
real-world distributions also exhibit this property.
The original ICA model (Bell and Sejnowski, 1995) was based on the optimiza-
tion of an objective function deﬁned by information maximization. One advantage
of a probabilistic latent variable formulation is that it helps to motivate and formu-
late generalizations of basic ICA. For instance, independent factor analysis (Attias,
1999a) considers a model in which the number of latent and observed variables can
differ, the observed variables are noisy, and the individual latent variables have ﬂex-
ible distributions modelled by mixtures of Gaussians. The log likelihood for this
model is maximized using EM, and the reconstruction of the latent variables is ap-
proximated using a variational approach. Many other types of model have been
considered, and there is now a huge literature on ICA and its applications (Jutten
and Herault, 1991; Comon et al., 1991; Amari et al., 1996; Pearlmutter and Parra,
1997; Hyv¨arinen and Oja, 1997; Hinton et al., 2001; Miskin and MacKay, 2001;
Hojen-Sorensen et al., 2002; Choudrey and Roberts, 2003; Chan et al., 2003; Stone,
2004).
