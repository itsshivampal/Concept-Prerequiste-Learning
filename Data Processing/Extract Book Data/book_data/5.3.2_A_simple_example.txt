The above derivation of the backpropagation procedure allowed for general
forms for the error function, the activation functions, and the network topology. In
order to illustrate the application of this algorithm, we shall consider a particular
example. This is chosen both for its simplicity and for its practical importance, be-
cause many applications of neural networks reported in the literature make use of
this type of network. Specifically, we shall consider a two-layer network of the form
illustrated in Figure 5.1, together with a sum-of-squares error, in which the output
units have linear activation functions, so that yk = ak, while the hidden units have
logistic sigmoid activation functions given by
where
A useful feature of this function is that its derivative can be expressed in a par-
ticularly simple form:
(5.60)
We also consider a standard sum-of-squares error function, so that for pattern n the
error is given by
h(a) = 1 − h(a)2.
h(a) ≡ tanh(a)
tanh(a) = ea − e−a
ea + e−a .
En =
1
2
(yk − tk)2
w(1)
ji xi
zj = tanh(aj)
yk =
w(2)
kj zj.
j=0
(5.58)
(5.59)
(5.61)
(5.62)
(5.63)
(5.64)
where yk is the activation of output unit k, and tk is the corresponding target, for a
particular input pattern xn.
For each pattern in the training set in turn, we first perform a forward propagation
using
aj =
K
Next we compute the δ’s for each output unit using
δk = yk − tk.
Then we backpropagate these to obtain δs for the hidden units using
(5.65)
(5.66)
246
5. NEURAL NETWORKS
δj = (1 − z2
j )
k=1
wkjδk.
Finally, the derivatives with respect to the first-layer and second-layer weights are
given by
∂En
∂w(1)
ji
= δjxi,
∂En
∂w(2)
kj
= δkzj.
(5.67)
