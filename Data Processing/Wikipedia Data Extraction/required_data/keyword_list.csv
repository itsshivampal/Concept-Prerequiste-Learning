,keyword,abbreviation
0,acceptance criterion,
1,activation function,
2,active constraint,
3,adaboost,
4,adaline,
5,adaptive rejection sampling,
6,assumed density ﬁltering,
7,akaike information criterion,aic
8,akaike information criterion,
9,ancestral sampling,
10,annular ﬂow,
11,autoregressive model,ar model
12,arc,
13,automatic relevance determination,ard
14,autoregressive moving average,arma
15,assumed density ﬁltering,
16,autoassociative networks,
17,automatic relevance determination,
18,autoregressive hidden markov model,
19,autoregressive model,
20,autoregressive moving average,
21,back-tracking,
22,backgammon,
23,backpropagation,
24,bagging,
25,basis function,
26,batch training,
27,baum-welch algorithm,
28,bayes’ theorem,
29,bayes,
30,bayesian analysis,
31,bayesian information criterion,
32,bayesian model comparison,
33,bayesian network,
34,bayesian probability,
35,belief propagation,
36,bernoulli distribution,
37,bernoulli,
38,beta distribution,
39,beta recursion,
40,between-class covariance,
41,bias,
42,bias parameter,
43,bias-variance trade-off,
44,bayesian information criterion,bic
45,binary entropy,
46,binomial distribution,
47,biological sequence,
48,bipartite graph,
49,bits,
50,blind source separation,
51,blocked path,
52,boltzmann distribution,
53,boltzmann,
54,boolean logic,
55,boosting,
56,bootstrap,
57,bootstrap ﬁlter,
58,box constraints,
59,box-muller method,
60,calculus of variations,
61,canonical correlation analysis,
62,canonical link function,
63,classiﬁcation and regression trees,cart
64,cauchy distribution,
65,causality,
66,canonical correlation analysis,cca
67,central differences,
68,central limit theorem,
69,chain graph,
70,chaining,
71,chapman-kolmogorov equations,
72,child node,
73,cholesky decomposition,
74,chunking,
75,circular normal,
76,classical probability,
77,classiﬁcation,
78,classiﬁcation and regression trees,
79,clique,
80,clustering,
81,clutter problem,
82,co-parents,
83,code-book vectors,
84,combining models,
85,committee,
86,complete data set,
87,completing the square,
88,computational learning theory,
89,concave function,
90,concentration parameter,
91,condensation algorithm,
92,conditional entropy,
93,conditional expectation,
94,conditional independence,
95,conditional mixture model,
96,conditional probability,
97,conjugate prior,
98,convex duality,
99,convex function,
100,convolutional neural network,
101,correlation matrix,
102,cost function,
103,covariance,
104,covariance matrix,
105,cox’s axioms,
106,credit assignment,
107,cross-entropy error function,
108,cross-validation,
109,cumulative distribution function,
110,curse of dimensionality,
111,curve ﬁtting,
112,directed acyclic graph,dag
113,dagsvm,
114,data augmentation,
115,data compression,
116,decision boundary,
117,decision region,
118,decision surface,
119,decision theory,
120,decision tree,
121,decomposition methods,
122,degrees of freedom,
123,degrees-of-freedom parameter,
124,density estimation,
125,density network,
126,dependency map,
127,descendant node,
128,design matrix,
129,differential entropy,
130,digamma function,
131,directed acyclic graph,
132,directed cycle,
133,directed factorization,
134,dirichlet distribution,
135,dirichlet,
136,discriminant function,
137,discriminative model,
138,distortion measure,
139,distributive law of multiplication,
140,dna,
141,document retrieval,
142,dual representation,
143,dual-energy gamma densitometry,
144,dynamic programming,
145,dynamical system,
146,early stopping,
147,expectation conditional maximization,ecm
148,edge,
149,effective number of observations,
150,effective number of parameters,
151,elliptical k-means,
152,expectation maximization,em
153,emission probability,
154,empirical bayes,
155,energy function,
156,entropy,
157,expectation propagation,ep
158,equality constraint,
159,equivalent kernel,
160,erf function,
161,error backpropagation,
162,error function,
163,error-correcting output codes,
164,euler,
165,euler-lagrange equations,
166,evidence approximation,
167,evidence function,
168,expectation,
169,expectation conditional maximization,
170,expectation maximization,
171,expectation propagation,
172,expectation step,
173,explaining away,
174,exploitation,
175,exploration,
176,exponential distribution,
177,exponential family,
178,extensive variables,
179,face detection,
180,face tracking,
181,factor analysis,
182,factor graph,
183,factor loading,
184,factorial hidden markov model,
185,factorized distribution,
186,feature extraction,
187,feature map,
188,feature space,
189,fisher information matrix,
190,fisher kernel,
191,fisher’s linear discriminant,
192,forward kinematics,
193,forward problem,
194,forward propagation,
195,forward-backward algorithm,
196,fractional belief propagation,
197,frequentist probability,
198,fuel system,
199,function interpolation,
200,functional,
201,gamma densitometry,
202,gamma distribution,
203,gamma function,
204,gating function,
205,gauss,
206,gaussian,
207,gaussian kernel,
208,gaussian process,
209,gaussian random ﬁeld,
210,gaussian-gamma distribution,
211,gaussian-wishart distribution,
212,generalization,
213,generalized linear model,
214,generalized maximum likelihood,
215,generative model,
216,generative topographic mapping,
217,geodesic distance,
218,gibbs sampling,
219,gibbs,
220,gini index,
221,global minimum,
222,gradient descent,
223,gram matrix,
224,graph-cut algorithm,
225,graphical model,
226,green’s function,
227,generative topographic mapping,gtm
228,hamilton,
229,hamiltonian dynamics,
230,hamiltonian function,
231,hammersley-clifford theorem,
232,handwriting recognition,
233,handwritten digit,
234,head-to-head path,
235,head-to-tail path,
236,heaviside step function,
237,hellinger distance,
238,hessian matrix,
239,heteroscedastic,
240,hidden markov model,
241,hidden unit,
242,hidden variable,
243,hierarchical bayesian model,
244,hierarchical mixture of experts,
245,hinge error function,
246,hinton diagram,
247,histogram density estimation,
248,hierarchical mixture of experts,hme
249,hold-out set,
250,homogeneous ﬂow,
251,homogeneous kernel,
252,homogeneous markov chain,
253,hooke’s law,
254,hybrid monte carlo,
255,hyperparameter,
256,hyperprior,
257,independent identically distributed,i.i.d.
258,independent component analysis,ica
259,iterated conditional modes,icm
260,id3,
261,identiﬁability,
262,image de-noising,
263,importance sampling,
264,importance weights,
265,improper prior,
266,imputation step,
267,imputation-posterior algorithm,
268,inactive constraint,
269,incomplete data set,
270,independence map,
271,independent component analysis,
272,independent factor analysis,
273,independent identically distributed,
274,independent variables,
275,independent,
276,induced factorization,
277,inequality constraint,
278,inference,
279,information criterion,
280,information geometry,
281,information theory,
282,input-output hidden markov model,
283,intensive variables,
284,intrinsic dimensionality,
285,invariance,
286,inverse gamma distribution,
287,inverse kinematics,
288,inverse problem,
289,inverse wishart distribution,
290,Imputation posterior algorithm,ip algorithm
291,iterative reweighted least squares,irls
292,ising model,
293,isomap,
294,isometric feature map,
295,iterated conditional modes,
296,iterative reweighted least squares,
297,jacobian matrix,
298,jensen’s inequality,
299,join tree,
300,junction tree algorithm,
301,k nearest neighbours,
302,k-means clustering algorithm,
303,k-medoids algorithm,
304,kalman ﬁlter,
305,kalman gain matrix,
306,kalman smoother,
307,karhunen-lo`eve transform,
308,karush-kuhn-tucker conditions,
309,kernel density estimator,
310,kernel function,
311,kernel pca,
312,kernel regression,
313,kernel substitution,
314,kernel trick,
315,kinetic energy,
316,karush-kuhn-tucker conditions,kkt
317,kullback-leibler divergence,kl divergence
318,kriging,
319,kullback-leibler divergence,
320,lagrange multiplier,
321,lagrange,
322,lagrangian,
323,laminar ﬂow,
324,laplace approximation,
325,laplace,
326,large margin,
327,lasso,
328,latent class analysis,
329,latent trait model,
330,latent variable,
331,lattice diagram,
332,lds,
333,leapfrog discretization,
334,learning,
335,learning rate parameter,
336,least-mean-squares algorithm,
337,leave-one-out,
338,likelihood function,
339,likelihood weighted sampling,
340,linear discriminant,
341,linear dynamical system,
342,linear independence,
343,linear regression,
344,linear smoother,
345,linear-gaussian model,
346,linearly separable,
347,link,
348,link function,
349,liouville’s theorem,
350,locally linear embedding,lle
351,lms algorithm,
352,local minimum,
353,local receptive ﬁeld,
354,locally linear embedding,
355,location parameter,
356,log odds,
357,logic sampling,
358,logistic regression,
359,logistic sigmoid,
360,logit function,
361,loopy belief propagation,
362,loss function,
363,loss matrix,
364,lossless data compression,
365,lossy data compression,
366,lower bound,
367,machine learning,
368,macrostate,
369,mahalanobis distance,
370,manifold,
371,map,
372,margin,
373,marginal likelihood,
374,marginal probability,
375,markov blanket,
376,markov boundary,
377,markov chain,
378,markov chain monte carlo,
379,markov model,
380,markov network,
381,markov random ﬁeld,
382,max-sum algorithm,
383,maximal clique,
384,maximal spanning tree,
385,maximization step,
386,maximum likelihood,
387,maximum margin,
388,maximum posterior,
389,markov chain monte carlo,mcmc
390,mixture density network,mdn
391,multidimensional scaling,mds
392,mean,
393,mean ﬁeld theory,
394,mean value theorem,
395,measure theory,
396,memory-based methods,
397,message passing,
398,metropolis algorithm,
399,metropolis-hastings algorithm,
400,microstate,
401,minimum risk,
402,minkowski loss,
403,missing at random,
404,missing data,
405,mixing coefﬁcient,
406,mixture component,
407,mixture density network,
408,mixture distribution,
409,mixture model,
410,mixture of experts,
411,mixture of gaussians,
412,multilayer perceptron,mlp
413,mnist data,
414,model comparison,
415,model evidence,
416,model selection,
417,moment matching,
418,momentum variable,
419,monte carlo em algorithm,
420,monte carlo sampling,
421,moore-penrose pseudo-inverse,
422,moralization,
423,markov random ﬁeld,mrf
424,multidimensional scaling,
425,multilayer perceptron,
426,multimodality,
427,multinomial distribution,
428,multiplicity,
429,mutual information,
430,nadaraya-watson,
431,naive bayes model,
432,nats,
433,natural language modelling,
434,natural parameters,
435,nearest-neighbour methods,
436,neural network,
437,newton-raphson,
438,node,
439,noiseless coding theorem,
440,nonidentiﬁability,
441,noninformative prior,
442,nonparametric methods,
443,normal distribution,
444,normal equations,
445,normal-gamma distribution,
446,normal-wishart distribution,
447,normalized exponential,
448,novelty detection,
449,ν-svm,
450,object recognition,
451,observed variable,
452,occam factor,
453,oil ﬂow data,
454,old faithful data,
455,on-line learning,
456,one-versus-one classiﬁer,
457,one-versus-the-rest classiﬁer,
458,ordered over-relaxation,
459,ornstein-uhlenbeck process,
460,orthogonal least squares,
461,outlier,
462,outliers,
463,over-ﬁtting,
464,over-relaxation,
465,pac learning,
466,pac-bayesian framework,
467,parameter shrinkage,
468,parent node,
469,particle ﬁlter,
470,partition function,
471,parzen estimator,
472,parzen window,
473,pattern recognition,
474,principal component analysis,pca
475,pending message,
476,perceptron,
477,perceptron criterion,
478,perfect map,
479,periodic variable,
480,phase space,
481,photon noise,
482,plate,
483,polynomial curve ﬁtting,
484,polytree,
485,position variable,
486,positive deﬁnite covariance,
487,positive deﬁnite matrix,
488,positive semideﬁnite covariance,
489,positive semideﬁnite matrix,
490,posterior probability,
491,posterior step,
492,potential energy,
493,potential function,
494,power ep,
495,power method,
496,precision matrix,
497,precision parameter,
498,predictive distribution,
499,preprocessing,
500,principal component analysis,
501,principal curve,
502,principal subspace,
503,principal surface,
504,prior,
505,probabilistic graphical model,
506,probabilistic pca,
507,probability,
508,probably approximately correct,
509,probit function,
510,probit regression,
511,product rule of probability,
512,proposal distribution,
513,protected conjugate gradients,
514,protein sequence,
515,pseudo-inverse,
516,pseudo-random numbers,
517,quadratic discriminant,
518,quality parameter,
519,radial basis function,
520,rauch-tung-striebel equations,
521,regression,
522,regression function,
523,regularization,
524,regularized least squares,
525,reinforcement learning,
526,reject option,
527,rejection sampling,
528,relative entropy,
529,relevance vector,
530,relevance vector machine,
531,responsibility,
532,ridge regression,
533,root-mean-square error,rms error
534,robbins-monro algorithm,
535,robot arm,
536,robustness,
537,root node,
538,root-mean-square error,
539,rosenblatt,
540,rotation invariance,
541,rauch-tung-striebel equations,rts equations
542,running intersection property,
543,relevance vector machine,rvm
544,sample mean,
545,sample variance,
546,sampling-importance-resampling,
547,scale invariance,
548,scale parameter,
549,scaling factor,
550,schwarz criterion,
551,self-organizing map,
552,sequential data,
553,sequential estimation,
554,sequential gradient descent,
555,sequential learning,
556,sequential minimal optimization,
557,serial message passing schedule,
558,shannon,
559,shared parameters,
560,shrinkage,
561,shur complement,
562,sigmoid,
563,simplex,
564,single-class support vector machine,
565,singular value decomposition,
566,sinusoidal data,
567,sampling-importance-resampling,sir
568,skip-layer connection,
569,slack variable,
570,slice sampling,
571,sequential minimal optimization,smo
572,smoother matrix,
573,smoothing parameter,
574,soft margin,
575,soft weight sharing,
576,softmax function,
577,self-organizing map,som
578,sparsity,
579,sparsity parameter,
580,spectrogram,
581,speech recognition,
582,sphereing,
583,spline functions,
584,standard deviation,
585,standardizing,
586,state space model,
587,stationary kernel,
588,statistical bias,
589,statistical independence,
590,statistical learning theory,
591,steepest descent,
592,stirling’s approximation,
593,stochastic,
594,stochastic em,
595,stochastic gradient descent,
596,stochastic process,
597,stratiﬁed ﬂow,
598,student’s t-distribution,
599,subsampling,
600,sufﬁcient statistics,
601,sum rule of probability,
602,sum-of-squares error,
603,sum-product algorithm,
604,supervised learning,
605,support vector,
606,support vector machine,
607,survival of the ﬁttest,
608,singular value decomposition,svd
609,support vector machine,svm
610,switching hidden markov model,
611,switching state space model,
612,synthetic data sets,
613,tail-to-tail path,
614,tangent distance,
615,tangent propagation,
616,tapped delay line,
617,target vector,
618,test set,
619,threshold parameter,
620,tied parameters,
621,tikhonov regularization,
622,time warping,
623,tomography,
624,training,
625,training set,
626,transition probability,
627,translation invariance,
628,tree-reweighted message passing,
629,treewidth,
630,trellis diagram,
631,triangulated graph,
632,type 2 maximum likelihood,
633,undetermined multiplier,
634,undirected graph,
635,uniform distribution,
636,uniform sampling,
637,uniquenesses,
638,unobserved variable,
639,unsupervised learning,
640,utility function,
641,validation set,
642,vapnik-chervonenkis dimension,
643,variance,
644,variational inference,
645,vapnik-chervonenkis dimension,vc dimension
646,soft,
647,between-class covariance,
648,within-class covariance,
649,partitioned covariance matrix,
650,conditional entropy,
651,differential entropy,
652,relative entropy,
653,functional derivative,
654,conditional gaussian,
655,gaussian marginal,
656,gaussian mixture,
657,directed graphical model,
658,undirected graphical model,
659,autoregressive hidden markov model,
660,factorial hidden markov model,
661,input-output hidden markov model,
662,left-to-right hidden markov model,
663,gaussian kernel function,
664,fisher linear discriminant,
665,variational linear regression,
666,bayesian logistic regression,
667,multiclass logistic regression,
668,margin error,
669,soft margin,
670,homogeneous markov chain,
671,message passing schedule,
672,variational message passing,
673,conditional mixture model,
674,convolutional neural network,
675,perceptron convergence theorem,
676,perceptron hardware,
677,conjugate prior,
678,consistent gaussian prior,
679,improper prior,
680,noninformative prior,
681,bayesian probability,
682,probability density,
683,probability mass function,
684,prior probability,
685,probability sum rule,
686,probability theory,
687,tikhonov regularization,
688,hierarchical bayesian analysis,
689,model averaging bayesian analysis,
690,mixture model bernoulli distribution,
691,diagonal covariance matrix,
692,isotropic covariance matrix,
693,positive deﬁnite covariance matrix,
694,gaussian mixture expectation maximization,
695,generalized expectation maximization,
696,sampling methods expectation maximization,
697,mixture model factor analysis,
698,maximum likelihood gaussian,
699,sequential estimation gaussian,
700,sufﬁcient statistics gaussian,
701,wrapped gaussian,
702,directional curvature generative topographic mapping,
703,magniﬁcation factor generative topographic mapping,
704,blocking gibbs sampling,
705,bipartite graphical model,
706,factorization graphical model,
707,fully connected graphical model,
708,inference graphical model,
709,tree graphical model,
710,treewidth graphical model,
711,triangulated graphical model,
712,diagonal approximation hessian matrix,
713,exact evaluation hessian matrix,
714,fast multiplication hessian matrix,
715,ﬁnite differences hessian matrix,
716,inverse hessian matrix,
717,outer product approximation hessian matrix,
718,forward-backward algorithm hidden markov model,
719,maximum likelihood hidden markov model,
720,scaling factor hidden markov model,
721,sum-product algorithm hidden markov model,
722,switching hidden markov model,
723,variational inference hidden markov model,
724,extended kalman ﬁlter,
725,fisher kernel function,
726,homogeneous kernel function,
727,nonvectorial inputs kernel function,
728,stationary kernel function,
729,inference linear dynamical system,
730,em linear regression,
731,mixture model linear regression,
732,mixture model logistic regression,
733,ﬁrst order markov chain,
734,second order markov chain,
735,homogeneous markov model,
736,gaussian mixture maximum likelihood,
737,singularities maximum likelihood,
738,type 2 maximum likelihood,
739,pending message message passing,
740,linear regression mixture model,
741,logistic regression mixture model,
742,symmetries mixture model,
743,regularization neural network,
744,relation to gaussian process neural network,
745,bayesian principal component analysis,
746,em algorithm principal component analysis,
747,gibbs sampling principal component analysis,
748,mixture distribution principal component analysis,
749,physical analogy principal component analysis,
750,classical probability,
751,frequentist probability,
752,product rule probability,
753,switching state space model,
754,for hidden markov model sum-product algorithm,
755,for regression support vector machine,
756,multiclass support vector machine,
757,for gaussian mixture variational inference,
758,for hidden markov model variational inference,
759,local variational inference,
760,vector quantization vapnik-chervonenkis dimension,
761,vertex vapnik-chervonenkis dimension,
762,visualization vapnik-chervonenkis dimension,
763,viterbi algorithm vapnik-chervonenkis dimension,
764,von mises distribution vapnik-chervonenkis dimension,
765,wavelets vapnik-chervonenkis dimension,
766,weak learner vapnik-chervonenkis dimension,
767,weight decay vapnik-chervonenkis dimension,
768,weight parameter vapnik-chervonenkis dimension,
769,weight sharing vapnik-chervonenkis dimension,
770,weight vector soft,
771,weight-space symmetry soft,
772,weighted least squares soft,
773,well-determined parameters soft,
774,whitening soft,
775,wishart distribution soft,
776,within-class covariance soft,
777,woodbury identity soft,
778,wrapped distribution soft,
779,yellowstone national park soft,
