Absolute value rectiﬁcation, 192
Accuracy, 425
Activation function, 170
Active constraint, 95
AdaGrad, 307
ADALINE, see adaptive linear element
Adam,
Adaptive linear element,
Adversarial example, 268
Adversarial training,
Aﬃne, 110
AIS, see annealed importance sampling
Almost everywhere, 71
Almost sure convergence, 130
Ancestral sampling
ANN, see Artiﬁcial neural network
Annealed importance sampling, 
Approximate Bayesian computation, 718
Approximate inference, 585
Artiﬁcial intelligence, 1
Artiﬁcial neural network, see Neural network
Bag of words, 473
Bagging, 256
Batch normalization,
Bayes error, 117
Bayes’ rule, 70
Bayesian hyperparameter optimization, 438
Bayesian network, see directed graphical model
Bayesian probability, 55
Bayesian statistics, 135
Belief network, see directed graphical model
Bernoulli distribution, 62
BFGS, 316
Bias,
Bias parameter, 110
Biased importance sampling, 595
Bigram, 464
Binary relation, 484
Block Gibbs sampling, 601
Boltzmann distribution, 572
Boltzmann machine,
BPTT, see back-propagation through time
Broadcasting, 34
Burn-in, 599
ASR, see automatic speech recognition
Asymptotically unbiased, 124
Audio,
Autoencoder,
Automatic speech recognition, 460
Back-propagation, 203
Back-propagation through time, 384
Backprop, see back-propagation
CAE, see contractive autoencoder
Calculus of variations, 179
Categorical distribution, see multinoulli distribution
CD, see contrastive divergence
Centering trick (DBM), 675
Central limit theorem, 63
Chain rule (calculus), 206
Chain rule of probability, 59
Chess, 2
Chord, 581
Chordal graph, 581
Class-based language models, 465
Classical dynamical system, 375
Classiﬁcation, 100
Clique potential, see factor (graphical model)
CNN, see convolutional neural network
Collaborative Filtering, 480
Collider, see explaining away
Color images, 360
Complex cell, 365
Computational graph, 204
Computer vision, 454
Concept drift, 540
Condition number, 279
Conditional computation, see dynamic structure
Conditional independence,
Conditional probability, 59
Conditional RBM, 687
Connectionism,17 445
Connectionist temporal classiﬁcation, 462
Consistency,
Constrained optimization,
Content-based addressing, 421
Content-based recommender systems, 482
Context-speciﬁc independence, 575
Contextual bandits, 482
Continuation methods, 327
Contractive autoencoder, 523
Contrast, 456
Contrastive divergence,
Convex optimization, 141
Convolution,
Convolutional network, 16
Convolutional neural network,
Coordinate descent,
Correlation, 61
Cost function, see objective function
Covariance,
Covariance matrix, 62
Coverage, 426
Critical temperature, 605
Cross-correlation, 332
Cross-entropy,,75 132
Cross-validation, 122
CTC, see connectionist temporal classiﬁcation
Curriculum learning, 328
Curse of dimensionality, 154
Cyc, 2
DAE, see denoising autoencoder
Data generating distribution,
Data generating process, 111
Data parallelism, 449
Dataset, 105
Dataset augmentation,
DBM, see deep Boltzmann machine
DCGAN,
Decision tree,
Decoder, 4
Deep belief network,
Deep Blue, 2
Deep Boltzmann machine,
Deep feedforward network,
Deep learning,
Denoising autoencoder,
Denoising score matching, 621
Density estimation, 103
Derivative,
Design matrix, 106
Detector layer, 339
Determinant, xii
Diagonal matrix, 41
Diﬀerential entropy,
Dirac delta function, 65
Directed graphical model,
Directional derivative, 85
Discriminative ﬁne-tuning, see supervised ﬁne-tuning
Discriminative RBM, 688
Distributed representation,
Domain adaptation
Dot product
Double backprop, 271
Doubly block circulant matrix, 333
Dream sleep,
DropConnect, 266
Dropout,
Dynamic structure,
Early stopping,
EBM, see energy-based model
24 27 405
Echo state network,
Eﬀective capacity, 114
Eigendecomposition, 42
Eigenvalue, 42
Eigenvector, 42
ELBO, see evidence lower bound
Element-wise product, see Hadamard product, see Hadamard product
Embedding, 518
Empirical distribution, 66
Empirical risk, 276
Empirical risk minimization, 276
Encoder, 4
Energy function, 571
Energy-based model,
Ensemble methods, 256
Epoch, 246
Equality constraint, 94
Equivariance, 338
Error function, see objective function
ESN, see echo state network
Euclidean norm, 39
Euler-Lagrange equation, 648
Evidence lower bound,
Example, 99
Expectation, 60
Expectation maximization, 636
Expected value, see expectation
Explaining away,
Exploitation, 483
Exploration, 483
Exponential distribution,
F-score, 425
Factor (graphical model), 569
Factor analysis, 492
Factor graph, 581
Factors of variation, 4
Feature, 99
Feature selection, 236
Feedforward neural network, 167
Fine-tuning, 323
Finite diﬀerences, 441
Forget gate, 306
Forward propagation, 203
Fourier transform,
Fovea, 366
FPCD, 616
Free energy,
Freebase, 485
Frequentist probability, 55
Frequentist statistics, 135
Frobenius norm, 46
Fully-visible Bayes network, 707
Functional derivatives, 647
FVBN, see fully-visible Bayes network
Gabor function, 368
GANs, see generative adversarial networks
Gated recurrent unit, 427
Gaussian distribution, see normal distribution
Gaussian kernel, 142
Gaussian mixture,
GCN, see global contrast normalization
GeneOntology, 485
Generalization, 110
Generalized Lagrange function, see generalized Lagrangian
Generalized Lagrangian, 94
Generative adversarial networks,
Generative moment matching networks, 705
Generator network, 695
Gibbs distribution, 570
Gibbs sampling,
Global contrast normalization, 456
GPU, see graphics processing unit
Gradient, 84
Gradient clipping,
Gradient descent,
Graph, xii
Graphical model, see structured probabilistic model
Graphics processing unit, 446
Greedy algorithm, 323
Greedy layer-wise unsupervised pretraining,
Greedy supervised pretraining, 323
Grid search, 434
Hadamard product,
Hard tanh 196
Harmonium, see restricted Boltzmann machine
Harmony theory, 573
Helmholtz free energy, see evidence lower bound
Hessian, 223
Hessian matrix,
Heteroscedastic, 187
Hidden layer
Hill climbing, 86
Hyperparameter optimization, 434
Hyperparameters,
Hypothesis space,
i.i.d. assumptions,
Identity matrix, 36
ILSVRC, see ImageNet Large-Scale Visual Recognition Challenge
ImageNet Large-Scale Visual Recognition Challenge, 23
Immorality, 579
Importance sampling,
Importance weighted autoencoder, 700
Independence,
Independent and identically distributed, see i.i.d. assumptions
Independent component analysis, 493
Independent subspace analysis, 495
Inequality constraint, 94
Inference
Information retrieval, 527
Initialization, 301
Integral, xiii
Invariance, 342
Isotropic, 65
Jacobian matrix,
Joint probability, 57
k-means,
k-nearest neighbors,
Karush-Kuhn-Tucker conditions,
Karush–Kuhn–Tucker, 94
Kernel (convolution),
Kernel machine, 550
Kernel trick, 141
KKT, see Karush–Kuhn–Tucker
KKT conditions, see Karush-Kuhn-Tucker conditions
KL divergence, see Kullback-Leibler divergence
Knowledge base,
Krylov methods, 223
Kullback-Leibler divergence,
Label smoothing, 243
Lagrange multipliers,
Lagrangian, see generalized Lagrangian
LAPGAN, 704
Laplace distribution,
Latent variable, 67
Layer (neural network), 167
LCN, see local contrast normalization
Leaky ReLU, 192
Leaky units, 408
Learning rate, 85
Line search
Linear combination, 37
Linear dependence, 38
Linear factor models, 491
Linear regression,
Link prediction, 486
Lipschitz constant, 92
Lipschitz continuous, 92
Liquid state machine, 405
Local conditional probability distribution
Local contrast normalization, 458
Logistic regression,
Logistic sigmoid,
Long short-term memory,
Loop, 581
Loopy belief propagation, 587
Loss function, see objective function
Lp norm, 39
LSTM, see long short-term memory
M-step, 636
Machine learning, 2
Machine translation, 101
Main diagonal, 33
Manifold, 160
Manifold hypothesis, 161
Manifold learning, 161
Manifold tangent classiﬁer, 272
MAP approximation,
,138 507
Marginal probability, 58
Markov chain, 597
Markov chain Monte Carlo, 597
Markov network, see undirected model
Markov random ﬁeld, see undirected model
Matrix
Matrix inverse, 36
Matrix product, 34
Max norm, 40
Max pooling, 339
Maximum likelihood, 131
Maxout,
MCMC, see Markov chain Monte Carlo
Mean ﬁeld,
Mean squared error, 108
Measure theory, 71
Measure zero, 71
Memory network,
Method of steepest descent, see gradient descent
Minibatch, 279
Missing inputs, 100
Mixing (Markov chain), 603
Mixture density networks, 188
Mixture distribution, 
Mixture model,
Mixture of experts
MLP, see multilayer perception
MNIST,
Model averaging, 256
Model compression, 450
Model identiﬁability, 284
Model parallelism, 449
Moment matching, 705
Moore-Penrose pseudoinverse,
Moralized graph, 579
MP-DBM, see multi-prediction DBM
MRF (Markov Random Field), see undirected model
MSE, see mean squared error
Multi-modal learning, 541
Multi-prediction DBM, 676
Multi-task learning,
Multilayer perception, 5
Multilayer perceptron, 27
Multinomial distribution, 62
Multinoulli distribution, 62
n-gram, 463
NADE, 710
Naive Bayes, 3
Nat, 73
Natural image, 561
Natural language processing, 463
Nearest neighbor regression, 115
Negative deﬁnite, 89
Negative phase,
Neocognitron,
Nesterov momentum, 300
Netﬂix Grand Prize,
Neural language model,
Neural network, 13
Neural Turing machine, 420
Neuroscience, 15
Newton’s method,
NLM, see neural language model
NLP, see natural language processing
No free lunch theorem, 116
Noise-contrastive estimation, 622
Non-parametric model, 114
Norm
Normal distribution,
Normal equations,
Normalized initialization, 303
Numerical diﬀerentiation, see ﬁnite diﬀerences
Object detection, 455
Object recognition, 455
Objective function, 82
OMP- ,k see orthogonal matching pursuit
One-shot learning, 540
Operation, 204
Optimization,
Orthodox statistics, see frequentist statistics
Orthogonal matching pursuit,
Orthogonal matrix, 42
Orthogonality, 41
Output layer, 167
Parallel distributed processing, 17
,301 407
Parameter initialization,
Parameter sharing
Parameter tying, see Parameter sharing
Parametric model, 114
Parametric ReLU, 192
Partial derivative, 84
Partition function,
PCA, see principal components analysis
PCD, see stochastic maximum likelihood
Perceptron,
Persistent contrastive divergence, see stochastic maximum likelihood
Perturbation analysis, see reparametrization trick
Point estimator, 122
Policy, 482
Pooling,
Positive deﬁnite, 89
Positive phase,
Precision, 425
Precision (of a normal distribution),
Predictive sparse decomposition, 525
Preprocessing, 455
Pretraining,
Primary visual cortex, 365
Principal components analysis,
Prior probability distribution, 135
Probabilistic max pooling, 685
Probabilistic PCA,
492 493 634
Probability density function, 58
Probability distribution, 56
Probability mass function, 56
Probability mass function estimation, 103
Product of experts, 572
Product rule of probability, see chain rule of probability
PSD, see predictive sparse decomposition
Pseudolikelihood, 617
Quadrature pair, 369
Quasi-Newton methods
Radial basis function, 196
Random search, 436
Random variable, 56
Ratio matching, 620
RBF, 196
RBM, see restricted Boltzmann machine
Recall, 425
Receptive ﬁeld, 337
Recommender Systems, 480
Rectiﬁed linear unit,
Recurrent network, 27
Recurrent neural network, 378
Regression, 101
Regularization,
Regularizer, 119
REINFORCE, 691
Reinforcement learning,
Relational database, 485
Reparametrization trick, 690
Representation learning, 3
Representational capacity, 114
Restricted Boltzmann machine
Ridge regression, see weight decay
Risk, 275
RNN-RBM, 687
Saddle points, 285
Sample mean, 125
Scalar,
Score matching,
Second derivative, 86
Second derivative test, 89
Self-information, 73
Semantic hashing, 527
Semi-supervised learning, 243
Separable convolution, 362
Separation (probabilistic modeling), 574
Set, xii
SGD, see stochastic gradient descent
Shannon entropy,
Shortlist, 468
Sigmoid,
Sigmoid belief network, 27
Simple cell, 365
Singular value, see singular value decomposition
Singular value decomposition,
Singular vector, see singular value decomposition
Slow feature analysis, 495
SML, see stochastic maximum likelihood
Softmax,
Softplus
Spam detection,
Sparse coding,
Sparse initialization,
Sparse representation,
Spearmint, 438
Spectral radius, 406
Speech recognition, see automatic speech recognition
Sphering, see whitening
Spike and slab restricted Boltzmann machine, 682
SPN, see sum-product network
Square matrix, 38
ssRBM, see spike and slab restricted Boltzmann machine
Standard deviation, 61
Standard error, 127
Standard error of the mean,
Statistic, 122
Statistical learning theory, 110
Steepest descent, see gradient descent
Stochastic back-propagation, see reparametrization trick
Stochastic gradient descent,
Stochastic maximum likelihood,
Stochastic pooling, 266
Structure learning, 584
Structured output,
Structured probabilistic model,
Sum rule of probability, 58
Sum-product network, 555
Supervised ﬁne-tuning,
Supervised learning, 105
Support vector machine, 140
Surrogate loss function, 276
SVD, see singular value decomposition
Symmetric matrix,
Tangent distance, 270
Tangent plane, 518
Tangent prop, 270
TDNN, see time-delay neural network
Teacher forcing,
Tempering, 605
Template matching, 141
Tensor,
Test set, 110
Tikhonov regularization, see weight decay
Tiled convolution, 352
Time-delay neural network,
Toeplitz matrix, 333
Topographic ICA, 495
Trace operator, 46
Training error, 110
Transcription, 101
Transfer learning, 538
Transpose,
Triangle inequality, 39
Triangulated graph, see chordal graph
Trigram, 464
Unbiased, 124
Undirected graphical model,
Undirected model, 568
Uniform distribution, 57
Unigram, 464
Unit norm, 41
Unit vector, 41
Universal approximation theorem, 197
Universal approximator, 555
Unnormalized probability distribution, 569
Unsupervised learning,
Unsupervised pretraining
V-structure, see explaining away
V1, 365
VAE, see variational autoencoder
Vapnik-Chervonenkis dimension, 114
Variance,
Variational autoencoder,
Variational derivatives, see functional derivtives
Variational free energy, see evidence lower bound
VC dimension, see Vapnik-Chervonenkis dimension
Vector,
Virtual adversarial examples, 269
Visible layer, 6
Volumetric data, 360
Wake-sleep,
Weight decay,
Weight space symmetry, 284
Weights,
Whitening, 457
Wikibase, 485
Wikibase, 485
Word embedding, 466
Word-sense disambiguation, 486
WordNet, 485
Zero-data learning, see zero-shot learning
Zero-shot learning, 540
