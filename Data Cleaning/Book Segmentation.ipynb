{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from num2words import num2words\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shivam/Documents/Personalise Learning Thesis/Data Cleaning/../Processed_Data/Book Segmentation/book_data.csv\n"
     ]
    }
   ],
   "source": [
    "dirpath = os.getcwd()\n",
    "required_file = dirpath + \"/../Processed_Data/Book Segmentation/book_data.csv\"\n",
    "print(required_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(required_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0 Section                                    Title  \\\n",
      "0             0       1                             Introduction   \n",
      "1             1     1.1        Example: Polynomial Curve Fitting   \n",
      "2             2     1.2                       Probability Theory   \n",
      "3             3   1.2.1                    Probability densities   \n",
      "4             4   1.2.2             Expectations and covariances   \n",
      "5             5   1.2.3                   Bayesian probabilities   \n",
      "6             6   1.2.4                The Gaussian distribution   \n",
      "7             7   1.2.5                 Curve fitting re-visited   \n",
      "8             8   1.2.6                   Bayesian curve fitting   \n",
      "9             9     1.3                          Model Selection   \n",
      "10           10     1.4              The Curse of Dimensionality   \n",
      "11           11     1.5                          Decision Theory   \n",
      "12           12   1.5.1    Minimizing the misclassification rate   \n",
      "13           13   1.5.2             Minimizing the expected loss   \n",
      "14           14   1.5.3                        The reject option   \n",
      "15           15   1.5.4                   Inference and decision   \n",
      "16           16   1.5.5            Loss functions for regression   \n",
      "17           17     1.6                       Information Theory   \n",
      "18           18   1.6.1  Relative entropy and mutual information   \n",
      "19           19       2                Probability Distributions   \n",
      "20           20     2.1                         Binary Variables   \n",
      "21           21   2.1.1                    The beta distribution   \n",
      "22           22     2.2                    Multinomial Variables   \n",
      "23           23   2.2.1               The Dirichlet distribution   \n",
      "24           24     2.3                The Gaussian Distribution   \n",
      "25           25   2.3.1       Conditional Gaussian distributions   \n",
      "26           26   2.3.2          Marginal Gaussian distributions   \n",
      "27           27   2.3.3    Bayes’ theorem for Gaussian variables   \n",
      "28           28   2.3.4      Maximum likelihood for the Gaussian   \n",
      "29           29   2.3.5                    Sequential estimation   \n",
      "..          ...     ...                                      ...   \n",
      "215         215    11.4                           Slice Sampling   \n",
      "216         216    11.5         The Hybrid Monte Carlo Algorithm   \n",
      "217         217  11.5.1                        Dynamical systems   \n",
      "218         218  11.5.2                       Hybrid Monte Carlo   \n",
      "219         219    11.6        Estimating the Partition Function   \n",
      "220         220      13                          Sequential Data   \n",
      "221         221    13.1                            Markov Models   \n",
      "222         222    13.2                     Hidden Markov Models   \n",
      "223         223  13.2.1           Maximum likelihood for the HMM   \n",
      "224         224  13.2.2           The forward-backward algorithm   \n",
      "225         225  13.2.3    The sum-product algorithm for the HMM   \n",
      "226         226  13.2.4                          Scaling factors   \n",
      "227         227  13.2.5                    The Viterbi algorithm   \n",
      "228         228  13.2.6    Extensions of the hidden Markov model   \n",
      "229         229    13.3                 Linear Dynamical Systems   \n",
      "230         230  13.3.1                         Inference in LDS   \n",
      "231         231  13.3.2                          Learning in LDS   \n",
      "232         232  13.3.3                        Extensions of LDS   \n",
      "233         233  13.3.4                         Particle filters   \n",
      "234         234      14                         Combining Models   \n",
      "235         235    14.1                 Bayesian Model Averaging   \n",
      "236         236    14.2                               Committees   \n",
      "237         237    14.3                                 Boosting   \n",
      "238         238  14.3.1             Minimizing exponential error   \n",
      "239         239  14.3.2             Error functions for boosting   \n",
      "240         240    14.4                        Tree-based Models   \n",
      "241         241    14.5               Conditional Mixture Models   \n",
      "242         242  14.5.1     Mixtures of linear regression models   \n",
      "243         243  14.5.2              Mixtures of logistic models   \n",
      "244         244  14.5.3                      Mixtures of experts   \n",
      "\n",
      "                                               Content  \n",
      "0    1 Introduction. The problem of searching for p...  \n",
      "1    We begin by introducing a simple regression pr...  \n",
      "2    A key concept in the field of pattern recognit...  \n",
      "3    As well as considering probabilities defined o...  \n",
      "4    One of the most important operations involving...  \n",
      "5    So far in this chapter, we have viewed probabi...  \n",
      "6    We shall devote the whole of Chapter 2 to a st...  \n",
      "7    We have seen how the problem of polynomial cur...  \n",
      "8    Although we have included a prior distribution...  \n",
      "9    1. x. In our example of polynomial curve fitti...  \n",
      "10   33. Figure 1.18 The technique of S-fold cross-...  \n",
      "11   We have seen in Section 1.2 how probability th...  \n",
      "12   Suppose that our goal is simply to make as few...  \n",
      "13   For many applications, our objective will be m...  \n",
      "14   We have seen that classification errors arise ...  \n",
      "15   We have broken the classification problem down...  \n",
      "16   So far, we have discussed decision theory in t...  \n",
      "17   49. q = 1. −1. 0. y − t. 1. 2. q = 10. −1. 0. ...  \n",
      "18   So far in this section, we have introduced a n...  \n",
      "19   In Chapter 1, we emphasized the central role p...  \n",
      "20   We begin by considering a single binary random...  \n",
      "21   We have seen in (2.8) that the maximum likelih...  \n",
      "22   Binary variables can be used to describe quant...  \n",
      "23   We now introduce a family of prior distributio...  \n",
      "24   The Gaussian, also known as the normal distrib...  \n",
      "25   An important property of the multivariate Gaus...  \n",
      "26   We have seen that if a joint distribution p(xa...  \n",
      "27   In Sections 2.3.1 and 2.3.2, we considered a G...  \n",
      "28   Given a data set X = (x1, . . . , xN )T in whi...  \n",
      "29   Our discussion of the maximum likelihood solut...  \n",
      "..                                                 ...  \n",
      "215  We have seen that one of the difficulties with...  \n",
      "216  As we have already noted, one of the major lim...  \n",
      "217  explain how this may be combined with the Metr...  \n",
      "218  for achieving this is called the leapfrog disc...  \n",
      "219  As we have seen, most of the sampling algorith...  \n",
      "220  So far in this book, we have focussed primaril...  \n",
      "221  Figure 13.2 The simplest approach to. modellin...  \n",
      "222  The hidden Markov model can be viewed as a spe...  \n",
      "223  If we have observed a data set X = {x1, . . . ...  \n",
      "224  Next we seek an efficient procedure for evalua...  \n",
      "225  The directed graph that represents the hidden ...  \n",
      "226  There is an important issue that must be addre...  \n",
      "227  In many applications of hidden Markov models, ...  \n",
      "228  The basic hidden Markov model, along with the ...  \n",
      "229  635. and so we can transform the model into an...  \n",
      "230  We now turn to the problem of finding the marg...  \n",
      "231  So far, we have considered the inference probl...  \n",
      "232  As with the hidden Markov model, there is cons...  \n",
      "233  For dynamical systems which do not have a line...  \n",
      "234  In earlier chapters, we have explored a range ...  \n",
      "235  Section 9.2. It is important to distinguish be...  \n",
      "236  655. p(x) =. πkN (x|µk, Σk). (14.4). with the ...  \n",
      "237  657. ECOM =. 1. M. EAV.. (14.14). This apparen...  \n",
      "238  Boosting was originally motivated using statis...  \n",
      "239  The exponential error function that is minimiz...  \n",
      "240  663. E(z). can be addressed by basing the boos...  \n",
      "241  We have seen that standard decision trees are ...  \n",
      "242  Chapter 9. 666. 14. COMBINING MODELS. of perfo...  \n",
      "243  Because the logistic regression model defines ...  \n",
      "244  In Section 14.5.1, we considered a mixture of ...  \n",
      "\n",
      "[245 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
