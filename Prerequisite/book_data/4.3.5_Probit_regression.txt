We have seen that, for a broad range of class-conditional distributions, described
by the exponential family, the resulting posterior class probabilities are given by a
logistic (or softmax) transformation acting on a linear function of the feature vari-
ables. However, not all choices of class-conditional density give rise to such a simple
form for the posterior probabilities (for instance, if the class-conditional densities are
modelled using Gaussian mixtures). This suggests that it might be worth exploring
other types of discriminative probabilistic model. For the purposes of this chapter,
however, we shall return to the two-class case, and again remain within the frame-
work of generalized linear models so that
p(t = 1|a) = f(a)
where a = wTφ, and f(·) is the activation function.
One way to motivate an alternative choice for the link function is to consider a
noisy threshold model, as follows. For each input φn, we evaluate an = wTφn and
then we set the target value according to
(4.111)
tn = 1 if an � θ
tn = 0 otherwise.
(4.112)
a
4.3. Probabilistic Discriminative Models
211
Figure 4.13 Schematic example of a probability density p(θ)
shown by the blue curve, given in this example by a mixture
of two Gaussians, along with its cumulative distribution function
f (a), shown by the red curve. Note that the value of the blue
curve at any point, such as that indicated by the vertical green
line, corresponds to the slope of the red curve at the same point.
Conversely, the value of the red curve at this point corresponds
to the area under the blue curve indicated by the shaded green
region. In the stochastic threshold model, the class label takes
the value t = 1 if the value of a = wTφ exceeds a threshold, oth-
erwise it takes the value t = 0. This is equivalent to an activation
function given by the cumulative distribution function f (a).
1
0.8
0.6
0.4
0.2
0
0
1
2
3
4
If the value of θ is drawn from a probability density p(θ), then the corresponding
activation function will be given by the cumulative distribution function
f(a) =
p(θ) dθ
a
as illustrated in Figure 4.13.
As a specific example, suppose that the density p(θ) is given by a zero mean,
unit variance Gaussian. The corresponding cumulative distribution function is given
by
Φ(a) =
N (θ|0, 1) dθ
which is known as the probit function. It has a sigmoidal shape and is compared
with the logistic sigmoid function in Figure 4.9. Note that the use of a more gen-
eral Gaussian distribution does not change the model because this is equivalent to
a re-scaling of the linear coefficients w. Many numerical packages provide for the
evaluation of a closely related function defined by
(4.113)
(4.114)
erf(a) =
2
a
0
exp(−θ2/2) dθ
(4.115)
Exercise 4.21
and known as the erf function or error function (not to be confused with the error
function of a machine learning model). It is related to the probit function by
Φ(a) =
1
2
1 +
1
√2
erf(a)
(4.116)
The generalized linear model based on a probit activation function is known as probit
regression.
We can determine the parameters of this model using maximum likelihood, by a
straightforward extension of the ideas discussed earlier. In practice, the results found
using probit regression tend to be similar to those of logistic regression. We shall,
212
4. LINEAR MODELS FOR CLASSIFICATION
however, find another use for the probit model when we discuss Bayesian treatments
of logistic regression in Section 4.5.
One issue that can occur in practical applications is that of outliers, which can
arise for instance through errors in measuring the input vector x or through misla-
belling of the target value t. Because such points can lie a long way to the wrong side
of the ideal decision boundary, they can seriously distort the classifier. Note that the
logistic and probit regression models behave differently in this respect because the
tails of the logistic sigmoid decay asymptotically like exp(−x) for x → ∞, whereas
for the probit activation function they decay like exp(−x2), and so the probit model
can be significantly more sensitive to outliers.
However, both the logistic and the probit models assume the data is correctly
labelled. The effect of mislabelling is easily incorporated into a probabilistic model
by introducing a probability 	 that the target value t has been flipped to the wrong
value (Opper and Winther, 2000a), leading to a target value distribution for data point
x of the form
p(t|x) = (1 − 	)σ(x) + 	(1 − σ(x))
(4.117)
where σ(x) is the activation function with input vector x. Here 	 may be set in
advance, or it may be treated as a hyperparameter whose value is inferred from the
data.
= 	 + (1 − 2	)σ(x)
