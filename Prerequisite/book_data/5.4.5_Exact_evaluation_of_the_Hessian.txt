So far, we have considered various approximation schemes for evaluating the
Hessian matrix or its inverse. The Hessian can also be evaluated exactly, for a net-
work of arbitrary feed-forward topology, using extension of the technique of back-
propagation used to evaluate first derivatives, which shares many of its desirable
features including computational efficiency (Bishop, 1991; Bishop, 1992). It can be
applied to any differentiable error function that can be expressed as a function of
the network outputs and to networks having arbitrary differentiable activation func-
tions. The number of computational steps needed to evaluate the Hessian scales
like O(W 2). Similar algorithms have also been considered by Buntine and Weigend
(1993).
Here we consider the specific case of a network having two layers of weights,
for which the required equations are easily derived. We shall use indices i and i
to denote inputs, indices j and j to denoted hidden units, and indices k and k to
denote outputs. We first define
δk = ∂En
∂ak
Mkk ≡
∂2En
∂ak∂ak
(5.92)
where En is the contribution to the error from data point n. The Hessian matrix for
this network can then be considered in three separate blocks as follows.
1. Both weights in the second layer:
∂2En
kj ∂w(2)
kj
∂w(2)
= zjzjMkk.
(5.93)
Exercise 5.22
k
k
254
5. NEURAL NETWORKS
2. Both weights in the first layer:
∂w(1)
= xixih(aj)Ijj
∂2En
ji ∂w(1)
ji
+xixih(aj)h(aj)
w(2)
kjδk
k
w(2)
kjw(2)
kj Mkk.
(5.94)
3. One weight in each layer:
∂2En
ji ∂w(2)
kj
∂w(1)
= xih(aj)
δkIjj + zj
w(2)
kjHkk
(5.95)
k
Exercise 5.23
Here Ijj is the j, j element of the identity matrix. If one or both of the weights is
a bias term, then the corresponding expressions are obtained simply by setting the
appropriate activation(s) to 1. Inclusion of skip-layer connections is straightforward.
