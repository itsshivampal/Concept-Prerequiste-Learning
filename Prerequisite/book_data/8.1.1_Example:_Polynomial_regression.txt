As an illustration of the use of directed graphs to describe probability distri-
butions, we consider the Bayesian polynomial regression model introduced in Sec-
N
N
Figure 8.3 Directed graphical model representing the joint
distribution (8.6) corresponding to the Bayesian
polynomial regression model
introduced in Sec-
tion 1.2.6.
8.1. Bayesian Networks
w
t1
363
tN
(8.6)
tion 1.2.6. The random variables in this model are the vector of polynomial coeffi-
cients w and the observed data t = (t1, . . . , tN )T. In addition, this model contains
the input data x = (x1, . . . , xN )T, the noise variance σ2, and the hyperparameter α
representing the precision of the Gaussian prior over w, all of which are parameters
of the model rather than random variables. Focussing just on the random variables
for the moment, we see that the joint distribution is given by the product of the prior
p(w) and N conditional distributions p(tn|w) for n = 1, . . . , N so that
p(t, w) = p(w)
p(tn|w).
n=1
This joint distribution can be represented by a graphical model shown in Figure 8.3.
When we start to deal with more complex models later in the book, we shall find
it inconvenient to have to write out multiple nodes of the form t1, . . . , tN explicitly as
in Figure 8.3. We therefore introduce a graphical notation that allows such multiple
nodes to be expressed more compactly, in which we draw a single representative
node tn and then surround this with a box, called a plate, labelled with N indicating
that there are N nodes of this kind. Re-writing the graph of Figure 8.3 in this way,
we obtain the graph shown in Figure 8.4.
We shall sometimes find it helpful to make the parameters of a model, as well as
its stochastic variables, explicit. In this case, (8.6) becomes
p(t, w|x, α, σ2) = p(w|α)
p(tn|w, xn, σ2).
n=1
Correspondingly, we can make x and α explicit in the graphical representation. To
do this, we shall adopt the convention that random variables will be denoted by open
circles, and deterministic parameters will be denoted by smaller solid circles. If we
take the graph of Figure 8.4 and include the deterministic parameters, we obtain the
graph shown in Figure 8.5.
When we apply a graphical model to a problem in machine learning or pattern
recognition, we will typically set some of the random variables to specific observed
Figure 8.4 An alternative, more compact, representation of the graph
shown in Figure 8.3 in which we have introduced a plate
(the box labelled N) that represents N nodes of which only
a single example tn is shown explicitly.
tn
N
w
N
N
364
8. GRAPHICAL MODELS
Figure 8.5 This shows the same model as in Figure 8.4 but
with the deterministic parameters shown explicitly
by the smaller solid nodes.
xn
w
N
σ2
tn
values, for example the variables {tn} from the training set in the case of polynomial
curve fitting. In a graphical model, we will denote such observed variables by shad-
ing the corresponding nodes. Thus the graph corresponding to Figure 8.5 in which
the variables {tn} are observed is shown in Figure 8.6. Note that the value of w is
not observed, and so w is an example of a latent variable, also known as a hidden
variable. Such variables play a crucial role in many probabilistic models and will
form the focus of Chapters 9 and 12.
Having observed the values {tn} we can, if desired, evaluate the posterior dis-
tribution of the polynomial coefficients w as discussed in Section 1.2.5. For the
moment, we note that this involves a straightforward application of Bayes’ theorem
p(w|T) ∝ p(w)
p(tn|w)
n=1
(8.7)
where again we have omitted the deterministic parameters in order to keep the nota-
tion uncluttered.
In general, model parameters such as w are of little direct interest in themselves,
because our ultimate goal is to make predictions for new input values. Suppose we
x and we wish to find the corresponding probability dis-
are given a new input value
t conditioned on the observed data. The graphical model that describes
tribution for
this problem is shown in Figure 8.7, and the corresponding joint distribution of all
of the random variables in this model, conditioned on the deterministic parameters,
is then given by
p(
t, t, w|
x, x, α, σ2) =
p(tn|xn, w, σ2)
p(w|α)p(
n=1
x, w, σ2).
t|
(8.8)
Figure 8.6 As in Figure 8.5 but with the nodes {tn} shaded
to indicate that
the corresponding random vari-
ables have been set to their observed (training set)
values.
xn
w
σ2
tn
N
b
b
tn
Figure 8.7 The polynomial regression model, corresponding
to Figure 8.6, showing also a new input value
x
together with the corresponding model prediction
t.
xn
8.1. Bayesian Networks
w
ˆt
N
σ2
365
ˆx
The required predictive distribution for
probability, by integrating out the model parameters w so that
t is then obtained, from the sum rule of
p(
t|
x, x, t, α, σ2) ∝
p(
t, t, w|
x, x, α, σ2) dw
where we are implicitly setting the random variables in t to the specific values ob-
served in the data set. The details of this calculation were discussed in Chapter 3.
