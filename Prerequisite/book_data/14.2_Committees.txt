655
p(x) =
πkN (x|µk, Σk)
(14.4)
with the usual interpretation of the symbols. This is an example of model combi-
nation. For independent, identically distributed data, we can use (14.3) to write the
marginal probability of a data set X = {x1, . . . , xN} in the form
p(X) =
n=1
p(xn) =
p(xn, zn)
(14.5)
n=1
zn
Thus we see that each observed data point xn has a corresponding latent variable zn.
Now suppose we have several different models indexed by h = 1, . . . , H with
prior probabilities p(h). For instance one model might be a mixture of Gaussians and
another model might be a mixture of Cauchy distributions. The marginal distribution
over the data set is given by
p(X) =
p(X|h)p(h).
h=1
(14.6)
This is an example of Bayesian model averaging. The interpretation of this summa-
tion over h is that just one model is responsible for generating the whole data set,
and the probability distribution over h simply reflects our uncertainty as to which
model that is. As the size of the data set increases, this uncertainty reduces, and
the posterior probabilities p(h|X) become increasingly focussed on just one of the
models.
This highlights the key difference between Bayesian model averaging and model
combination, because in Bayesian model averaging the whole data set is generated
by a single model. By contrast, when we combine multiple models, as in (14.5), we
see that different data points within the data set can potentially be generated from
different values of the latent variable z and hence by different components.
Although we have considered the marginal probability p(X), the same consid-
erations apply for the predictive density p(x|X) or for conditional distributions such
as p(t|x, X, T).
Exercise 14.1
14.2. Committees
Section 3.2
The simplest way to construct a committee is to average the predictions of a set of
individual models. Such a procedure can be motivated from a frequentist perspective
by considering the trade-off between bias and variance, which decomposes the er-
ror due to a model into the bias component that arises from differences between the
model and the true function to be predicted, and the variance component that repre-
sents the sensitivity of the model to the individual data points. Recall from Figure 3.5
1
M
Ex
m=1
M
M
M
M
m=1
1
M
1
M
m=1
1
M
2
656
14. COMBINING MODELS
that when we trained multiple polynomials using the sinusoidal data, and then aver-
aged the resulting functions, the contribution arising from the variance term tended to
cancel, leading to improved predictions. When we averaged a set of low-bias mod-
els (corresponding to higher order polynomials), we obtained accurate predictions
for the underlying sinusoidal function from which the data were generated.
In practice, of course, we have only a single data set, and so we have to find
a way to introduce variability between the different models within the committee.
One approach is to use bootstrap data sets, discussed in Section 1.2.3. Consider a
regression problem in which we are trying to predict the value of a single continuous
variable, and suppose we generate M bootstrap data sets and then use each to train
a separate copy ym(x) of a predictive model where m = 1, . . . , M. The committee
prediction is given by
yCOM(x) =
ym(x).
(14.7)
This procedure is known as bootstrap aggregation or bagging (Breiman, 1996).
Suppose the true regression function that we are trying to predict is given by
h(x), so that the output of each of the models can be written as the true value plus
an error in the form
ym(x) = h(x) + 	m(x).
The average sum-of-squares error then takes the form
Ex
(14.9)
where Ex[·] denotes a frequentist expectation with respect to the distribution of the
input vector x. The average error made by the models acting individually is therefore
{ym(x) − h(x)}2
m(x)2
= Ex
EAV =
m(x)2
Similarly, the expected error from the committee (14.7) is given by
ECOM = Ex
= Ex
2
ym(x) − h(x)
m(x)
m=1
If we assume that the errors have zero mean and are uncorrelated, so that
Ex [	m(x)] = 0
Ex [	m(x)	l(x)] = 0,
m = l
(14.8)
(14.10)
(14.11)
(14.12)
(14.13)
Exercise 14.2
then we obtain
