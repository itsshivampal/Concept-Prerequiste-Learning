Probabilities play a central role in modern pattern recognition. We have seen in
Chapter 1 that probability theory can be expressed in terms of two simple equations
corresponding to the sum rule and the product rule. All of the probabilistic infer-
ence and learning manipulations discussed in this book, no matter how complex,
amount to repeated application of these two equations. We could therefore proceed
to formulate and solve complicated probabilistic models purely by algebraic ma-
nipulation. However, we shall find it highly advantageous to augment the analysis
using diagrammatic representations of probability distributions, called probabilistic
graphical models. These offer several useful properties:
1. They provide a simple way to visualize the structure of a probabilistic model
and can be used to design and motivate new models.
2. Insights into the properties of the model, including conditional independence
properties, can be obtained by inspection of the graph.
359
360
8. GRAPHICAL MODELS
3. Complex computations, required to perform inference and learning in sophis-
ticated models, can be expressed in terms of graphical manipulations, in which
underlying mathematical expressions are carried along implicitly.
A graph comprises nodes (also called vertices) connected by links (also known
as edges or arcs). In a probabilistic graphical model, each node represents a random
variable (or group of random variables), and the links express probabilistic relation-
ships between these variables. The graph then captures the way in which the joint
distribution over all of the random variables can be decomposed into a product of
factors each depending only on a subset of the variables. We shall begin by dis-
cussing Bayesian networks, also known as directed graphical models, in which the
links of the graphs have a particular directionality indicated by arrows. The other
major class of graphical models are Markov random fields, also known as undirected
graphical models, in which the links do not carry arrows and have no directional
significance. Directed graphs are useful for expressing causal relationships between
random variables, whereas undirected graphs are better suited to expressing soft con-
straints between random variables. For the purposes of solving inference problems,
it is often convenient to convert both directed and undirected graphs into a different
representation called a factor graph.
In this chapter, we shall focus on the key aspects of graphical models as needed
for applications in pattern recognition and machine learning. More general treat-
ments of graphical models can be found in the books by Whittaker (1990), Lauritzen
(1996), Jensen (1996), Castillo et al. (1997), Jordan (1999), Cowell et al. (1999),
and Jordan (2007).
