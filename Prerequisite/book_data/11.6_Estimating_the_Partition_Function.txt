As we have seen, most of the sampling algorithms considered in this chapter re-
quire only the functional form of the probability distribution up to a multiplicative
constant. Thus if we write
pE(z) =
1
ZE
exp(−E(z))
(11.71)
then the value of the normalization constant ZE, also known as the partition func-
tion, is not needed in order to draw samples from p(z). However, knowledge of the
value of ZE can be useful for Bayesian model comparison since it represents the
model evidence (i.e., the probability of the observed data given the model), and so
it is of interest to consider how its value might be obtained. We assume that direct
evaluation by summing, or integrating, the function exp(−E(z)) over the state space
of z is intractable.
For model comparison, it is actually the ratio of the partition functions for two
models that is required. Multiplication of this ratio by the ratio of prior probabilities
gives the ratio of posterior probabilities, which can then be used for model selection
or model averaging.
One way to estimate a ratio of partition functions is to use importance sampling
from a distribution with energy function G(z)
z exp(−E(z))
z exp(−G(z))
z exp(−E(z) + G(z)) exp(−G(z))
ZE
ZG
z exp(−G(z))
= EG(z)[exp(−E + G)]
exp(−E(z(l)) + G(z(l)))
(11.72)
L
where {z(l)} are samples drawn from the distribution defined by pG(z). If the dis-
tribution pG is one for which the partition function can be evaluated analytically, for
example a Gaussian, then the absolute value of ZE can be obtained.
This approach will only yield accurate results if the importance sampling distri-
bution pG is closely matched to the distribution pE, so that the ratio pE/pG does not
have wide variations. In practice, suitable analytically specified importance sampling
distributions cannot readily be found for the kinds of complex models considered in
this book.
An alternative approach is therefore to use the samples obtained from a Markov
chain to define the importance-sampling distribution. If the transition probability for
the Markov chain is given by T (z, z), and the sample set is given by z(1), . . . , z(L),
then the sampling distribution can be written as
1
ZG
exp (−G(z)) =
which can be used directly in (11.72).
T (z(l), z)
l=1
(11.73)
11.6. Estimating the Partition Function
555
Methods for estimating the ratio of two partition functions require for their suc-
cess that the two corresponding distributions be reasonably closely matched. This is
especially problematic if we wish to find the absolute value of the partition function
for a complex distribution because it is only for relatively simple distributions that
the partition function can be evaluated directly, and so attempting to estimate the
ratio of partition functions directly is unlikely to be successful. This problem can be
tackled using a technique known as chaining (Neal, 1993; Barber and Bishop, 1997),
which involves introducing a succession of intermediate distributions p2, . . . , pM−1
that interpolate between a simple distribution p1(z) for which we can evaluate the
normalization coefficient Z1 and the desired complex distribution pM (z). We then
have
ZM
Z1
= Z2
Z1
Z3
Z2 ···
ZM
ZM−1
(11.74)
in which the intermediate ratios can be determined using Monte Carlo methods as
discussed above. One way to construct such a sequence of intermediate systems
is to use an energy function containing a continuous parameter 0 � α � 1 that
interpolates between the two distributions
Eα(z) = (1 − α)E1(z) + αEM (z).
(11.75)
If the intermediate ratios in (11.74) are to be found using Monte Carlo, it may be
more efficient to use a single Markov chain run than to restart the Markov chain for
each ratio. In this case, the Markov chain is run initially for the system p1 and then
after some suitable number of steps moves on to the next distribution in the sequence.
Note, however, that the system must remain close to the equilibrium distribution at
each stage.
556
11. SAMPLING METHODS
Exercises
11.1 () www Show that the finite sample estimator
f defined by (11.2) has mean
equal to E[f] and variance given by (11.3).
11.2 () Suppose that z is a random variable with uniform distribution over (0, 1) and
that we transform z using y = h−1(z) where h(y) is given by (11.6). Show that y
has the distribution p(y).
11.3 () Given a random variable z that is uniformly distributed over (0, 1), find a trans-
formation y = f(z) such that y has a Cauchy distribution given by (11.8).
11.4 ( )
Suppose that z1 and z2 are uniformly distributed over the unit circle, as
shown in Figure 11.3, and that we make the change of variables given by (11.10)
and (11.11). Show that (y1, y2) will be distributed according to (11.12).
11.5 () www Let z be a D-dimensional random variable having a Gaussian distribu-
tion with zero mean and unit covariance matrix, and suppose that the positive definite
symmetric matrix Σ has the Cholesky decomposition Σ = LLT where L is a lower-
triangular matrix (i.e., one with zeros above the leading diagonal). Show that the
variable y = µ + Lz has a Gaussian distribution with mean µ and covariance Σ.
This provides a technique for generating samples from a general multivariate Gaus-
sian using samples from a univariate Gaussian having zero mean and unit variance.
p(z)/kq(z) where
11.6 ( ) www In this exercise, we show more carefully that rejection sampling does
indeed draw samples from the desired distribution p(z). Suppose the proposal dis-
tribution is q(z) and show that the probability of a sample value z being accepted is
given by
p is any unnormalized distribution that is proportional to
p(z), and the constant k is set to the smallest value that ensures kq(z) �
p(z) for all
values of z. Note that the probability of drawing a value z is given by the probability
of drawing that value from q(z) times the probability of accepting that value given
that it has been drawn. Make use of this, along with the sum and product rules of
probability, to write down the normalized form for the distribution over z, and show
that it equals p(z).
11.7 () Suppose that z has a uniform distribution over the interval [0, 1]. Show that the
variable y = b tan z + c has a Cauchy distribution given by (11.16).
11.8 ( ) Determine expressions for the coefficients ki in the envelope distribution
(11.17) for adaptive rejection sampling using the requirements of continuity and nor-
malization.
11.9 ( ) By making use of the technique discussed in Section 11.1.1 for sampling
from a single exponential distribution, devise an algorithm for sampling from the
piecewise exponential distribution defined by (11.17).
11.10 () Show that the simple random walk over the integers defined by (11.34), (11.35),
and (11.36) has the property that E[(z(τ ))2] = E[(z(τ−1))2] + 1/2 and hence by
induction that E[(z(τ ))2] = τ /2.
Figure 11.15 A probability distribution over two variables z1
and z2 that is uniform over the shaded regions
and that is zero everywhere else.
z2
Exercises
557
z1
11.11 ( ) www Show that the Gibbs sampling algorithm, discussed in Section 11.3,
satisfies detailed balance as defined by (11.40).
11.12 () Consider the distribution shown in Figure 11.15. Discuss whether the standard
Gibbs sampling procedure for this distribution is ergodic, and therefore whether it
would sample correctly from this distribution
11.13 ( ) Consider the simple 3-node graph shown in Figure 11.16 in which the observed
node x is given by a Gaussian distribution N (x|µ, τ−1) with mean µ and precision
τ . Suppose that the marginal distributions over the mean and precision are given
by N (µ|µ0, s0) and Gam(τ|a, b), where Gam(·|·,·) denotes a gamma distribution.
Write down expressions for the conditional distributions p(µ|x, τ) and p(τ|x, µ) that
would be required in order to apply Gibbs sampling to the posterior distribution
p(µ, τ|x).
11.14 () Verify that the over-relaxation update (11.50), in which zi has mean µi and
variance σi, and where ν has zero mean and unit variance, gives a value zi with
mean µi and variance σ2
i .
11.15 () www Using (11.56) and (11.57), show that the Hamiltonian equation (11.58)
is equivalent to (11.53). Similarly, using (11.57) show that (11.59) is equivalent to
(11.55).
11.16 () By making use of (11.56), (11.57), and (11.63), show that the conditional dis-
tribution p(r|z) is a Gaussian.
Figure 11.16 A graph involving an observed Gaussian variable x with
prior distributions over its mean µ and precision τ.
x
558
11. SAMPLING METHODS
11.17 () www Verify that the two probabilities (11.68) and (11.69) are equal, and hence
that detailed balance holds for the hybrid Monte Carlo algorithm.
