An elegant and powerful method for finding maximum likelihood solutions for
models with latent variables is called the expectation-maximization algorithm, or EM
algorithm (Dempster et al., 1977; McLachlan and Krishnan, 1997). Later we shall
give a general treatment of EM, and we shall also show how EM can be generalized
to obtain the variational inference framework. Initially, we shall motivate the EM
algorithm by giving a relatively informal treatment in the context of the Gaussian
mixture model. We emphasize, however, that EM has broad applicability, and indeed
it will be encountered in the context of a variety of different models in this book.
Let us begin by writing down the conditions that must be satisfied at a maximum
of the likelihood function. Setting the derivatives of ln p(X|π, µ, Σ) in (9.14) with
respect to the means µk of the Gaussian components to zero, we obtain
0 = −
n=1
πkN (xn|µk, Σk)
j πjN (xn|µj, Σj)
γ(znk)
Σk(xn − µk)
(9.16)
where we have made use of the form (2.43) for the Gaussian distribution. Note that
the posterior probabilities, or responsibilities, given by (9.13) appear naturally on
the right-hand side. Multiplying by Σ−1
(which we assume to be nonsingular) and
k
rearranging we obtain
µk =
where we have defined
1
Nk
γ(znk)xn
Nk =
γ(znk).
n=1
(9.17)
(9.18)
Section 10.1
n=1
N
N
n=1
K
436
9. MIXTURE MODELS AND EM
We can interpret Nk as the effective number of points assigned to cluster k. Note
carefully the form of this solution. We see that the mean µk for the kth Gaussian
component is obtained by taking a weighted mean of all of the points in the data set,
in which the weighting factor for data point xn is given by the posterior probability
γ(znk) that component k was responsible for generating xn.
If we set the derivative of ln p(X|π, µ, Σ) with respect to Σk to zero, and follow
a similar line of reasoning, making use of the result for the maximum likelihood
solution for the covariance matrix of a single Gaussian, we obtain
Σk =
1
Nk
γ(znk)(xn − µk)(xn − µk)T
(9.19)
which has the same form as the corresponding result for a single Gaussian fitted to
the data set, but again with each data point weighted by the corresponding poste-
rior probability and with the denominator given by the effective number of points
associated with the corresponding component.
Finally, we maximize ln p(X|π, µ, Σ) with respect to the mixing coefficients
πk. Here we must take account of the constraint (9.9), which requires the mixing
coefficients to sum to one. This can be achieved using a Lagrange multiplier and
maximizing the following quantity
Section 2.3.4
Appendix E
ln p(X|π, µ, Σ) + λ
πk − 1
k=1
which gives
0 =
N (xn|µk, Σk)
j πjN (xn|µj, Σj)
(9.20)
(9.21)
where again we see the appearance of the responsibilities. If we now multiply both
sides by πk and sum over k making use of the constraint (9.9), we find λ = −N.
Using this to eliminate λ and rearranging we obtain
πk = Nk
N
(9.22)
so that the mixing coefficient for the kth component is given by the average respon-
sibility which that component takes for explaining the data points.
It is worth emphasizing that the results (9.17), (9.19), and (9.22) do not con-
stitute a closed-form solution for the parameters of the mixture model because the
responsibilities γ(znk) depend on those parameters in a complex way through (9.13).
However, these results do suggest a simple iterative scheme for finding a solution to
the maximum likelihood problem, which as we shall see turns out to be an instance
of the EM algorithm for the particular case of the Gaussian mixture model. We
first choose some initial values for the means, covariances, and mixing coefficients.
Then we alternate between the following two updates that we shall call the E step
9.2. Mixtures of Gaussians
437
2
0
−2
L = 1
2
0
−2
−2
0
(a)
2
−2
0
(b)
2
−2
0
(c)
2
L = 2
L = 5
2
0
−2
L = 20
2
0
−2
2
0
−2
2
0
−2
−2
0
(d)
2
−2
0
(e)
2
−2
0
(f)
2
Figure 9.8 Illustration of the EM algorithm using the Old Faithful set as used for the illustration of the K-means
algorithm in Figure 9.1. See the text for details.
Section 9.4
and the M step, for reasons that will become apparent shortly. In the expectation
step, or E step, we use the current values for the parameters to evaluate the posterior
probabilities, or responsibilities, given by (9.13). We then use these probabilities in
the maximization step, or M step, to re-estimate the means, covariances, and mix-
ing coefficients using the results (9.17), (9.19), and (9.22). Note that in so doing
we first evaluate the new means using (9.17) and then use these new values to find
the covariances using (9.19), in keeping with the corresponding result for a single
Gaussian distribution. We shall show that each update to the parameters resulting
from an E step followed by an M step is guaranteed to increase the log likelihood
function. In practice, the algorithm is deemed to have converged when the change
in the log likelihood function, or alternatively in the parameters, falls below some
threshold. We illustrate the EM algorithm for a mixture of two Gaussians applied to
the rescaled Old Faithful data set in Figure 9.8. Here a mixture of two Gaussians
is used, with centres initialized using the same values as for the K-means algorithm
in Figure 9.1, and with precision matrices initialized to be proportional to the unit
matrix. Plot (a) shows the data points in green, together with the initial configura-
tion of the mixture model in which the one standard-deviation contours for the two
438
9. MIXTURE MODELS AND EM
j=1
Gaussian components are shown as blue and red circles. Plot (b) shows the result
of the initial E step, in which each data point is depicted using a proportion of blue
ink equal to the posterior probability of having been generated from the blue com-
ponent, and a corresponding proportion of red ink given by the posterior probability
of having been generated by the red component. Thus, points that have a significant
probability for belonging to either cluster appear purple. The situation after the first
M step is shown in plot (c), in which the mean of the blue Gaussian has moved to
the mean of the data set, weighted by the probabilities of each data point belonging
to the blue cluster, in other words it has moved to the centre of mass of the blue ink.
Similarly, the covariance of the blue Gaussian is set equal to the covariance of the
blue ink. Analogous results hold for the red component. Plots (d), (e), and (f) show
the results after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the
algorithm is close to convergence.
Note that the EM algorithm takes many more iterations to reach (approximate)
convergence compared with the K-means algorithm, and that each cycle requires
significantly more computation. It is therefore common to run the K-means algo-
rithm in order to find a suitable initialization for a Gaussian mixture model that is
subsequently adapted using EM. The covariance matrices can conveniently be ini-
tialized to the sample covariances of the clusters found by the K-means algorithm,
and the mixing coefficients can be set to the fractions of data points assigned to the
respective clusters. As with gradient-based approaches for maximizing the log like-
lihood, techniques must be employed to avoid singularities of the likelihood function
in which a Gaussian component collapses onto a particular data point. It should be
emphasized that there will generally be multiple local maxima of the log likelihood
function, and that EM is not guaranteed to find the largest of these maxima. Because
the EM algorithm for Gaussian mixtures plays such an important role, we summarize
it below.
EM for Gaussian Mixtures
Given a Gaussian mixture model, the goal is to maximize the likelihood function
with respect to the parameters (comprising the means and covariances of the
components and the mixing coefficients).
1. Initialize the means µk, covariances Σk and mixing coefficients πk, and
evaluate the initial value of the log likelihood.
2. E step. Evaluate the responsibilities using the current parameter values
γ(znk) = πkN (xn|µk, Σk)
πjN (xn|µj, Σj)
K
(9.23)
n=1
N
N
n=1
γ(znk)xn
Nk =
n=1
n=1
ln
k=1
K
N
N
(9.24)
)T
(9.25)
(9.26)
(9.27)
(9.28)
