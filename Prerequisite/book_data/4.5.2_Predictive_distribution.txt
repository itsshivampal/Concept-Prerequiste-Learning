The predictive distribution for class C1, given a new feature vector φ(x), is
obtained by marginalizing with respect to the posterior distribution p(w|t), which is
itself approximated by a Gaussian distribution q(w) so that
p(C1|φ, t) =
p(C1|φ, w)p(w|t) dw 
σ(wTφ)q(w) dw
(4.145)
with the corresponding probability for class C2 given by p(C2|φ, t) = 1− p(C1|φ, t).
To evaluate the predictive distribution, we first note that the function σ(wTφ) de-
pends on w only through its projection onto φ. Denoting a = wTφ, we have
σ(wTφ) =
δ(a − wTφ)σ(a) da
where δ(·) is the Dirac delta function. From this we obtain
(4.146)
σ(wTφ)q(w) dw =
σ(a)p(a) da
(4.147)
4.5. Bayesian Logistic Regression
219
(4.148)
where
p(a) =
δ(a − wTφ)q(w) dw.
We can evaluate p(a) by noting that the delta function imposes a linear constraint
on w and so forms a marginal distribution from the joint distribution q(w) by inte-
grating out all directions orthogonal to φ. Because q(w) is Gaussian, we know from
Section 2.3.2 that the marginal distribution will also be Gaussian. We can evaluate
the mean and covariance of this distribution by taking moments, and interchanging
the order of integration over a and w, so that
µa = E[a] =
p(a)a da =
q(w)wTφ dw = wT
MAPφ
(4.149)
where we have used the result (4.144) for the variational posterior distribution q(w).
Similarly
a = var[a] =
σ2
p(a)
da
a2 − E[a]2
N φ)2
q(w)
(wTφ)2 − (mT
dw = φTSN φ.
(4.150)
Note that the distribution of a takes the same form as the predictive distribution
(3.58) for the linear regression model, with the noise variance set to zero. Thus our
variational approximation to the predictive distribution becomes
p(C1|t) =
σ(a)p(a) da =
σ(a)N (a|µa, σ2
a) da.
(4.151)
This result can also be derived directly by making use of the results for the marginal
of a Gaussian distribution given in Section 2.3.2.
The integral over a represents the convolution of a Gaussian with a logistic sig-
moid, and cannot be evaluated analytically. We can, however, obtain a good approx-
imation (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b; Barber and Bishop,
1998a) by making use of the close similarity between the logistic sigmoid function
σ(a) defined by (4.59) and the probit function Φ(a) defined by (4.114). In order to
obtain the best approximation to the logistic function we need to re-scale the hori-
zontal axis, so that we approximate σ(a) by Φ(λa). We can find a suitable value of
λ by requiring that the two functions have the same slope at the origin, which gives
λ2 = π/8. The similarity of the logistic sigmoid and the probit function, for this
choice of λ, is illustrated in Figure 4.9.
The advantage of using a probit function is that its convolution with a Gaussian
can be expressed analytically in terms of another probit function. Specifically we
can show that
Φ(λa)N (a|µ, σ2) da = Φ
(λ−2 + σ2)1/2
(4.152)
Exercise 4.24
Exercise 4.25
Exercise 4.26
n
220
4. LINEAR MODELS FOR CLASSIFICATION
We now apply the approximation σ(a)  Φ(λa) to the probit functions appearing
on both sides of this equation, leading to the following approximation for the convo-
lution of a logistic sigmoid with a Gaussian
σ(a)N (a|µ, σ2) da  σ
κ(σ2)µ
(4.153)
where we have defined
(4.154)
Applying this result to (4.151) we obtain the approximate predictive distribution
κ(σ2) = (1 + πσ2/8)−1/2.
in the form
κ(σ2
a)µa
p(C1|φ, t) = σ
where µa and σ2
fined by (4.154).
a are defined by (4.149) and (4.150), respectively, and κ(σ2
(4.155)
a) is de-
Note that the decision boundary corresponding to p(C1|φ, t) = 0.5 is given by
µa = 0, which is the same as the decision boundary obtained by using the MAP
value for w. Thus if the decision criterion is based on minimizing misclassifica-
tion rate, with equal prior probabilities, then the marginalization over w has no ef-
fect. However, for more complex decision criteria it will play an important role.
Marginalization of the logistic sigmoid model under a Gaussian approximation to
the posterior distribution will be illustrated in the context of variational inference in
Figure 10.13.
Exercises
4.1 ( ) Given a set of data points {xn}, we can define the convex hull to be the set of
(4.156)
all points x given by
x =
αnxn
where αn � 0 and
n αn = 1. Consider a second set of points {yn} together with
their corresponding convex hull. By definition, the two sets of points will be linearly
wTxn + w0 > 0 for all
separable if there exists a vector
wTyn + w0 < 0 for all yn. Show that if their convex hulls intersect, the two
xn, and
sets of points cannot be linearly separable, and conversely that if they are linearly
separable, their convex hulls do not intersect.
w and a scalar w0 such that
4.2 ( ) www Consider the minimization of a sum-of-squares error function (4.15),
and suppose that all of the target vectors in the training set satisfy a linear constraint
aTtn + b = 0
(4.157)
where tn corresponds to the nth row of the matrix T in (4.15). Show that as a
consequence of this constraint, the elements of the model prediction y(x) given by
the least-squares solution (4.17) also satisfy this constraint, so that
aTy(x) + b = 0.
(4.158)
Exercises
221
To do so, assume that one of the basis functions φ0(x) = 1 so that the corresponding
parameter w0 plays the role of a bias.
4.3 ( ) Extend the result of Exercise 4.2 to show that if multiple linear constraints
are satisfied simultaneously by the target vectors, then the same constraints will also
be satisfied by the least-squares prediction of a linear model.
4.4 () www Show that maximization of the class separation criterion given by (4.23)
with respect to w, using a Lagrange multiplier to enforce the constraint wTw = 1,
leads to the result that w ∝ (m2 − m1).
4.5 () By making use of (4.20), (4.23), and (4.24), show that the Fisher criterion (4.25)
can be written in the form (4.26).
4.6 () Using the definitions of the between-class and within-class covariance matrices
given by (4.27) and (4.28), respectively, together with (4.34) and (4.36) and the
choice of target values described in Section 4.1.5, show that the expression (4.33)
that minimizes the sum-of-squares error function can be written in the form (4.37).
4.7 () www Show that the logistic sigmoid function (4.59) satisfies the property
σ(−a) = 1 − σ(a) and that its inverse is given by σ−1(y) = ln{y/(1 − y)}.
4.8 () Using (4.57) and (4.58), derive the result (4.65) for the posterior class probability
in the two-class generative model with Gaussian densities, and verify the results
(4.66) and (4.67) for the parameters w and w0.
4.9 () www Consider a generative classification model for K classes defined by
prior class probabilities p(Ck) = πk and general class-conditional densities p(φ|Ck)
where φ is the input feature vector. Suppose we are given a training data set {φn, tn}
where n = 1, . . . , N, and tn is a binary target vector of length K that uses the 1-of-
K coding scheme, so that it has components tnj = Ijk if pattern n is from class Ck.
Assuming that the data points are drawn independently from this model, show that
the maximum-likelihood solution for the prior probabilities is given by
N
πk = Nk
N
where Nk is the number of data points assigned to class Ck.
4.10 ( ) Consider the classification model of Exercise 4.9 and now suppose that the
class-conditional densities are given by Gaussian distributions with a shared covari-
ance matrix, so that
p(φ|Ck) = N (φ|µk, Σ).
Show that the maximum likelihood solution for the mean of the Gaussian distribution
for class Ck is given by
µk =
1
Nk
tnkφn
n=1
(4.161)
(4.159)
(4.160)
K
k=1
N
222
4. LINEAR MODELS FOR CLASSIFICATION
which represents the mean of those feature vectors assigned to class Ck. Similarly,
show that the maximum likelihood solution for the shared covariance matrix is given
by
Nk
N
Sk
(4.162)
(4.163)
where
Sk =
1
Nk
tnk(φn − µk)(φn − µk)T.
n=1
Thus Σ is given by a weighted average of the covariances of the data associated with
each class, in which the weighting coefficients are given by the prior probabilities of
the classes.
4.11 ( ) Consider a classification problem with K classes for which the feature vector
φ has M components each of which can take L discrete states. Let the values of the
components be represented by a 1-of-L binary coding scheme. Further suppose that,
conditioned on the class Ck, the M components of φ are independent, so that the
class-conditional density factorizes with respect to the feature vector components.
Show that the quantities ak given by (4.63), which appear in the argument to the
softmax function describing the posterior class probabilities, are linear functions of
the components of φ. Note that this represents an example of the naive Bayes model
which is discussed in Section 8.2.2.
4.12 () www Verify the relation (4.88) for the derivative of the logistic sigmoid func-
tion defined by (4.59).
4.13 () www By making use of the result (4.88) for the derivative of the logistic sig-
moid, show that the derivative of the error function (4.90) for the logistic regression
model is given by (4.91).
4.14 () Show that for a linearly separable data set, the maximum likelihood solution
for the logistic regression model is obtained by finding a vector w whose decision
boundary wTφ(x) = 0 separates the classes and then taking the magnitude of w to
infinity.
4.15 ( ) Show that the Hessian matrix H for the logistic regression model, given by
(4.97), is positive definite. Here R is a diagonal matrix with elements yn(1 − yn),
and yn is the output of the logistic regression model for input vector xn. Hence show
that the error function is a concave function of w and that it has a unique minimum.
4.16 () Consider a binary classification problem in which each observation xn is known
to belong to one of two classes, corresponding to t = 0 and t = 1, and suppose that
the procedure for collecting training data is imperfect, so that training points are
sometimes mislabelled. For every data point xn, instead of having a value t for the
class label, we have instead a value πn representing the probability that tn = 1.
Given a probabilistic model p(t = 1|φ), write down the log likelihood function
appropriate to such a data set.
Exercises
223
4.17 () www Show that the derivatives of the softmax activation function (4.104),
where the ak are defined by (4.105), are given by (4.106).
4.18 () Using the result (4.91) for the derivatives of the softmax activation function,
show that the gradients of the cross-entropy error (4.108) are given by (4.109).
4.19 () www Write down expressions for the gradient of the log likelihood, as well
as the corresponding Hessian matrix, for the probit regression model defined in Sec-
tion 4.3.5. These are the quantities that would be required to train such a model using
IRLS.
4.20 ( ) Show that the Hessian matrix for the multiclass logistic regression problem,
defined by (4.110), is positive semidefinite. Note that the full Hessian matrix for
this problem is of size M K × M K, where M is the number of parameters and K
is the number of classes. To prove the positive semidefinite property, consider the
product uTHu where u is an arbitrary vector of length M K, and then apply Jensen’s
inequality.
4.21 () Show that the probit function (4.114) and the erf function (4.115) are related by
(4.116).
4.22 () Using the result (4.135), derive the expression (4.137) for the log model evi-
dence under the Laplace approximation.
4.23 ( ) www In this exercise, we derive the BIC result (4.139) starting from the
Laplace approximation to the model evidence given by (4.137). Show that if the
prior over parameters is Gaussian of the form p(θ) = N (θ|m, V0), the log model
evidence under the Laplace approximation takes the form
0 (θMAP − m) −
1
2
1
2
(θMAP − m)TV−1
ln p(D)  ln p(D|θMAP) −
ln|H| + const
where H is the matrix of second derivatives of the log likelihood ln p(D|θ) evaluated
at θMAP. Now assume that the prior is broad so that V−1
is small and the second
0
term on the right-hand side above can be neglected. Furthermore, consider the case
of independent, identically distributed data so that H is the sum of terms one for each
data point. Show that the log model evidence can then be written approximately in
the form of the BIC expression (4.139).
4.24 ( ) Use the results from Section 2.3.2 to derive the result (4.151) for the marginal-
ization of the logistic regression model with respect to a Gaussian posterior distribu-
tion over the parameters w.
4.25 ( ) Suppose we wish to approximate the logistic sigmoid σ(a) defined by (4.59)
by a scaled probit function Φ(λa), where Φ(a) is defined by (4.114). Show that if
λ is chosen so that the derivatives of the two functions are equal at a = 0, then
λ2 = π/8.
224
4. LINEAR MODELS FOR CLASSIFICATION
4.26 ( )
In this exercise, we prove the relation (4.152) for the convolution of a probit
function with a Gaussian distribution. To do this, show that the derivative of the left-
hand side with respect to µ is equal to the derivative of the right-hand side, and then
integrate both sides with respect to µ and then show that the constant of integration
vanishes. Note that before differentiating the left-hand side, it is convenient first
to introduce a change of variable given by a = µ + σz so that the integral over a
is replaced by an integral over z. When we differentiate the left-hand side of the
relation (4.152), we will then obtain a Gaussian integral over z that can be evaluated
analytically.
