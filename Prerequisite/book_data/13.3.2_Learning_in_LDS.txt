So far, we have considered the inference problem for linear dynamical systems,
assuming that the model parameters θ = {A, Γ, C, Σ, µ0, V0} are known. Next, we
consider the determination of these parameters using maximum likelihood (Ghahra-
mani and Hinton, 1996b). Because the model has latent variables, this can be ad-
dressed using the EM algorithm, which was discussed in general terms in Chapter 9.
We can derive the EM algorithm for the linear dynamical system as follows. Let
us denote the estimated parameter values at some particular cycle of the algorithm
by θold. For these parameter values, we can run the inference algorithm to determine
the posterior distribution of the latent variables p(Z|X, θold), or more precisely those
local posterior marginals that are required in the M step.
In particular, we shall
require the following expectations
E [zn] =
µn
E
znzT
E
n−1
znzT
n
= Jn−1
Vn +
Vn +
µT
n
µn
µT
µn
n−1
(13.105)
(13.106)
(13.107)
where we have used (13.104).
Now we consider the complete-data log likelihood function, which is obtained
by taking the logarithm of (13.6) and is therefore given by
ln p(X, Z|θ) = ln p(z1|µ0, V0) +
ln p(zn|zn−1, A, Γ)
n=1
ln p(xn|zn, C, Σ)
(13.108)
in which we have made the dependence on the parameters explicit. We now take the
expectation of the complete-data log likelihood with respect to the posterior distri-
bution p(Z|X, θold) which defines the function
Q(θ, θold) = EZ|θold [ln p(X, Z|θ)] .
(13.109)
In the M step, this function is maximized with respect to the components of θ.
Consider first the parameters µ0 and V0. If we substitute for p(z1|µ0, V0) in
(13.108) using (13.77), and then take the expectation with respect to Z, we obtain
Q(θ, θold) = −
1
2
ln|V0| − EZ|θold
1
2
(z1 − µ0)TV−1
0 (z1 − µ0)
+ const
where all terms not dependent on µ0 or V0 have been absorbed into the additive
constant. Maximization with respect to µ0 and V0 is easily performed by making
use of the maximum likelihood solution for a Gaussian distribution discussed in
Section 2.3.4, giving
Exercise 13.32
µnew
Vnew
N − 1
ln|Σ|
n=2
1
xnE
N
2
1
2
n=2
n=1
n=1
E
2
N
N
N
N
N
N
1
N
−xnE
n=1
zT
n
N − 1
znzT
n−1
n=2
N
N
−1
13.3. Linear Dynamical Systems
643
0
(13.110)
(13.111)
Similarly, to optimize A and Γ, we substitute for p(zn|zn−1, A, Γ) in (13.108)
1 ] − E[z1]E[zT
1 ].
= E[z1]
= E[z1zT
0
using (13.75) giving
Q(θ, θold) = −
ln|Γ|
−EZ|θold
1
2
(zn − Azn−1)TΓ−1(zn − Azn−1)
+ const
(13.112)
in which the constant comprises terms that are independent of A and Γ. Maximizing
with respect to these parameters then gives
Anew =
Γnew =
−E
znzT
n−1
E
zn−1zT
n−1
E
znzT
n
− AnewE
zn−1zT
n
n=2
Anew + AnewE
zn−1zT
n−1
(Anew)T
(13.113)
(13.114)
Note that Anew must be evaluated first, and the result can then be used to determine
Γnew.
Finally, in order to determine the new values of C and Σ, we substitute for
p(xn|zn, C, Σ) in (13.108) using (13.76) giving
Q(θ, θold) = −
−EZ|θold
(xn − Czn)TΣ−1(xn − Czn)
+ const.
Maximizing with respect to C and Σ then gives
Cnew =
Σnew =
zT
n
E
n=1
znzT
n
xnxT
n − CnewE [zn] xT
Cnew + CnewE
znzT
n
n
−1
(13.115)
Cnew
(13.116)
Exercise 13.33
Exercise 13.34
644
Chapter 10
13. SEQUENTIAL DATA
We have approached parameter learning in the linear dynamical system using
maximum likelihood. Inclusion of priors to give a MAP estimate is straightforward,
and a fully Bayesian treatment can be found by applying the analytical approxima-
tion techniques discussed in Chapter 10, though a detailed treatment is precluded
here due to lack of space.
