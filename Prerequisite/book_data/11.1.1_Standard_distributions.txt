We first consider how to generate random numbers from simple nonuniform dis-
tributions, assuming that we already have available a source of uniformly distributed
random numbers. Suppose that z is uniformly distributed over the interval (0, 1),
and that we transform the values of z using some function f(·) so that y = f(z).
The distribution of y will be governed by
where, in this case, p(z) = 1. Our goal is to choose the function f(z) such that the
resulting values of y have some specific desired distribution p(y). Integrating (11.5)
we obtain
p(y) = p(z)
z = h(y) ≡
p(
y) d
y
(11.5)
(11.6)
526
11. SAMPLING METHODS
methods for statistical inference.
Diagnostic tests for convergence of Markov chain Monte Carlo algorithms are
summarized in Robert and Casella (1999), and some practical guidance on the use of
sampling methods in the context of machine learning is given in Bishop and Nabney
(2008).
Exercise 11.2
which is the indefinite integral of p(y). Thus, y = h−1(z), and so we have to
transform the uniformly distributed random numbers using a function which is the
inverse of the indefinite integral of the desired distribution. This is illustrated in
Figure 11.2.
Consider for example the exponential distribution
p(y) = λ exp(−λy)
(11.7)
where 0 � y < ∞. In this case the lower limit of the integral in (11.6) is 0, and so
h(y) = 1 − exp(−λy). Thus, if we transform our uniformly distributed variable z
using y = −λ−1 ln(1 − z), then y will have an exponential distribution.
 ∂(z1, . . . , zM )
y
∂(y1, . . . , yM )
p(y)
11.1. Basic Sampling Algorithms
527
h(y)
Figure 11.2 Geometrical
interpretation of
the trans-
formation method for generating nonuni-
formly distributed random numbers. h(y)
is the indefinite integral of the desired dis-
tribution p(y).
If a uniformly distributed
random variable z is transformed using
y = h−1(z), then y will be distributed ac-
cording to p(y).
1
0
Another example of a distribution to which the transformation method can be
applied is given by the Cauchy distribution
p(y) =
1
1
1 + y2 .
(11.8)
Exercise 11.3
In this case, the inverse of the indefinite integral can be expressed in terms of the
‘tan’ function.
The generalization to multiple variables is straightforward and involves the Ja-
cobian of the change of variables, so that
p(y1, . . . , yM ) = p(z1, . . . , zM )
(11.9)
As a final example of the transformation method we consider the Box-Muller
method for generating samples from a Gaussian distribution. First, suppose we gen-
erate pairs of uniformly distributed random numbers z1, z2 ∈ (−1, 1), which we can
do by transforming a variable distributed uniformly over (0, 1) using z → 2z − 1.
2 � 1. This leads to a uniform
Next we discard each pair unless it satisfies z2
distribution of points inside the unit circle with p(z1, z2) = 1/π, as illustrated in
Figure 11.3. Then, for each pair z1, z2 we evaluate the quantities
1 + z2
Figure 11.3 The Box-Muller method for generating Gaussian dis-
tributed random numbers starts by generating samples
from a uniform distribution inside the unit circle.
1
z2
−1−1
z1
1
 ∂(z1, z2)
∂(y1, y2)
r2
r2
1
Zp
528
11. SAMPLING METHODS
Exercise 11.4
Exercise 11.5
y1 = z1
y2 = z2
1/2
1/2
−2 ln z1
−2 ln z2
(11.10)
(11.11)
where r2 = z2
1 + z2
2. Then the joint distribution of y1 and y2 is given by
p(y1, y2) = p(z1, z2)
1
√2π
exp(−y2
1/2)
1
√2π
exp(−y2
2/2)
(11.12)
and so y1 and y2 are independent and each has a Gaussian distribution with zero
mean and unit variance.
If y has a Gaussian distribution with zero mean and unit variance, then σy + µ
will have a Gaussian distribution with mean µ and variance σ2. To generate vector-
valued variables having a multivariate Gaussian distribution with mean µ and co-
variance Σ, we can make use of the Cholesky decomposition, which takes the form
Σ = LLT (Press et al., 1992). Then, if z is a vector valued random variable whose
components are independent and Gaussian distributed with zero mean and unit vari-
ance, then y = µ + Lz will have mean µ and covariance Σ.
Obviously, the transformation technique depends for its success on the ability
to calculate and then invert the indefinite integral of the required distribution. Such
operations will only be feasible for a limited number of simple distributions, and so
we must turn to alternative approaches in search of a more general strategy. Here
we consider two techniques called rejection sampling and importance sampling. Al-
though mainly limited to univariate distributions and thus not directly applicable to
complex problems in many dimensions, they do form important components in more
general strategies.
