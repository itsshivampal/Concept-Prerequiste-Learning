We have seen how the problem of polynomial curve fitting can be expressed in
terms of error minimization. Here we return to the curve fitting example and view it
from a probabilistic perspective, thereby gaining some insights into error functions
and regularization, as well as taking us towards a full Bayesian treatment.
The goal in the curve fitting problem is to be able to make predictions for the
target variable t given some new value of the input variable x on the basis of a set of
training data comprising N input values x = (x1, . . . , xN )T and their corresponding
target values t = (t1, . . . , tN )T. We can express our uncertainty over the value of
the target variable using a probability distribution. For this purpose, we shall assume
that, given the value of x, the corresponding value of t has a Gaussian distribution
with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). Thus
we have
where, for consistency with the notation in later chapters, we have defined a preci-
sion parameter β corresponding to the inverse variance of the distribution. This is
illustrated schematically in Figure 1.16.
p(t|x, w, β) = N
t|y(x, w), β−1
(1.60)
N
n=1
t
N
N
n=1
Figure 1.16 Schematic illustration of a Gaus-
sian conditional distribution for t given x given by
(1.60), in which the mean is given by the polyno-
mial function y(x, w), and the precision is given
by the parameter β, which is related to the vari-
ance by β−1 = σ2.
y(x0, w)
1.2. Probability Theory
29
y(x, w)
p(t|x0, w, β)
2σ
x
x0
We now use the training data {x, t} to determine the values of the unknown
parameters w and β by maximum likelihood. If the data are assumed to be drawn
independently from the distribution (1.60), then the likelihood function is given by
p(t|x, w, β) =
N
tn|y(xn, w), β−1
(1.61)
As we did in the case of the simple Gaussian distribution earlier, it is convenient to
maximize the logarithm of the likelihood function. Substituting for the form of the
Gaussian distribution, given by (1.46), we obtain the log likelihood function in the
form
ln p(t|x, w, β) = −
2
{y(xn, w) − tn}2 + N
2
ln β −
N
2
ln(2π).
(1.62)
Consider first the determination of the maximum likelihood solution for the polyno-
mial coefficients, which will be denoted by wML. These are determined by maxi-
mizing (1.62) with respect to w. For this purpose, we can omit the last two terms
on the right-hand side of (1.62) because they do not depend on w. Also, we note
that scaling the log likelihood by a positive constant coefficient does not alter the
location of the maximum with respect to w, and so we can replace the coefficient
β/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently
minimize the negative log likelihood. We therefore see that maximizing likelihood is
equivalent, so far as determining w is concerned, to minimizing the sum-of-squares
error function defined by (1.2). Thus the sum-of-squares error function has arisen as
a consequence of maximizing likelihood under the assumption of a Gaussian noise
distribution.
We can also use maximum likelihood to determine the precision parameter β of
the Gaussian conditional distribution. Maximizing (1.62) with respect to β gives
1
βML
1
N
{y(xn, wML) − tn}2 .
n=1
(1.63)
N
n=1
30
1. INTRODUCTION
Section 1.2.4
Again we can first determine the parameter vector wML governing the mean and sub-
sequently use this to find the precision βML as was the case for the simple Gaussian
distribution.
Having determined the parameters w and β, we can now make predictions for
new values of x. Because we now have a probabilistic model, these are expressed
in terms of the predictive distribution that gives the probability distribution over t,
rather than simply a point estimate, and is obtained by substituting the maximum
likelihood parameters into (1.60) to give
p(t|x, wML, βML) = N
t|y(x, wML), β−1
(1.64)
ML
Now let us take a step towards a more Bayesian approach and introduce a prior
distribution over the polynomial coefficients w. For simplicity, let us consider a
Gaussian distribution of the form
p(w|α) = N (w|0, α−1I) =
2π
(M +1)/2
exp
2
wTw
(1.65)
where α is the precision of the distribution, and M +1 is the total number of elements
in the vector w for an M th order polynomial. Variables such as α, which control
the distribution of model parameters, are called hyperparameters. Using Bayes’
theorem, the posterior distribution for w is proportional to the product of the prior
distribution and the likelihood function
p(w|x, t, α, β) ∝ p(t|x, w, β)p(w|α).
(1.66)
We can now determine w by finding the most probable value of w given the data,
in other words by maximizing the posterior distribution. This technique is called
maximum posterior, or simply MAP. Taking the negative logarithm of (1.66) and
combining with (1.62) and (1.65), we find that the maximum of the posterior is
given by the minimum of
2
{y(xn, w) − tn}2 + α
2
wTw.
(1.67)
Thus we see that maximizing the posterior distribution is equivalent to minimizing
the regularized sum-of-squares error function encountered earlier in the form (1.4),
with a regularization parameter given by λ = α/β.
