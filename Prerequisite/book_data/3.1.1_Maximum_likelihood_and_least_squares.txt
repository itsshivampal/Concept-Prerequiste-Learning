In Chapter 1, we fitted polynomial functions to data sets by minimizing a sum-
of-squares error function. We also showed that this error function could be motivated
as the maximum likelihood solution under an assumed Gaussian noise model. Let
us return to this discussion and consider the least squares approach, and its relation
to maximum likelihood, in more detail.
As before, we assume that the target variable t is given by a deterministic func-
tion y(x, w) with additive Gaussian noise so that
t = y(x, w) +
(3.7)
where 	 is a zero mean Gaussian random variable with precision (inverse variance)
β. Thus we can write
Section 1.5.5
Recall that, if we assume a squared loss function, then the optimal prediction, for a
new value of x, will be given by the conditional mean of the target variable. In the
case of a Gaussian conditional distribution of the form (3.8), the conditional mean
p(t|x, w, β) = N (t|y(x, w), β−1).
(3.8)
1
2
n=1
n=1
N
N
n=1
N
N
3.1. Linear Basis Function Models
will be simply
E[t|x] =
tp(t|x) dt = y(x, w).
Note that the Gaussian noise assumption implies that the conditional distribution of
t given x is unimodal, which may be inappropriate for some applications. An ex-
tension to mixtures of conditional Gaussian distributions, which permit multimodal
conditional distributions, will be discussed in Section 14.5.1.
Now consider a data set of inputs X = {x1, . . . , xN} with corresponding target
values t1, . . . , tN . We group the target variables {tn} into a column vector that we
denote by t where the typeface is chosen to distinguish it from a single observation
of a multivariate target, which would be denoted t. Making the assumption that
these data points are drawn independently from the distribution (3.8), we obtain the
following expression for the likelihood function, which is a function of the adjustable
parameters w and β, in the form
p(t|X, w, β) =
N (tn|wTφ(xn), β−1)
(3.10)
where we have used (3.3). Note that in supervised learning problems such as regres-
sion (and classification), we are not seeking to model the distribution of the input
variables. Thus x will always appear in the set of conditioning variables, and so
from now on we will drop the explicit x from expressions such as p(t|x, w, β) in or-
der to keep the notation uncluttered. Taking the logarithm of the likelihood function,
and making use of the standard form (1.46) for the univariate Gaussian, we have
141
(3.9)
(3.11)
(3.12)
ln p(t|w, β) =
lnN (tn|wTφ(xn), β−1)
= N
2
ln β −
N
2
ln(2π) − βED(w)
where the sum-of-squares error function is defined by
ED(w) =
{tn − wTφ(xn)}2.
Having written down the likelihood function, we can use maximum likelihood to
determine w and β. Consider first the maximization with respect to w. As observed
already in Section 1.2.5, we see that maximization of the likelihood function under a
conditional Gaussian noise distribution for a linear model is equivalent to minimizing
a sum-of-squares error function given by ED(w). The gradient of the log likelihood
function (3.11) takes the form
∇ ln p(t|w, β) =
n=1
tn − wTφ(xn)
φ(xn)T.
(3.13)
n=1
j=1
N
M−1
M−1
wjφj
j=1
φj =
1
N
N
⎛⎜⎜⎝ φ0(x1)
φ0(x2)
1
2
n=1
N
N
N
142
3. LINEAR MODELS FOR REGRESSION
Setting this gradient to zero gives
n=1
Solving for w we obtain
0 =
tnφ(xn)T − wT
φ(xn)φ(xn)T
(3.14)
(3.15)
which are known as the normal equations for the least squares problem. Here Φ is an
N×M matrix, called the design matrix, whose elements are given by Φnj = φj(xn),
so that
wML =
ΦTΦ
−1 ΦTt
The quantity
φ1(x1)
φ1(x2)
φ0(xN ) φ1(xN )
φM−1(x1)
φM−1(x2)
··· φM−1(xN )
−1 ΦT
ΦTΦ
(3.17)
is known as the Moore-Penrose pseudo-inverse of the matrix Φ (Rao and Mitra,
1971; Golub and Van Loan, 1996). It can be regarded as a generalization of the
notion of matrix inverse to nonsquare matrices. Indeed, if Φ is square and invertible,
then using the property (AB)−1 = B−1A−1 we see that Φ† ≡ Φ−1.
we make the bias parameter explicit, then the error function (3.12) becomes
At this point, we can gain some insight into the role of the bias parameter w0. If
(3.16)
ED(w) =
{tn − w0 −
wjφj(xn)}2.
Setting the derivative with respect to w0 equal to zero, and solving for w0, we obtain
w0 = t −
where we have defined
t =
1
N
N
tn,
n=1
φj(xn).
n=1
Thus the bias w0 compensates for the difference between the averages (over the
training set) of the target values and the weighted sum of the averages of the basis
function values.
We can also maximize the log likelihood function (3.11) with respect to the noise
precision parameter β, giving
1
βML
1
N
{tn − wT
MLφ(xn)}2
n=1
(3.21)
(3.18)
(3.19)
(3.20)
3.1. Linear Basis Function Models
143
Figure 3.2 Geometrical interpretation of the least-squares
solution, in an N-dimensional space whose axes
are the values of t1, . . . , tN . The least-squares
regression function is obtained by finding the or-
thogonal projection of the data vector t onto the
subspace spanned by the basis functions φj(x)
in which each basis function is viewed as a vec-
tor ϕj of length N with elements φj(xn).
S
ϕ1
y
ϕ2
t
and so we see that the inverse of the noise precision is given by the residual variance
of the target values around the regression function.
