So far in this section, we have introduced a number of concepts from information
theory, including the key notion of entropy. We now start to relate these ideas to
pattern recognition. Consider some unknown distribution p(x), and suppose that
we have modelled this using an approximating distribution q(x). If we use q(x) to
construct a coding scheme for the purpose of transmitting values of x to a receiver,
then the average additional amount of information (in nats) required to specify the
value of x (assuming we choose an efficient coding scheme) as a result of using q(x)
instead of the true distribution p(x) is given by
KL(pq) = −
p(x) ln q(x) dx −
p(x) ln
q(x)
p(x)
dx.
p(x) ln p(x) dx
(1.113)
This is known as the relative entropy or Kullback-Leibler divergence, or KL diver-
gence (Kullback and Leibler, 1951), between the distributions p(x) and q(x). Note
that it is not a symmetrical quantity, that is to say KL(pq) ≡ KL(qp).
We now show that the Kullback-Leibler divergence satisfies KL(pq) � 0 with
equality if, and only if, p(x) = q(x). To do this we first introduce the concept of
convex functions. A function f(x) is said to be convex if it has the property that
every chord lies on or above the function, as shown in Figure 1.31. Any value of x
in the interval from x = a to x = b can be written in the form λa + (1 − λ)b where
0 � λ � 1. The corresponding point on the chord is given by λf(a) + (1 − λ)f(b),
Claude Shannon
1916–2001
After graduating from Michigan and
MIT, Shannon joined the AT&T Bell
Telephone laboratories in 1941. His
paper ‘A Mathematical Theory of
Communication’ published in the
Bell System Technical Journal in
1948 laid the foundations for modern information the-
ory. This paper introduced the word ‘bit’, and his con-
cept that information could be sent as a stream of 1s
and 0s paved the way for the communications revo-
lution.
It is said that von Neumann recommended to
Shannon that he use the term entropy, not only be-
cause of its similarity to the quantity used in physics,
but also because “nobody knows what entropy really
is, so in any discussion you will always have an advan-
tage”.
i=1
M
f
i=1
M
56
1. INTRODUCTION
Figure 1.31 A convex function f (x) is one for which ev-
ery chord (shown in blue) lies on or above
the function (shown in red).
f(x)
xλ
xλ
b
x
chord
a
and the corresponding value of the function is f (λa + (1 − λ)b). Convexity then
implies
(1.114)
f(λa + (1 − λ)b) � λf(a) + (1 − λ)f(b).
This is equivalent to the requirement that the second derivative of the function be
everywhere positive. Examples of convex functions are x ln x (for x > 0) and x2. A
function is called strictly convex if the equality is satisfied only for λ = 0 and λ = 1.
If a function has the opposite property, namely that every chord lies on or below the
function, it is called concave, with a corresponding definition for strictly concave. If
a function f(x) is convex, then −f(x) will be concave.
convex function f(x) satisfies
Using the technique of proof by induction, we can show from (1.114) that a
Exercise 1.36
Exercise 1.38
λixi
λif(xi)
(1.115)
where λi � 0 and
i λi = 1, for any set of points {xi}. The result (1.115) is
known as Jensen’s inequality. If we interpret the λi as the probability distribution
over a discrete variable x taking the values {xi}, then (1.115) can be written
f (E[x]) � E[f(x)]
(1.116)
where E[·] denotes the expectation. For continuous variables, Jensen’s inequality
takes the form
(1.117)
f(x)p(x) dx.
xp(x) dx
f
We can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler
divergence (1.113) to give
KL(pq) = −
p(x) ln
q(x)
p(x)
dx � − ln
q(x) dx = 0
(1.118)
n=1
N
1.6. Information Theory
57
where we have used the fact that − ln x is a convex function, together with the nor-
q(x) dx = 1. In fact, − ln x is a strictly convex function,
malization condition
so the equality will hold if, and only if, q(x) = p(x) for all x. Thus we can in-
terpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two
distributions p(x) and q(x).
We see that there is an intimate relationship between data compression and den-
sity estimation (i.e., the problem of modelling an unknown probability distribution)
because the most efficient compression is achieved when we know the true distri-
bution. If we use a distribution that is different from the true one, then we must
necessarily have a less efficient coding, and on average the additional information
that must be transmitted is (at least) equal to the Kullback-Leibler divergence be-
tween the two distributions.
Suppose that data is being generated from an unknown distribution p(x) that we
wish to model. We can try to approximate this distribution using some parametric
distribution q(x|θ), governed by a set of adjustable parameters θ, for example a
multivariate Gaussian. One way to determine θ is to minimize the Kullback-Leibler
divergence between p(x) and q(x|θ) with respect to θ. We cannot do this directly
because we don’t know p(x). Suppose, however, that we have observed a finite set
of training points xn, for n = 1, . . . , N, drawn from p(x). Then the expectation
with respect to p(x) can be approximated by a finite sum over these points, using
(1.35), so that
KL(pq) 
{− ln q(xn|θ) + ln p(xn)} .
(1.119)
The second term on the right-hand side of (1.119) is independent of θ, and the first
term is the negative log likelihood function for θ under the distribution q(x|θ) eval-
uated using the training set. Thus we see that minimizing this Kullback-Leibler
divergence is equivalent to maximizing the likelihood function.
Now consider the joint distribution between two sets of variables x and y given
by p(x, y). If the sets of variables are independent, then their joint distribution will
factorize into the product of their marginals p(x, y) = p(x)p(y). If the variables are
not independent, we can gain some idea of whether they are ‘close’ to being indepen-
dent by considering the Kullback-Leibler divergence between the joint distribution
and the product of the marginals, given by
I[x, y] ≡ KL(p(x, y)p(x)p(y))
p(x, y) ln
p(x)p(y)
p(x, y)
dx dy
(1.120)
Exercise 1.41
which is called the mutual information between the variables x and y. From the
properties of the Kullback-Leibler divergence, we see that I(x, y) � 0 with equal-
ity if, and only if, x and y are independent. Using the sum and product rules of
probability, we see that the mutual information is related to the conditional entropy
through
I[x, y] = H[x] − H[x|y] = H[y] − H[y|x].
(1.121)
M
N
N
58
1. INTRODUCTION
Thus we can view the mutual information as the reduction in the uncertainty about x
by virtue of being told the value of y (or vice versa). From a Bayesian perspective,
we can view p(x) as the prior distribution for x and p(x|y) as the posterior distribu-
tion after we have observed new data y. The mutual information therefore represents
the reduction in uncertainty about x as a consequence of the new observation y.
Exercises
1.1 () www Consider the sum-of-squares error function given by (1.2) in which
the function y(x, w) is given by the polynomial (1.1). Show that the coefficients
w = {wi} that minimize this error function are given by the solution to the following
set of linear equations
Aijwj = Ti
j=0
(1.122)
where
Aij =
(xn)i+j,
n=1
Ti =
(xn)itn.
(1.123)
n=1
Here a suffix i or j denotes the index of a component, whereas (x)i denotes x raised
to the power of i.
1.2 () Write down the set of coupled linear equations, analogous to (1.122), satisfied
by the coefficients wi which minimize the regularized sum-of-squares error function
given by (1.4).
1.3 ( ) Suppose that we have three coloured boxes r (red), b (blue), and g (green).
Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange,
and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes. If a box is chosen
at random with probabilities p(r) = 0.2, p(b) = 0.2, p(g) = 0.6, and a piece of
fruit is removed from the box (with equal probability of selecting any of the items in
the box), then what is the probability of selecting an apple? If we observe that the
selected fruit is in fact an orange, what is the probability that it came from the green
box?
1.4 ( ) www Consider a probability density px(x) defined over a continuous vari-
able x, and suppose that we make a nonlinear change of variable using x = g(y),
so that the density transforms according to (1.27). By differentiating (1.27), show
that the location
y of the maximum of the density in y is not in general related to the
location
x of the maximum of the density over x by the simple functional relation
x = g(
y) as a consequence of the Jacobian factor. This shows that the maximum
of a probability density (in contrast to a simple function) is dependent on the choice
of variable. Verify that, in the case of a linear transformation, the location of the
maximum transforms in the same way as the variable itself.
1.5 () Using the definition (1.38) show that var[f(x)] satisfies (1.39).
Exercises
59
1.6 () Show that if two variables x and y are independent, then their covariance is
zero.
1.7 ( ) www In this exercise, we prove the normalization condition (1.48) for the
univariate Gaussian. To do this consider, the integral
I =
exp
1
2σ2 x2
dx
(1.124)
which we can evaluate by first writing its square in the form
I 2 =
exp
1
2σ2 x2 −
1
2σ2 y2
dx dy.
(1.125)
Now make the transformation from Cartesian coordinates (x, y) to polar coordinates
(r, θ) and then substitute u = r2. Show that, by performing the integrals over θ and
u, and then taking the square root of both sides, we obtain
I =
2πσ2
1/2
(1.126)
Finally, use this result to show that the Gaussian distribution N (x|µ, σ2) is normal-
ized.
1.8 ( ) www By using a change of variables, verify that the univariate Gaussian
distribution given by (1.46) satisfies (1.49). Next, by differentiating both sides of the
normalization condition
N
x|µ, σ2
dx = 1
(1.127)
with respect to σ2, verify that the Gaussian satisfies (1.50). Finally, show that (1.51)
holds.
1.9 () www Show that the mode (i.e. the maximum) of the Gaussian distribution
(1.46) is given by µ. Similarly, show that the mode of the multivariate Gaussian
(1.52) is given by µ.
1.10 () www Suppose that the two variables x and z are statistically independent.
Show that the mean and variance of their sum satisfies
E[x + z] = E[x] + E[z]
var[x + z] = var[x] + var[z].
(1.128)
(1.129)
1.11 () By setting the derivatives of the log likelihood function (1.54) with respect to µ
and σ2 equal to zero, verify the results (1.55) and (1.56).
60
1. INTRODUCTION
D
D
iM−1
j=1
i=1
D
D
D
i2=1
j=1
i=1
D
D
D
D
D
i1
1.12 ( ) www Using the results (1.49) and (1.50), show that
E[xnxm] = µ2 + Inmσ2
(1.130)
where xn and xm denote data points sampled from a Gaussian distribution with mean
µ and variance σ2, and Inm satisfies Inm = 1 if n = m and Inm = 0 otherwise.
Hence prove the results (1.57) and (1.58).
1.13 () Suppose that the variance of a Gaussian is estimated using the result (1.56) but
with the maximum likelihood estimate µML replaced with the true value µ of the
mean. Show that this estimator has the property that its expectation is given by the
true variance σ2.
1.14 ( )
Show that an arbitrary square matrix with elements wij can be written in
the form wij = wS
ij are symmetric and anti-symmetric
matrices, respectively, satisfying wS
ji for all i and j. Now
consider the second order term in a higher order polynomial in D dimensions, given
by
ij = −wA
ij where wS
ij and wA
ji and wA
ij = wS
ij + wA
Show that
wijxixj.
wijxixj =
wS
ijxixj
i=1
j=1
(1.131)
(1.132)
so that the contribution from the anti-symmetric matrix vanishes. We therefore see
that, without loss of generality, the matrix of coefficients wij can be chosen to be
symmetric, and so not all of the D2 elements of this matrix can be chosen indepen-
ij is given
dently. Show that the number of independent parameters in the matrix wS
by D(D + 1)/2.
1.15 (  ) www In this exercise and the next, we explore how the number of indepen-
dent parameters in a polynomial grows with the order M of the polynomial and with
the dimensionality D of the input space. We start by writing down the M th order
term for a polynomial in D dimensions in the form
wi1i2···iM xi1xi2 ··· xiM .
i1=1
iM =1
(1.133)
The coefficients wi1i2···iM comprise DM elements, but the number of independent
parameters is significantly fewer due to the many interchange symmetries of the
factor xi1xi2 ··· xiM . Begin by showing that the redundancy in the coefficients can
be removed by rewriting this M th order term in the form
iM =1
wi1i2···iM xi1xi2 ··· xiM .
i1=1
i2=1
(1.134)
D
i=1
i=1
D
M
Exercises
61
w coefficients and w coefficients need
Note that the precise relationship between the
not be made explicit. Use this result to show that the number of independent param-
eters n(D, M), which appear at order M, satisfies the following recursion relation
(1.135)
(1.136)
(1.137)
Next use proof by induction to show that the following result holds
n(D, M) =
n(i, M − 1).
(i + M − 2)!
(i − 1)! (M − 1)!
(D + M − 1)!
(D − 1)! M!
which can be done by first proving the result for D = 1 and arbitrary M by making
use of the result 0! = 1, then assuming it is correct for dimension D and verifying
that it is correct for dimension D + 1. Finally, use the two previous results, together
with proof by induction, to show
n(D, M) =
(D + M − 1)!
(D − 1)! M! .
To do this, first show that the result is true for M = 2, and any value of D � 1,
by comparison with the result of Exercise 1.14. Then make use of (1.135), together
with (1.136), to show that, if the result holds at order M − 1, then it will also hold at
order M
1.16 (  ) In Exercise 1.15, we proved the result (1.135) for the number of independent
parameters in the M th order term of a D-dimensional polynomial. We now find an
expression for the total number N(D, M) of independent parameters in all of the
terms up to and including the M6th order. First show that N(D, M) satisfies
N(D, M) =
n(D, m)
(1.138)
m=0
where n(D, m) is the number of independent parameters in the term of order m.
Now make use of the result (1.137), together with proof by induction, to show that
N(d, M) =
(D + M)!
D! M!
(1.139)
This can be done by first proving that the result holds for M = 0 and arbitrary
D � 1, then assuming that it holds at order M, and hence showing that it holds at
order M + 1. Finally, make use of Stirling’s approximation in the form
n!  nne−n
(1.140)
for large n to show that, for D
M, the quantity N(D, M) grows like DM ,
and for M
D it grows like M D. Consider a cubic (M = 3) polynomial in D
dimensions, and evaluate numerically the total number of independent parameters
for (i) D = 10 and (ii) D = 100, which correspond to typical small-scale and
medium-scale machine learning applications.
D
62
1. INTRODUCTION
1.17 ( ) www The gamma function is defined by
Γ(x) ≡
0
ux−1e−u du.
(1.141)
Using integration by parts, prove the relation Γ(x + 1) = xΓ(x). Show also that
Γ(1) = 1 and hence that Γ(x + 1) = x! when x is an integer.
1.18 ( ) www We can use the result (1.126) to derive an expression for the surface
area SD, and the volume VD, of a sphere of unit radius in D dimensions. To do this,
consider the following result, which is obtained by transforming from Cartesian to
polar coordinates
e−x2
i dxi = SD
0
e−r2
rD−1 dr.
(1.142)
i=1
Using the definition (1.141) of the Gamma function, together with (1.126), evaluate
both sides of this equation, and hence show that
SD =
2πD/2
Γ(D/2) .
(1.143)
Next, by integrating with respect to radius from 0 to 1, show that the volume of the
unit sphere in D dimensions is given by
(1.144)
Finally, use the results Γ(1) = 1 and Γ(3/2) = √π/2 to show that (1.143) and
(1.144) reduce to the usual expressions for D = 2 and D = 3.
VD = SD
D
1.19 ( ) Consider a sphere of radius a in D-dimensions together with the concentric
hypercube of side 2a, so that the sphere touches the hypercube at the centres of each
of its sides. By using the results of Exercise 1.18, show that the ratio of the volume
of the sphere to the volume of the cube is given by
volume of sphere
volume of cube
πD/2
D2D−1Γ(D/2) .
(1.145)
Now make use of Stirling’s formula in the form
Γ(x + 1)  (2π)1/2e−xxx+1/2
(1.146)
which is valid for x
1, to show that, as D → ∞, the ratio (1.145) goes to zero.
Show also that the ratio of the distance from the centre of the hypercube to one of
the corners, divided by the perpendicular distance to one of the sides, is √D, which
therefore goes to ∞ as D → ∞. From these results we see that, in a space of high
dimensionality, most of the volume of a cube is concentrated in the large number of
corners, which themselves become very long ‘spikes’!
Exercises
63
1.20 ( ) www In this exercise, we explore the behaviour of the Gaussian distribution
in high-dimensional spaces. Consider a Gaussian distribution in D dimensions given
by
p(x) =
(1.147)
1
(2πσ2)D/2
exp
−x2
2σ2
We wish to find the density with respect to radius in polar coordinates in which the
direction variables have been integrated out. To do this, show that the integral of
the probability density over a thin shell of radius r and thickness 	, where 	  1, is
given by p(r)	 where
p(r) = SDrD−1
(2πσ2)D/2
exp
r2
2σ2
(1.148)
(1.149)
where SD is the surface area of a unit sphere in D dimensions. Show that the function
p(r) has a single stationary point located, for large D, at
r + 	) where
p(
r  √Dσ. By considering
r, show that for large D,
p(
r + 	) = p(
r) exp
3	2
2σ2
r is a maximum of the radial probability density and also that p(r)
which shows that
r with length scale σ. We have
decays exponentially away from its maximum at
already seen that σ
r for large D, and so we see that most of the probability
mass is concentrated in a thin shell at large radius. Finally, show that the probability
r by a factor of exp(D/2).
density p(x) is larger at the origin than at the radius
We therefore see that most of the probability mass in a high-dimensional Gaussian
distribution is located at a different radius from the region of high probability density.
This property of distributions in spaces of high dimensionality will have important
consequences when we consider Bayesian inference of model parameters in later
chapters.
1.21 ( ) Consider two nonnegative numbers a and b, and show that, if a � b, then
a � (ab)1/2. Use this result to show that, if the decision regions of a two-class
classification problem are chosen to minimize the probability of misclassification,
this probability will satisfy
p(mistake) �
{p(x,C1)p(x,C2)}1/2 dx.
(1.150)
1.22 () www Given a loss matrix with elements Lkj, the expected risk is minimized
if, for each x, we choose the class that minimizes (1.81). Verify that, when the
loss matrix is given by Lkj = 1 − Ikj, where Ikj are the elements of the identity
matrix, this reduces to the criterion of choosing the class having the largest posterior
probability. What is the interpretation of this form of loss matrix?
1.23 () Derive the criterion for minimizing the expected loss when there is a general
loss matrix and general prior probabilities for the classes.
64
1. INTRODUCTION
1.24 ( ) www Consider a classification problem in which the loss incurred when
an input vector from class Ck is classified as belonging to class Cj is given by the
loss matrix Lkj, and for which the loss incurred in selecting the reject option is λ.
Find the decision criterion that will give the minimum expected loss. Verify that this
reduces to the reject criterion discussed in Section 1.5.3 when the loss matrix is given
by Lkj = 1 − Ikj. What is the relationship between λ and the rejection threshold θ?
1.25 () www Consider the generalization of the squared loss function (1.87) for a
single target variable t to the case of multiple target variables described by the vector
t given by
E[L(t, y(x))] =
y(x) − t2p(x, t) dx dt.
(1.151)
Using the calculus of variations, show that the function y(x) for which this expected
loss is minimized is given by y(x) = Et[t|x]. Show that this result reduces to (1.89)
for the case of a single target variable t.
1.26 () By expansion of the square in (1.151), derive a result analogous to (1.90) and
hence show that the function y(x) that minimizes the expected squared loss for the
case of a vector t of target variables is again given by the conditional expectation of
t.
1.27 ( ) www Consider the expected loss for regression problems under the Lq loss
function given by (1.91). Write down the condition that y(x) must satisfy in order
to minimize E[Lq]. Show that, for q = 1, this solution represents the conditional
median, i.e., the function y(x) such that the probability mass for t < y(x) is the
same as for t � y(x). Also show that the minimum expected Lq loss for q → 0 is
given by the conditional mode, i.e., by the function y(x) equal to the value of t that
maximizes p(t|x) for each x.
1.28 () In Section 1.6, we introduced the idea of entropy h(x) as the information gained
on observing the value of a random variable x having distribution p(x). We saw
that, for independent variables x and y for which p(x, y) = p(x)p(y), the entropy
functions are additive, so that h(x, y) = h(x) + h(y). In this exercise, we derive the
relation between h and p in the form of a function h(p). First show that h(p2) =
2h(p), and hence by induction that h(pn) = nh(p) where n is a positive integer.
Hence show that h(pn/m) = (n/m)h(p) where m is also a positive integer. This
implies that h(px) = xh(p) where x is a positive rational number, and hence by
continuity when it is a positive real number. Finally, show that this implies h(p)
must take the form h(p) ∝ ln p.
1.29 () www Consider an M-state discrete random variable x, and use Jensen’s in-
equality in the form (1.115) to show that the entropy of its distribution p(x) satisfies
H[x] � ln M.
1.30 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians
p(x) = N (x|µ, σ2) and q(x) = N (x|m, s2).
Table 1.3 The joint distribution p(x, y) for two binary variables
x and y used in Exercise 1.39.
Exercises
65
y
0
1/3
0
1
1/3
1/3
x
0
1
1.31 ( ) www Consider two variables x and y having joint distribution p(x, y). Show
that the differential entropy of this pair of variables satisfies
with equality if, and only if, x and y are statistically independent.
H[x, y] � H[x] + H[y]
(1.152)
1.32 () Consider a vector x of continuous variables with distribution p(x) and corre-
sponding entropy H[x]. Suppose that we make a nonsingular linear transformation
of x to obtain a new variable y = Ax. Show that the corresponding entropy is given
by H[y] = H[x] + ln|A| where |A| denotes the determinant of A.
1.33 ( ) Suppose that the conditional entropy H[y|x] between two discrete random
variables x and y is zero. Show that, for all values of x such that p(x) > 0, the
variable y must be a function of x, in other words for each x there is only one value
of y such that p(y|x) = 0.
1.34 ( ) www Use the calculus of variations to show that the stationary point of the
functional (1.108) is given by (1.108). Then use the constraints (1.105), (1.106),
and (1.107) to eliminate the Lagrange multipliers and hence show that the maximum
entropy solution is given by the Gaussian (1.109).
1.35 () www Use the results (1.106) and (1.107) to show that the entropy of the
univariate Gaussian (1.109) is given by (1.110).
1.36 () A strictly convex function is defined as one for which every chord lies above
the function. Show that this is equivalent to the condition that the second derivative
of the function be positive.
1.37 () Using the definition (1.111) together with the product rule of probability, prove
the result (1.112).
1.38 ( ) www Using proof by induction, show that the inequality (1.114) for convex
functions implies the result (1.115).
1.39 (  ) Consider two binary variables x and y having the joint distribution given in
Table 1.3.
Evaluate the following quantities
(a) H[x]
(b) H[y]
(c) H[y|x]
(d) H[x|y]
(e) H[x, y]
I[x, y].
(f)
Draw a diagram to show the relationship between these various quantities.
66
1. INTRODUCTION
1.40 () By applying Jensen’s inequality (1.115) with f(x) = ln x, show that the arith-
metic mean of a set of real numbers is never less than their geometrical mean.
1.41 () www Using the sum and product rules of probability, show that the mutual
information I(x, y) satisfies the relation (1.121).
