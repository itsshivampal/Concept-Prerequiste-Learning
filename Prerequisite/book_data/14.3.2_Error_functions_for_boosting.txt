The exponential error function that is minimized by the AdaBoost algorithm
differs from those considered in previous chapters. To gain some insight into the
nature of the exponential error function, we first consider the expected error given
by
Ex,t [exp{−ty(x)}] =
t
exp{−ty(x)}p(t|x)p(x) dx.
(14.27)
If we perform a variational minimization with respect to all possible functions y(x),
we obtain
y(x) =
ln
1
2
p(t = 1|x)
p(t = −1|x)
(14.28)
Exercise 14.6
Exercise 14.7
662
14. COMBINING MODELS
Figure 14.3 Plot of the exponential (green) and
rescaled cross-entropy (red) error
functions along with the hinge er-
ror
(blue) used in support vector
machines, and the misclassification
for large
error (black). Note that
negative values of z = ty(x),
the
cross-entropy gives a linearly in-
creasing penalty, whereas the expo-
nential loss gives an exponentially in-
creasing penalty.
E(z)
−2
−1
0
1
z
2
which is half the log-odds. Thus the AdaBoost algorithm is seeking the best approx-
imation to the log odds ratio, within the space of functions represented by the linear
combination of base classifiers, subject to the constrained minimization resulting
from the sequential optimization strategy. This result motivates the use of the sign
function in (14.19) to arrive at the final classification decision.
We have already seen that the minimizer y(x) of the cross-entropy error (4.90)
for two-class classification is given by the posterior class probability. In the case
of a target variable t ∈ {−1, 1}, we have seen that the error function is given by
ln(1 + exp(−yt)). This is compared with the exponential error function in Fig-
ure 14.3, where we have divided the cross-entropy error by a constant factor ln(2)
so that it passes through the point (0, 1) for ease of comparison. We see that both
can be seen as continuous approximations to the ideal misclassification error func-
tion. An advantage of the exponential error is that its sequential minimization leads
to the simple AdaBoost scheme. One drawback, however, is that it penalizes large
negative values of ty(x) much more strongly than cross-entropy. In particular, we
see that for large negative values of ty, the cross-entropy grows linearly with |ty|,
whereas the exponential error function grows exponentially with |ty|. Thus the ex-
ponential error function will be much less robust to outliers or misclassified data
points. Another important difference between cross-entropy and the exponential er-
ror function is that the latter cannot be interpreted as the log likelihood function of
any well-defined probabilistic model. Furthermore, the exponential error does not
generalize to classification problems having K > 2 classes, again in contrast to the
cross-entropy for a probabilistic model, which is easily generalized to give (4.108).
The interpretation of boosting as the sequential optimization of an additive model
under an exponential error (Friedman et al., 2000) opens the door to a wide range
of boosting-like algorithms, including multiclass extensions, by altering the choice
of error function. It also motivates the extension to regression problems (Friedman,
2001). If we consider a sum-of-squares error function for regression, then sequential
minimization of an additive model of the form (14.21) simply involves fitting each
new base classifier to the residual errors tn−fm−1(xn) from the previous model. As
we have noted, however, the sum-of-squares error is not robust to outliers, and this
Section 7.1.2
Exercise 14.8
Section 4.3.4
Exercise 14.9
Figure 14.4 Comparison of
the squared error
(green) with the absolute error (red)
showing how the latter places much
less emphasis on large errors and
hence is more robust to outliers and
mislabelled data points.
