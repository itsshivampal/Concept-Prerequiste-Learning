We begin our discussion of the Bayesian treatment of linear regression by in-
troducing a prior probability distribution over the model parameters w. For the mo-
ment, we shall treat the noise precision parameter β as a known constant. First note
that the likelihood function p(t|w) defined by (3.10) is the exponential of a quadratic
function of w. The corresponding conjugate prior is therefore given by a Gaussian
distribution of the form
having mean m0 and covariance S0.
p(w) = N (w|m0, S0)
(3.48)
N
n=1
3.3. Bayesian Linear Regression
153
Next we compute the posterior distribution, which is proportional to the product
of the likelihood function and the prior. Due to the choice of a conjugate Gaus-
sian prior distribution, the posterior will also be Gaussian. We can evaluate this
distribution by the usual procedure of completing the square in the exponential, and
then finding the normalization coefficient using the standard result for a normalized
Gaussian. However, we have already done the necessary work in deriving the gen-
eral result (2.116), which allows us to write down the posterior distribution directly
in the form
where
p(w|t) = N (w|mN , SN )
mN = SN
S−1
N = S−1
0 + βΦTΦ.
S−1
0 m0 + βΦTt
Note that because the posterior distribution is Gaussian, its mode coincides with its
mean. Thus the maximum posterior weight vector is simply given by wMAP = mN .
If we consider an infinitely broad prior S0 = α−1I with α → 0, the mean mN
of the posterior distribution reduces to the maximum likelihood value wML given
by (3.15). Similarly, if N = 0, then the posterior distribution reverts to the prior.
Furthermore, if data points arrive sequentially, then the posterior distribution at any
stage acts as the prior distribution for the subsequent data point, such that the new
posterior distribution is again given by (3.49).
For the remainder of this chapter, we shall consider a particular form of Gaus-
sian prior in order to simplify the treatment. Specifically, we consider a zero-mean
isotropic Gaussian governed by a single precision parameter α so that
and the corresponding posterior distribution over w is then given by (3.49) with
p(w|α) = N (w|0, α−1I)
mN = βSN ΦTt
S−1
N = αI + βΦTΦ.
(3.49)
(3.50)
(3.51)
(3.52)
(3.53)
(3.54)
Exercise 3.7
Exercise 3.8
The log of the posterior distribution is given by the sum of the log likelihood and
the log of the prior and, as a function of w, takes the form
ln p(w|t) = −
2
{tn − wTφ(xn)}2 −
2
wTw + const.
(3.55)
Maximization of this posterior distribution with respect to w is therefore equiva-
lent to the minimization of the sum-of-squares error function with the addition of a
quadratic regularization term, corresponding to (3.27) with λ = α/β.
We can illustrate Bayesian learning in a linear basis function model, as well as
the sequential update of a posterior distribution, using a simple example involving
straight-line fitting. Consider a single input variable x, a single target variable t and
154
3. LINEAR MODELS FOR REGRESSION
a linear model of the form y(x, w) = w0 + w1x. Because this has just two adap-
tive parameters, we can plot the prior and posterior distributions directly in parameter
space. We generate synthetic data from the function f(x, a) = a0 + a1x with param-
eter values a0 = −0.3 and a1 = 0.5 by first choosing values of xn from the uniform
distribution U(x|−1, 1), then evaluating f(xn, a), and finally adding Gaussian noise
with standard deviation of 0.2 to obtain the target values tn. Our goal is to recover
the values of a0 and a1 from such data, and we will explore the dependence on the
size of the data set. We assume here that the noise variance is known and hence we
set the precision parameter to its true value β = (1/0.2)2 = 25. Similarly, we fix
the parameter α to 2.0. We shall shortly discuss strategies for determining α and
β from the training data. Figure 3.7 shows the results of Bayesian learning in this
model as the size of the data set is increased and demonstrates the sequential nature
of Bayesian learning in which the current posterior distribution forms the prior when
a new data point is observed. It is worth taking time to study this figure in detail as
it illustrates several important aspects of Bayesian inference. The first row of this
figure corresponds to the situation before any data points are observed and shows a
plot of the prior distribution in w space together with six samples of the function
y(x, w) in which the values of w are drawn from the prior. In the second row, we
see the situation after observing a single data point. The location (x, t) of the data
point is shown by a blue circle in the right-hand column. In the left-hand column is a
plot of the likelihood function p(t|x, w) for this data point as a function of w. Note
that the likelihood function provides a soft constraint that the line must pass close to
the data point, where close is determined by the noise precision β. For comparison,
the true parameter values a0 = −0.3 and a1 = 0.5 used to generate the data set
are shown by a white cross in the plots in the left column of Figure 3.7. When we
multiply this likelihood function by the prior from the top row, and normalize, we
obtain the posterior distribution shown in the middle plot on the second row. Sam-
ples of the regression function y(x, w) obtained by drawing samples of w from this
posterior distribution are shown in the right-hand plot. Note that these sample lines
all pass close to the data point. The third row of this figure shows the effect of ob-
serving a second data point, again shown by a blue circle in the plot in the right-hand
column. The corresponding likelihood function for this second data point alone is
shown in the left plot. When we multiply this likelihood function by the posterior
distribution from the second row, we obtain the posterior distribution shown in the
middle plot of the third row. Note that this is exactly the same posterior distribution
as would be obtained by combining the original prior with the likelihood function
for the two data points. This posterior has now been influenced by two data points,
and because two points are sufficient to define a line this already gives a relatively
compact posterior distribution. Samples from this posterior distribution give rise to
the functions shown in red in the third column, and we see that these functions pass
close to both of the data points. The fourth row shows the effect of observing a total
of 20 data points. The left-hand plot shows the likelihood function for the 20th data
point alone, and the middle plot shows the resulting posterior distribution that has
now absorbed information from all 20 observations. Note how the posterior is much
sharper than in the third row. In the limit of an infinite number of data points, the
3.3. Bayesian Linear Regression
155
Figure 3.7 Illustration of sequential Bayesian learning for a simple linear model of the form y(x, w) =
w0 + w1x. A detailed description of this figure is given in the text.
M
156
3. LINEAR MODELS FOR REGRESSION
posterior distribution would become a delta function centred on the true parameter
values, shown by the white cross.
Other forms of prior over the parameters can be considered. For instance, we
can generalize the Gaussian prior to give
p(w|α) =
q
2
2
1/q
1
Γ(1/q)
M
exp
2
|wj|q
j=1
(3.56)
in which q = 2 corresponds to the Gaussian distribution, and only in this case is the
prior conjugate to the likelihood function (3.10). Finding the maximum of the poste-
rior distribution over w corresponds to minimization of the regularized error function
(3.29). In the case of the Gaussian prior, the mode of the posterior distribution was
equal to the mean, although this will no longer hold if q = 2.
