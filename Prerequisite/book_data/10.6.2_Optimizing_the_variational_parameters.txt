We now have a normalized Gaussian approximation to the posterior distribution,
which we shall use shortly to evaluate the predictive distribution for new data points.
First, however, we need to determine the variational parameters {ξn} by maximizing
the lower bound on the marginal likelihood.
To do this, we substitute the inequality (10.152) back into the marginal likeli-
hood to give
ln p(t) = ln
p(t|w)p(w) dw � ln
h(w, ξ)p(w) dw = L(ξ).
(10.159)
As with the optimization of the hyperparameter α in the linear regression model of
Section 3.5, there are two approaches to determining the ξn. In the first approach, we
recognize that the function L(ξ) is defined by an integration over w and so we can
view w as a latent variable and invoke the EM algorithm. In the second approach,
we integrate over w analytically and then perform a direct maximization over ξ. Let
us begin by considering the EM approach.
The EM algorithm starts by choosing some initial values for the parameters
In the E step of the EM algorithm,
{ξn}, which we denote collectively by ξold.
N
n=1
10.6. Variational Logistic Regression
501
we then use these parameter values to find the posterior distribution over w, which
is given by (10.156). In the M step, we then maximize the expected complete-data
log likelihood which is given by
Q(ξ, ξold) = E [ln h(w, ξ)p(w)]
(10.160)
where the expectation is taken with respect to the posterior distribution q(w) evalu-
ated using ξold. Noting that p(w) does not depend on ξ, and substituting for h(w, ξ)
we obtain
Q(ξ, ξold) =
ln σ(ξn) − ξn/2 − λ(ξn)(φT
nE[wwT]φn − ξ2
n)
+ const
(10.161)
where ‘const’ denotes terms that are independent of ξ. We now set the derivative with
respect to ξn equal to zero. A few lines of algebra, making use of the definitions of
σ(ξ) and λ(ξ), then gives
0 = λ(ξn)(φT
n).
nE[wwT]φn − ξ2
(10.162)
We now note that λ(ξ) is a monotonic function of ξ for ξ � 0, and that we can
restrict attention to nonnegative values of ξ without loss of generality due to the
symmetry of the bound around ξ = 0. Thus λ(ξ) = 0, and hence we obtain the
following re-estimation equations
(ξnew
n )2 = φT
nE[wwT]φn = φT
n
SN + mN mT
N
φn
(10.163)
where we have used (10.156).
Let us summarize the EM algorithm for finding the variational posterior distri-
bution. We first initialize the variational parameters ξold. In the E step, we evaluate
the posterior distribution over w given by (10.156), in which the mean and covari-
ance are defined by (10.157) and (10.158). In the M step, we then use this variational
posterior to compute a new value for ξ given by (10.163). The E and M steps are
repeated until a suitable convergence criterion is satisfied, which in practice typically
requires only a few iterations.
An alternative approach to obtaining re-estimation equations for ξ is to note
that in the integral over w in the definition (10.159) of the lower bound L(ξ), the
integrand has a Gaussian-like form and so the integral can be evaluated analytically.
Having evaluated the integral, we can then differentiate with respect to ξn. It turns
out that this gives rise to exactly the same re-estimation equations as does the EM
approach given by (10.163).
As we have emphasized already, in the application of variational methods it is
useful to be able to evaluate the lower bound L(ξ) given by (10.159). The integration
over w can be performed analytically by noting that p(w) is Gaussian and h(w, ξ)
is the exponential of a quadratic function of w. Thus, by completing the square
and making use of the standard result for the normalization coefficient of a Gaussian
distribution, we can obtain a closed form solution which takes the form
Exercise 10.33
Exercise 10.34
Exercise 10.35
N
6
4
2
0
−2
−4
−6
−4
10. APPROXIMATE INFERENCE
502
6
4
2
0
−2
−4
−6
−4
5
2
0
50
7
9
9
0
1
0.0
−2
0
2
4
−2
0
2
4
Figure 10.13 Illustration of the Bayesian approach to logistic regression for a simple linearly separable data
set. The plot on the left shows the predictive distribution obtained using variational inference. We see that
the decision boundary lies roughly mid way between the clusters of data points, and that the contours of the
predictive distribution splay out away from the data reflecting the greater uncertainty in the classification of such
regions. The plot on the right shows the decision boundaries corresponding to five samples of the parameter
vector w drawn from the posterior distribution p(w|t).
L(ξ) =
1
2
ln |SN|
|S0| −
1
2
mT
N S−1
N mN +
1
2
mT
0 S−1
0 m0
ln σ(ξn) −
n=1
1
2 ξn − λ(ξn)ξ2
n
(10.164)
This variational framework can also be applied to situations in which the data
In this case we maintain a
is arriving sequentially (Jaakkola and Jordan, 2000).
Gaussian posterior distribution over w, which is initialized using the prior p(w). As
each data point arrives, the posterior is updated by making use of the bound (10.151)
and then normalized to give an updated posterior distribution.
The predictive distribution is obtained by marginalizing over the posterior dis-
tribution, and takes the same form as for the Laplace approximation discussed in
Section 4.5.2. Figure 10.13 shows the variational predictive distributions for a syn-
thetic data set. This example provides interesting insights into the concept of ‘large
margin’, which was discussed in Section 7.1 and which has qualitatively similar be-
haviour to the Bayesian solution.
