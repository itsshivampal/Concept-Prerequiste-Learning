We can use the outer-product approximation to develop a computationally ef-
ficient procedure for approximating the inverse of the Hessian (Hassibi and Stork,
1993). First we write the outer-product approximation in matrix notation as
HN =
bnbT
n
n=1
(5.86)
where bn ≡ ∇wan is the contribution to the gradient of the output unit activation
arising from data point n. We now derive a sequential procedure for building up the
Hessian by including data points one at a time. Suppose we have already obtained
the inverse Hessian using the first L data points. By separating off the contribution
from data point L + 1, we obtain
In order to evaluate the inverse of the Hessian, we now consider the matrix identity
HL+1 = HL + bL+1bT
L+1.
(5.87)
M + vvT
−1 = M−1 −
(M−1v)
vTM−1
1 + vTM−1v
(5.88)
where I is the unit matrix, which is simply a special case of the Woodbury identity
(C.7). If we now identify HL with M and bL+1 with v, we obtain
H−1
L+1 = H−1
L −
H−1
1 + bT
L bL+1bT
L+1H−1
L+1H−1
L bL+1
L
(5.89)
Exercise 5.21
In this way, data points are sequentially absorbed until L+1 = N and the whole data
set has been processed. This result therefore represents a procedure for evaluating
the inverse of the Hessian using a single pass through the data set. The initial matrix
H0 is chosen to be αI, where α is a small quantity, so that the algorithm actually
finds the inverse of H + αI. The results are not particularly sensitive to the precise
value of α. Extension of this algorithm to networks having more than one output is
straightforward.
We note here that the Hessian matrix can sometimes be calculated indirectly as
part of the network training algorithm. In particular, quasi-Newton nonlinear opti-
mization algorithms gradually build up an approximation to the inverse of the Hes-
sian during training. Such algorithms are discussed in detail in Bishop and Nabney
(2008).
