One way to reduce the effective complexity of a network with a large number
of weights is to constrain weights within certain groups to be equal. This is the
technique of weight sharing that was discussed in Section 5.5.6 as a way of building
translation invariance into networks used for image interpretation. It is only appli-
cable, however, to particular problems in which the form of the constraints can be
specified in advance. Here we consider a form of soft weight sharing (Nowlan and
Hinton, 1992) in which the hard constraint of equal weights is replaced by a form
of regularization in which groups of weights are encouraged to have similar values.
Furthermore, the division of weights into groups, the mean weight value for each
group, and the spread of values within the groups are all determined as part of the
learning process.
Exercise 5.28
j=1
j=1
M
M
i
270
5. NEURAL NETWORKS
Section 2.3.9
Exercise 5.29
Recall that the simple weight decay regularizer, given in (5.112), can be viewed
as the negative log of a Gaussian prior distribution over the weights. We can encour-
age the weight values to form several groups, rather than just one group, by consid-
ering instead a probability distribution that is a mixture of Gaussians. The centres
and variances of the Gaussian components, as well as the mixing coefficients, will be
considered as adjustable parameters to be determined as part of the learning process.
Thus, we have a probability density of the form
where
p(w) =
p(wi)
p(wi) =
πjN (wi|µj, σ2
j )
(5.136)
(5.137)
and πj are the mixing coefficients. Taking the negative logarithm then leads to a
regularization function of the form
Ω(w) = −
ln
i
πjN (wi|µj, σ2
j )
The total error function is then given by
E(w) = E(w) + λΩ(w)
(5.138)
(5.139)
where λ is the regularization coefficient. This error is minimized both with respect
to the weights wi and with respect to the parameters {πj, µj, σj} of the mixture
model. If the weights were constant, then the parameters of the mixture model could
be determined by using the EM algorithm discussed in Chapter 9. However, the dis-
tribution of weights is itself evolving during the learning process, and so to avoid nu-
merical instability, a joint optimization is performed simultaneously over the weights
and the mixture-model parameters. This can be done using a standard optimization
algorithm such as conjugate gradients or quasi-Newton methods.
In order to minimize the total error function, it is necessary to be able to evaluate
its derivatives with respect to the various adjustable parameters. To do this it is con-
venient to regard the {πj} as prior probabilities and to introduce the corresponding
posterior probabilities which, following (2.192), are given by Bayes’ theorem in the
form
γj(w) =
πjN (w|µj, σ2
j )
k) .
k πkN (w|µk, σ2
(5.140)
The derivatives of the total error function with respect to the weights are then given
by
E
∂wi
= ∂E
∂wi
γj(wi)
j
(wi − µj)
σ2
j
(5.141)
account of the constraints
easily computed to give
E
∂µj
E
∂σj
i
i
Exercise 5.30
Exercise 5.31
5.5. Regularization in Neural Networks
271
The effect of the regularization term is therefore to pull each weight towards the
centre of the jth Gaussian, with a force proportional to the posterior probability of
that Gaussian for the given weight. This is precisely the kind of effect that we are
seeking.
Derivatives of the error with respect to the centres of the Gaussians are also
γj(wi)
(µi − wj)
σ2
j
(5.142)
which has a simple intuitive interpretation, because it pushes µj towards an aver-
age of the weight values, weighted by the posterior probabilities that the respective
weight parameters were generated by component j. Similarly, the derivatives with
respect to the variances are given by
γj(wi)
1
σj −
(wi − µj)2
σ3
j
(5.143)
which drives σj towards the weighted average of the squared deviations of the weights
around the corresponding centre µj, where the weighting coefficients are again given
by the posterior probability that each weight is generated by component j. Note that
in a practical implementation, new variables ηj defined by
j = exp(ηj)
σ2
(5.144)
are introduced, and the minimization is performed with respect to the ηj. This en-
sures that the parameters σj remain positive. It also has the effect of discouraging
pathological solutions in which one or more of the σj goes to zero, corresponding
to a Gaussian component collapsing onto one of the weight parameter values. Such
solutions are discussed in more detail in the context of Gaussian mixture models in
Section 9.2.1.
For the derivatives with respect to the mixing coefficients πj, we need to take
πj = 1,
0 � πi � 1
(5.145)
j
which follow from the interpretation of the πj as prior probabilities. This can be
done by expressing the mixing coefficients in terms of a set of auxiliary variables
{ηj} using the softmax function given by
πj =
exp(ηj)
M
k=1 exp(ηk)
(5.146)
Exercise 5.32
The derivatives of the regularized error function with respect to the {ηj} then take
the form
272
5. NEURAL NETWORKS
Figure 5.18 The left figure shows a two-link robot arm,
in which the Cartesian coordinates (x1, x2) of the end ef-
fector are determined uniquely by the two joint angles θ1
and θ2 and the (fixed) lengths L1 and L2 of the arms. This
is know as the forward kinematics of the arm.
In prac-
tice, we have to find the joint angles that will give rise to a
desired end effector position and, as shown in the right fig-
ure, this inversekinematicshas two solutions correspond-
ing to ‘elbow up’ and ‘elbow down’.
L2
θ2
L1
E
∂ηj
i
{πj − γj(wi)} .
Exercise 5.33
We see that πj is therefore driven towards the average posterior probability for com-
ponent j.
