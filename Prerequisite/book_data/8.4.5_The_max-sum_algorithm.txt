The sum-product algorithm allows us to take a joint distribution p(x) expressed
as a factor graph and efficiently find marginals over the component variables. Two
other common tasks are to find a setting of the variables that has the largest prob-
ability and to find the value of that probability. These can be addressed through a
closely related algorithm called max-sum, which can be viewed as an application of
dynamic programming in the context of graphical models (Cormen et al., 2001).
A simple approach to finding latent variable values having high probability
would be to run the sum-product algorithm to obtain the marginals p(xi) for ev-
ery variable, and then, for each marginal in turn, to find the value x
i that maximizes
that marginal. However, this would give the set of values that are individually the
most probable. In practice, we typically wish to find the set of values that jointly
have the largest probability, in other words the vector xmax that maximizes the joint
distribution, so that
xmax = arg max
p(x)
for which the corresponding value of the joint probability will be given by
x
p(xmax) = max
x
p(x).
(8.87)
(8.88)
In general, xmax is not the same as the set of x
i values, as we can easily show using
a simple example. Consider the joint distribution p(x, y) over two binary variables
x, y ∈ {0, 1} given in Table 8.1. The joint distribution is maximized by setting x =
1 and y = 0, corresponding the value 0.4. However, the marginal for p(x), obtained
by summing over both values of y, is given by p(x = 0) = 0.6 and p(x = 1) = 0.4,
and similarly the marginal for y is given by p(y = 0) = 0.7 and p(y = 1) = 0.3,
and so the marginals are maximized by x = 0 and y = 0, which corresponds to a
value of 0.3 for the joint distribution. In fact, it is not difficult to construct examples
for which the set of individually most probable values has probability zero under the
joint distribution.
We therefore seek an efficient algorithm for finding the value of x that maxi-
mizes the joint distribution p(x) and that will allow us to obtain the value of the
joint distribution at its maximum. To address the second of these problems, we shall
simply write out the max operator in terms of its components
max
x
p(x) = max
x1
. . . max
xM
p(x)
(8.89)
Exercise 8.27
412
8. GRAPHICAL MODELS
where M is the total number of variables, and then substitute for p(x) using its
expansion in terms of a product of factors. In deriving the sum-product algorithm,
we made use of the distributive law (8.53) for multiplication. Here we make use of
the analogous law for the max operator
max(ab, ac) = a max(b, c)
(8.90)
which holds if a � 0 (as will always be the case for the factors in a graphical model).
This allows us to exchange products with maximizations.
Consider first the simple example of a chain of nodes described by (8.49). The
evaluation of the probability maximum can be written as
max
x
p(x) =
1
Z
1
Z
max
x1
max
x1
··· max
ψ1,2(x1, x2)
xN
[ψ1,2(x1, x2)··· ψN−1,N (xN−1, xN )]
··· max
ψN−1,N (xN−1, xN )
xN
As with the calculation of marginals, we see that exchanging the max and product
operators results in a much more efficient computation, and one that is easily inter-
preted in terms of messages passed from node xN backwards along the chain to node
x1.
We can readily generalize this result to arbitrary tree-structured factor graphs
by substituting the expression (8.59) for the factor graph expansion into (8.89) and
again exchanging maximizations with products. The structure of this calculation is
identical to that of the sum-product algorithm, and so we can simply translate those
results into the present context. In particular, suppose that we designate a particular
variable node as the ‘root’ of the graph. Then we start a set of messages propagating
inwards from the leaves of the tree towards the root, with each node sending its
message towards the root once it has received all incoming messages from its other
neighbours. The final maximization is performed over the product of all messages
arriving at the root node, and gives the maximum value for p(x). This could be called
the max-product algorithm and is identical to the sum-product algorithm except that
summations are replaced by maximizations. Note that at this stage, messages have
been sent from leaves to the root, but not in the other direction.
In practice, products of many small probabilities can lead to numerical under-
flow problems, and so it is convenient to work with the logarithm of the joint distri-
bution. The logarithm is a monotonic function, so that if a > b then ln a > ln b, and
hence the max operator and the logarithm function can be interchanged, so that
ln
max
x
p(x)
= max
x
ln p(x).
The distributive property is preserved because
max(a + b, a + c) = a + max(b, c).
(8.91)
(8.92)
Thus taking the logarithm simply has the effect of replacing the products in the
max-product algorithm with sums, and so we obtain the max-sum algorithm. From
(8.93)
(8.94)
(8.95)
(8.96)
(8.97)
s∈ne(x)
8.4. Inference in Graphical Models
413
the results (8.66) and (8.69) derived earlier for the sum-product algorithm, we can
readily write down the max-sum algorithm in terms of message passing simply by
replacing ‘sum’ with ‘max’ and replacing products with sums of logarithms to give
µf→x(x) = max
x1,...,xM
ln f(x, x1, . . . , xM ) +
µxm→f (xm)
m∈ne(fs)\x
µx→f (x) =
l∈ne(x)\f
µfl→x(x).
The initial messages sent by the leaf nodes are obtained by analogy with (8.70) and
(8.71) and are given by
while at the root node the maximum probability can then be computed, by analogy
with (8.63), using
µx→f (x) = 0
µf→x(x) = ln f(x)
pmax = max
x
µfs→x(x)
So far, we have seen how to find the maximum of the joint distribution by prop-
agating messages from the leaves to an arbitrarily chosen root node. The result will
be the same irrespective of which node is chosen as the root. Now we turn to the
second problem of finding the configuration of the variables for which the joint dis-
tribution attains this maximum value. So far, we have sent messages from the leaves
to the root. The process of evaluating (8.97) will also give the value xmax for the
most probable value of the root node variable, defined by
xmax = arg max
x
µfs→x(x)
s∈ne(x)
(8.98)
At this point, we might be tempted simply to continue with the message passing al-
gorithm and send messages from the root back out to the leaves, using (8.93) and
(8.94), then apply (8.98) to all of the remaining variable nodes. However, because
we are now maximizing rather than summing, it is possible that there may be mul-
tiple configurations of x all of which give rise to the maximum value for p(x). In
such cases, this strategy can fail because it is possible for the individual variable
values obtained by maximizing the product of messages at each node to belong to
different maximizing configurations, giving an overall configuration that no longer
corresponds to a maximum.
The problem can be resolved by adopting a rather different kind of message
passing from the root node to the leaves. To see how this works, let us return once
again to the simple chain example of N variables x1, . . . , xN each having K states,
414
8. GRAPHICAL MODELS
Figure 8.53 A lattice, or trellis, diagram show-
ing explicitly the K possible states (one per row
of the diagram) for each of the variables xn in the
chain model.
In this illustration K = 3. The ar-
row shows the direction of message passing in the
max-product algorithm. For every state k of each
variable xn (corresponding to column n of the dia-
gram) the function φ(xn) defines a unique state at
the previous variable, indicated by the black lines.
The two paths through the lattice correspond to
configurations that give the global maximum of the
joint probability distribution, and either of these
can be found by tracing back along the black lines
in the opposite direction to the arrow.
k = 1
k = 2
k = 3
n − 2
n − 1
n
n + 1
corresponding to the graph shown in Figure 8.38. Suppose we take node xN to be
the root node. Then in the first phase, we propagate messages from the leaf node x1
to the root node using
µxn→fn,n+1(xn) = µfn−1,n→xn(xn)
µfn−1,n→xn(xn) = max
xn−1
ln fn−1,n(xn−1, xn) + µxn−1→f n−1,n(xn)
which follow from applying (8.94) and (8.93) to this particular graph. The initial
message sent from the leaf node is simply
The most probable value for xN is then given by
µx1→f1,2(x1) = 0.
N = arg max
xmax
xN
µfN−1,N→xN (xN )
(8.99)
(8.100)
Now we need to determine the states of the previous variables that correspond to the
same maximizing configuration. This can be done by keeping track of which values
of the variables gave rise to the maximum state of each variable, in other words by
storing quantities given by
φ(xn) = arg max
xn−1
ln fn−1,n(xn−1, xn) + µxn−1→f n−1,n(xn)
(8.101)
To understand better what is happening, it is helpful to represent the chain of vari-
ables in terms of a lattice or trellis diagram as shown in Figure 8.53. Note that this
is not a probabilistic graphical model because the nodes represent individual states
of variables, while each variable corresponds to a column of such states in the di-
agram. For each state of a given variable, there is a unique state of the previous
variable that maximizes the probability (ties are broken either systematically or at
random), corresponding to the function φ(xn) given by (8.101), and this is indicated
8.4. Inference in Graphical Models
415
by the lines connecting the nodes. Once we know the most probable value of the fi-
nal node xN , we can then simply follow the link back to find the most probable state
of node xN−1 and so on back to the initial node x1. This corresponds to propagating
a message back down the chain using
n−1 = φ(xmax
n )
xmax
(8.102)
and is known as back-tracking. Note that there could be several values of xn−1 all
of which give the maximum value in (8.101). Provided we chose one of these values
when we do the back-tracking, we are assured of a globally consistent maximizing
configuration.
In Figure 8.53, we have indicated two paths, each of which we shall suppose
corresponds to a global maximum of the joint probability distribution. If k = 2
and k = 3 each represent possible values of xmax
N , then starting from either state
and tracing back along the black lines, which corresponds to iterating (8.102), we
obtain a valid global maximum configuration. Note that if we had run a forward
pass of max-sum message passing followed by a backward pass and then applied
(8.98) at each node separately, we could end up selecting some states from one path
and some from the other path, giving an overall configuration that is not a global
maximizer. We see that it is necessary instead to keep track of the maximizing states
during the forward pass using the functions φ(xn) and then use back-tracking to find
a consistent solution.
The extension to a general tree-structured factor graph should now be clear. If
a message is sent from a factor node f to a variable node x, a maximization is
performed over all other variable nodes x1, . . . , xM that are neighbours of that fac-
tor node, using (8.93). When we perform this maximization, we keep a record of
which values of the variables x1, . . . , xM gave rise to the maximum. Then in the
back-tracking step, having found xmax, we can then use these stored values to as-
sign consistent maximizing states xmax
M . The max-sum algorithm, with
back-tracking, gives an exact maximizing configuration for the variables provided
the factor graph is a tree. An important application of this technique is for finding
the most probable sequence of hidden states in a hidden Markov model, in which
case it is known as the Viterbi algorithm.
, . . . , xmax
1
As with the sum-product algorithm, the inclusion of evidence in the form of
observed variables is straightforward. The observed variables are clamped to their
observed values, and the maximization is performed over the remaining hidden vari-
ables. This can be shown formally by including identity functions for the observed
variables into the factor functions, as we did for the sum-product algorithm.
It is interesting to compare max-sum with the iterated conditional modes (ICM)
algorithm described on page 389. Each step in ICM is computationally simpler be-
cause the ‘messages’ that are passed from one node to the next comprise a single
value consisting of the new state of the node for which the conditional distribution
is maximized. The max-sum algorithm is more complex because the messages are
functions of node variables x and hence comprise a set of K values for each pos-
sible state of x. Unlike max-sum, however, ICM is not guaranteed to find a global
maximum even for tree-structured graphs.
Section 13.2
416
8. GRAPHICAL MODELS
