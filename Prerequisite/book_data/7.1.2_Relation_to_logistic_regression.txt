As with the separable case, we can re-cast the SVM for nonseparable distri-
butions in terms of the minimization of a regularized error function. This will also
allow us to highlight similarities, and differences, compared to the logistic regression
model.
We have seen that for data points that are on the correct side of the margin
boundary, and which therefore satisfy yntn � 1, we have ξn = 0, and for the
Section 4.3.2
n=1
N
N
Figure 7.5 Plot of the ‘hinge’ error function used
in support vector machines, shown
in blue, along with the error function
for logistic regression, rescaled by a
factor of 1/ ln(2) so that it passes
through the point (0, 1), shown in red.
Also shown are the misclassification
error in black and the squared error
in green.
7.1. Maximum Margin Classifiers
337
E(z)
−2
−1
0
1
z
2
remaining points we have ξn = 1 − yntn. Thus the objective function (7.21) can be
written (up to an overall multiplicative constant) in the form
ESV(yntn) + λw2
(7.44)
where λ = (2C)−1, and ESV(·) is the hinge error function defined by
ESV(yntn) = [1 − yntn]+
(7.45)
where [· ]+ denotes the positive part. The hinge error function, so-called because
of its shape, is plotted in Figure 7.5. It can be viewed as an approximation to the
misclassification error, i.e., the error function that ideally we would like to minimize,
which is also shown in Figure 7.5.
When we considered the logistic regression model in Section 4.3.2, we found it
convenient to work with target variable t ∈ {0, 1}. For comparison with the support
vector machine, we first reformulate maximum likelihood logistic regression using
the target variable t ∈ {−1, 1}. To do this, we note that p(t = 1|y) = σ(y) where
y(x) is given by (7.1), and σ(y) is the logistic sigmoid function defined by (4.59). It
follows that p(t = −1|y) = 1 − σ(y) = σ(−y), where we have used the properties
of the logistic sigmoid function, and so we can write
p(t|y) = σ(yt).
(7.46)
Exercise 7.6
From this we can construct an error function by taking the negative logarithm of the
likelihood function that, with a quadratic regularizer, takes the form
where
ELR(yntn) + λw2.
n=1
ELR(yt) = ln (1 + exp(−yt)) .
(7.47)
(7.48)
338
7. SPARSE KERNEL MACHINES
For comparison with other error functions, we can divide by ln(2) so that the error
function passes through the point (0, 1). This rescaled error function is also plotted
in Figure 7.5 and we see that it has a similar form to the support vector error function.
The key difference is that the flat region in ESV(yt) leads to sparse solutions.
Both the logistic error and the hinge loss can be viewed as continuous approx-
imations to the misclassification error. Another continuous error function that has
sometimes been used to solve classification problems is the squared error, which
is again plotted in Figure 7.5. It has the property, however, of placing increasing
emphasis on data points that are correctly classified but that are a long way from
the decision boundary on the correct side. Such points will be strongly weighted at
the expense of misclassified points, and so if the objective is to minimize the mis-
classification rate, then a monotonically decreasing error function would be a better
choice.
