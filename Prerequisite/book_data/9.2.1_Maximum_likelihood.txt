Suppose we have a data set of observations {x1, . . . , xN}, and we wish to model
this data using a mixture of Gaussians. We can represent this data set as an N × D
N
1
K
0.5
9.2. Mixtures of Gaussians
433
1
1
(a)
1
(b)
0.5
0
0.5
0
0
0.5
1
0
0.5
1
(c)
0.5
0
0
Figure 9.5 Example of 500 points drawn from the mixture of 3 Gaussians shown in Figure 2.23. (a) Samples
from the joint distribution p(z)p(x|z) in which the three states of z, corresponding to the three components of the
mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution
p(x), which is obtained by simply ignoring the values of z and just plotting the x values. The data set in (a) is
said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the
value of the responsibilities γ(znk) associated with data point xn, obtained by plotting the corresponding point
using proportions of red, blue, and green ink given by γ(znk) for k = 1, 2, 3, respectively
matrix X in which the nth row is given by xT
n. Similarly, the corresponding latent
variables will be denoted by an N × K matrix Z with rows zT
n. If we assume that
the data points are drawn independently from the distribution, then we can express
the Gaussian mixture model for this i.i.d. data set using the graphical representation
shown in Figure 9.6. From (9.7) the log of the likelihood function is given by
ln p(X|π, µ, Σ) =
ln
n=1
k=1
πkN (xn|µk, Σk)
(9.14)
Before discussing how to maximize this function, it is worth emphasizing that
there is a significant problem associated with the maximum likelihood framework
applied to Gaussian mixture models, due to the presence of singularities. For sim-
plicity, consider a Gaussian mixture whose components have covariance matrices
given by Σk = σ2
kI, where I is the unit matrix, although the conclusions will hold
for general covariance matrices. Suppose that one of the components of the mixture
model, let us say the jth component, has its mean µj exactly equal to one of the data
Figure 9.6 Graphical representation of a Gaussian mixture model
for a set of N i.i.d. data points {xn}, with corresponding
latent points {zn}, where n = 1, . . . , N.
zn
xn
N
434
9. MIXTURE MODELS AND EM
Figure 9.7 Illustration of how singularities in the
likelihood function arise with mixtures
of Gaussians. This should be com-
pared with the case of a single Gaus-
sian shown in Figure 1.14 for which no
singularities arise.
p(x)
x
points so that µj = xn for some value of n. This data point will then contribute a
term in the likelihood function of the form
N (xn|xn, σ2
j I) =
1
(2π)1/2
1
σj
(9.15)
If we consider the limit σj → 0, then we see that this term goes to infinity and
so the log likelihood function will also go to infinity. Thus the maximization of
the log likelihood function is not a well posed problem because such singularities
will always be present and will occur whenever one of the Gaussian components
‘collapses’ onto a specific data point. Recall that this problem did not arise in the
case of a single Gaussian distribution. To understand the difference, note that if a
single Gaussian collapses onto a data point it will contribute multiplicative factors
to the likelihood function arising from the other data points and these factors will go
to zero exponentially fast, giving an overall likelihood that goes to zero rather than
infinity. However, once we have (at least) two components in the mixture, one of
the components can have a finite variance and therefore assign finite probability to
all of the data points while the other component can shrink onto one specific data
point and thereby contribute an ever increasing additive value to the log likelihood.
This is illustrated in Figure 9.7. These singularities provide another example of the
severe over-fitting that can occur in a maximum likelihood approach. We shall see
that this difficulty does not occur if we adopt a Bayesian approach. For the moment,
however, we simply note that in applying maximum likelihood to Gaussian mixture
models we must take steps to avoid finding such pathological solutions and instead
seek local maxima of the likelihood function that are well behaved. We can hope to
avoid the singularities by using suitable heuristics, for instance by detecting when a
Gaussian component is collapsing and resetting its mean to a randomly chosen value
while also resetting its covariance to some large value, and then continuing with the
optimization.
A further issue in finding maximum likelihood solutions arises from the fact
that for any given maximum likelihood solution, a K-component mixture will have
a total of K! equivalent solutions corresponding to the K! ways of assigning K
sets of parameters to K components. In other words, for any given (nondegenerate)
point in the space of parameter values there will be a further K!−1 additional points
all of which give rise to exactly the same distribution. This problem is known as
Section 10.1
N
n=1
N
N
9.2. Mixtures of Gaussians
435
identifiability (Casella and Berger, 2002) and is an important issue when we wish to
interpret the parameter values discovered by a model. Identifiability will also arise
when we discuss models having continuous latent variables in Chapter 12. However,
for the purposes of finding a good density model, it is irrelevant because any of the
equivalent solutions is as good as any other.
Maximizing the log likelihood function (9.14) for a Gaussian mixture model
turns out to be a more complex problem than for the case of a single Gaussian. The
difficulty arises from the presence of the summation over k that appears inside the
logarithm in (9.14), so that the logarithm function no longer acts directly on the
Gaussian. If we set the derivatives of the log likelihood to zero, we will no longer
obtain a closed form solution, as we shall see shortly.
One approach is to apply gradient-based optimization techniques (Fletcher, 1987;
Nocedal and Wright, 1999; Bishop and Nabney, 2008). Although gradient-based
techniques are feasible, and indeed will play an important role when we discuss
mixture density networks in Chapter 5, we now consider an alternative approach
known as the EM algorithm which has broad applicability and which will lay the
foundations for a discussion of variational inference techniques in Chapter 10.
