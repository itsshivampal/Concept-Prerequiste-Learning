In Chapter 2, we discussed the important role played by the exponential family of
distributions and their conjugate priors. For many of the models discussed in this
book, the complete-data likelihood is drawn from the exponential family. However,
in general this will not be the case for the marginal likelihood function for the ob-
served data. For example, in a mixture of Gaussians, the joint distribution of obser-
vations xn and corresponding hidden variables zn is a member of the exponential
family, whereas the marginal distribution of xn is a mixture of Gaussians and hence
is not.
Up to now we have grouped the variables in the model into observed variables
and hidden variables. We now make a further distinction between latent variables,
denoted Z, and parameters, denoted θ, where parameters are intensive (fixed in num-
ber independent of the size of the data set), whereas latent variables are extensive
(scale in number with the size of the data set). For example, in a Gaussian mixture
model, the indicator variables zkn (which specify which component k is responsible
for generating data point xn) represent the latent variables, whereas the means µk,
precisions Λk and mixing proportions πk represent the parameters.
Consider the case of independent identically distributed data. We denote the
data values by X = {xn}, where n = 1, . . . N, with corresponding latent variables
Z = {zn}. Now suppose that the joint distribution of observed and latent variables
is a member of the exponential family, parameterized by natural parameters η so that
p(X, Z|η) =
n=1
h(xn, zn)g(η) exp
ηTu(xn, zn)
(10.113)
We shall also use a conjugate prior for η, which can be written as
p(η|ν0, v0) = f(ν0, χ0)g(η)ν0 exp
νoηTχ0
(10.114)
Recall that the conjugate prior distribution can be interpreted as a prior number ν0
of observations all having the value χ0 for the u vector. Now consider a variational
n=1
N
n=1
10.4. Exponential Family Distributions
491
distribution that factorizes between the latent variables and the parameters, so that
q(Z, η) = q(Z)q(η). Using the general result (10.9), we can solve for the two
factors as follows
ln q(Z) = Eη[ln p(X, Z|η)] + const
ln h(xn, zn) + E[ηT]u(xn, zn)
+ const. (10.115)
Section 10.2.5
Thus we see that this decomposes into a sum of independent terms, one for each
value of n, and hence the solution for q(Z) will factorize over n so that q(Z) =
n q(zn). This is an example of an induced factorization. Taking the exponential
of both sides, we have
q(zn) = h(xn, zn)g (E[η]) exp
E[ηT]u(xn, zn)
(10.116)
where the normalization coefficient has been re-instated by comparison with the
standard form for the exponential family.
Similarly, for the variational distribution over the parameters, we have
ln q(η) = ln p(η|ν0, χ0) + EZ[ln p(X, Z|η)] + const
ln g(η) + ηTEzn[u(xn, zn)]
= ν0 ln g(η) + ηTχ0 +
N
(10.117)
+ const. (10.118)
Again, taking the exponential of both sides, and re-instating the normalization coef-
ficient by inspection, we have
q(η) = f(νN , χN )g(η)νN exp
ηTχN
(10.119)
where we have defined
νN = ν0 + N
N
χN = χ0 +
Ezn[u(xn, zn)].
n=1
(10.120)
(10.121)
Note that the solutions for q(zn) and q(η) are coupled, and so we solve them iter-
atively in a two-stage procedure. In the variational E step, we evaluate the expected
sufficient statistics E[u(xn, zn)] using the current posterior distribution q(zn) over
the latent variables and use this to compute a revised posterior distribution q(η) over
the parameters. Then in the subsequent variational M step, we use this revised pa-
rameter posterior distribution to find the expected natural parameters E[ηT], which
gives rise to a revised variational distribution over the latent variables.
