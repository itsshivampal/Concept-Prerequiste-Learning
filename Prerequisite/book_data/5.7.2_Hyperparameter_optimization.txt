So far, we have assumed that the hyperparameters α and β are fixed and known.
We can make use of the evidence framework, discussed in Section 3.5, together with
the Gaussian approximation to the posterior obtained using the Laplace approxima-
tion, to obtain a practical procedure for choosing the values of such hyperparameters.
The marginal likelihood, or evidence, for the hyperparameters is obtained by
integrating over the network weights
ln p(D|α, β)  −E(wMAP) −
where W is the total number of parameters in w, and the regularized error function
is defined by
ln(2π) (5.175)
ln β −
ln|A| + W
2
ln α + N
2
N
2
1
2
E(wMAP) = β
2
{y(xn, wMAP) − tn}2 + α
2
wT
MAPwMAP.
(5.176)
We see that this takes the same form as the corresponding result (3.86) for the linear
regression model.
In the evidence framework, we make point estimates for α and β by maximizing
ln p(D|α, β). Consider first the maximization with respect to α, which can be done
by analogy with the linear regression case discussed in Section 3.5.2. We first define
the eigenvalue equation
(5.177)
where H is the Hessian matrix comprising the second derivatives of the sum-of-
squares error function, evaluated at w = wMAP. By analogy with (3.92), we obtain
βHui = λiui
wT
MAPwMAP
(5.178)
i=1
W
5.7. Bayesian Neural Networks
where γ represents the effective number of parameters and is defined by
λi
α + λi
281
(5.179)
Section 3.5.3
Section 5.1.1
Note that this result was exact for the linear regression case. For the nonlinear neural
network, however, it ignores the fact that changes in α will cause changes in the
Hessian H, which in turn will change the eigenvalues. We have therefore implicitly
ignored terms involving the derivatives of λi with respect to α.
Similarly, from (3.95) we see that maximizing the evidence with respect to β
gives the re-estimation formula
1
N
1
N − γ
n=1
{y(xn, wMAP) − tn}2.
(5.180)
As with the linear model, we need to alternate between re-estimation of the hyper-
parameters α and β and updating of the posterior distribution. The situation with
a neural network model is more complex, however, due to the multimodality of the
posterior distribution. As a consequence, the solution for wMAP found by maximiz-
ing the log posterior will depend on the initialization of w. Solutions that differ only
as a consequence of the interchange and sign reversal symmetries in the hidden units
are identical so far as predictions are concerned, and it is irrelevant which of the
equivalent solutions is found. However, there may be inequivalent solutions as well,
and these will generally yield different values for the optimized hyperparameters.
In order to compare different models, for example neural networks having differ-
ent numbers of hidden units, we need to evaluate the model evidence p(D). This can
be approximated by taking (5.175) and substituting the values of α and β obtained
from the iterative optimization of these hyperparameters. A more careful evaluation
is obtained by marginalizing over α and β, again by making a Gaussian approxima-
tion (MacKay, 1992c; Bishop, 1995a). In either case, it is necessary to evaluate the
determinant |A| of the Hessian matrix. This can be problematic in practice because
the determinant, unlike the trace, is sensitive to the small eigenvalues that are often
difficult to determine accurately.
The Laplace approximation is based on a local quadratic expansion around a
mode of the posterior distribution over weights. We have seen in Section 5.1.1 that
any given mode in a two-layer network is a member of a set of M!2M equivalent
modes that differ by interchange and sign-change symmetries, where M is the num-
ber of hidden units. When comparing networks having different numbers of hid-
den units, this can be taken into account by multiplying the evidence by a factor of
M!2M .
