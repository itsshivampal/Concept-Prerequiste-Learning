There is an important issue that must be addressed before we can make use of the
forward backward algorithm in practice. From the recursion relation (13.36), we note
that at each step the new value α(zn) is obtained from the previous value α(zn−1)
by multiplying by quantities p(zn|zn−1) and p(xn|zn). Because these probabilities
are often significantly less than unity, as we work our way forward along the chain,
the values of α(zn) can go to zero exponentially quickly. For moderate lengths of
chain (say 100 or so), the calculation of the α(zn) will soon exceed the dynamic
range of the computer, even if double precision floating point is used.
In the case of i.i.d. data, we implicitly circumvented this problem with the eval-
uation of likelihood functions by taking logarithms. Unfortunately, this will not help
here because we are forming sums of products of small numbers (we are in fact im-
plicitly summing over all possible paths through the lattice diagram of Figure 13.7).
We therefore work with re-scaled versions of α(zn) and β(zn) whose values remain
of order unity. As we shall see, the corresponding scaling factors cancel out when
we use these re-scaled quantities in the EM algorithm.
In (13.34), we defined α(zn) = p(x1, . . . , xn, zn) representing the joint distri-
bution of all the observations up to xn and the latent variable zn. Now we define a
normalized version of α given by
α(zn) = p(zn|x1, . . . , xn) =
α(zn)
p(x1, . . . , xn)
(13.55)
which we expect to be well behaved numerically because it is a probability distribu-
tion over K variables for any value of n. In order to relate the scaled and original al-
pha variables, we introduce scaling factors defined by conditional distributions over
the observed variables
cn = p(xn|x1, . . . , xn−1).
(13.56)
n
n
zn−1
N
N
n=1
m=n+1
zn+1
628
13. SEQUENTIAL DATA
From the product rule, we then have
p(x1, . . . , xn) =
cm
m=1
(13.57)
and so
α(zn) = p(zn|x1, . . . , xn)p(x1, . . . , xn) =
cm
α(zn).
(13.58)
m=1
We can then turn the recursion equation (13.36) for α into one for
α given by
cn
α(zn) = p(xn|zn)
α(zn−1)p(zn|zn−1).
(13.59)
α(zn),
Note that at each stage of the forward message passing phase, used to evaluate
we have to evaluate and store cn, which is easily done because it is the coefficient
that normalizes the right-hand side of (13.59) to give
We can similarly define re-scaled variables
α(zn).
β(zn) using
β(zn) =
cm
β(zn)
(13.60)
which will again remain within machine precision because, from (13.35), the quan-
tities
β(zn) are simply the ratio of two conditional probabilities
β(zn) =
p(xn+1, . . . , xN|zn)
p(xn+1, . . . , xN|x1, . . . , xn) .
(13.61)
The recursion result (13.38) for β then gives the following recursion for the re-scaled
variables
cn+1
β(zn) =
β(zn+1)p(xn+1|zn+1)p(zn+1|zn).
(13.62)
In applying this recursion relation, we make use of the scaling factors cn that were
previously computed in the α phase.
From (13.57), we see that the likelihood function can be found using
p(X) =
cn.
(13.63)
Exercise 13.15
Similarly, using (13.33) and (13.43), together with (13.63), we see that the required
marginals are given by
γ(zn) =
α(zn)
β(zn)
ξ(zn−1, zn) = cn
α(zn−1)p(xn|zn)p(zn|z−1)
β(zn).
(13.64)
(13.65)
Section 13.3
13.2. Hidden Markov Models
629
α(zn)
β(zn) instead of using
Finally, we note that there is an alternative formulation of the forward-backward
algorithm (Jordan, 2007) in which the backward pass is defined by a recursion based
the quantities γ(zn) =
β(zn). This α–γ recursion
α(zn)
requires that the forward pass be completed first so that all the quantities
are available for the backward pass, whereas the forward and backward passes of
the α–β algorithm can be done independently. Although these two algorithms have
comparable computational cost, the α–β version is the most commonly encountered
one in the case of hidden Markov models, whereas for linear dynamical systems a
recursion analogous to the α–γ form is more usual.
