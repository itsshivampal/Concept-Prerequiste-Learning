Suppose that our goal is simply to make as few misclassifications as possible.
We need a rule that assigns each value of x to one of the available classes. Such a
rule will divide the input space into regions Rk called decision regions, one for each
class, such that all points in Rk are assigned to class Ck. The boundaries between
decision regions are called decision boundaries or decision surfaces. Note that each
decision region need not be contiguous but could comprise some number of disjoint
regions. We shall encounter examples of decision boundaries and decision regions in
later chapters. In order to find the optimal decision rule, consider first of all the case
of two classes, as in the cancer problem for instance. A mistake occurs when an input
vector belonging to class C1 is assigned to class C2 or vice versa. The probability of
this occurring is given by
p(mistake) = p(x âˆˆ R1,C2) + p(x âˆˆ R2,C1)
p(x,C2) dx +
R1
p(x,C1) dx.
R2
(1.78)
We are free to choose the decision rule that assigns each point x to one of the two
classes. Clearly to minimize p(mistake) we should arrange that each x is assigned to
whichever class has the smaller value of the integrand in (1.78). Thus, if p(x,C1) >
p(x,C2) for a given value of x, then we should assign that x to class C1. From the
product rule of probability we have p(x,Ck) = p(Ck|x)p(x). Because the factor
p(x) is common to both terms, we can restate this result as saying that the minimum
x
b
x0
k=1
K
K
b
b
b
b
b
b
b
40
1. INTRODUCTION
p(x,C1)
R1
p(x,C2)
R2
x
x. Values of x ï¿½
Figure 1.24 Schematic illustration of the joint probabilities p(x,Ck) for each of two classes plotted
against x, together with the decision boundary x =
x are classified as
class C2 and hence belong to decision region R2, whereas points x <
x are classified
as C1 and belong to R1. Errors arise from the blue, green, and red regions, so that for
x the errors are due to points from class C2 being misclassified as C1 (represented by
x <
the sum of the red and green regions), and conversely for points in the region x ï¿½
x the
errors are due to points from class C1 being misclassified as C2 (represented by the blue
region). As we vary the location
x of the decision boundary, the combined areas of the
blue and green regions remains constant, whereas the size of the red region varies. The
optimal choice for
x is where the curves for p(x,C1) and p(x, C2) cross, corresponding to
x = x0, because in this case the red region disappears. This is equivalent to the minimum
misclassification rate decision rule, which assigns each value of x to the class having the
higher posterior probability p(Ck|x).
probability of making a mistake is obtained if each value of x is assigned to the class
for which the posterior probability p(Ck|x) is largest. This result is illustrated for
two classes, and a single input variable x, in Figure 1.24.
For the more general case of K classes, it is slightly easier to maximize the
probability of being correct, which is given by
p(correct) =
p(x âˆˆ Rk,Ck)
k=1
Rk
p(x,Ck) dx
(1.79)
which is maximized when the regions Rk are chosen such that each x is assigned
to the class for which p(x,Ck) is largest. Again, using the product rule p(x,Ck) =
p(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see
that each x should be assigned to the class having the largest posterior probability
p(Ck|x).
 cancer
Figure 1.25 An example of a loss matrix with ele-
ments Lkj for the cancer treatment problem. The rows
correspond to the true class, whereas the columns cor-
respond to the assignment of class made by our deci-
sion criterion.
cancer
normal
0
1
normal
1000
0
1.5. Decision Theory
41
