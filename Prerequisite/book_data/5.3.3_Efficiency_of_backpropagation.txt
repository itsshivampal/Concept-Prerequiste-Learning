One of the most important aspects of backpropagation is its computational effi-
ciency. To understand this, let us examine how the number of computer operations
required to evaluate the derivatives of the error function scales with the total number
W of weights and biases in the network. A single evaluation of the error function
(for a given input pattern) would require O(W ) operations, for sufficiently large W .
This follows from the fact that, except for a network with very sparse connections,
the number of weights is typically much greater than the number of units, and so the
bulk of the computational effort in forward propagation is concerned with evaluat-
ing the sums in (5.48), with the evaluation of the activation functions representing a
small overhead. Each term in the sum in (5.48) requires one multiplication and one
addition, leading to an overall computational cost that is O(W ).
An alternative approach to backpropagation for computing the derivatives of the
error function is to use finite differences. This can be done by perturbing each weight
in turn, and approximating the derivatives by the expression
∂En
∂wji
= En(wji + 	) − En(wji)
+ O(	)
(5.68)
where 	  1. In a software simulation, the accuracy of the approximation to the
derivatives can be improved by making 	 smaller, until numerical roundoff problems
arise. The accuracy of the finite differences method can be improved significantly
by using symmetrical central differences of the form
∂En
∂wji
= En(wji + 	) − En(wji − 	)
2
+ O(	2).
(5.69)
Exercise 5.14
In this case, the O(	) corrections cancel, as can be verified by Taylor expansion on
the right-hand side of (5.69), and so the residual corrections are O(	2). The number
of computational steps is, however, roughly doubled compared with (5.68).
The main problem with numerical differentiation is that the highly desirable
O(W ) scaling has been lost. Each forward propagation requires O(W ) steps, and
Figure 5.8 Illustration of a modular pattern
recognition system in which the
Jacobian matrix can be used
to backpropagate error signals
from the outputs through to ear-
lier modules in the system.
u
x
5.3. Error Backpropagation
247
v
z
w
y
there are W weights in the network each of which must be perturbed individually, so
that the overall scaling is O(W 2).
However, numerical differentiation plays an important role in practice, because a
comparison of the derivatives calculated by backpropagation with those obtained us-
ing central differences provides a powerful check on the correctness of any software
implementation of the backpropagation algorithm. When training networks in prac-
tice, derivatives should be evaluated using backpropagation, because this gives the
greatest accuracy and numerical efficiency. However, the results should be compared
with numerical differentiation using (5.69) for some test cases in order to check the
correctness of the implementation.
