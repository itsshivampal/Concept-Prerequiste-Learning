Because the logistic regression model defines a conditional distribution for the
target variable, given the input vector, it is straightforward to use it as the component
distribution in a mixture model, thereby giving rise to a richer family of conditional
distributions compared to a single logistic regression model. This example involves
a straightforward combination of ideas encountered in earlier sections of the book
and will help consolidate these for the reader.
The conditional distribution of the target variable, for a probabilistic mixture of
K logistic regression models, is given by
p(t|φ, θ) =
πkyt
k [1 − yk]1−t
(14.45)
where φ is the feature vector, yk = σ
denotes the adjustable parameters namely {πk} and {wk}.
wT
k φ
Now suppose we are given a data set {φn, tn}. The corresponding likelihood
is the output of component k, and θ
k=1
K
K
N
N
n=1
k=1
n=1
k=1
14.5. Conditional Mixture Models
671
Figure 14.9 The left plot shows the predictive conditional density corresponding to the converged solution in
Figure 14.8. This gives a log likelihood value of −3.0. A vertical slice through one of these plots at a particular
value of x represents the corresponding conditional distribution p(t|x), which we see is bimodal. The plot on the
right shows the predictive density for a single linear regression model fitted to the same data set using maximum
likelihood. This model has a smaller log likelihood of −27.6.
function is then given by
πkytn
p(t|θ) =
where ynk = σ(wT
k φn) and t = (t1, . . . , tN )T. We can maximize this likelihood
function iteratively by making use of the EM algorithm. This involves introducing
latent variables znk that correspond to a 1-of-K coded binary indicator variable for
each data point n. The complete-data likelihood function is then given by
nk [1 − ynk]1−tn
(14.46)
n=1
p(t, Z|θ) =
πkytn
nk [1 − ynk]1−tn
znk
(14.47)
where Z is the matrix of latent variables with elements znk. We initialize the EM
algorithm by choosing an initial value θold for the model parameters. In the E step,
we then use these parameter values to evaluate the posterior probabilities of the com-
ponents k for each data point n, which are given by
γnk = E[znk] = p(k|φn, θold) = πkytn
j πjytn
nk [1 − ynk]1−tn
nj [1 − ynj]1−tn
(14.48)
These responsibilities are then used to find the expected complete-data log likelihood
as a function of θ, given by
Q(θ, θold) = EZ [ln p(t, Z|θ)]
K
N
γnk {ln πk + tn ln ynk + (1 − tn) ln (1 − ynk)} .
(14.49)
N
1
N
n=1
n=1
N
N
K
672
14. COMBINING MODELS
Section 4.3.3
Section 4.3.3
Exercise 14.16
The M step involves maximization of this function with respect to θ, keeping θold,
and hence γnk, fixed. Maximization with respect to πk can be done in the usual way,
k πk = 1, giving
with a Lagrange multiplier to enforce the summation constraint
the familiar result
πk =
γnk.
(14.50)
To determine the {wk}, we note that the Q(θ, θold) function comprises a sum
over terms indexed by k each of which depends only on one of the vectors wk, so
that the different vectors are decoupled in the M step of the EM algorithm. In other
words, the different components interact only via the responsibilities, which are fixed
during the M step. Note that the M step does not have a closed-form solution and
must be solved iteratively using, for instance, the iterative reweighted least squares
(IRLS) algorithm. The gradient and the Hessian for the vector wk are given by
∇kQ =
n=1
γnk(tn − ynk)φn
Hk = −∇k∇kQ =
γnkynk(1 − ynk)φnφT
n
(14.51)
(14.52)
where ∇k denotes the gradient with respect to wk. For fixed γnk, these are indepen-
dent of {wj} for j = k and so we can solve for each wk separately using the IRLS
algorithm. Thus the M-step equations for component k correspond simply to fitting
a single logistic regression model to a weighted data set in which data point n carries
a weight γnk. Figure 14.10 shows an example of the mixture of logistic regression
models applied to a simple classification problem. The extension of this model to a
mixture of softmax models for more than two classes is straightforward.
