The result (3.92) has an elegant interpretation (MacKay, 1992a), which provides
insight into the Bayesian solution for α. To see this, consider the contours of the like-
lihood function and the prior as illustrated in Figure 3.15. Here we have implicitly
transformed to a rotated set of axes in parameter space aligned with the eigenvec-
tors ui defined in (3.87). Contours of the likelihood function are then axis-aligned
ellipses. The eigenvalues λi measure the curvature of the likelihood function, and
so in Figure 3.15 the eigenvalue λ1 is small compared with λ2 (because a smaller
curvature corresponds to a greater elongation of the contours of the likelihood func-
tion). Because βΦTΦ is a positive definite matrix, it will have positive eigenvalues,
and so the ratio λi/(λi + α) will lie between 0 and 1. Consequently, the quantity γ
defined by (3.91) will lie in the range 0 � γ � M. For directions in which λi
the corresponding parameter wi will be close to its maximum likelihood value, and
the ratio λi/(λi + α) will be close to 1. Such parameters are called well determined
because their values are tightly constrained by the data. Conversely, for directions
in which λi  α, the corresponding parameters wi will be close to zero, as will the
ratios λi/(λi + α). These are directions in which the likelihood function is relatively
insensitive to the parameter value and so the parameter has been set to a small value
by the prior. The quantity γ defined by (3.91) therefore measures the effective total
number of well determined parameters.
We can obtain some insight into the result (3.95) for re-estimating β by com-
paring it with the corresponding maximum likelihood result given by (3.21). Both
of these formulae express the variance (the inverse precision) as an average of the
squared differences between the targets and the model predictions. However, they
differ in that the number of data points N in the denominator of the maximum like-
lihood result is replaced by N − γ in the Bayesian result. We recall from (1.56) that
the maximum likelihood estimate of the variance for a Gaussian distribution over a
n=1
N
3.5. The Evidence Approximation
single variable x is given by
ML =
σ2
1
N
(xn − µML)2
and that this estimate is biased because the maximum likelihood solution µML for
the mean has fitted some of the noise on the data. In effect, this has used up one
degree of freedom in the model. The corresponding unbiased estimate is given by
(1.59) and takes the form
171
(3.96)
MAP =
σ2
(xn − µML)2.
(3.97)
N
1
N − 1
n=1
We shall see in Section 10.1.3 that this result can be obtained from a Bayesian treat-
ment in which we marginalize over the unknown mean. The factor of N − 1 in the
denominator of the Bayesian result takes account of the fact that one degree of free-
dom has been used in fitting the mean and removes the bias of maximum likelihood.
Now consider the corresponding results for the linear regression model. The mean
of the target distribution is now given by the function wTφ(x), which contains M
parameters. However, not all of these parameters are tuned to the data. The effective
number of parameters that are determined by the data is γ, with the remaining M −γ
parameters set to small values by the prior. This is reflected in the Bayesian result
for the variance that has a factor N − γ in the denominator, thereby correcting for
the bias of the maximum likelihood result.
We can illustrate the evidence framework for setting hyperparameters using the
sinusoidal synthetic data set from Section 1.1, together with the Gaussian basis func-
tion model comprising 9 basis functions, so that the total number of parameters in
the model is given by M = 10 including the bias. Here, for simplicity of illustra-
tion, we have set β to its true value of 11.1 and then used the evidence framework to
determine α, as shown in Figure 3.16.
We can also see how the parameter α controls the magnitude of the parameters
{wi}, by plotting the individual parameters versus the effective number γ of param-
eters, as shown in Figure 3.17.
If we consider the limit N
M in which the number of data points is large in
relation to the number of parameters, then from (3.87) all of the parameters will be
well determined by the data because ΦTΦ involves an implicit sum over data points,
and so the eigenvalues λi increase with the size of the data set. In this case, γ = M,
and the re-estimation equations for α and β become
M
2EW (mN )
N
2ED(mN )
(3.98)
(3.99)
where EW and ED are defined by (3.25) and (3.26), respectively. These results
can be used as an easy-to-compute approximation to the full evidence re-estimation
172
3. LINEAR MODELS FOR REGRESSION
−5
0
ln α
5
−5
0
ln α
5
Figure 3.16 The left plot shows γ (red curve) and 2αEW (mN ) (blue curve) versus ln α for the sinusoidal
synthetic data set. It is the intersection of these two curves that defines the optimum value for α given by the
evidence procedure. The right plot shows the corresponding graph of log evidence ln p(t|α, β) versus ln α (red
curve) showing that the peak coincides with the crossing point of the curves in the left plot. Also shown is the
test set error (blue curve) showing that the evidence maximum occurs close to the point of best generalization.
formulae, because they do not require evaluation of the eigenvalue spectrum of the
Hessian.
Figure 3.17 Plot of
the 10 parameters wi
from the Gaussian basis function
model versus the effective num-
ber of parameters γ, in which the
hyperparameter α is varied in the
range 0 � α � ∞ causing γ to
vary in the range 0 � γ � M.
wi
2
1
0
−1
−2
0
8
4
5
2
6
3
1
7
9
0
2
4
6
8
10
