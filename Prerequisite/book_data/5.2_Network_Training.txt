So far, we have viewed neural networks as a general class of parametric nonlinear
functions from a vector x of input variables to a vector y of output variables. A
simple approach to the problem of determining the network parameters is to make an
analogy with the discussion of polynomial curve fitting in Section 1.1, and therefore
to minimize a sum-of-squares error function. Given a training set comprising a set
of input vectors {xn}, where n = 1, . . . , N, together with a corresponding set of
n=1
N
N
N
N
n=1
target vectors {tn}, we minimize the error function
E(w) =
1
2
y(xn, w) − tn2.
5.2. Network Training
233
(5.11)
However, we can provide a much more general view of network training by first
giving a probabilistic interpretation to the network outputs. We have already seen
many advantages of using probabilistic predictions in Section 1.5.4. Here it will also
provide us with a clearer motivation both for the choice of output unit nonlinearity
and the choice of error function.
We start by discussing regression problems, and for the moment we consider
a single target variable t that can take any real value. Following the discussions
in Section 1.2.5 and 3.1, we assume that t has a Gaussian distribution with an x-
dependent mean, which is given by the output of the neural network, so that
p(t|x, w) = N
t|y(x, w), β−1
(5.12)
where β is the precision (inverse variance) of the Gaussian noise. Of course this
is a somewhat restrictive assumption, and in Section 5.6 we shall see how to extend
this approach to allow for more general conditional distributions. For the conditional
distribution given by (5.12), it is sufficient to take the output unit activation function
to be the identity, because such a network can approximate any continuous function
from x to y. Given a data set of N independent, identically distributed observations
X = {x1, . . . , xN}, along with corresponding target values t = {t1, . . . , tN}, we
can construct the corresponding likelihood function
p(t|X, w, β) =
p(tn|xn, w, β).
n=1
Taking the negative logarithm, we obtain the error function
2
{y(xn, w) − tn}2 −
N
2
ln β + N
2
ln(2π)
(5.13)
which can be used to learn the parameters w and β. In Section 5.7, we shall dis-
cuss the Bayesian treatment of neural networks, while here we consider a maximum
likelihood approach. Note that in the neural networks literature, it is usual to con-
sider the minimization of an error function rather than the maximization of the (log)
likelihood, and so here we shall follow this convention. Consider first the determi-
nation of w. Maximizing the likelihood function is equivalent to minimizing the
sum-of-squares error function given by
E(w) =
1
2
{y(xn, w) − tn}2
n=1
(5.14)
n=1
N
N
234
5. NEURAL NETWORKS
where we have discarded additive and multiplicative constants. The value of w found
by minimizing E(w) will be denoted wML because it corresponds to the maximum
likelihood solution. In practice, the nonlinearity of the network function y(xn, w)
causes the error E(w) to be nonconvex, and so in practice local maxima of the
likelihood may be found, corresponding to local minima of the error function, as
discussed in Section 5.2.1.
Having found wML, the value of β can be found by minimizing the negative log
likelihood to give
1
βML
1
N
{y(xn, wML) − tn}2.
(5.15)
Note that this can be evaluated once the iterative optimization required to find wML
is completed. If we have multiple target variables, and we assume that they are inde-
pendent conditional on x and w with shared noise precision β, then the conditional
distribution of the target values is given by
p(t|x, w) = N
t|y(x, w), β−1I
(5.16)
Following the same argument as for a single target variable, we see that the maximum
likelihood weights are determined by minimizing the sum-of-squares error function
(5.11). The noise precision is then given by
Exercise 5.2
Exercise 5.3
1
βML
1
N K
y(xn, wML) − tn2
n=1
(5.17)
where K is the number of target variables. The assumption of independence can be
dropped at the expense of a slightly more complex optimization problem.
Recall from Section 4.3.6 that there is a natural pairing of the error function
(given by the negative log likelihood) and the output unit activation function. In the
regression case, we can view the network as having an output activation function that
is the identity, so that yk = ak. The corresponding sum-of-squares error function
has the property
∂E
∂ak
= yk − tk
(5.18)
which we shall make use of when discussing error backpropagation in Section 5.3.
Now consider the case of binary classification in which we have a single target
variable t such that t = 1 denotes class C1 and t = 0 denotes class C2. Following
the discussion of canonical link functions in Section 4.3.6, we consider a network
having a single output whose activation function is a logistic sigmoid
y = σ(a) ≡
1
1 + exp(−a)
(5.19)
so that 0 � y(x, w) � 1. We can interpret y(x, w) as the conditional probability
p(C1|x), with p(C2|x) given by 1 − y(x, w). The conditional distribution of targets
given inputs is then a Bernoulli distribution of the form
p(t|x, w) = y(x, w)t {1 − y(x, w)}1−t .
(5.20)
n=1
K
N
K
k=1
N
N
K
If we consider a training set of independent observations, then the error function,
which is given by the negative log likelihood, is then a cross-entropy error function
of the form
E(w) = −
{tn ln yn + (1 − tn) ln(1 − yn)}
(5.21)
where yn denotes y(xn, w). Note that there is no analogue of the noise precision β
because the target values are assumed to be correctly labelled. However, the model
is easily extended to allow for labelling errors. Simard et al. (2003) found that using
the cross-entropy error function instead of the sum-of-squares for a classification
problem leads to faster training as well as improved generalization.
If we have K separate binary classifications to perform, then we can use a net-
work having K outputs each of which has a logistic sigmoid activation function.
Associated with each output is a binary class label tk ∈ {0, 1}, where k = 1, . . . , K.
If we assume that the class labels are independent, given the input vector, then the
conditional distribution of the targets is
p(t|x, w) =
yk(x, w)tk [1 − yk(x, w)]1−tk .
(5.22)
Taking the negative logarithm of the corresponding likelihood function then gives
the following error function
E(w) = −
n=1
k=1
{tnk ln ynk + (1 − tnk) ln(1 − ynk)}
(5.23)
where ynk denotes yk(xn, w). Again, the derivative of the error function with re-
spect to the activation for a particular output unit takes the form (5.18) just as in the
regression case.
It is interesting to contrast the neural network solution to this problem with the
corresponding approach based on a linear classification model of the kind discussed
in Chapter 4. Suppose that we are using a standard two-layer network of the kind
shown in Figure 5.1. We see that the weight parameters in the first layer of the
network are shared between the various outputs, whereas in the linear model each
classification problem is solved independently. The first layer of the network can
be viewed as performing a nonlinear feature extraction, and the sharing of features
between the different outputs can save on computation and can also lead to improved
generalization.
Finally, we consider the standard multiclass classification problem in which each
input is assigned to one of K mutually exclusive classes. The binary target variables
tk ∈ {0, 1} have a 1-of-K coding scheme indicating the class, and the network
outputs are interpreted as yk(x, w) = p(tk = 1|x), leading to the following error
function
E(w) = −
n=1
k=1
tkn ln yk(xn, w).
(5.24)
5.2. Network Training
235
Exercise 5.4
Exercise 5.5
Exercise 5.6
236
5. NEURAL NETWORKS
Figure 5.5 Geometrical view of the error function E(w) as
a surface sitting over weight space. Point wA is
a local minimum and wB is the global minimum.
At any point wC, the local gradient of the error
surface is given by the vector ∇E.
E(w)
wA
wB
wC
w1
w2
∇E
Following the discussion of Section 4.3.4, we see that the output unit activation
function, which corresponds to the canonical link, is given by the softmax function
yk(x, w) =
(5.25)
exp(ak(x, w))
exp(aj(x, w))
j
which satisfies 0 � yk � 1 and
k yk = 1. Note that the yk(x, w) are unchanged
if a constant is added to all of the ak(x, w), causing the error function to be constant
for some directions in weight space. This degeneracy is removed if an appropriate
regularization term (Section 5.5) is added to the error function.
Once again, the derivative of the error function with respect to the activation for
Exercise 5.7
a particular output unit takes the familiar form (5.18).
In summary, there is a natural choice of both output unit activation function
and matching error function, according to the type of problem being solved. For re-
gression we use linear outputs and a sum-of-squares error, for (multiple independent)
binary classifications we use logistic sigmoid outputs and a cross-entropy error func-
tion, and for multiclass classification we use softmax outputs with the corresponding
multiclass cross-entropy error function. For classification problems involving two
classes, we can use a single logistic sigmoid output, or alternatively we can use a
network with two outputs having a softmax output activation function.
