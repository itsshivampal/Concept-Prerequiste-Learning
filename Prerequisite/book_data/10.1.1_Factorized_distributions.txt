Here we consider an alternative way in which to restrict the family of distri-
butions q(Z). Suppose we partition the elements of Z into disjoint groups that we
denote by Zi where i = 1, . . . , M. We then assume that the q distribution factorizes
with respect to these groups, so that
q(Z) =
qi(Zi).
i=1
(10.5)
qj
qi
i
i
i=j
10.1. Variational Inference
465
It should be emphasized that we are making no further assumptions about the distri-
bution. In particular, we place no restriction on the functional forms of the individual
factors qi(Zi). This factorized form of variational inference corresponds to an ap-
proximation framework developed in physics called mean field theory (Parisi, 1988).
Amongst all distributions q(Z) having the form (10.5), we now seek that distri-
bution for which the lower bound L(q) is largest. We therefore wish to make a free
form (variational) optimization of L(q) with respect to all of the distributions qi(Zi),
which we do by optimizing with respect to each of the factors in turn. To achieve
this, we first substitute (10.5) into (10.3) and then dissect out the dependence on one
of the factors qj(Zj). Denoting qj(Zj) by simply qj to keep the notation uncluttered,
we then obtain
L(q) =
ln p(X, Z) −
ln qi
dZ
ln p(X, Z)
qi dZi
qj ln qj dZj + const
dZj −
i=j
p(X, Zj) dZj −
where we have defined a new distribution
qj ln
qj ln qj dZj + const
(10.6)
p(X, Zj) by the relation
ln
p(X, Zj) = Ei=j[ln p(X, Z)] + const.
(10.7)
Here the notation Ei=j[··· ] denotes an expectation with respect to the q distributions
over all variables zi for i = j, so that
Ei=j[ln p(X, Z)] =
ln p(X, Z)
qi dZi.
(10.8)
Now suppose we keep the {qi=j} fixed and maximize L(q) in (10.6) with re-
spect to all possible forms for the distribution qj(Zj). This is easily done by rec-
ognizing that (10.6) is a negative Kullback-Leibler divergence between qj(Zj) and
p(X, Zj). Thus maximizing (10.6) is equivalent to minimizing the Kullback-Leibler
Leonhard Euler
1707–1783
Euler was a Swiss mathematician
and physicist who worked in St.
Petersburg and Berlin and who is
widely considered to be one of the
greatest mathematicians of all time.
He is certainly the most prolific, and
his collected works fill 75 volumes. Amongst his many
contributions, he formulated the modern theory of the
function, he developed (together with Lagrange) the
calculus of variations, and he discovered the formula
eiπ = −1, which relates four of the most important
numbers in mathematics. During the last 17 years of
his life, he was almost totally blind, and yet he pro-
duced nearly half of his results during this period.
466
10. APPROXIMATE INFERENCE
divergence, and the minimum occurs when qj(Zj) =
general expression for the optimal solution q
j (Zj) given by
j (Zj) = Ei=j[ln p(X, Z)] + const.
ln q
p(X, Zj). Thus we obtain a
(10.9)
It is worth taking a few moments to study the form of this solution as it provides the
basis for applications of variational methods. It says that the log of the optimal so-
lution for factor qj is obtained simply by considering the log of the joint distribution
over all hidden and visible variables and then taking the expectation with respect to
all of the other factors {qi} for i = j.
Thus if we take the exponential of both sides and normalize, we have
The additive constant in (10.9) is set by normalizing the distribution q
j (Zj).
j (Zj) =
q
exp (Ei=j[ln p(X, Z)])
exp (Ei=j[ln p(X, Z)]) dZj
In practice, we shall find it more convenient to work with the form (10.9) and then re-
instate the normalization constant (where required) by inspection. This will become
clear from subsequent examples.
The set of equations given by (10.9) for j = 1, . . . , M represent a set of con-
sistency conditions for the maximum of the lower bound subject to the factorization
constraint. However, they do not represent an explicit solution because the expres-
j (Zj) depends on expectations
sion on the right-hand side of (10.9) for the optimum q
computed with respect to the other factors qi(Zi) for i = j. We will therefore seek
a consistent solution by first initializing all of the factors qi(Zi) appropriately and
then cycling through the factors and replacing each in turn with a revised estimate
given by the right-hand side of (10.9) evaluated using the current estimates for all of
the other factors. Convergence is guaranteed because bound is convex with respect
to each of the factors qi(Zi) (Boyd and Vandenberghe, 2004).
