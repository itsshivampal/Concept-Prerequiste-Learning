113
where X = {x1, . . . , xN}. We immediately see that the situation is now much
more complex than with a single Gaussian, due to the presence of the summation
over k inside the logarithm. As a result, the maximum likelihood solution for the
parameters no longer has a closed-form analytical solution. One approach to maxi-
mizing the likelihood function is to use iterative numerical optimization techniques
(Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008). Alterna-
tively we can employ a powerful framework called expectation maximization, which
will be discussed at length in Chapter 9.
2.4. The Exponential Family
The probability distributions that we have studied so far in this chapter (with the
exception of the Gaussian mixture) are specific examples of a broad class of distri-
butions called the exponential family (Duda and Hart, 1973; Bernardo and Smith,
1994). Members of the exponential family have many important properties in com-
mon, and it is illuminating to discuss these properties in some generality.
The exponential family of distributions over x, given parameters η, is defined to
be the set of distributions of the form
p(x|η) = h(x)g(η) exp
ηTu(x)
(2.194)
where x may be scalar or vector, and may be discrete or continuous. Here η are
called the natural parameters of the distribution, and u(x) is some function of x.
The function g(η) can be interpreted as the coefficient that ensures that the distribu-
tion is normalized and therefore satisfies
g(η)
h(x) exp
ηTu(x)
dx = 1
(2.195)
where the integration is replaced by summation if x is a discrete variable.
We begin by taking some examples of the distributions introduced earlier in
the chapter and showing that they are indeed members of the exponential family.
Consider first the Bernoulli distribution
p(x|µ) = Bern(x|µ) = µx(1 − µ)1−x.
Expressing the right-hand side as the exponential of the logarithm, we have
p(x|µ) = exp{x ln µ + (1 − x) ln(1 − µ)}
ln
x
= (1 − µ) exp
1 − µ
Comparison with (2.194) allows us to identify
(2.196)
(2.197)
(2.198)
η = ln
1 − µ
114
2. PROBABILITY DISTRIBUTIONS
M
M
k=1
u(x) = x
h(x) = 1
g(η) = σ(−η).
u(x) = x
h(x) = 1
g(η) = 1.
µk = 1
k=1
M
M−1
k=1
which we can solve for µ to give µ = σ(η), where
1
1 + exp(−η)
is called the logistic sigmoid function. Thus we can write the Bernoulli distribution
using the standard representation (2.194) in the form
p(x|η) = σ(−η) exp(ηx)
(2.200)
where we have used 1 − σ(η) = σ(−η), which is easily proved from (2.199). Com-
parison with (2.194) shows that
Next consider the multinomial distribution that, for a single observation x, takes
the form
p(x|µ) =
k=1
µxk
k = exp
xk ln µk
(2.204)
where x = (x1, . . . , xN )T. Again, we can write this in the standard representation
(2.194) so that
(2.205)
where ηk = ln µk, and we have defined η = (η1, . . . , ηM )T. Again, comparing with
(2.194) we have
p(x|η) = exp(ηTx)
Note that the parameters ηk are not independent because the parameters µk are sub-
ject to the constraint
so that, given any M − 1 of the parameters µk, the value of the remaining parameter
is fixed. In some circumstances, it will be convenient to remove this constraint by
expressing the distribution in terms of only M − 1 parameters. This can be achieved
by using the relationship (2.209) to eliminate µM by expressing it in terms of the
remaining {µk} where k = 1, . . . , M − 1, thereby leaving M − 1 parameters. Note
that these remaining parameters are still subject to the constraints
0 � µk � 1,
µk � 1.
(2.210)
(2.199)
(2.201)
(2.202)
(2.203)
(2.206)
(2.207)
(2.208)
(2.209)
M
ln
µk
M−1
j=1 µj
exp(ηk)
exp(ηk)
1 −
M−1
M−1
M−1
j µj
exp
1 +
µk
k=1
k=1
k=1
1 −
k=1
2.4. The Exponential Family
115
Making use of the constraint (2.209), the multinomial distribution in this representa-
tion then becomes
exp
xk ln µk
k=1
= exp
= exp
M−1
k=1
M−1
k=1
We now identify
xk ln
1 −
xk ln µk +
xk
ln
M−1
k=1
M−1
µk
µk
1 −
1 −
+ ln
= ηk
(2.211)
(2.212)
which we can solve for µk by first summing both sides over k and then rearranging
and back-substituting to give
µk =
j exp(ηj) .
(2.213)
This is called the softmax function, or the normalized exponential. In this represen-
tation, the multinomial distribution therefore takes the form
p(x|η) =
1 +
−1
exp(ηTx).
(2.214)
This is the standard form of the exponential family, with parameter vector η =
(η1, . . . , ηM−1)T in which
u(x) = x
h(x) = 1
g(η) =
1 +
exp(ηk)
−1
(2.215)
(2.216)
(2.217)
Finally, let us consider the Gaussian distribution. For the univariate Gaussian,
we have
p(x|µ, σ2) =
1
(2πσ2)1/2
1
(2πσ2)1/2
exp
1
2σ2 (x − µ)2
1
2σ2 x2 + µ
σ2 x −
1
2σ2 µ2
(2.218)
(2.219)
dx
ηT
N
η2
1
4η2
N
N
116
2. PROBABILITY DISTRIBUTIONS
which, after some simple rearrangement, can be cast in the standard exponential
family form (2.194) with
Exercise 2.57
Exercise 2.58
(2.220)
(2.221)
(2.222)
(2.223)
(2.224)
(2.225)
(2.226)
u(x) =
µ/σ2
−1/2σ2
x
x2
h(x) = (2π)−1/2
g(η) = (−2η2)1/2 exp
