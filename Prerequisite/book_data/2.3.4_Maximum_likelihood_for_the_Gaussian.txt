Given a data set X = (x1, . . . , xN )T in which the observations {xn} are as-
sumed to be drawn independently from a multivariate Gaussian distribution, we can
estimate the parameters of the distribution by maximum likelihood. The log likeli-
hood function is given by
ln p(X|µ, Σ) = −
N D
2
ln(2π)−
N
2
ln|Σ|−
(xn−µ)TΣ−1(xn−µ). (2.118)
By simple rearrangement, we see that the likelihood function depends on the data set
only through the two quantities
Appendix C
These are known as the sufficient statistics for the Gaussian distribution. Using
(C.19), the derivative of the log likelihood with respect to µ is given by
xn,
n=1
xnxT
n.
(2.119)
ln p(X|µ, Σ) =
Σ−1(xn − µ)
(2.120)
and setting this derivative to zero, we obtain the solution for the maximum likelihood
estimate of the mean given by
N
µML =
1
N
xn
n=1
(2.121)
n=1
N
94
2. PROBABILITY DISTRIBUTIONS
Exercise 2.34
Exercise 2.35
which is the mean of the observed set of data points. The maximization of (2.118)
with respect to Σ is rather more involved. The simplest approach is to ignore the
symmetry constraint and show that the resulting solution is symmetric as required.
Alternative derivations of this result, which impose the symmetry and positive defi-
niteness constraints explicitly, can be found in Magnus and Neudecker (1999). The
result is as expected and takes the form
ΣML =
1
N
(xn − µML)(xn − µML)T
(2.122)
which involves µML because this is the result of a joint maximization with respect
to µ and Σ. Note that the solution (2.121) for µML does not depend on ΣML, and so
we can first evaluate µML and then use this to evaluate ΣML.
If we evaluate the expectations of the maximum likelihood solutions under the
true distribution, we obtain the following results
E[µML] = µ
E[ΣML] = N − 1
N
(2.123)
(2.124)
We see that the expectation of the maximum likelihood estimate for the mean is equal
to the true mean. However, the maximum likelihood estimate for the covariance has
an expectation that is less than the true value, and hence it is biased. We can correct
this bias by defining a different estimator
Σ given by
N
1
N − 1
n=1
(xn − µML)(xn − µML)T.
(2.125)
Clearly from (2.122) and (2.124), the expectation of
Σ is equal to Σ.
