So far in our general discussion of EP, we have allowed the factors fi(θ) in the
distribution p(θ) to be functions of all of the components of θ, and similarly for the
f(θ) in the approximating distribution q(θ). We now consider
approximating factors
situations in which the factors depend only on subsets of the variables. Such restric-
tions can be conveniently expressed using the framework of probabilistic graphical
models, as discussed in Chapter 8. Here we use a factor graph representation because
this encompasses both directed and undirected graphs.
10. APPROXIMATE INFERENCE
514
100
r
o
r
r
E
10−5
Posterior mean
laplace
vb
104
ep
106
FLOPS
10−200
r
o
r
r
E
10−202
laplace
10−204
104
Evidence
vb
ep
106
FLOPS
Figure 10.17 Comparison of expectation propagation, variational inference, and the Laplace approximation on
the clutter problem. The left-hand plot shows the error in the predicted posterior mean versus the number of
floating point operations, and the right-hand plot shows the corresponding results for the model evidence.
Section 8.4.4
We shall focus on the case in which the approximating distribution is fully fac-
torized, and we shall show that in this case expectation propagation reduces to loopy
belief propagation (Minka, 2001a). To start with, we show this in the context of a
simple example, and then we shall explore the general case.
First of all, recall from (10.17) that if we minimize the Kullback-Leibler diver-
gence KL(pq) with respect to a factorized distribution q, then the optimal solution
for each factor is simply the corresponding marginal of p.
Now consider the factor graph shown on the left in Figure 10.18, which was
introduced earlier in the context of the sum-product algorithm. The joint distribution
is given by
p(x) = fa(x1, x2)fb(x2, x3)fc(x2, x4).
We seek an approximation q(x) that has the same factorization, so that
(10.225)
(10.226)
q(x) ∝
fa(x1, x2)
fb(x2, x3)
fc(x2, x4).
Note that normalization constants have been omitted, and these can be re-instated at
the end by local normalization, as is generally done in belief propagation. Now sup-
pose we restrict attention to approximations in which the factors themselves factorize
with respect to the individual variables so that
q(x) ∝
fa1(x1)
fa2(x2)
fb2(x2)
fb3(x3)
fc2(x2)
fc4(x4)
(10.227)
which corresponds to the factor graph shown on the right in Figure 10.18. Because
the individual factors are factorized, the overall distribution q(x) is itself fully fac-
torized.
Now we apply the EP algorithm using the fully factorized approximation. Sup-
pose that we have initialized all of the factors and that we choose to refine factor
x3
10.7. Expectation Propagation
515
x2
x3
x1
x2
x3
x1
fa
fb
fc
x4
˜fa1
˜fa2
˜fb2
˜fb3
˜fc2
˜fc4
x4
Figure 10.18 On the left is a simple factor graph from Figure 8.51 and reproduced here for convenience. On
the right is the corresponding factorized approximation.
fb(x2, x3) =
distribution to give
fb2(x2)
fb3(x3). We first remove this factor from the approximating
q\b(x) =
fa1(x1)
fa2(x2)
fc2(x2)
fc4(x4)
(10.228)
and we then multiply this by the exact factor fb(x2, x3) to give
fa1(x1)
fa2(x2)
p(x) = q\b(x)fb(x2, x3) =
(10.229)
pqnew).
We now find qnew(x) by minimizing the Kullback-Leibler divergence KL(
The result, as noted above, is that qnew(z) comprises the product of factors, one for
each variable xi, in which each factor is given by the corresponding marginal of
p(x). These four marginals are given by
fc4(x4)fb(x2, x3).
fc2(x2)
p(x1) ∝
p(x2) ∝
p(x3) ∝
p(x4) ∝
fa1(x1)
fa2(x2)
x2
fc4(x4)
fc2(x2)
fb(x2, x3)
fb(x2, x3)
fc2(x2)
x3
fa2(x2)
and qnew(x) is obtained by multiplying these marginals together. We see that the
fb(x2, x3) are those that involve
only factors in q(x) that change when we update
fb(x2, x3) =
the variables in fb namely x2 and x3. To obtain the refined factor
fb2(x2)
fb3(x3) we simply divide qnew(x) by q\b(x), which gives
fb2(x2) ∝
fb(x2, x3)
fb3(x3) ∝
x2
fb(x2, x3)
fa2(x2)
fc2(x2)
(10.235)
(10.230)
(10.231)
(10.232)
(10.233)
(10.234)
i=j
k
i
k
i
516
10. APPROXIMATE INFERENCE
Section 8.4.4
These are precisely the messages obtained using belief propagation in which mes-
sages from variable nodes to factor nodes have been folded into the messages from
fb2(x2) corresponds to the message
factor nodes to variable nodes. In particular,
µfb→x2(x2) sent by factor node fb to variable node x2 and is given by (8.81). Simi-
fa2(x2) corre-
larly, if we substitute (8.78) into (8.79), we obtain (10.235) in which
sponds to µfa→x2(x2) and
fc2(x2) corresponds to µfc→x2(x2), giving the message
fb3(x3) which corresponds to µfb→x3(x3).
This result differs slightly from standard belief propagation in that messages are
passed in both directions at the same time. We can easily modify the EP procedure
to give the standard form of the sum-product algorithm by updating just one of the
fb2(x2) is unchanged
factors at a time, for instance if we refine only
fb3(x3) is again given by (10.235). If
by definition, while the refined version of
we are refining only one term at a time, then we can choose the order in which the
refinements are done as we wish. In particular, for a tree-structured graph we can
follow a two-pass update scheme, corresponding to the standard belief propagation
schedule, which will result in exact inference of the variable and factor marginals.
The initialization of the approximation factors in this case is unimportant.
fb3(x3), then
Now let us consider a general factor graph corresponding to the distribution
p(θ) =
fi(θi)
(10.236)
where θi represents the subset of variables associated with factor fi. We approximate
this using a fully factorized distribution of the form
q(θ) ∝
fik(θk)
(10.237)
where θk corresponds to an individual variable node. Suppose that we wish to refine
fjl(θl) keeping all other terms fixed. We first remove the term
the particular term
fj(θj) from q(θ) to give
q\j(θ) ∝
fik(θk)
(10.238)
fjl(θl),
and then multiply by the exact factor fj(θj). To determine the refined term
we need only consider the functional dependence on θl, and so we simply find the
corresponding marginal of
q\j(θ)fj(θj).
(10.239)
Up to a multiplicative constant, this involves taking the marginal of fj(θj) multiplied
by any terms from q\j(θ) that are functions of any of the variables in θj. Terms that
fi(θi) for i = j will cancel between numerator and
correspond to other factors
denominator when we subsequently divide by q\j(θ). We therefore obtain
fjl(θl) ∝
fj(θj)
fkm(θm).
(10.240)
θm=l∈θj
k
m=l
Exercises
517
We recognize this as the sum-product rule in the form in which messages from vari-
able nodes to factor nodes have been eliminated, as illustrated by the example shown
fjm(θm) corresponds to the message µfj→θm(θm),
in Figure 8.50. The quantity
which factor node j sends to variable node m, and the product over k in (10.240)
is over all factors that depend on the variables θm that have variables (other than
variable θl) in common with factor fj(θj). In other words, to compute the outgoing
message from a factor node, we take the product of all the incoming messages from
other factor nodes, multiply by the local factor, and then marginalize.
Thus, the sum-product algorithm arises as a special case of expectation propa-
gation if we use an approximating distribution that is fully factorized. This suggests
that more flexible approximating distributions, corresponding to partially discon-
nected graphs, could be used to achieve higher accuracy. Another generalization is
to group factors fi(θi) together into sets and to refine all the factors in a set together
at each iteration. Both of these approaches can lead to improvements in accuracy
(Minka, 2001b). In general, the problem of choosing the best combination of group-
ing and disconnection is an open research issue.
We have seen that variational message passing and expectation propagation op-
timize two different forms of the Kullback-Leibler divergence. Minka (2005) has
shown that a broad range of message passing algorithms can be derived from a com-
mon framework involving minimization of members of the alpha family of diver-
gences, given by (10.19). These include variational message passing, loopy belief
propagation, and expectation propagation, as well as a range of other algorithms,
which we do not have space to discuss here, such as tree-reweighted message pass-
ing (Wainwright et al., 2005), fractional belief propagation (Wiegerinck and Heskes,
2003), and power EP (Minka, 2004).
Exercises
10.1 () www Verify that the log marginal distribution of the observed data ln p(X)
can be decomposed into two terms in the form (10.2) where L(q) is given by (10.3)
and KL(qp) is given by (10.4).
10.2 () Use the properties E[z1] = m1 and E[z2] = m2 to solve the simultaneous equa-
tions (10.13) and (10.15), and hence show that, provided the original distribution
p(z) is nonsingular, the unique solution for the means of the factors in the approxi-
mation distribution is given by E[z1] = µ1 and E[z2] = µ2.
10.3 ( ) www Consider a factorized variational distribution q(Z) of the form (10.5).
By using the technique of Lagrange multipliers, verify that minimization of the
Kullback-Leibler divergence KL(pq) with respect to one of the factors qi(Zi),
keeping all other factors fixed, leads to the solution (10.17).
10.4 ( ) Suppose that p(x) is some fixed distribution and that we wish to approximate
it using a Gaussian distribution q(x) = N (x|µ, Σ). By writing down the form of
the KL divergence KL(pq) for a Gaussian q(x) and then differentiating, show that
518
10. APPROXIMATE INFERENCE
minimization of KL(pq) with respect to µ and Σ leads to the result that µ is given
by the expectation of x under p(x) and that Σ is given by the covariance.
10.5 ( ) www Consider a model in which the set of all hidden stochastic variables, de-
noted collectively by Z, comprises some latent variables z together with some model
parameters θ. Suppose we use a variational distribution that factorizes between la-
tent variables and parameters so that q(z, θ) = qz(z)qθ(θ), in which the distribution
qθ(θ) is approximated by a point estimate of the form qθ(θ) = δ(θ − θ0) where θ0
is a vector of free parameters. Show that variational optimization of this factorized
distribution is equivalent to an EM algorithm, in which the E step optimizes qz(z),
and the M step maximizes the expected complete-data log posterior distribution of θ
with respect to θ0.
10.6 ( ) The alpha family of divergences is defined by (10.19). Show that the Kullback-
Leibler divergence KL(pq) corresponds to α → 1. This can be done by writing
p = exp(	 ln p) = 1 + 	 ln p + O(	2) and then taking 	 → 0. Similarly show that
KL(qp) corresponds to α → −1.
10.7 ( ) Consider the problem of inferring the mean and precision of a univariate Gaus-
sian using a factorized variational approximation, as considered in Section 10.1.3.
Show that the factor qµ(µ) is a Gaussian of the form N (µ|µN , λ−1
N ) with mean and
precision given by (10.26) and (10.27), respectively. Similarly show that the factor
qτ (τ) is a gamma distribution of the form Gam(τ|aN , bN ) with parameters given by
(10.29) and (10.30).
10.8 () Consider the variational posterior distribution for the precision of a univariate
Gaussian whose parameters are given by (10.29) and (10.30). By using the standard
results for the mean and variance of the gamma distribution given by (B.27) and
(B.28), show that if we let N → ∞, this variational posterior distribution has a
mean given by the inverse of the maximum likelihood estimator for the variance of
the data, and a variance that goes to zero.
10.9 ( ) By making use of the standard result E[τ] = aN /bN for the mean of a gamma
distribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result
(10.33) for the reciprocal of the expected precision in the factorized variational treat-
ment of a univariate Gaussian.
10.10 () www Derive the decomposition given by (10.34) that is used to find approxi-
mate posterior distributions over models using variational inference.
10.11 ( ) www By using a Lagrange multiplier to enforce the normalization constraint
on the distribution q(m), show that the maximum of the lower bound (10.35) is given
by (10.36).
10.12 ( ) Starting from the joint distribution (10.41), and applying the general result
(10.9), show that the optimal variational distribution q(Z) over the latent variables
for the Bayesian mixture of Gaussians is given by (10.48) by verifying the steps
given in the text.
Exercises
519
10.13 ( ) www Starting from (10.54), derive the result (10.59) for the optimum vari-
ational posterior distribution over µk and Λk in the Bayesian mixture of Gaussians,
and hence verify the expressions for the parameters of this distribution given by
(10.60)–(10.63).
10.14 ( ) Using the distribution (10.59), verify the result (10.64).
10.15 () Using the result (B.17), show that the expected value of the mixing coefficients
in the variational mixture of Gaussians is given by (10.69).
10.16 ( ) www Verify the results (10.71) and (10.72) for the first two terms in the
lower bound for the variational Gaussian mixture model given by (10.70).
10.17 (  ) Verify the results (10.73)–(10.77) for the remaining terms in the lower bound
for the variational Gaussian mixture model given by (10.70).
10.18 (  )
In this exercise, we shall derive the variational re-estimation equations for
the Gaussian mixture model by direct differentiation of the lower bound. To do this
we assume that the variational distribution has the factorization defined by (10.42)
and (10.55) with factors given by (10.48), (10.57), and (10.59). Substitute these into
(10.70) and hence obtain the lower bound as a function of the parameters of the varia-
tional distribution. Then, by maximizing the bound with respect to these parameters,
derive the re-estimation equations for the factors in the variational distribution, and
show that these are the same as those obtained in Section 10.2.1.
10.19 ( ) Derive the result (10.81) for the predictive distribution in the variational treat-
ment of the Bayesian mixture of Gaussians model.
10.20 ( ) www This exercise explores the variational Bayes solution for the mixture of
Gaussians model when the size N of the data set is large and shows that it reduces (as
we would expect) to the maximum likelihood solution based on EM derived in Chap-
ter 9. Note that results from Appendix B may be used to help answer this exercise.
First show that the posterior distribution q(Λk) of the precisions becomes sharply
peaked around the maximum likelihood solution. Do the same for the posterior dis-
tribution of the means q(µk|Λk). Next consider the posterior distribution q(π)
for the mixing coefficients and show that this too becomes sharply peaked around
the maximum likelihood solution. Similarly, show that the responsibilities become
equal to the corresponding maximum likelihood values for large N, by making use
of the following asymptotic result for the digamma function for large x
ψ(x) = ln x + O (1/x) .
(10.241)
Finally, by making use of (10.80), show that for large N, the predictive distribution
becomes a mixture of Gaussians.
10.21 () Show that the number of equivalent parameter settings due to interchange sym-
metries in a mixture model with K components is K!.
520
10. APPROXIMATE INFERENCE
10.22 ( ) We have seen that each mode of the posterior distribution in a Gaussian mix-
ture model is a member of a family of K! equivalent modes. Suppose that the result
of running the variational inference algorithm is an approximate posterior distribu-
tion q that is localized in the neighbourhood of one of the modes. We can then
approximate the full posterior distribution as a mixture of K! such q distributions,
once centred on each mode and having equal mixing coefficients. Show that if we
assume negligible overlap between the components of the q mixture, the resulting
lower bound differs from that for a single component q distribution through the ad-
dition of an extra term ln K!.
10.23 ( ) www Consider a variational Gaussian mixture model in which there is no
prior distribution over mixing coefficients {πk}. Instead, the mixing coefficients are
treated as parameters, whose values are to be found by maximizing the variational
lower bound on the log marginal likelihood. Show that maximizing this lower bound
with respect to the mixing coefficients, using a Lagrange multiplier to enforce the
constraint that the mixing coefficients sum to one, leads to the re-estimation result
(10.83). Note that there is no need to consider all of the terms in the lower bound but
only the dependence of the bound on the {πk}.
10.24 ( ) www We have seen in Section 10.2 that the singularities arising in the max-
imum likelihood treatment of Gaussian mixture models do not arise in a Bayesian
treatment. Discuss whether such singularities would arise if the Bayesian model
were solved using maximum posterior (MAP) estimation.
10.25 ( ) The variational treatment of the Bayesian mixture of Gaussians, discussed in
Section 10.2, made use of a factorized approximation (10.5) to the posterior distribu-
tion. As we saw in Figure 10.2, the factorized assumption causes the variance of the
posterior distribution to be under-estimated for certain directions in parameter space.
Discuss qualitatively the effect this will have on the variational approximation to the
model evidence, and how this effect will vary with the number of components in
the mixture. Hence explain whether the variational Gaussian mixture will tend to
under-estimate or over-estimate the optimal number of components.
10.26 (  ) Extend the variational treatment of Bayesian linear regression to include
a gamma hyperprior Gam(β|c0, d0) over β and solve variationally, by assuming a
factorized variational distribution of the form q(w)q(α)q(β). Derive the variational
update equations for the three factors in the variational distribution and also obtain
an expression for the lower bound and for the predictive distribution.
10.27 ( ) By making use of the formulae given in Appendix B show that the variational
lower bound for the linear basis function regression model, defined by (10.107), can
be written in the form (10.107) with the various terms defined by (10.108)–(10.112).
10.28 (  ) Rewrite the model for the Bayesian mixture of Gaussians, introduced in
Section 10.2, as a conjugate model from the exponential family, as discussed in
Section 10.4. Hence use the general results (10.115) and (10.119) to derive the
specific results (10.48), (10.57), and (10.59).
Exercises
521
10.29 () www Show that the function f(x) = ln(x) is concave for 0 < x < ∞
by computing its second derivative. Determine the form of the dual function g(λ)
defined by (10.133), and verify that minimization of λx − g(λ) with respect to λ
according to (10.132) indeed recovers the function ln(x).
10.30 () By evaluating the second derivative, show that the log logistic function f(x) =
− ln(1 + e−x) is concave. Derive the variational upper bound (10.137) directly by
making a second order Taylor expansion of the log logistic function around a point
x = ξ.
10.31 ( ) By finding the second derivative with respect to x, show that the function
f(x) = − ln(ex/2 + e−x/2) is a concave function of x. Now consider the second
derivatives with respect to the variable x2 and hence show that it is a convex function
of x2. Plot graphs of f(x) against x and against x2. Derive the lower bound (10.144)
on the logistic sigmoid function directly by making a first order Taylor series expan-
sion of the function f(x) in the variable x2 centred on the value ξ2.
10.32 ( ) www Consider the variational treatment of logistic regression with sequen-
tial learning in which data points are arriving one at a time and each must be pro-
cessed and discarded before the next data point arrives. Show that a Gaussian ap-
proximation to the posterior distribution can be maintained through the use of the
lower bound (10.151), in which the distribution is initialized using the prior, and as
each data point is absorbed its corresponding variational parameter ξn is optimized.
10.33 () By differentiating the quantity Q(ξ, ξold) defined by (10.161) with respect to
the variational parameter ξn show that the update equation for ξn for the Bayesian
logistic regression model is given by (10.163).
10.34 ( )
In this exercise we derive re-estimation equations for the variational parame-
ters ξ in the Bayesian logistic regression model of Section 4.5 by direct maximization
of the lower bound given by (10.164). To do this set the derivative of L(ξ) with re-
spect to ξn equal to zero, making use of the result (3.117) for the derivative of the log
of a determinant, together with the expressions (10.157) and (10.158) which define
the mean and covariance of the variational posterior distribution q(w).
10.35 ( ) Derive the result (10.164) for the lower bound L(ξ) in the variational logistic
regression model. This is most easily done by substituting the expressions for the
Gaussian prior q(w) = N (w|m0, S0), together with the lower bound h(w, ξ) on
the likelihood function, into the integral (10.159) which defines L(ξ). Next gather
together the terms which depend on w in the exponential and complete the square
to give a Gaussian integral, which can then be evaluated by invoking the standard
result for the normalization coefficient of a multivariate Gaussian. Finally take the
logarithm to obtain (10.164).
10.36 ( ) Consider the ADF approximation scheme discussed in Section 10.7, and show
that inclusion of the factor fj(θ) leads to an update of the model evidence of the
form
pj(D)  pj−1(D)Zj
(10.242)
522
10. APPROXIMATE INFERENCE
where Zj is the normalization constant defined by (10.197). By applying this result
recursively, and initializing with p0(D) = 1, derive the result
p(D) 
Zj.
j
(10.243)
10.37 () www Consider the expectation propagation algorithm from Section 10.7, and
suppose that one of the factors f0(θ) in the definition (10.188) has the same expo-
nential family functional form as the approximating distribution q(θ). Show that if
f0(θ) leaves
the factor
f0(θ) unchanged. This situation typically arises when one of the factors is the prior
p(θ), and so we see that the prior factor can be incorporated once exactly and does
not need to be refined.
f0(θ) is initialized to be f0(θ), then an EP update to refine
10.38 (  )
In this exercise and the next, we shall verify the results (10.214)–(10.224)
for the expectation propagation algorithm applied to the clutter problem. Begin by
using the division formula (10.205) to derive the expressions (10.214) and (10.215)
by completing the square inside the exponential to identify the mean and variance.
Also, show that the normalization constant Zn, defined by (10.206), is given for the
clutter problem by (10.216). This can be done by making use of the general result
(2.115).
10.39 (  ) Show that the mean and variance of qnew(θ) for EP applied to the clutter
problem are given by (10.217) and (10.218). To do this, first prove the following
results for the expectations of θ and θθT under qnew(θ)
E[θ] = m\n + v\n∇m\n ln Zn
E[θTθ] = 2(v\n)2∇v\n ln Zn + 2E[θ]Tm\n − m\n2
(10.244)
(10.245)
and then make use of the result (10.216) for Zn. Next, prove the results (10.220)–
(10.222) by using (10.207) and completing the square in the exponential. Finally,
use (10.208) to derive the result (10.223).
