In practice, we are not usually interested in the value of w itself but rather in
making predictions of t for new values of x. This requires that we evaluate the
predictive distribution defined by
p(t|t, α, β) =
p(t|w, β)p(w|t, α, β) dw
(3.57)
in which t is the vector of target values from the training set, and we have omitted the
corresponding input vectors from the right-hand side of the conditioning statements
to simplify the notation. The conditional distribution p(t|x, w, β) of the target vari-
able is given by (3.8), and the posterior weight distribution is given by (3.49). We
see that (3.57) involves the convolution of two Gaussian distributions, and so making
use of the result (2.115) from Section 8.1.4, we see that the predictive distribution
takes the form
where the variance σ2
p(t|x, t, α, β) = N (t|mT
N (x) of the predictive distribution is given by
N φ(x), σ2
N (x))
(3.58)
(3.59)
N (x) =
σ2
1
+ φ(x)TSN φ(x).
The first term in (3.59) represents the noise on the data whereas the second term
reflects the uncertainty associated with the parameters w. Because the noise process
and the distribution of w are independent Gaussians, their variances are additive.
Note that, as additional data points are observed, the posterior distribution becomes
N +1(x) �
narrower. As a consequence it can be shown (Qazaz et al., 1997) that σ2
N (x). In the limit N → ∞, the second term in (3.59) goes to zero, and the variance
σ2
of the predictive distribution arises solely from the additive noise governed by the
parameter β.
As an illustration of the predictive distribution for Bayesian linear regression
models, let us return to the synthetic sinusoidal data set of Section 1.1. In Figure 3.8,
Exercise 3.10
Exercise 3.11
t
1
0
−1
t
1
0
−1
0
0
3.3. Bayesian Linear Regression
157
t
1
0
−1
1
x
0
1
x
t
1
0
−1
1
x
0
1
x
Figure 3.8 Examples of the predictive distribution (3.58) for a model consisting of 9 Gaussian basis functions
of the form (3.4) using the synthetic sinusoidal data set of Section 1.1. See the text for a detailed discussion.
we fit a model comprising a linear combination of Gaussian basis functions to data
sets of various sizes and then look at the corresponding posterior distributions. Here
the green curves correspond to the function sin(2πx) from which the data points
were generated (with the addition of Gaussian noise). Data sets of size N = 1,
N = 2, N = 4, and N = 25 are shown in the four plots by the blue circles. For
each plot, the red curve shows the mean of the corresponding Gaussian predictive
distribution, and the red shaded region spans one standard deviation either side of
the mean. Note that the predictive uncertainty depends on x and is smallest in the
neighbourhood of the data points. Also note that the level of uncertainty decreases
as more data points are observed.
The plots in Figure 3.8 only show the point-wise predictive variance as a func-
tion of x. In order to gain insight into the covariance between the predictions at
different values of x, we can draw samples from the posterior distribution over w,
and then plot the corresponding functions y(x, w), as shown in Figure 3.9.
158
3. LINEAR MODELS FOR REGRESSION
t
1
0
−1
t
1
0
−1
0
0
t
1
0
−1
1
x
0
1
x
t
1
0
−1
1
x
0
1
x
Figure 3.9 Plots of the function y(x, w) using samples from the posterior distributions over w corresponding to
the plots in Figure 3.8.
If we used localized basis functions such as Gaussians, then in regions away
from the basis function centres, the contribution from the second term in the predic-
tive variance (3.59) will go to zero, leaving only the noise contribution β−1. Thus,
the model becomes very confident in its predictions when extrapolating outside the
region occupied by the basis functions, which is generally an undesirable behaviour.
This problem can be avoided by adopting an alternative Bayesian approach to re-
gression known as a Gaussian process.
Note that, if both w and β are treated as unknown, then we can introduce a
conjugate prior distribution p(w, β) that, from the discussion in Section 2.3.6, will
be given by a Gaussian-gamma distribution (Denison et al., 2002). In this case, the
predictive distribution is a Student’s t-distribution.
Section 6.4
Exercise 3.12
Exercise 3.13
3.3. Bayesian Linear Regression
159
Figure 3.10 The equivalent ker-
nel k(x, x) for the Gaussian basis
functions in Figure 3.1, shown as
a plot of x versus x, together with
three slices through this matrix cor-
responding to three different values
of x. The data set used to generate
this kernel comprised 200 values of
x equally spaced over the interval
(−1, 1).
N
N
