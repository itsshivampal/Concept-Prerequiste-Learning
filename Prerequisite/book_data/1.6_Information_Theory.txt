49
q = 1
−1
0
y − t
1
2
q = 10
−1
0
y − t
1
2
2
0
−2
0
−2
2
q
t
1
y
q
t
1
y
Figure 1.29 Plots of the quantity Lq = |y − t|q for various values of q.
h(x) = − log2 p(x)
(1.92)
where the negative sign ensures that information is positive or zero. Note that low
probability events x correspond to high information content. The choice of basis
for the logarithm is arbitrary, and for the moment we shall adopt the convention
prevalent in information theory of using logarithms to the base of 2. In this case, as
we shall see shortly, the units of h(x) are bits (‘binary digits’).
Now suppose that a sender wishes to transmit the value of a random variable to
a receiver. The average amount of information that they transmit in the process is
obtained by taking the expectation of (1.92) with respect to the distribution p(x) and
is given by
H[x] = −
x
p(x) log2 p(x).
(1.93)
This important quantity is called the entropy of the random variable x. Note that
limp→0 p ln p = 0 and so we shall take p(x) ln p(x) = 0 whenever we encounter a
value for x such that p(x) = 0.
So far we have given a rather heuristic motivation for the definition of informa-
50
1. INTRODUCTION
tion (1.92) and the corresponding entropy (1.93). We now show that these definitions
indeed possess useful properties. Consider a random variable x having 8 possible
states, each of which is equally likely. In order to communicate the value of x to
a receiver, we would need to transmit a message of length 3 bits. Notice that the
entropy of this variable is given by
H[x] = −8 ×
1
8
log2
1
8
= 3 bits.
1
8
1
64
Now consider an example (Cover and Thomas, 1991) of a variable having 8 pos-
sible states {a, b, c, d, e, f, g, h} for which the respective probabilities are given by
( 1
2 , 1
8 , 1
4 , 1
16 , 1
64 , 1
1
2
64). The entropy in this case is given by
64 , 1
64 , 1
1
4
1
2 −
4
64
1
16 −
1
4 −
1
8 −
1
16
log2
log2
log2
log2
log2
= 2 bits.
H[x] = −
We see that the nonuniform distribution has a smaller entropy than the uniform one,
and we shall gain some insight into this shortly when we discuss the interpretation of
entropy in terms of disorder. For the moment, let us consider how we would transmit
the identity of the variable’s state to a receiver. We could do this, as before, using
a 3-bit number. However, we can take advantage of the nonuniform distribution by
using shorter codes for the more probable events, at the expense of longer codes for
the less probable events, in the hope of getting a shorter average code length. This
can be done by representing the states {a, b, c, d, e, f, g, h} using, for instance, the
following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111.
The average length of the code that has to be transmitted is then
1
2 × 1 +
1
4 × 2 +
average code length =
1
64 × 6 = 2 bits
which again is the same as the entropy of the random variable. Note that shorter code
strings cannot be used because it must be possible to disambiguate a concatenation
of such strings into its component parts. For instance, 11001110 decodes uniquely
into the state sequence c, a, d.
1
16 × 4 + 4 ×
1
8 × 3 +
This relation between entropy and shortest coding length is a general one. The
noiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound
on the number of bits needed to transmit the state of a random variable.
From now on, we shall switch to the use of natural logarithms in defining en-
tropy, as this will provide a more convenient link with ideas elsewhere in this book.
In this case, the entropy is measured in units of ‘nats’ instead of bits, which differ
simply by a factor of ln 2.
We have introduced the concept of entropy in terms of the average amount of
information needed to specify the state of a random variable. In fact, the concept of
entropy has much earlier origins in physics where it was introduced in the context
of equilibrium thermodynamics and later given a deeper interpretation as a measure
of disorder through developments in statistical mechanics. We can understand this
alternative view of entropy by considering a set of N identical objects that are to be
divided amongst a set of bins, such that there are ni objects in the ith bin. Consider
W = N!
ni
N
1
N
1
N
i
i
1
N
1.6. Information Theory
51
the number of different ways of allocating the objects to the bins. There are N
ways to choose the first object, (N − 1) ways to choose the second object, and
so on, leading to a total of N! ways to allocate all N objects to the bins, where N!
(pronounced ‘factorial N’) denotes the product N ×(N −1)×···×2×1. However,
we don’t wish to distinguish between rearrangements of objects within each bin. In
the ith bin there are ni! ways of reordering the objects, and so the total number of
ways of allocating the N objects to the bins is given by
which is called the multiplicity. The entropy is then defined as the logarithm of the
multiplicity scaled by an appropriate constant
i ni!
(1.94)
H =
ln W =
ln N! −
ln ni!.
(1.95)
We now consider the limit N → ∞, in which the fractions ni/N are held fixed, and
apply Stirling’s approximation
ln N!  N ln N − N
(1.96)
which gives
ni
N
i
ln
H = − lim
N→∞
i ni = N. Here pi = limN→∞(ni/N) is the probability
where we have used
of an object being assigned to the ith bin. In physics terminology, the specific ar-
rangements of objects in the bins is called a microstate, and the overall distribution
of occupation numbers, expressed through the ratios ni/N, is called a macrostate.
The multiplicity W is also known as the weight of the macrostate.
pi ln pi
(1.97)
We can interpret the bins as the states xi of a discrete random variable X, where
p(X = xi) = pi. The entropy of the random variable X is then
H[p] = −
i
p(xi) ln p(xi).
(1.98)
Distributions p(xi) that are sharply peaked around a few values will have a relatively
low entropy, whereas those that are spread more evenly across many values will
have higher entropy, as illustrated in Figure 1.30. Because 0 � pi � 1, the entropy
is nonnegative, and it will equal its minimum value of 0 when one of the pi =
1 and all other pj=i = 0. The maximum entropy configuration can be found by
maximizing H using a Lagrange multiplier to enforce the normalization constraint
on the probabilities. Thus we maximize
H = −
i
p(xi) ln p(xi) + λ
p(xi) − 1
i
(1.99)
Appendix E
H = 1.77
H = 3.09
52
1. INTRODUCTION
0.5
s
e
i
t
i
l
i
b
a
b
o
r
p
0.25
0
s
e
i
t
i
l
i
b
a
b
o
r
p
0.25
0.5
0
Figure 1.30 Histograms of two probability distributions over 30 bins illustrating the higher value of the entropy
H for the broader distribution. The largest entropy would arise from a uniform distribution that would give H =
− ln(1/30) = 3.40.
Exercise 1.29
from which we find that all of the p(xi) are equal and are given by p(xi) = 1/M
where M is the total number of states xi. The corresponding value of the entropy
is then H = ln M. This result can also be derived from Jensen’s inequality (to be
discussed shortly). To verify that the stationary point is indeed a maximum, we can
evaluate the second derivative of the entropy, which gives
H
∂p(xi)∂p(xj)
= −Iij
1
pi
(1.100)
where Iij are the elements of the identity matrix.
We can extend the definition of entropy to include distributions p(x) over con-
tinuous variables x as follows. First divide x into bins of width ∆. Then, assuming
p(x) is continuous, the mean value theorem (Weisstein, 1999) tells us that, for each
such bin, there must exist a value xi such that
(i+1)∆
i∆
p(x) dx = p(xi)∆.
(1.101)
We can now quantize the continuous variable x by assigning any value x to the value
xi whenever x falls in the ith bin. The probability of observing the value xi is then
p(xi)∆. This gives a discrete distribution for which the entropy takes the form
H∆ = −
i
p(xi)∆ ln (p(xi)∆) = −
p(xi)∆ ln p(xi) − ln ∆
(1.102)
i
i p(xi)∆ = 1, which follows from (1.101). We now omit
where we have used
the second term − ln ∆ on the right-hand side of (1.102) and then consider the limit
1.6. Information Theory
53
∆ → 0. The first term on the right-hand side of (1.102) will approach the integral of
p(x) ln p(x) in this limit so that
lim
∆→0
i
p(xi)∆ ln p(xi)
p(x) ln p(x) dx
(1.103)
where the quantity on the right-hand side is called the differential entropy. We see
that the discrete and continuous forms of the entropy differ by a quantity ln ∆, which
diverges in the limit ∆ → 0. This reflects the fact that to specify a continuous
variable very precisely requires a large number of bits. For a density defined over
multiple continuous variables, denoted collectively by the vector x, the differential
entropy is given by
H[x] = −
p(x) ln p(x) dx.
(1.104)
In the case of discrete distributions, we saw that the maximum entropy con-
figuration corresponded to an equal distribution of probabilities across the possible
states of the variable. Let us now consider the maximum entropy configuration for
a continuous variable. In order for this maximum to be well defined, it will be nec-
essary to constrain the first and second moments of p(x) as well as preserving the
normalization constraint. We therefore maximize the differential entropy with the
Ludwig Boltzmann
1844–1906
Ludwig Eduard Boltzmann was an
Austrian physicist who created the
field of statistical mechanics. Prior
to Boltzmann,
the concept of en-
tropy was already known from
classical thermodynamics where it
quantifies the fact that when we take energy from a
system, not all of that energy is typically available
to do useful work. Boltzmann showed that the ther-
modynamic entropy S, a macroscopic quantity, could
be related to the statistical properties at the micro-
scopic level. This is expressed through the famous
equation S = k ln W in which W represents the
number of possible microstates in a macrostate, and
k  1.38 × 10−23 (in units of Joules per Kelvin) is
known as Boltzmann’s constant. Boltzmann’s ideas
were disputed by many scientists of they day. One dif-
ficulty they saw arose from the second law of thermo-
dynamics, which states that the entropy of a closed
system tends to increase with time. By contrast, at
the microscopic level the classical Newtonian equa-
tions of physics are reversible, and so they found it
difficult to see how the latter could explain the for-
mer. They didn’t fully appreciate Boltzmann’s argu-
ments, which were statistical in nature and which con-
cluded not that entropy could never decrease over
time but simply that with overwhelming probability it
would generally increase. Boltzmann even had a long-
running dispute with the editor of the leading German
physics journal who refused to let him refer to atoms
and molecules as anything other than convenient the-
oretical constructs. The continued attacks on his work
lead to bouts of depression, and eventually he com-
mitted suicide. Shortly after Boltzmann’s death, new
experiments by Perrin on colloidal suspensions veri-
fied his theories and confirmed the value of the Boltz-
mann constant. The equation S = k ln W is carved on
Boltzmann’s tombstone.
1
1
2
54
1. INTRODUCTION
three constraints
Appendix E
Appendix D
Exercise 1.34
Exercise 1.35
p(x) dx = 1
xp(x) dx = µ
(x − µ)2p(x) dx = σ2.
(1.105)
(1.106)
(1.107)
The constrained maximization can be performed using Lagrange multipliers so that
we maximize the following functional with respect to p(x)
p(x) ln p(x) dx + λ1
+λ2
p(x) dx − 1
xp(x) dx − µ
+ λ3
(x − µ)2p(x) dx − σ2
Using the calculus of variations, we set the derivative of this functional to zero giving
p(x) = exp
−1 + λ1 + λ2x + λ3(x − µ)2
(1.108)
The Lagrange multipliers can be found by back substitution of this result into the
three constraint equations, leading finally to the result
p(x) =
(2πσ2)1/2
exp
(x − µ)2
2σ2
(1.109)
and so the distribution that maximizes the differential entropy is the Gaussian. Note
that we did not constrain the distribution to be nonnegative when we maximized the
entropy. However, because the resulting distribution is indeed nonnegative, we see
with hindsight that such a constraint is not necessary.
If we evaluate the differential entropy of the Gaussian, we obtain
H[x] =
1 + ln(2πσ2)
(1.110)
Thus we see again that the entropy increases as the distribution becomes broader,
i.e., as σ2 increases. This result also shows that the differential entropy, unlike the
discrete entropy, can be negative, because H(x) < 0 in (1.110) for σ2 < 1/(2πe).
Suppose we have a joint distribution p(x, y) from which we draw pairs of values
of x and y. If a value of x is already known, then the additional information needed
to specify the corresponding value of y is given by − ln p(y|x). Thus the average
additional information needed to specify y can be written as
H[y|x] = −
p(y, x) ln p(y|x) dy dx
(1.111)
Exercise 1.37
which is called the conditional entropy of y given x. It is easily seen, using the
product rule, that the conditional entropy satisfies the relation
1.6. Information Theory
55
H[x, y] = H[y|x] + H[x]
(1.112)
where H[x, y] is the differential entropy of p(x, y) and H[x] is the differential en-
tropy of the marginal distribution p(x). Thus the information needed to describe x
and y is given by the sum of the information needed to describe x alone plus the
additional information required to specify y given x.
