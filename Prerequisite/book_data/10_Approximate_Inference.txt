A central task in the application of probabilistic models is the evaluation of the pos-
terior distribution p(Z|X) of the latent variables Z given the observed (visible) data
variables X, and the evaluation of expectations computed with respect to this dis-
tribution. The model might also contain some deterministic parameters, which we
will leave implicit for the moment, or it may be a fully Bayesian model in which any
unknown parameters are given prior distributions and are absorbed into the set of
latent variables denoted by the vector Z. For instance, in the EM algorithm we need
to evaluate the expectation of the complete-data log likelihood with respect to the
posterior distribution of the latent variables. For many models of practical interest, it
will be infeasible to evaluate the posterior distribution or indeed to compute expec-
tations with respect to this distribution. This could be because the dimensionality of
the latent space is too high to work with directly or because the posterior distribution
has a highly complex form for which expectations are not analytically tractable. In
the case of continuous variables, the required integrations may not have closed-form
461
462
10. APPROXIMATE INFERENCE
analytical solutions, while the dimensionality of the space and the complexity of the
integrand may prohibit numerical integration. For discrete variables, the marginal-
izations involve summing over all possible configurations of the hidden variables,
and though this is always possible in principle, we often find in practice that there
may be exponentially many hidden states so that exact calculation is prohibitively
expensive.
In such situations, we need to resort to approximation schemes, and these fall
broadly into two classes, according to whether they rely on stochastic or determin-
istic approximations. Stochastic techniques such as Markov chain Monte Carlo, de-
scribed in Chapter 11, have enabled the widespread use of Bayesian methods across
many domains. They generally have the property that given infinite computational
resource, they can generate exact results, and the approximation arises from the use
of a finite amount of processor time. In practice, sampling methods can be compu-
tationally demanding, often limiting their use to small-scale problems. Also, it can
be difficult to know whether a sampling scheme is generating independent samples
from the required distribution.
In this chapter, we introduce a range of deterministic approximation schemes,
some of which scale well to large applications. These are based on analytical ap-
proximations to the posterior distribution, for example by assuming that it factorizes
in a particular way or that it has a specific parametric form such as a Gaussian. As
such, they can never generate exact results, and so their strengths and weaknesses
are complementary to those of sampling methods.
In Section 4.4, we discussed the Laplace approximation, which is based on a
local Gaussian approximation to a mode (i.e., a maximum) of the distribution. Here
we turn to a family of approximation techniques called variational inference or vari-
ational Bayes, which use more global criteria and which have been widely applied.
We conclude with a brief introduction to an alternative variational framework known
as expectation propagation.
