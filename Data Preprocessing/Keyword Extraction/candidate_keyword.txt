introduction
problem
patterns
data
fundamental one
successful history
instance
extensive astronomical observations
tycho brahe
16th century
johannes kepler
empirical laws
planetary motion
turn
springboard
development
clas- sical mechanics
discovery
regularities
key role
development
veriﬁcation
quantum physics
early twenti- eth century
ﬁeld
pattern recognition
automatic discov- ery
regularities
data
use
computer algorithms
use
regularities
actions
data
different categories
example
handwritten digits
figure
digit corresponds
pixel image
vector x
real numbers
goal
machine
vector x
input
identity
digit
output
nontrivial problem
wide variability
introduction figure
examples
hand-written dig-
us zip codes
rules
heuristics
digits
shapes
strokes
practice
approach
proliferation
rules
exceptions
rules
poor results
far
results
machine
approach
large set
n digits
x1
training set
parameters
adaptive model
categories
digits
training
advance
category
digit
target vector t
identity
corresponding digit
suitable techniques
cate- gories
terms
vectors
note
such target vector t
digit image x
result
machine
algorithm
function y
x
new digit image x
input
output vector y
same way
target vectors
precise form
function y
x
training phase
learning phase
basis
training data
model
termine
identity
new digit images
test set
ability
new examples
train- ing
generalization
practical applications
variability
input vectors
training data
tiny fraction
possible input vectors
generalization
central goal
pattern recognition
practical applications
original input variables
new space
variables
pattern recognition problem
instance
digit recogni- tion problem
images
digits
digit
box
ﬁxed size
variability
digit class
location
scale
digits
subsequent pattern recognition algorithm
different classes
pre-processing stage
feature extraction
new test data
same steps
training data
pre-processing
order
computation
example
goal
real-time face detection
high-resolution video stream
computer
huge numbers
pixels
second
complex pattern recognition algorithm
aim
useful features
introduction
useful discriminatory information
non-faces
features
inputs
pattern recognition algorithm
instance
average value
image intensity
rectangular subregion
viola
jones
set
such features
fast face detection
number
such features
number
pixels
kind
pre-processing repre- sents
form
dimensionality reduction
information
information
solution
problem
overall accuracy
system
applications
training data comprises examples
input vectors
corresponding target vectors
prob- lems
cases
digit recognition example
aim
input vector
ﬁnite number
discrete categories
classiﬁcation problems
output
continuous variables
task
regression
example
regression problem
pre- diction
yield
chemical manufacturing process
inputs consist
concentrations
reactants
temperature
pressure
other pattern recognition problems
training data consists
set
input vectors
corresponding target values
goal
problems
groups
similar examples
data
clustering
distribution
data
input space
density estimation
data
high-dimensional space
dimensions
purpose
visualization
technique
reinforcement learning
sutton
barto
problem
suitable actions
situation
order
reward
learning algorithm
examples
optimal outputs
contrast
learning
process
trial
error
sequence
states
actions
learning algorithm
environment
many cases
current action
immediate reward
impact
re- ward
subsequent time steps
example
appropriate reinforcement
techniques
neural network
game
backgammon
high standard
tesauro
network
board position
input
result
dice throw
strong move
output
network play
copy
games
major challenge
game
backgammon
dozens
moves
end
game
reward
form
victory
reward
moves
moves
good ones
others
example
credit assignment problem
general feature
re- inforcement learning
trade-off
exploration
system
new kinds
actions
exploitation
system
use
actions
high reward
too
focus
exploration
exploitation
poor results
reinforcement
active area
machine
research
introduction figure
plot
training data set
n =
points
blue circles
observation
input variable x
corresponding target variable t.
green curve
function sin
gener- ate
data
goal
pre- dict
value
t
new value
x
knowledge
green curve
−1
x
treatment
scope
book
tasks
own tools
techniques
key ideas
such problems
main goals
chapter
informal way
concepts
simple examples
book
same ideas re-emerge
context
sophisti- cated models
real-world pattern recognition applications
chapter
self-contained introduction
important tools
book
probability theory
decision theory
infor- mation theory
topics
fact straightforward
clear understanding
machine learning techniques
effect
practical applications
example
polynomial curve fitting
simple regression problem
run- ning example
chapter
number
key concepts
sup- pose
real-valued input variable x
observation
value
real-valued target variable t.
present purposes
artiﬁcial example
data
precise process
data
comparison
learned model
data
example
function sin
random noise
target values
detail
appendix a
training
n observations
x
x ≡
x1
xn
t
corresponding observations
values
t
t ≡
t1
tn
t. figure
plot
training
n
data points
input data
x
figure
values
xn
n =
n
range [
]
target data
t
corresponding values
function 
 m    n
example
polynomial curve fitting sin
small level
random noise
gaussian distri- bution
gaussian distribution
section
such point
order
corresponding value tn
data
way
property
many real data sets
underlying regularity
individual observations
random noise
noise
random
pro- cesses
radioactive decay
sources
variability
goal
training
order
predictions
value t
target variable
new value x
input
underlying function sin
difﬁcult problem
ﬁnite data set
data
noise
x t. probability theory
uncertainty
appropriate value
section
framework
such uncertainty
precise
quantitative manner
decision theory
section
probabilistic representation
order
predictions
appropriate criteria
moment
simple approach
curve
data
polynomial function
form y
w0 + w1x + w2x2 +
+ wm xm = wjxj
j=0
m
order
polynomial
xj
x
power
j
polynomial coefﬁcients w0
vector w. note
polynomial function y
nonlinear function
x
linear function
coefﬁcients w. functions
polynomial
unknown parameters
important properties
linear models
chapters
values
coefﬁcients
polynomial
training data
error function
misﬁt
function y
value
w
training
data points
simple choice
error function
sum
squares
errors
predictions
w
data point xn
corresponding target values
e
w
y
w
− tn
n=1
factor
convenience
mo- tivation
choice
error function
chapter
moment
nonnegative quantity
introduction figure
error function
corre- sponds
half
sum
squares
displacements
vertical green bars
data point
function y
t tn y
w
x function y
training data point
geomet- rical interpretation
sum-of-squares error function
figure
curve
problem
value
w
e
w
error function
quadratic function
coefﬁcients w
derivatives
respect
coefﬁcients
elements
w
minimization
error function
unique solution
w
closed form
function y
problem
order m
polynomial
example
important concept
model comparison
model selection
figure
examples
results
polynomials
orders m
data set
figure
m =
ﬁrst order
m =
polynomials
poor ﬁts
data
poor representations
function sin
third order
m =
polynomial
ﬁt
function sin
examples
figure
order polynomial
m =
excellent ﬁt
training data
fact
polynomial passes
data point
e
w
ﬁtted curve oscillates
poor representation
function sin
latter behaviour
over-ﬁtting
goal
good generalization
accurate predictions
new data
quantitative insight
dependence
generalization performance
m
separate test
data points
same procedure
training
points
new choices
random noise values
target values
choice
m
residual value
e
w
training data
e
w
test data set
root-mean-square  figure
plots
polynomials
various orders m
red curves
data set
figure
rms
error
erms
w
/n
division
n
different sizes
data sets
equal footing
square root
erms
same scale
same units
target variable t. graphs
training
test set rms errors
various values
m
figure
test set error
measure
values
t
new data observations
x
figure
small values
m
large values
test set error
fact
corresponding polynomials
oscillations
function sin
values
m
range
� m
give small values
test set error
reasonable representations
function sin
case
m =
figure
example
polynomial curve fitting
−1
−1
m
t
m
t
m
x m =
x
introduction figure
graphs
root-mean-square error
training set
inde- pendent test
various values
m. training test
s m r e
m
m =
training
error
degrees
freedom
coefﬁcients
w9
data points
training set
test set error
figure
corresponding function y
wild oscillations
polynomial
order
order polynomials
special cases
m =
polynomial
capa- ble
results
m =
polynomial
predictor
new data
function sin
data
case
power series expansion
function sin
terms
orders
results
m.
insight
problem
values
co- efﬁcients
polynomials
various order
m
magnitude
coefﬁcients
m =
polynomial
coefﬁcients
data
negative values
correspond- table
table
coefﬁcients
polynomials
various order
typical mag- nitude
coefﬁcients in- creases
or- der
polynomial increases
w
w
w
w
w
w
w
w
w
m
m
m
-25.43
-1.27 m =
t
−1
example
polynomial curve fitting
n
n
t
x figure
plots
solutions
sum-of-squares error function
m =
polynomial
n =
data points
left plot
n
data points
right plot
size
data set
over-ﬁtting problem
polynomial function matches
data points
data points
ends
range
function
large oscilla- tions
figure
ﬂexible polynomials
values
m
random noise
target values
behaviour
model
size
data set
figure
model complexity
over-ﬁtting problem become
size
data set increases
way
data set
other words
model
data
rough heuristic
number
data points
number
adaptive parameters
model
chapter
number
parameters
appropriate measure
model complexity
something
number
parameters
model
size
available training set
complexity
model
com- plexity
problem
squares
model parameters
speciﬁc case
maximum likelihood
section
over-ﬁtting problem
general property
maximum likelihood
bayesian approach
over-ﬁtting problem
difﬁculty
bayesian perspective
models
number
parameters
number
data points
bayesian model
effective number
parameters adapts
size
data set
moment
current approach
practice
sets
limited size
  t n
−1
ﬂexible models
technique
over-ﬁtting phenomenon
such cases
regularization
penalty term
error function
order
coefﬁcients
large values
such penalty term
form
sum
squares
coefﬁcients
modiﬁed error function
form e
w
y
w
− tn
+ λ
+
+ w2
+ w2
w2 ≡ wtw = w2 m
coefﬁcient λ
rel- ative importance
regularization term
sum-of-squares error term
note
coefﬁcient w0
regularizer
inclusion
results
choice
origin
target
hastie
own regularization coefﬁcient
topic
detail
section
error function
closed form
techniques
statistics literature
shrinkage methods
value
coefﬁcients
particular case
quadratic regularizer
ridge regres- sion
hoerl
kennard
context
neural networks
approach
weight decay
figure
results
polynomial
order m
same data set
regularized error function
value
ln λ = −18
over-ﬁtting
representation
underlying function sin
value
poor ﬁt
figure
ln λ
corresponding coefﬁcients
polynomials
regularization
desired effect
exercise
introduction
ln λ =
x ln λ
x figure
plots
m =
polynomials
data set
figure
regularized error function
values
regularization parameter λ
λ = −18
ln λ =
case
regularizer
λ =
λ = −∞
bottom right
figure
example
polynomial curve fitting
table
table
coefﬁcients
m =
polynomials
various values
regularization parameter λ
note
λ = −∞
model
regularization
graph
bottom right
fig- ure
value
λ increases
typical magnitude
coefﬁcients
λ = −∞
λ =
-0.77 -31.97
-45.95
ln λ =
-0.05 -0.06 -0.05 -0.03 -0.02 -0.01
w
w
w
w
w
w
w
w
w
w
magnitude
coefﬁcients
impact
regularization term
generalization error
value
rms error
training
test sets
ln λ
figure
effect λ
effective complexity
model
hence
degree
over-ﬁtting
issue
model complexity
length
section
practical application
approach
error function
way
suitable value
model complexity
results
simple way
available data
training set
coefﬁcients w
separate validation
hold-out set
model complexity
m
λ
many cases
valuable training data
sophisticated approaches
discussion
polynomial curve ﬁtting
in- tuition
principled approach
problems
pattern recognition
discussion
probability theory
foundation
subsequent developments
book
section
figure
graph
root-mean-square er- ror
versus ln λ
m =
polynomial
test s m r e
−35 −30 −25
λ −20
introduction
important insights
concepts
con- text
polynomial curve
complex situations
probability theory a key concept
ﬁeld
pattern recognition
uncertainty
noise
measurements
ﬁnite size
data sets
prob- ability theory
consistent framework
quantiﬁcation
manipula- tion
uncertainty
central foundations
pattern recognition
decision theory
section
optimal predictions
information
infor- mation
basic concepts
probability theory
sim- ple example
imagine
boxes
red
blue
red box
apples
oranges
blue box
apples
orange
figure
boxes
box
item
fruit
sort
fruit
box
process many times
red box
%
time
blue
%
time
item
fruit
box
pieces
fruit
box
example
identity
box
random variable
b
random
possible values
r
red box
b
blue box
identity
fruit
random
f
values
apple
orange
probability
event
fraction
times
event
total number
trials
limit
total number
trials
inﬁnity
probability
red box
figure
simple example
coloured boxes
fruit
apples
or- anges
orange
intro- duce
basic ideas
probability
figure
sum
product rules
probability
random variables
x
values
xi
m
y
values
yj
j =
l.
illustration
m =
l =
total number n
instances
variables
number
instances
x = xi
y = yj
nij
number
points
corresponding cell
array
number
points
column i
x = xi
ci
number
points
row j
y = yj
rj
probability theory
rj
ci
probability
blue box
probabilities
p
b = r
p
b = b
note
deﬁnition
probabilities
interval
]
events
possible outcomes
instance
example
box
blue
probabilities
events
questions
“
overall probability
se- lection procedure
apple
orange
probability
box
”
questions
complex questions
problems
pattern recognition
el- ementary rules
probability
sum rule
product rule
rules
boxes
fruit example
order
rules
probability
general ex- ample
figure
random variables x
y
instance
box
fruit variables
x
values
i
m
y
values
j
l. consider
total
n trials
variables x
y
number
such trials
x = xi
y =
number
trials
x
value xi
irrespective
value
y
ci
number
trials
y
value
rj
probability
x
value xi
y
value yj
x = xi
y = yj
joint probability
x = xi
y = yj
number
points
cell i
j
fraction
total number
points
limit n → ∞
probability
x
value xi irrespective
value
y
p
x = xi
fraction
total number
points
column i
p
x = xi
y = yj
nij n
p
x = xi
ci n
number
instances
column i
figure
sum
number
instances
cell
column
= j nij
 l
introduction
x = xi
p
x = xi
y = yj
sum rule
probability
note
p
x = xi
marginal probability
other variables
case y
instances
x = xi
fraction
such instances
y = yj
y = yj|x = xi
conditional probability
y = yj
x = xi
fraction
points
column i
fall
cell i
j
hence
p
y = yj|x = xi
following relationship  ci p
x = xi
y = yj
nij n =
ci n · = p
y = yj|x = xi
p
x = xi
product rule
probability
distinction
box b
fruit example
values
random
example r
box
red one
probability
b
value r
p
b = r
ambiguity
cumbersome notation
many cases
need
such pedantry
p
b
distribution
ran- dom variable b
p
r
distribution
particular value r
interpretation
context
compact notation
fundamental rules
probability theory
following form
rules
probability sum rule p
x
p
x
y
y product rule p
x
y
p
y |x
p
x
p
x
y
joint probability
“
probability
x
y ”
quantity p
y |x
conditional probability
“
probability
y
x ”
quantity p
x
marginal probability 
probability
x ”
simple rules
basis
probabilistic machinery
book
product rule
symmetry property p
x
y
p
y
x
relationship
conditional probabilities
y |x
p
x|y
p
y
p
x
probability theory
bayes ’ theorem
central role
pattern recognition
machine learning
sum rule
denominator
bayes ’ theorem
terms
quantities
numerator p
x
p
x|y
p
y
y
denominator
bayes ’ theorem
normalization constant
sum
conditional probability
left-hand side
values
y equals
figure
simple example
joint distribution
variables
concept
conditional distributions
ﬁnite sample
n =
data points
joint distribution
top left
top right
histogram
fractions
data points
values
y
deﬁnition
probability
fractions
corresponding probabilities
y
limit n → ∞
histogram
simple way
probability distribution
ﬁnite number
points
distribution
distributions
data lies
heart
statistical pattern recognition
great detail
book
plots
figure
show
corresponding histogram estimates
p
x
p
x|y =
example
boxes
fruit
moment
random variables
instantiations
probabilities
blue boxes
p
b = r
p
b = b
note
satisfy p
b = r
+ p
b = b
box
random
blue box
probability
apple
fraction
apples
blue box
f = a|b = b
fact
conditional probabilities
type
fruit
box p
f = a|b = r
p
f =
r
p
f = a|b = b
p
f =
b
introduction p
x
y
p
y
y
y
x p
x
p
x|y =
x figure
illustration
distribution
variables
x
possible values
y
possible values
top left ﬁgure
sample
points
joint probability distri- bution
variables
ﬁgures
histogram estimates
marginal distributions
x
p
y
conditional distribution p
x|y =
corresponding
bottom row
top left ﬁgure
again
note
probabilities
p
f = a|b = r
+ p
f =
r
p
f = a|b = b
+ p
f =
b
sum
product rules
probability
overall probability
apple
p
f =
= p
f = a|b = r
p
b = r
+ p
f = a|b = b
p
b = b
×
+
×
=
sum rule
p
f = o
−
=
probability theory
 suppose
piece
fruit
orange
box
probability distribution
boxes
identity
fruit
probabilities
–
probability distribution
fruit
identity
box
problem
conditional probability
bayes ’ theorem
×
×
= =
p
f = o
p
b = r|f = o
= p
f =
r
p
b = r
sum rule
p
b = b|f = o
−
=
important interpretation
bayes ’ theorem
box
identity
selected item
fruit
complete information
probability p
b
prior probability
probability
identity
fruit
fruit
orange
bayes ’ theorem
probability p
b|f
posterior probability
probability
f
note
example
prior probability
red box
blue box
red one
piece
fruit
orange
posterior probability
red box
box
fact
red one
result accords
intuition
proportion
oranges
red box
blue box
observation
fruit
orange
signiﬁcant evidence
red box
fact
evidence
red box
blue one
joint distribution
variables
product
marginals
p
x
y
p
x
p
y
x
y
product rule
p
y |x
p
y
conditional distribution
y
x
value
x
instance
boxes
fruit example
box
same fraction
apples
oranges
p
f|b
p
f
probability
selecting
apple
box
probability densities as
probabilities
discrete sets
events
probabilities
respect
continuous variables
informal discussion
probability
real-valued variable x
interval
x + δx
p
x
δx
δx →
p
x
probability density
x
figure
probability
x
interval
b
p
x ∈
b
=
p
x
dx
    
introduction figure
concept
probability
discrete variables
probability density p
x
continuous variable x
probability
x lying
inter- val
x + δx
p
x
δx
δx →
probability density
derivative
cumulative distri- bution function p
x
p
x
p
x
x
δx p
x
p
x
−∞
probabilities
value
x
real axis
probability density p
conditions
nonlinear change
probability density transforms
simple function
jacobian factor
instance
f
g
change
variables x = g
y
function f
x
probability density px
x
density py
y
respect
new variable y
sufﬁces
fact
px
x
py
y
different densities
observations
range
x + δx
small values
δx
range
y + δy
px
x
 py
y
δy
py
y
= px
x
px
g
y
|
consequence
property
concept
maximum
probability density
choice
probability
interval
z
cumulative distribution function
p
z
= p
x
z −∞
p 
x
p
x
figure
several continuous variables
vector x
joint probability density p
x
p
such exercise
    x
probability theory
probability
inﬁnitesimal volume δx
point x
p
x
δx
multivariate probability density
p
x
p
x
integral
whole
x space
joint probability distributions
combination
continuous variables
note
x
discrete variable
p
x
probability mass function
set
‘ probability masses
allowed values
x
sum
product rules
probability
bayes ’ theorem
case
probability densities
combinations
con- tinuous variables
instance
real variables
sum
product rules
form p
x
p
dy p
p
y|x
p
x
formal justiﬁcation
sum
product rules
continuous variables
feller
branch
mathematics
measure theory
lies
scope
book
validity
intervals
width ∆
discrete probability dis- tribution
intervals
limit ∆
sums
integrals
desired result
expectations
covariances
important operations
probabilities
weighted averages
functions
average value
function f
probability distribution p
x
expectation
f
x
e [ f ]
discrete distribution
e [
] = p
x
f
x
average
relative probabilities
different values
x
case
continuous variables
expectations
terms
integration
respect
corresponding probability density e [
] = p
x
f
x
dx
case
ﬁnite number n
points
probability distribution
probability density
expectation
  n=1 n 
introduction ﬁnite sum
points
[ f ]
n f
xn
extensive use
result
methods
chapter
approximation
becomes
limit n → ∞
sometimes
expectations
functions
several variables
case
subscript
variable
instance
average
function f
respect
distribution
x
note
ex [ f
]
function
y. ex [ f
conditional expectation
respect
conditional distribution
p
x|y
f
x
analogous deﬁnition
continuous variables
ex [ f|y ] =
variance
f
x
var [ f ] = e
f
x
e [ f
x
]
measure
much variability
f
x
mean value e [ f
x
]
square
variance
terms
expectations
f
x
x
variance
variable x
var [ f ] = e [ f
x
] − e [ f
x
var [ x ] = e [ x2 ] − e [ x
random variables
covariance
cov [ x
y ] = ex
[
x − e [ x ]
y − e [ y ]
] = ex
y [ xy ] − e [ x ] e [ y ]
extent
x
y
indepen- dent
covariance vanishes
case
vectors
random variables x
covariance
matrix cov [ x
y ] = ex
x − e [ x ]
yt − e [ yt ]
= ex
y [ xyt ] − e [ x ] e [ yt ]
covariance
components
vector x
simpler notation cov [ x ] ≡ cov [ x
x ]
exercise
exercise
probability theory
bayesian probabilities
chapter
probabilities
terms
frequencies
random
repeatable events
frequentist interpretation
probability
general bayesian view
probabilities
quantiﬁcation
uncertainty
uncertain event
example
moon
own orbit
sun
arctic ice cap
end
century
events
numerous times
order
notion
probability
context
boxes
fruit
idea
example
polar ice
fresh evidence
instance
new earth observation satellite gathering novel forms
diagnostic information
opinion
rate
ice loss
assessment
such matters
actions
instance
extent
emission
greenhouse gasses
such circumstances
expression
uncertainty
precise revisions
uncertainty
light
new evidence
optimal actions
decisions
consequence
elegant
bayesian interpretation
probability
use
probability
uncertainty
ad-hoc choice
common sense
rational coherent inferences
instance
cox
numerical values
degrees
belief
simple set
axioms
common sense properties
such beliefs
set
rules
degrees
belief
sum
product rules
probability
ﬁrst rigorous proof
probability theory
extension
boolean logic
situations
uncertainty
jaynes
numerous other authors
different sets
properties
axioms
such measures
uncertainty
ramsey
good
savage
definetti
lindley
case
numerical quantities
pre-
rules
probability
quantities
bayesian
probabilities
ﬁeld
pattern recognition
general no- thomas bayes
thomas bayes
tun- bridge wells
clergyman
amateur scientist
mathematician
theology
edinburgh univer- sity
fellow
royal society
18th century
is- sues
probability arose
connection
gambling
new concept
insurance
important problem
so-called in- verse probability
solution
thomas bayes
paper ‘ essay towards
problem
doctrine
chances ’
years
death
philo- sophical transactions
royal society
fact
bayes
theory
case
uni- form
pierre-simon laplace
theory
general form
broad applicability
introduction  tion
probability
example
polynomial curve
section
frequentist notion
probability
random values
observed variables tn
uncertainty
appropriate choice
model param- eters
bayesian perspective
machinery
probability theory
uncertainty
model parameters
w
choice
model
bayes ’ theorem
new signiﬁcance
boxes
fruit example
observation
identity
fruit
relevant information
probability
chosen box
red one
example
bayes ’ theorem
prior probability
posterior probability
evidence
observed data
detail
similar approach
inferences
quantities
parameters
polynomial curve
example
assumptions
w
data
form
prior probability distribution p
w
effect
observed data d =
t1
conditional probability p
d|w
section
bayes ’ theorem
form p
w|d
= p
d|w
p
w
p
d
uncertainty
w
d
form
posterior probability p
w|d
quantity p
d|w
right-hand side
bayes ’ theorem
observed data
d
function
parameter vector w
case
likelihood function
observed data set
different settings
parameter vector w. note
likelihood
probability distribution
w
respect
equal one
deﬁnition
likelihood
state bayes ’ theorem
words posterior ∝ likelihood ×
quantities
functions
w.
denominator
normalization constant
posterior distribution
left-hand side
valid probability density
sides
respect
denominator
bayes ’ theorem
terms
prior distribution
likelihood function p
d
p
d|w
p
w
dw
bayesian
frequentist paradigms
likelihood function p
d|w
central role
manner
dif- ferent
approaches
frequentist setting
w
ﬁxed parameter
value
form
‘ estimator ’
error bars
probability theory
estimate
distribution
possible data sets d.
contrast
bayesian viewpoint
single data set d
one
uncertainty
parameters
probability distribution
w. a
frequentist estimator
maximum likelihood
w
value
likelihood function p
d|w
value
w
probability
observed data set
machine learning literature
negative log
likelihood function
error function
negative logarithm
function
likelihood
error
approach
frequentist error bars
bootstrap
efron
hastie
al.
multiple data sets
original data set
n data points x =
x1
new data
xb
n points
random
x
replacement
points
x
xb
other points
x
xb
process
l times
l data sets
size n
original data
x
statistical accuracy
parameter estimates
variability
predictions
different bootstrap data sets
advantage
bayesian viewpoint
inclusion
prior knowl- edge arises
suppose
instance
fair-looking coin
times
heads
time
classical maximum likelihood estimate
probability
heads
future tosses
heads
contrast
bayesian approach
reasonable prior
extreme conclusion
much controversy
debate
relative mer-
frequentist
bayesian paradigms
fact
unique frequentist
viewpoint
instance
common criticism
bayesian approach
prior distribution
basis
mathematical convenience
reﬂection
prior beliefs
subjective nature
conclusions
de- pendence
choice
prior
source
difﬁculty
dependence
prior
motivation
so-called noninformative priors
difﬁculties
different models
bayesian methods
poor choices
poor results
high conﬁdence
frequentist evaluation methods
protection
such prob- lems
techniques
cross-validation
areas
model comparison
book
strong emphasis
bayesian viewpoint
huge growth
practical importance
bayesian methods
past few years
useful frequentist concepts
bayesian framework
origins
18th century
prac- tical application
bayesian methods
long time
difﬁculties
full bayesian procedure
need
sum
integrate
whole
parameter space
section
section
introduction see
order
predictions
different models
development
methods
markov chain monte carlo
chapter
dramatic improvements
speed
memory capacity
computers
door
practical use
bayesian techniques
im- pressive range
problem domains
monte carlo methods
wide range
models
small-scale problems
efﬁcient deterministic approximation schemes
variational bayes
expectation propagation
chapter
complementary alternative
methods
bayesian techniques
large-scale applications
blei
gaussian distribution
whole
chapter
study
various probability dis- tributions
key properties
important probability distributions
continuous variables
gaussian distribution
extensive use
distribution
remainder
chapter
book
case
single real-valued variable x
gaussian distribution
n x|µ
σ2
x − µ
−
parameters
µ
mean
σ2
vari- ance
square root
variance
σ
standard deviation
reciprocal
variance
β =
precision
motivation
terms
figure
plot
gaussian distribution
form
gaussian distribution satisﬁes n
x|µ
σ2
exercise
gaussian
pierre-simon laplace
laplace
modesty
point
france
time
claim
mathe- matics
numerous contributions
as- tronomy
nebular hypothesis
earth
condensa- tion
cooling
disk
gas
dust
ﬁrst edition
th´eorie analytique
probabilit´es
laplace
“ probability theory
nothing
common sense
calculation ”
work
discus- sion
inverse probability calculation
bayes ’ theorem
poincar´e
problems
life expectancy
jurisprudence
planetary masses
triangulation
error estimation
 
µ
x
figure
plot
mean µ
standard deviation σ. n
x|µ
σ2
probability theory exercise
exercise
∞ −∞ n x|µ
σ2
thus
requirements
valid probability density
expectations
functions
x
gaussian distribu- tion
average value
x
e [ x ] = n x|µ
σ2 x dx = µ
−∞
parameter µ
average value
x
distribution
mean
second order moment e [ x2 ] = ∞ −∞ n x|µ
σ2 x2 dx = µ2 + σ2
from
variance
x
var [ x ] = e [ x2 ] − e [ x
= σ2
σ2
variance parameter
maximum
distribution
mode
mode
mean
gaussian distribution
d-dimensional vector x
continuous variables
n
x|µ
σ
d/2 |σ|1/2
−
x − µ
tς−1
x − µ
d-dimensional vector µ
mean
d × d matrix σ
covariance
|σ|
determinant
σ
use
multivariate gaussian distribution brieﬂy
chapter
properties
detail
section
introduction figure
illustration
likelihood function
gaussian distribution
red curve
black points
data set
values
xn
likelihood function
corresponds
product
blue values
likelihood in- volves
mean
vari- ance
gaussian
maxi- mize
product
p
x
n
xn|µ
σ2
x  n
data set
observations
=
x1
xn
t
n observations
scalar variable x
type- face x
single observation
vector-valued variable
xd
t
x
observations
gaussian distribution
mean µ
variance σ2
parameters
data set
data points
same distribution
joint probability
independent events
product
marginal probabilities
event
data
x
probability
data set
µ
σ2
form section
p
σ2
n xn|µ
σ2
n=1
function
µ
σ2
likelihood function
gaus- sian
figure
common criterion
parameters
probability distribu- tion
observed data set
parameter values
likelihood function
strange criterion
discussion
probability theory
probability
parameters
data
probability
data
parameters
fact
criteria
context
curve ﬁtting
moment
values
unknown parame- ters
σ2
likelihood function
prac- tice
log
likelihood function
logarithm
function
argument
maximization
log
function
maximization
function
log
subsequent mathematical analysis
product
large number
small probabilities
numerical precision
computer
sum
log probabilities
from
log likelihood   n
n n=1 n   n
n n
xn n=1 n n  exercise
section
exercise
probability theory
function
form ln p x|µ
= −
xn − µ
− ln σ2 − n
ln
maximizing
respect
maximum likelihood solution
µml =
sample mean
i.e.
mean
observed values
xn
respect
maximum likelihood solution
variance
form ml =
n
xn − µml
sample variance
respect
sample mean µml
note
joint maximization
respect
σ2
case
gaussian distribution
solution
µ decouples
σ2
result
chapter
subsequent chapters
sig- niﬁcant limitations
maximum likelihood approach
indication
problem
context
solutions
maximum likelihood param- eter settings
univariate gaussian distribution
maximum likelihood approach
variance
distribution
example
phenomenon
bias
problem
over-ﬁtting
context
polynomial curve
maximum likelihood solutions
ml
functions
data set values x1
xn
expectations
quantities
respect
data set values
gaussian distribution
parameters µ
σ2
e [ µml ] = µ e [ σ2 ml ] = σ2
average
maximum likelihood estimate
correct mean
true variance
factor
n −
/n
intuition
result
figure
from
following estimate
variance parameter
unbiased σ2 = n ml = n
n
n=1
xn − µml
introduction figure
illustration
bias arises
max- imum likelihood
variance
gaussian
green curve
true gaussian distribution
data
red curves
gaussian distributions
data sets
consist- ing
data points
maximum likelihood results
data sets
mean
variance
sample mean
true mean
b
c
section
section
result
bayesian approach
note
bias
maximum likelihood solution
number n
data points
limit n →
maximum likelihood solution
variance
true variance
distribution
data
practice
anything
small n
bias
serious problem
book
complex models
many parameters
bias problems
maximum likelihood
fact
issue
bias
maximum likelihood lies
root
over-ﬁtting problem
context
polynomial curve
curve
problem
polynomial curve ﬁtting
terms
error minimization
curve
example
view
probabilistic perspective
insights
error functions
regularization
full bayesian treatment
goal
curve
problem
predictions
target variable t
new value
input variable x
basis
set
data
n input values
=
x1
xn
t
corresponding target values
=
t1
tn
uncertainty
value
target
probability distribution
purpose
value
x
value
t
gaussian distribution
value y
polynomial curve
consistency
notation
later chapters
preci- sion parameter β corresponding
inverse variance
distribution
figure
p
t|x
w
β
n t|y
β−1
 n
t   n n n=1 figure
schematic illustration
gaus- sian conditional distribution
t
x
mean
polyno- mial function y
precision
parameter β
vari- ance
β−1 = σ2
y
w
probability theory
y
p
t|x0
w
β
x x0
training data
x
t
values
unknown parameters
maximum likelihood
data
distribution
likelihood function
p
t|x
w
β
n tn|y
w
β−1
case
simple gaussian distribution
logarithm
likelihood function
form
gaussian distribution
log likelihood function
form ln p
t|x
w
β
y
w
− tn
+ n
ln β − n
ln
determination
maximum likelihood solution
polyno- mial coefﬁcients
wml
maxi- mizing
respect
purpose
terms
right-hand side
w.
log likelihood
positive constant coefﬁcient
location
respect
coefﬁcient β/2
log likelihood
negative log likelihood
likelihood
w
sum-of-squares error function
sum-of-squares error function
consequence
likelihood
assumption
gaussian noise distribution
maximum likelihood
precision parameter β
gaussian conditional distribution
maximizing
respect
βml
n
y
wml
− tn
n=1
     n
introduction section
again
parameter vector wml
mean
sub-
precision βml
case
simple gaussian distribution
parameters
β
predictions
new values
x
probabilistic model
terms
predictive distribution
probability distribution
t
point estimate
maximum likelihood parameters
p
t|x
wml
βml
n t|y
β−1
ml
step
bayesian approach
prior distribution
polynomial coefﬁcients
simplicity
gaussian distribution
form p
w|α
= n
w|0
α−1i
m +1
− wtw
α
precision
distribution
m +1
total number
elements
vector w
m th order polynomial
variables
α
distribution
model parameters
hyperparameters
bayes ’ theorem
posterior distribution
w
product
prior distribution
likelihood function p
w|x
t
α
β
p
t|x
w
β
p
w|α
w
probable value
w
data
other words
posterior distribution
technique
maximum posterior
map
negative logarithm
maximum
posterior
minimum
y
w
− tn
wtw
posterior distribution
sum-of-squares error function
form
regularization parameter
λ = α/β
bayesian curve
prior distribution p
w|α
point estimate
w
bayesian treatment
bayesian approach
sum
product rules
probability
val- ues
w. such marginalizations
heart
bayesian methods
pattern recognition
 n  n
probability theory
curve
problem
training data x
t
new test point x
goal
value
t.
predictive distribution p
t|x
x
t
parameters
β
advance
chapters
such parameters
data
bayesian setting
bayesian treatment
consistent application
sum
product rules
probability
predictive distribution
form p
t|x
x
t
= p
t|x
w
p
w|x
t
dw
p
t|x
w
dependence
α
β
notation
p
w|x
t
posterior distribution
param- eters
right-hand side
section
problems
curve-ﬁtting example
posterior distribution
integra- tion
result
predictive distribution
gaussian
form p
t|x
x
t
= n
mean
variance
t|m
x
x
matrix s
m
x
βφ
x
ts φ
xn
tn n=1 s2
x
β−1 + φ
x
tsφ
x
s−1 = αi + β φ
xn
φ
x
t
unit matrix
vector φ
x
elements φi
x
xi
m.
variance
mean
predictive distribution
x
ﬁrst term
uncertainty
predicted value
t
noise
target variables
maximum likelihood predictive distribution
β−1 ml
second term arises
uncertainty
parameters
consequence
bayesian treatment
predictive distribution
synthetic sinusoidal regression problem
figure
introduction figure
predictive distribution result- ing
bayesian treatment
polynomial curve
m =
polynomial
ﬁxed parameters
β =
noise variance
red curve
mean
predictive distribution
red region corresponds
stan- dard deviation
mean
−1
model selection
x
example
polynomial curve
squares
optimal order
generalization
order
number
free parameters
model
model complexity
regularized least squares
regularization coefﬁcient λ
effective complexity
model
whereas
complex models
mixture distributions
neural networks
multiple pa- rameters
complexity
practical application
values
such parameters
principal objective
predictive performance
new data
appropriate values
complexity parameters
model
range
different types
model
order
one
particular application
maximum likelihood approach
perfor- mance
training set
good indicator
predictive performance
data
problem
over-ﬁtting
data
approach
available data
range
models
model
range
values
complexity parameters
independent data
validation
predictive performance
model design
many times
lim- ited size data set
over-ﬁtting
validation data
third test set
performance
model
many applications
supply
data
training
testing
order
good models
available data
training
validation set
noisy estimate
predictive performance
solution
dilemma
cross-validation
figure
proportion
s −
/s
available data
use
curse
dimensionality
figure
technique
s-fold cross-validation
case
s =
available data
s groups
case
equal size
s −
groups
set
models
re- maining group
procedure
s possible choices
held-out group
red blocks
perfor- mance scores
s runs
run
run
run
data
performance
data
case s = n
n
total number
data points
leave-one-out technique
major drawback
cross-validation
number
training runs
factor
s
models
training
further problem
techniques
cross-validation
use separate data
performance
multiple complexity parameters
single model
in- stance
several regularization parameters
combinations
settings
such parameters
case
number
training runs
number
parameters
ap- proach
training data
multiple hyperparameters
model types
single training run
need
measure
performance
training data
bias
historically various ‘ information criteria
attempt
bias
maximum likelihood
addition
penalty term
over-ﬁtting
complex models
example
akaike information criterion
aic
akaike
model
quan- tity ln p
d|wml
m
p
d|wml
best-ﬁt log likelihood
m
number
adjustable parameters
model
variant
quantity
bayesian information criterion
bic
section
such criteria
account
uncertainty
model parameters
practice
simple models
section
bayesian approach
complexity penalties
principled way
curse
dimensionality
polynomial curve
example
input variable x
prac- tical applications
pattern recognition
spaces
introduction figure
scatter plot
oil ﬂow data
input variables x6
x7
‘ homoge- nous ’ class
‘ annular ’ class
‘ laminar ’ class
goal
new test point de-
‘ × ’
x7
x6
high dimensionality
many input variables
serious challenges
important factor
design
pattern recognition techniques
order
problem
data
measurements
pipeline
mixture
oil
wa- ter
gas
bishop
james
materials
different geometrical conﬁgurations
‘ homogenous ’
‘ annular ’
‘ laminar ’
fractions
materials
data point
12-dimensional input vector
measurements
gamma ray densitometers
attenuation
gamma rays
nar- row beams
pipe
data set
detail
appendix a
figure
points
data set
plot
mea- surements
x7
ten input values
purposes
illustration
data point
geomet- rical classes
goal
data
training
order
new observation
x7
cross
figure
cross
numerous red points
red class
plenty
green points
green class
blue class
intuition
identity
cross
nearby points
training set
distant points
fact
intuition
later chapters
intuition
algorithm
simple ap- proach
input space
regular cells
fig- ure
test point
class
cell
training data points
curse
dimensionality
figure
illustration
simple approach
solution
classiﬁcation problem
input space
cells
new test point
class
majority number
rep- resentatives
same cell
test point
simplistic approach
severe shortcomings
x7
x6
fall
same cell
identity
test point
class
number
points
same cell
test point
ties
random
numerous problems
naive approach
se- vere becomes
extension
problems
numbers
input variables
spaces
dimensionality
origin
problem
figure
region
space
regular cells
number
such cells
dimensionality
space
problem
large number
cells
large quantity
data
order
cells
hope
technique
space
few variables
sophisticated approach
further insight
problems
high-dimensional spaces
example
polynomial curve ﬁtting
figure
illustration
curse
dimensionality
number
regions
regular grid grows
dimensionality d
space
clarity
subset
cubical regions
d =
x2 x2 x1 d
x1 x3 d
d
x1  d  d  d  d  d  d
introduction
approach
input spaces
several variables
d input variables
general polynomial
coefﬁcients
order
form exercise
y
w0 + wixi + wijxixj + wijkxixjxk
i=1 i=1 j=1 i=1 j=1 k=1
d
number
independent coefﬁcients
coefﬁcients
symmetries
x variables
d3
practice
complex dependencies
data
higher-order polynomial
polynomial
order m
growth
number
coefﬁcients
dm
power law growth
exponential growth
method
limited practical utility
geometrical intuitions
life
space
di- mensions
spaces
dimensionality
simple example
sphere
radius r
space
d dimensions
fraction
volume
sphere
radius r
fraction
volume
sphere
radius r
d dimensions
rd
vd
r
= kdrd
exercise
constant kd
d. thus
required fraction
exercise
vd
− vd
−
vd
−
−
d
function
various values
d
figure
large d
fraction
small values
thus
spaces
high dimensionality
volume
sphere
thin shell
surface
further example
direct relevance
recognition
behaviour
gaussian distribution
high-dimensional space
coordinates
directional variables
expression
density p
r
function
radius r
origin
p
r
δr
probability mass
thin shell
thickness δr
radius r.
distribution
various values
d
figure
large d
probability mass
gaussian
thin shell
severe difﬁculty
spaces
many dimensions
curse
dimensionality
bellman
book
ex- tensive use
illustrative examples
input spaces
dimensions
techniques
reader
intuitions
spaces
low dimensionality
spaces
many dimensions
curse
dimensionality
figure
plot
fraction
volume
sphere lying
range r
various values
dimensionality d.
o i
r f e m u o v
d
d
d
d
curse
dimensionality
important issues
pat- tern recognition applications
effective techniques
high-dimensional spaces
reasons
first
real data
region
space
effective dimension- ality
directions
important variations
target variables
second
real data
smoothness properties
part small changes
input variables
small changes
target variables
ploit local interpolation-like techniques
predictions
target variables
new values
input variables
successful pattern recognition tech- niques
properties
example
application
manufacturing
images
identical planar objects
con- veyor belt
goal
orientation
image
point figure
plot
probability density
respect
r
gaus- sian distribution
various values
high-dimensional space
probability mass
thin shell
speciﬁc radius
dimensionality d.
r
d
d
d
r
introduction
high-dimensional space
dimensionality
number
pixels
objects
different positions
image
different orientations
degrees
freedom
variability
images
set
images
dimensional manifold
high-dimensional space
due
complex relationships
object position
orientation
pixel intensities
manifold
goal
model
input image
output
orientation
object irrespective
position
degree
freedom
variability
manifold
decision theory
section
probability theory
consistent mathematical framework
uncertainty
discussion
decision theory
probability theory
optimal decisions
situations
uncertainty
pattern recognition
suppose
input vector
corresponding vector t
target variables
goal
new value
x
regression problems
t
continuous variables
whereas
classiﬁcation problems
class labels
joint probability distribution p
complete summary
uncertainty
variables
determination
p
set
training data
example
inference
difﬁcult problem
solution
subject
book
practical application
speciﬁc prediction
value
t
speciﬁc action
understanding
values t
aspect
subject
decision theory
consider
example
medical diagnosis problem
x-ray image
patient
patient
cancer
case
input vector x
set
pixel intensities
image
output variable t
presence
cancer
class c1
absence
cancer
class c2
instance
t
binary variable
corresponds
class c1
corresponds
class c2
choice
label values
probabilistic models
general inference problem
joint distribution p
ck
t
complete probabilistic description
situation
informative quantity
end
treatment
patient
choice
appropriate sense
duda
hart
decision step
subject
decision theory
optimal decisions
appropriate probabilities
decision stage
inference problem
introduction
key ideas
decision theory
 
decision theory
rest
book
further background
detailed accounts
berger
bather
detailed analysis
probabilities
role
decisions
x-ray image x
new patient
goal
classes
image
probabilities
classes
image
p
ck|x
bayes ’ theorem
probabilities
form p
ck|x
p
x|ck
p
ck
p
x
note
quantities
bayes ’ theorem
joint distribution p
ck
marginalizing
conditioning
respect
appropriate variables
p
ck
prior probability
class ck
p
ck|x
corresponding posterior probability
p
c1
repre- sents
probability
person
cancer
x-ray measurement
p
c1|x
corresponding probability
bayes ’ theorem
light
information
x-ray
aim
chance
x
wrong class
class
posterior probability
intuition
general criteria
decisions
misclassiﬁcation rate suppose
goal
few misclassiﬁcations
rule
value
x
available classes
rule
input space
regions rk
decision regions
class
points
rk
class ck
boundaries
decision regions
decision boundaries
decision surfaces
note
decision region
number
disjoint regions
examples
decision boundaries
decision regions
later chapters
order
optimal decision rule
ﬁrst
case
classes
cancer problem
instance
mistake
input vector
class c1
class c2
vice versa
probability
occurring
p
mistake
= p
x ∈ r1
c2
p
x ∈ r2
c1
p
c2
dx + r1 p
c1
dx
r2
decision rule
point x
classes
p
mistake
x
class
value
integrand
c1
p
c2
value
x
x
class c1
product rule
probability
ck
p
ck|x
p
x
factor p
x
terms
result
minimum  x b x0   k=1 k k  b b b b b b b
introduction p
c1
r1 p
c2
r2 x x
values
x � figure
schematic illustration
joint probabilities
ck
classes
x
decision boundary x = x
class c2
decision region r2
whereas points
< x
c1
r1
errors
blue
red regions
errors
points
class c2
c1
x <
sum
green regions
points
region x �
errors
points
class c1
c2
blue region
location x
decision boundary
combined areas
blue
green regions
size
red region varies
optimal choice
x
curves
p
c1
p
c2
cross
= x0
case
red region
minimum misclassiﬁcation rate decision rule
value
x
class
posterior probability p
ck|x
probability
mistake
value
x
class
posterior probability p
ck|x
result
classes
single input variable x
figure
general case
k classes
probability
correct
p
p
x ∈ rk
ck
k=1 rk p
ck
dx
regions rk
x
class
p
ck
again
product rule p
ck
p
ck|x
p
x
factor
p
x
terms
x
class
posterior probability p
ck|x
 cancer      figure
example
loss matrix
ele- ments lkj
cancer treatment problem
rows correspond
true class
columns cor- respond
assignment
class
deci- sion criterion
cancer
decision theory
expected loss
many applications
objective
number
misclassiﬁcations
medical diagnosis problem
patient
cancer
cancer
consequences
patient distress
need
further investigations
patient
cancer
result
premature death
treatment
consequences
types
mistake
mistakes
second kind
expense
mistakes
ﬁrst kind
such issues
introduction
loss function
cost function
overall measure
loss
available decisions
actions
goal
total loss
note
authors
utility function
value
equivalent concepts
utility
negative
loss
text
loss function convention
new value
x
true class
ck
class cj
j
level
loss
lkj
k
j element
loss matrix
instance
cancer example
loss matrix
form
figure
particular loss matrix
loss
correct decision
loss
healthy patient
cancer
loss
patient
cancer
optimal solution
loss function
loss function
true class
input vector x
uncertainty
true class
joint probability distribution p
ck
average loss
average
respect
distribution
e [ l ] = k j rj lkjp
x
ck
dx
x
decision regions rj
goal
regions rj
order
expected loss
lkjp
x
ck
implies
x
product rule p
ck
p
ck|x
p
x
common factor
p
x
decision rule
expected loss
one
new x
class j
quantity inputs x
poste- rior probabilities
threshold θ
introduction figure
illustration
reject option
p
c1|x
p
c2|x
region
lkjp
ck|x
k
minimum
posterior class proba- bilities
ck|x
reject option
classiﬁcation errors
regions
input space
posterior probabilities
ck|x
unity
joint distributions
ck
comparable values
regions
class membership
applications
decisions
difﬁcult cases
anticipation
error rate
examples
classiﬁcation de- cision
reject option
example
hypothetical medical illustration
automatic system
x-ray images
little doubt
correct class
human expert
ambiguous cases
threshold θ
inputs
posterior probabilities
ck|x
case
classes
single continuous input variable x
figure
note
θ
examples
k classes
θ
examples
fraction
examples
value
θ
reject criterion
expected loss
loss matrix
account
loss
reject decision
inference
decision
classiﬁcation problem
separate stages
inference stage
data
model
p
ck|x
exercise
 p
ck|x
p
x|ck
p
ck
p
x
posterior class probabilities
ck|x
denominator
bayes ’ theorem
terms
quantities
numerator
p
x
p
x|ck
p
ck
k
joint distribution p
ck
posterior probabilities
posterior probabilities
decision theory
class membership
new input x
approaches
distribution
inputs
outputs
generative models
synthetic data points
input space
b
first
inference problem
posterior class probabilities
ck|x
decision theory
new x
classes
approaches
posterior probabilities
discriminative models
c
function f
x
discriminant function
input
class label
instance
case
two-class problems
f
·
class c1
class c2
case
probabilities
role
relative merits
alternatives
approach
joint distribution
x
ck
many applications
x
high dimensionality
large training
order
class-conditional densities
reasonable accuracy
note
class priors
ck
fractions
training
data points
classes
advantage
approach
marginal density
data p
x
new data points
low probability
model
predictions
decision theory
subsequent decision stage
posterior probabilities
op- timal class assignments
alternative possibility
problems
function
inputs
decisions
function
discriminant function
fact
distinct approaches
decision problems
practical applications
order
complexity
first
inference problem
class-conditional densities
x|ck
class ck
prior class probabilities
ck
bayes ’ theorem
form
introduction
e i
i
n e d s
c l
p
x|c2
p
x|c1
x
p
c1|x
p
c2|x
x
figure
example
class-conditional densities
classes
single input variable x
left plot
corresponding posterior probabilities
right plot
note
left-hand mode
class-conditional density p
x|c1
blue
left plot
effect
posterior probabilities
vertical green line
right plot
decision boundary
x
minimum misclassiﬁcation rate
low accuracy
outlier detection
novelty detection
bishop
tarassenko
classiﬁcation decisions
waste- ful
computational resources
data
joint distribution p
ck
fact
posterior probabilities
ck|x
approach
b
class- conditional densities
lot
structure
little effect
pos- terior probabilities
figure
much interest
relative merits
discriminative approaches
machine learning
ways
jebara
lasserre
al.
approach
c
training data
discriminant function f
x
x
class label
inference
decision stages
single learning problem
example
figure
value
x
vertical green line
decision boundary
minimum probability
misclassiﬁcation
option
c
access
posterior probabilities
ck|x
many powerful reasons
posterior probabilities
decisions
risk
problem
elements
loss matrix
time
time
decision theory
application
posterior probabilities
minimum risk decision criterion
discriminant function
change
loss matrix
training data
classiﬁcation problem afresh
reject option
posterior probabilities
rejection criterion
misclassiﬁcation rate
expected loss
fraction
data points
class priors
medical x-ray problem
large number
x-ray images
gen- eral population
use
training data
order
screening system
cancer
general population
examples corresponds
presence
can- cer
data set
adaptive model
severe difﬁculties
small proportion
cancer class
instance
classiﬁer
point
normal class
% accuracy
trivial solution
large data set
few examples
x-ray images
cancer
learning algorithm
broad range
examples
such images
hence
balanced data set
equal numbers
exam- ples
classes
accurate model
effects
modiﬁcations
training data
suppose
modiﬁed data set
models
posterior probabilities
bayes ’ theorem
posterior probabilities
prior probabilities
fractions
points
class
posterior probabilities
data set
ﬁrst divide
class fractions
data set
class fractions
population
model
new posterior probabilities
procedure
discriminant function
posterior probabilities
models
complex applications
problem
number
subproblems
sep- arate module
example
hypothetical medical diagnosis problem
information
blood tests
x-ray im- ages
heterogeneous information
huge input space
system
x- ray images
blood data
models
posterior probabilities
classes
outputs
rules
probability
simple way
class
distributions
inputs
x-ray images
xi
blood data
xb
    
p
xb|ck
p
xi|ck
p
xb|ck
example
conditional independence property
indepen- dence
distribution
class ck
posterior probability
x-ray
blood data
p
ck|xi
xb
p
xb|ck
p
ck
p
ck|xi
p
ck|xb
p
xi|ck
p
xb|ck
p
ck
p
ck
class prior probabilities
ck
fractions
data points
class
posterior probabilities
particular condi- tional independence assumption
example
naive bayes model
note
joint marginal distribution p
xb
model
later chapters
models
data
conditional independence assumption
loss functions
regression so
decision theory
context
classiﬁcation prob- lems
case
regression problems
curve
example
decision stage
speciﬁc esti- mate y
x
value
t
input x
suppose
loss l
t
y
x
loss
e [ l ] = l
t
y
x
p
dx dt
common choice
loss function
regression problems
squared loss
l
t
y
x
y
x
case
expected loss
e [ l ] =
y
x
x
t
dx dt
goal
y
x
e [ l ]
ﬂexible function y
x
calculus
variations
δe [ l ] δy
x
y
x
p
dt
y
x
sum
product rules
probability
tp
dt y
x
p
x
tp
t|x
dt = et [ t|x ]
introduction section
section
section
appendix d   figure
regression function y
x
expected squared loss
mean
conditional distri- bution p
t|x
t y
x0
decision theory
y
x
p
t|x0
x exercise
conditional average
t
x
regression function
result
figure
mul- tiple target variables
vector t
case
optimal solution
conditional average y
x
et [ t|x ]
result
different way
light
nature
regression problem
knowledge
optimal solution
conditional expectation
square term
y
x
y
x
e [ t|x ] + e [ t|x ] − t
y
x
e [ t|x ]
y
x
e [ t|x ]
e [ t|x ] − t
+
e [ t|x ] − t
notation
e [ t|x ]
et [ t|x ]
loss function
t
cross-term vanishes
expression
loss function
form e [ l ] =
y
x
e [ t|x ]
p
x
e [ t|x ] − t
x
dx
function y
enters
ﬁrst term
y
x
e [ t|x ]
case
term
result
squares predictor
conditional mean
second term
variance
distribution
t
x
intrinsic variability
target data
noise
y
x
irreducible minimum value
loss function
classiﬁcation problem
appropriate prob- abilities
optimal decisions
models
decisions
distinct approaches
regression problems
order
complexity
first
inference problem
joint density p
conditional density p
t|x
conditional mean
 section
exercise
e [ lq ] = |y
x
t|qp
dx dt
squared loss
function |y − t|q
t
various values
q
figure
minimum
e [ lq ]
conditional mean
q =
conditional median
q =
conditional mode
information theory
chapter
variety
concepts
probability theory
decision theory
foundations
subsequent discussion
book
chapter
additional concepts
ﬁeld
information theory
development
pattern recognition
machine learning techniques
again
key concepts
reader
detailed discussions
viterbi
omura
cover
thomas
mackay
discrete random variable x
much information
speciﬁc value
amount
information
‘ degree
surprise ’
value
x
improbable event
information
likely event
event
information
measure
information content
depend
probability distribution p
x
quantity h
x
monotonic function
probability p
x
information content
form
h
events
information gain
sum
information
h
h
x
h
y
unrelated events
= p
x
p
y
relationships
h
logarithm
p
x
exercise
introduction
b
first
inference problem
conditional density p
t|x
conditional mean
c
regression function y
x
training data
relative merits
approaches
same lines
classiﬁca- tion problems
squared loss
only possible choice
loss function
regression
situations
loss
poor results
sophisticated approaches
important example concerns situations
conditional distribution p
t|x
solution
inverse problems
simple generalization
squared loss
minkowski loss
expectation
q
− y
q |
− y | q
y −
q
−2
y −
information theory
q
y −
q
y −
q |
− y | q |
− y | figure
plots
quantity lq = |y − t|q
various values
q. h
x
− log2 p
x
negative sign
information
zero
low probability events
correspond
high information content
choice
basis
logarithm
moment
convention prevalent
information theory
logarithms
base
case
units
h
x
bits
‘ binary digits ’
sender
value
random
receiver
average amount
information
process
expectation
respect
distribution p
x
h [ x ] = − x p
x
p
x
important quantity
entropy
random variable x
limp→0 p ln p
p
x
p
x
whenever
value
x
p
x
heuristic motivation
deﬁnition
introduction tion
corresponding entropy
deﬁnitions
useful properties
random variable x
possible states
order
value
x
receiver
message
length
bits
notice
entropy
h [ x ] = −8
log2
=
bits
example
cover
thomas
pos- sible states
b
c
d
e
f
g
h
respective probabilities
entropy
case
−
−
−
−
log2 log2 log2 log2
bits
h [ x ] = −
nonuniform distribution
entropy
insight
interpretation
entropy
terms
disorder
moment
identity
variable ’ s state
receiver
3-bit number
advantage
nonuniform distribution
codes
probable events
expense
codes
probable events
hope
average code length
states
b
c
d
e
f
g
h
instance
following set
code strings
average length
code
+ average code length
bits
entropy
random
note
code strings
concatenation
such strings
component parts
instance
decodes
state sequence c
d.
+
relation
entropy
length
noiseless coding theorem
shannon
entropy
bound
number
bits
state
random
use
natural logarithms
en- tropy
convenient link
ideas
book
case
entropy
units
‘ nats
bits
factor
concept
entropy
terms
average amount
information
state
random
fact
concept
entropy
origins
physics
context
equilibrium thermodynamics
interpretation
measure
disorder
developments
statistical mechanics
alternative view
entropy
set
n identical objects
set
bins
ni objects
ith bin
w = n
    
n
n      =
n i
n  
information theory
number
different ways
objects
bins
n ways
ﬁrst object
n −
ways
second object
total
n
ways
n objects
bins
n
pronounced ‘ factorial n ’
product n ×
n −1
×···×2×1
’ t wish
rearrangements
objects
bin
ith bin
ways
objects
total number
ways
n
bins
multiplicity
entropy
logarithm
multiplicity
appropriate constant i ni
h =
w =
n
− ln ni
limit n → ∞
fractions
ﬁxed
’ s approximation ln n
 n
n − n
ni n i
h = −
n→∞ i
= n.
= limn→∞
probability
ith bin
physics terminology
speciﬁc ar- rangements
objects
bins
microstate
overall distribution
occupation numbers
ratios ni/n
macrostate
multiplicity w
weight
macrostate
pi ln pi
bins
states xi
discrete random variable x
p
x = xi
pi
entropy
random variable x
h [ p ] = − i p
xi
ln p
xi
distributions
xi
few values
low entropy
many values
entropy
figure
� pi
entropy
minimum value
pi
other pj=i
maximum entropy conﬁguration
h
lagrange multiplier
normalization constraint
probabilities
h = − i p
xi
ln p
xi
λ p
xi
i
appendix e h =
h
introduction
s e i
i l i
b o r
 s e i
i l i
b o r
    figure
histograms
probability distributions
bins
value
entropy h
distribution
entropy
uniform distribution
h = − ln
=
exercise
p
xi
p
xi
m
total number
states xi
corresponding value
entropy
h = ln m.
result
jensen ’ s inequality
stationary point
second derivative
entropy
h ∂ ∂p
xi
∂p
xj
pi
iij
elements
identity matrix
deﬁnition
entropy
distributions
x
con- tinuous variables
first divide x
bins
width ∆
p
x
mean value theorem
weisstein
such bin
value xi
i+1
i∆ p
x
= p
xi
∆
continuous variable x
value x
value xi
x
ith bin
probability
value xi
xi
∆
discrete distribution
entropy
form h∆ = − i p
xi
ln
p
xi
∆
− p
xi
ln p
xi
ln ∆
i
p
xi
second term −
∆
right-hand side
limit   
information theory
∆
ﬁrst term
right-hand side
p
x
p
x
limit
lim ∆→0 i p
xi
ln p
xi
− p
x
p
x
dx
quantity
right-hand side
differential entropy
discrete
continuous forms
entropy differ
quantity ln ∆
limit ∆
fact
large number
bits
density
multiple continuous variables
vector x
differential entropy
h [ x ] = − p
x
p
x
dx
case
discrete distributions
maximum entropy con- ﬁguration
equal distribution
probabilities
possible states
maximum entropy conﬁguration
continuous variable
order
maximum
ﬁrst
second moments
p
normalization constraint
differential entropy
ludwig boltzmann
ludwig eduard boltzmann
austrian physicist
ﬁeld
statistical mechanics
prior
boltzmann
concept
en- tropy
classical thermodynamics
fact
energy
system
energy
useful work
boltzmann
ther- modynamic entropy s
macroscopic quantity
statistical properties
micro- scopic level
famous equation s = k ln w
w
number
possible microstates
macrostate

×
units
joules
kelvin
boltzmann ’ s constant
boltzmann ’
ideas
many scientists
day
dif- ﬁculty
second law
thermo- dynamics
entropy
closed system
time
contrast
microscopic level
classical newtonian equa- tions
physics
latter
for- mer
’ t
appreciate boltzmann ’
argu- ments
nature
con-
entropy
time
overwhelming probability
boltzmann
dispute
editor
leading german physics journal
atoms
molecules
anything
convenient the- oretical constructs
continued attacks
work lead
bouts
depression
suicide
boltzmann ’
death
new experiments
perrin
colloidal suspensions veri- ﬁed
theories
value
boltz- mann constant
equation s = k ln w
boltzmann ’
tombstone
 ∞    ∞   
  ∞   
introduction
constraints appendix e appendix d exercise
exercise
p
x
xp
x
= µ −∞ ∞ −∞
x − µ
x
dx = σ2
−∞
constrained maximization
lagrange multipliers
respect
x
p
x
p
x
+ λ1 − +λ2 −∞ ∞ −∞ ∞ −∞ p
x
∞ −∞ xp
x
− µ + λ3
x − µ
x
dx − σ2
calculus
variations
derivative
p
x
exp −1 + λ1 + λ2x + λ3
x − µ
lagrange multipliers
back substitution
result
constraint equations
result p
x
=
exp −
x − µ
distribution
differential entropy
note
distribution
entropy
resulting distribution
hindsight
constraint
differential entropy
gaussian
h [ x ]
+ ln
entropy
distribution
σ2 increases
result
differential entropy
discrete entropy
h
x
σ2 <
suppose
joint distribution p
pairs
values
x
y
value
x
additional information
corresponding value
y
− ln p
y|x
average additional information
y
h [ y|x ] = − p
x
p
y|x
dy dx
     exercise
conditional entropy
y
x
product rule
conditional entropy
relation
information theory
h [ x
y ] = h [ y|x ] + h [ x ]
h [ x
y ]
differential entropy
p
h [ x ]
differential en- tropy
marginal distribution p
x
information
x
y
sum
information
x
additional information
x
relative entropy
mutual information so
section
number
concepts
information theory
key notion
entropy
ideas
recognition
unknown distribution p
x
distribution q
x
coding scheme
purpose
values
x
receiver
average additional amount
information
nats
value
x
efﬁcient
scheme
result
q
x
true distribution p
x
kl
pq
− = − p
x
q
x
− p
x
q
x
p
x
dx
p
x
p
x
dx −
relative entropy
kullback-leibler divergence
kl diver- gence
kullback
leibler
distributions
x
x
note
symmetrical quantity
kl
pq
≡ kl
qp
kullback-leibler divergence satisﬁes kl
pq
equality
x
q
x
concept
convex functions
function f
x
property
chord
function
figure
value
x
interval
x
= b
form λa +
− λ
b
� λ �
corresponding point
chord
λf
+
− λ
f
b
claude shannon
michigan
mit
shannon
at
t bell telephone laboratories
paper
mathematical theory
communication ’
bell system technical journal
foundations
modern information the- ory
paper
word ‘ bit ’
con- cept
information
stream
way
communications revo- lution
von neumann
shannon
term entropy
be- cause
similarity
quantity
physics
“ nobody
entropy
discussion
advan- tage ”
 i=1 m 
    i=1 m  
introduction figure
a convex function f
x
ev- ery chord
blue
function
f
x
xλ b x chord
corresponding value
function
λa +
− λ
b
convexity
f
λa +
− λ
b
� λf
+
− λ
f
b
requirement
second derivative
function
examples
convex functions
x ln x
x >
function
equality
λ =
λ =
function
opposite property
chord
function
concave
corresponding deﬁnition
function f
x
−f
x
convex function f
x
technique
proof
induction
exercise
exercise
λixi � λif
xi
λi �
i λi =
set
points
xi
result
jensen ’ s inequality
λi
probability distribution
discrete variable x
values
xi
e [ x ]
e [ f
x
]
e [ · ]
expectation
continuous variables
jensen ’ s inequality
form
f
x
p
x
dx
xp
x
jensen ’ s inequality
form
kullback-leibler divergence
kl
pq
− p
x
q
x
p
x
� − ln q
x
 
n 
information theory
fact
−
x
convex function
nor- q
x
fact
− ln x
convex function
malization condition
equality
x
p
x
x
kullback-leibler divergence
measure
dissimilarity
distributions
x
x
intimate relationship
data compression
den- sity estimation
problem
unknown probability distribution
efﬁcient compression
true distri- bution
distribution
efﬁcient coding
additional information
kullback-leibler divergence be-
distributions
data
unknown distribution p
x
distribution
parametric distribution q
x|θ
set
adjustable parameters
example
multivariate gaussian
way
θ
kullback-leibler divergence
p
x
x|θ
respect
’ t
p
x
suppose
ﬁnite set
points
n =
n
drawn
p
x
expectation
respect
ﬁnite sum
points
kl
pq
− ln q
xn|θ
ln p
xn
second term
right-hand side
θ
ﬁrst term
negative log likelihood function
θ
distribution q
eval-
training set
kullback-leibler divergence
likelihood function
joint distribution
sets
variables x
p
sets
variables
joint distribution
product
marginals
p
x
p
y
variables
idea
’
indepen- dent
kullback-leibler divergence
joint distribution
product
marginals
≡ kl
p
x
p
− p
p
x
p
y
p
dx dy
exercise
mutual information
variables x
properties
kullback-leibler divergence
equal- ity
y
sum
product rules
probability
mutual information
conditional entropy
= h [ x ] − h [ x|y ] = h [ y ] − h [ y|x ]
m  n  n   
introduction
mutual information
reduction
uncertainty
x
virtue
value
y
vice versa
bayesian perspective
p
prior distribution
x
p
posterior distribu- tion
new data y
mutual information therefore
reduction
uncertainty
x
consequence
new observation y

sum-of-squares error function
function y
polynomial
show
coefﬁcients
error function
solution
following set
linear equations aijwj = ti j=0
aij =
xn
i+j
n=1 ti =
xn
itn
n=1
sufﬁx i
j
index
component
whereas
x
i denotes
power
i

write
set
coupled linear equations
coefﬁcients
regularized sum-of-squares error function
 
boxes r
b
blue
g
box r
apples
oranges
limes
box b
apple
orange
limes
box g
apples
oranges
limes
box
random
probabilities
r
p
b
p
g
piece
fruit
box
equal probability
items
box
probability
apple
fruit
fact
orange
probability
green box
 
www
probability density px
x
continuous vari- able x
nonlinear change
x = g
y
density
location y
maximum
density
y
location x
maximum
density
x
simple functional relation x = g
y
consequence
jacobian factor
maximum
probability density
contrast
simple function
choice
case
linear transformation
location
maximum transforms
same way

deﬁnition
show
var [ f
x
satisﬁes
       

variables
y
covariance
 
www
exercise
normalization condition
univariate gaussian
consider
exp ∞ −∞
x2 − dx
square
form
= ∞ −∞ ∞
−∞
x2 −
y2 dx dy
transformation
cartesian coordinates
x
coordinates
r
θ
u = r2
show
integrals
θ
u
square root
sides
use
result
gaussian distribution n
x|µ
σ2
 
www
change
variables
univariate gaussian distribution
satisﬁes
sides
normalization condition ∞ −∞ n x|µ
σ2
respect
gaussian satisﬁes

www show
mode
i.e
gaussian distribution
µ
mode
multivariate gaussian
µ

www suppose
variables
z
mean
variance
sum satisﬁes e [ x + z ] = e [ x ] + e [ z ] var [ x + z ] = var [ x ] + var [ z ]

derivatives
log likelihood function
respect
results
introduction  d  d      im−1 j=1 i=1 d d d       i2=1 j=1 i=1 d d d d d
 
www
results
e [ xnxm ] = µ2 + inmσ2
xn
xm denote data points
gaussian distribution
mean µ
variance σ2
inm
inm
n = m
inm =
hence
results

variance
gaussian
result
maximum likelihood estimate µml
true value µ
mean
estimator
property
expectation
true variance σ2
 
show
arbitrary square matrix
elements
form wij = ws ij
anti-symmetric matrices
ws ji
i
j
second order term
order polynomial
d dimensions
ij = −wa ij
ws ij
wa ji
wa ij = ws ij + wa show
wijxixj
wijxixj = ws ijxixj i=1 j=1
contribution
anti-symmetric matrix vanishes
loss
generality
matrix
coefﬁcients
d2 elements
matrix
indepen- ij
number
independent parameters
matrix ws
d
d +
/2
  
www
exercise
number
indepen- dent parameters
polynomial grows
order m
dimensionality d
input space
m th order term
polynomial
d dimensions
form ··· wi1i2···im xi1xi2 ··· xim
i1=1 im =1
coefﬁcients
comprise dm elements
number
independent parameters
many interchange symmetries
factor xi1xi2 ··· xim
redundancy
coefﬁcients
m th order term
form ··· im =1 wi1i2···im xi1xi2 ··· xim
i1=1 i2=1
 d i=1    i=1 d m
w coefﬁcients
w coefﬁcients
precise relationship
explicit
result
number
independent param- eters
d
m
order m
following recursion relation
next use proof
induction
following result
d
m
m −
m −
m −
=
d + m −
d −
m
ﬁrst
result
d =
arbitrary m
use
result
=
dimension d
dimension d
previous results
proof
induction
d
m
=
d + m −
d −
m
show
result
m =
value
d �
comparison
result
exercise
use
result
order m
order m
  
exercise
result
number
independent parameters
m th order term
d-dimensional polynomial
expression
total number n
d
m
independent parameters
terms
m6th order
first
n
d
m
satisﬁes n
d
m
d
m
m=0
n
d
m
number
independent parameters
term
order m.
use
result
proof
induction
n
d
m
=
d + m
d
m
ﬁrst
result
m =
arbitrary d �
order m
order m
use
’ s approximation
form n
 nne−n
large n
d m
quantity n
d
m
dm
m d
m d. consider
cubic
m =
polynomial
d dimensions
total number
independent parameters
i
d
ii
d
medium-scale machine learning applications
   d
introduction
 
gamma function
γ
x
ux−1e−u du
integration
parts
relation γ
x +
= xγ
x
show
γ
=
γ
x +
= x
x
integer
 
result
expression
surface area sd
volume vd
sphere
unit radius
d dimensions
following result
coordinates
e−x2 i
= sd ∞
e−r2 rd−1 dr.
i=1 −∞
deﬁnition
gamma function
sides
equation
show
sd
γ
d/2
next
respect
volume
unit
d dimensions
results γ
=
γ
= √π/2
usual expressions
d =
d =
vd = sd d
 
sphere
radius
d-dimensions
concentric hypercube
side
hypercube
centres
sides
results
exercise
ratio
volume
sphere
volume
cube
volume
sphere volume
cube = πd/2 d2d−1γ
d/2
use
’ s formula
form γ
x +

d → ∞
ratio
show
ratio
distance
centre
hypercube
corners
perpendicular distance
sides
√d
d → ∞
results
space
high dimensionality
volume
cube
large number
corners
‘ spikes
               
 
www
exercise
behaviour
gaussian distribution
high-dimensional spaces
gaussian distribution
d dimensions
p
x
=
d/2
density
respect
polar coordinates
direction variables
integral
probability density
thin shell
radius r
thickness
p
r
p
r
= sdrd−1
d/2
−
sd
surface area
unit
d dimensions
function p
r
single stationary point
large d
r +
p
r  √dσ
r
large d
p
+
p
r
− r
maximum
radial probability density
p
r
r
length scale σ
decays
maximum
σ r
large d
probability mass
thin shell
large radius
probability r
factor
exp
d/2
density p
x
origin
radius
probability mass
high-dimensional gaussian distribution
different radius
region
high probability density
property
distributions
spaces
high dimensionality
important consequences
bayesian inference
model parameters
later chapters
 
consider
nonnegative numbers
b
show
� b
�
ab
result
decision regions
two-class classiﬁcation problem
probability
misclassiﬁcation
probability
p
mistake
p
c1
p
c2
dx

loss matrix
elements lkj
risk
x
class
loss matrix
lkj =
− ikj
ikj
elements
identity matrix
criterion
class
posterior probability
interpretation
form
loss matrix

criterion
loss
general loss matrix
general prior probabilities
classes
introduction
 
www
classiﬁcation problem
loss
input vector
class ck
class cj
loss matrix lkj
loss
reject option
decision criterion
minimum expected loss
reject criterion
section
loss matrix
lkj =
− ikj
relationship
λ
rejection
θ

generalization
squared loss function
single target variable t
case
multiple target variables
vector t
e [ l
t
y
x
= y
x
t2p
dx dt
calculus
variations
function y
x
loss
y
x
et [ t|x ]
result
case
single target variable t.

expansion
square
result
show
function y
x
expected squared loss
case
vector t
target variables
conditional expectation
t.
 
expected loss
regression problems
lq loss function
write
condition
y
order
e [ lq ]
show
q =
solution
conditional median
i.e.
function y
probability mass
t < y
x
t � y
x
minimum
lq loss
conditional mode
function y
x
value
t
p
t|x
x

section
idea
entropy h
x
information
value
random variable x
distribution p
x
independent variables
y
p
p
x
p
y
entropy functions
h
h
x
h
y
exercise
relation
h
p
form
function h
p
first
h
p2
p
induction
h
pn
= nh
p
n
positive integer
hence show
h
pn/m
=
h
p
m
positive integer
h
px
= xh
p
x
positive rational number
continuity
positive real number
h
p
form h
p
ln p.

www consider
m-state discrete random variable x
jensen ’
in- equality
form
entropy
distribution p
x
h [ x ] �
m.
 
kullback-leibler divergence
gaussians
x
n
x|µ
σ2
x
n
x|m
s2
joint distribution p
binary variables
y
exercise
exercises
x
 
consider
variables
joint distribution p
show
differential entropy
pair
variables
equality
y
h [ x
y ] � h [ x ] + h [ y ]

vector x
continuous variables
distribution p
x
corre- sponding entropy h [ x ]
suppose
nonsingular linear transformation
x
new variable y = ax
show
corresponding entropy
h [ y ] = h [ x ] + ln|a|
|a|
determinant
a
 
conditional entropy h [
discrete random variables x
y
values
x
p
x
variable y
function
x
other words
x
value
y
p
y|x
 
www use
calculus
variations
stationary point
constraints
lagrange multipliers
hence show
maximum entropy solution
gaussian

www use
results
entropy
univariate gaussian

convex function
chord
function
condition
second derivative
function

deﬁnition
product rule
probability
result
 
proof
induction
inequality
convex functions
result
  
binary variables
joint distribution
following quantities
h [ x ]
b
h [ y ]
c
h [ y|x ]
d
h [ x|y ]
e
h [ x
y ]
y ]
diagram
relationship
various quantities
introduction

jensen ’ s inequality
f
x
ln x
arith- metic mean
set
real numbers
geometrical mean

www
sum
product rules
probability
mutual information
relation
probability distributions
chapter
central role
probability theory
solution
pattern recognition problems
exploration
particular examples
probability distributions
properties
be- ing
great interest
own right
distributions
building blocks
complex models
book
distributions
chapter
important purpose
opportunity
key statistical concepts
bayesian inference
context
simple models
complex situations
later chapters
role
distributions
chapter
prob- ability distribution p
x
random variable x
ﬁnite set x1
xn
observations
problem
density estimation
purposes
chapter
data points
problem
density estimation
probability distributions
many probability distributions
rise
ﬁnite data set
distribution p
x
data points x1
potential candidate
issue
appropriate distribution
problem
model selec- tion
context
polynomial curve
chapter
central issue
pattern recognition
binomial
multinomial distributions
discrete random variables
gaussian distribution
continuous random variables
speciﬁc examples
parametric distributions
small number
adaptive parameters
mean
variance
case
example
such models
problem
density estimation
procedure
suitable values
parameters
observed data set
frequentist treatment
speciﬁc values
parameters
criterion
likelihood function
contrast
bayesian treatment
prior distributions
parameters
bayes ’ theorem
corresponding posterior distribution
observed data
important role
conjugate priors
distributions
same functional form
lead
simpliﬁed bayesian analysis
example
conjugate
parameters
multinomial distribution
dirichlet distribution
conjugate
mean
gaussian
distributions
examples
exponential family
distributions
number
important properties
detail
limitation
parametric approach
speciﬁc functional form
distribution
particular application
alternative approach
nonparametric density estimation methods
form
distribution
size
data set
such models
parameters
model complexity
form
distribution
chapter
nonparametric methods
histograms
nearest-neighbours
kernels
binary variables
single binary random variable x ∈
example
x
outcome
coin
x =
‘ heads
x =
‘ tails
coin
probability
heads
tails
probability
x =
parameter µ
p
x =
= µ
 n  n   n n  
n
binary variables
� µ �
p
x =
− µ
probability distribution
x
form bern
x|µ
µx
− µ
bernoulli distribution
distribution
e [ x ] = µ var [ x ] = µ
− µ
data set d =
x1
observed values
x
likelihood function
function
µ
assumption
observations
p
x|µ
p
d|µ
p
xn|µ
n=1 n=1 µxn
− µ
frequentist setting
value
µ
likelihood function
logarithm
likelihood
case
bernoulli distribution
log likelihood function
ln p
d|µ
ln p
xn|µ
xn ln µ +
− xn
ln
− µ
point
log likelihood function
n observations
sum
xn
sum
example
sufﬁcient statistic
data
distribution
impor- tant role
sufﬁcient statistics
detail
derivative
ln p
d|µ
respect
maximum likelihood estimator µml
n xn n=1
exercise
section
jacob bernoulli
jacob bernoulli
jacques
james bernoulli
swiss mathematician
ﬁrst
bernoulli family
career
science
mathematics
philosophy
theology
parents
order
scientists
time
boyle
hooke
england
switzerland
mechanics
professor
mathematics
basel
rivalry
jacob
brother johann
productive collabora- tion
public dispute
jacob ’
sig- niﬁcant contributions
mathematics
artofconjecture
years
death
topics
probability the- ory
bernoulli distribution

m
sample mean
number
observations
x =
heads
data
m
form µml = m n
probability
heads
maximum likelihood frame- work
fraction
observations
heads
data set
coin
times
heads
n =
µml =
case
maximum likelihood result
future observations
heads
common sense
fact
extreme example
maximum likelihood
sensible conclusions
introduction
prior distribution
µ
distribution
number m
observations
x =
data set
size n.
binomial distribution
− µ
n−m
order
normalization coefﬁcient
n coin ﬂips
possible ways
m heads
binomial distribution
bin
m|n
µ
n m µm
− µ
n−m n
n m ≡
n − m
m
number
ways
m objects
total
n identical objects
figure
plot
binomial distribution
n =
µ =
mean
variance
binomial distribution
result
exercise
independent events
mean
sum
sum
means
variance
sum
sum
variances
m = x1 +
+ xn
observation
mean
variance
probability distributions figure
histogram plot
binomial dis- tribution
function
m
n =
µ =
exercise
 n  n 
binary variables
e [ m ] ≡ mbin
m|n
µ
n µ m=0
var [ m ] ≡ m=0
m − e [ m ]
bin
m|n
µ
n µ
− µ
results
calculus
beta distribution
maximum likelihood
parameter µ
bernoulli distribution
binomial distribution
fraction
observations
data set
x
over-ﬁtted results
small data sets
order
bayesian treatment
problem
prior distribution p
µ
parameter µ
form
prior distribution
simple interpretation
useful analytical properties
likelihood function
form
product
factors
form µx
− µ
powers
µ
− µ
posterior distribution
product
likelihood function
same functional form
property
conjugacy
several examples
chapter
prior
beta distribution
beta
µ|a
b
= γ
+ b
γ
γ
b
µa−1
− µ
b−1
γ
x
gamma function
coefﬁcient
beta distribution
exercise
exercise
exercise
mean
variance
beta distribution
beta
µ|a
b
dµ
e [ µ ] = var [ µ ]
+ b
+ b
+ b +
ab
parameters
b
hyperparameters
distribution
parameter µ
figure
plots
beta distribution
various values
hyperparameters
posterior distribution
µ
beta
binomial likelihood function
factors
µ
posterior distribution
form p
l
b
∝ µm+a−1
− µ
l+b−1
probability distributions
=
b =
=
b =
µ
=
b =
µ
=
b =
µ
figure
plots
beta distribution beta
µ|a
b
function
µ
various values
hyperparameters
b.
l = n − m
corresponds
number
‘ tails
coin example
same functional dependence
µ
prior distribution
conjugacy properties
respect
like- lihood function
beta distribution
normalization coefﬁcient
comparison
p
l
b
= γ
+ l + b
γ
γ
l + b
µm+a−1
− µ
l+b−1
effect
data set
m observations
x =
l observations
x =
value
m
value
b
l
prior distribution
posterior distribution
simple interpretation
hyperparameters
b
effective number
observations
x =
x =
note
integers
posterior distribution
additional data
observations
time
observation
current posterior   likelihood function
µ
µ
binary variables
figure
illustration
step
sequential bayesian inference
beta distribution
parameters
b =
likelihood function
n =
corresponds
single observation
x =
posterior
beta distribution
parameters
b =
section
distribution
likelihood function
new observation
posterior distribution
stage
posterior
beta distribution
total number
values
x =
x =
parameters
b. incorporation
additional observation
x =
corresponds
value
whereas
observation
x =
figure
step
process
sequential approach
arises
bayesian viewpoint
choice
likelihood function
assumption
i.i.d
data
sequential methods
use
observations
time
small batches
next observations
example
real-time learning scenarios
steady stream
data
predictions
data
whole data set
memory
sequential methods
large data sets
maximum likelihood methods
sequential framework
goal
outcome
next trial
predictive distribution
x
observed data
d.
sum
product rules
probability
form p
x =
p
x =
p
µ|d
µp
µ|d
dµ = e [ µ|d ]
result
posterior distribution p
µ|d
result
mean
beta distribution
p
x =
+ l + b
simple interpretation
total fraction
observations
real ob- servations
ﬁctitious prior observations
correspond
note
limit
inﬁnitely large data
m
l →
result
reduces
maximum likelihood result
general property
bayesian
maximum likelihood results
limit
inﬁnitely  
probability distributions exercise
exercise
large data set
ﬁnite data set
posterior mean
µ
prior mean
maximum likelihood estimate
µ corresponding
relative frequencies
events
figure
number
observations increases
posterior distribution
result
variance
beta distribution
variance
→ ∞
b → ∞
fact
general property
bayesian learning
data
uncertainty
posterior distribution
frequentist view
bayesian learning
show
average
property
general bayesian inference problem
parameter θ
data set d
joint distribution p
d
following result eθ [ θ ] = ed [ eθ [ θ|d ] ] p
θ
θ dθ
eθ [ θ ] ≡ ed [ eθ [ θ|d ] ] ≡ θp
θ|d
dθ p
d
dd
posterior mean
θ
distribution
data
prior mean
θ
varθ [ θ ] = ed [ varθ [ θ|d ] ] + vard [ eθ [ θ|d ] ]
term
left-hand side
prior variance
θ
right- hand side
ﬁrst term
average posterior variance
θ
second term
variance
posterior mean
θ
variance
positive quantity
result
average
posterior variance
θ
prior variance
reduction
variance
variance
posterior mean
note
result
average
particular observed data
posterior variance
prior variance
multinomial variables binary variables
quantities
possible values
often
discrete variables
k
exclusive states
various alternative ways
such variables
convenient represen- tation
1-of-k scheme
variable
k-dimensional vector x
elements
equals
elements equal       
k n x x k k=1       p xnk k=1 k=1 k=1
k µ k k k n k  k 
multinomial variables
instance
k =
states
particular observation
variable happens
state
x3
x
note
such vectors
parameter µk
distribution
x
x =
t.
k=1 xk
probability
xk =
k
µ =
µk
t
parameters
µk �
k µk =
probabilities
distribution
generalization
bernoulli distribution
outcomes
distribution
p
x|µ
µxk k p
x|µ
e [ x|µ ] = p
x|µ
=
µ1
µm
t = µ
data set d
n independent observations
xn
corresponding likelihood function
form p
d|µ
µxnk k =
xnk
µmk k
k=1
likelihood function
n data points
k quantities
=
section
appendix e
number
observations
xk =
sufﬁcient statistics
distribution
order
maximum likelihood solution
µ
ln p
d|µ
respect
account
constraint
µk
lagrange multiplier λ
mk ln µk + λ k=1 k=1 µk
derivative
respect
µk = −mk/λ
     k  k   k k=1    = k k
lagrange multiplier λ
constraint k µk
λ = −n
thus
maximum likelihood solution
form k = mk µml n
fraction
n observations
xk
joint distribution
quantities m1
mk
parameters
total number n
observations
from
form mult
m1
m2
mk|µ
n
n m1m2
mk µmk k k=1
multinomial distribution
normalization coefﬁcient
number
ways
n
k groups
size m1
mk
n m1m2
mk n
m1
m2
mk
note
variables mk
constraint
dirichlet distribution
family
prior distributions
parameters
multinomial distribution
inspection
form
multinomial distribution
conjugate prior
mk = n. p
µ|α
µαk−1 k
exercise
� µk �
µk =
parameters
distribution
denotes
α1
αk
t. note
summation constraint
distribution
space
µk
simplex
dimensionality k −
k =
figure
normalized form
distribution
dir
µ|α
γ
α0
γ
α1
γ
αk
k k=1
dirichlet distribution
γ
x
gamma function
α0 = αk
k=1
p  k  k µ3
multinomial variables figure
dirichlet distribution
variables
µ2
µ3
simplex
linear manifold
form
consequence
constraints
� µk �
µk =
µ1 plots
dirichlet distribution
simplex
various settings
param- eters
figure
likelihood function
posterior distribution
parameters
µk
form p
α
p
d|µ
p
µ|α
k=1 µαk+mk−1 k
posterior distribution
form
dirichlet distribution
dirichlet
conjugate prior
multinomial
normalization coefﬁcient
comparison
p
α
dir
µ|α + m
= γ
α0 + n
γ
α1 + m1
··· γ
αk + mk
k=1 µαk+mk−1 k
m =
m1
mk
t.
case
binomial distribution
beta
parameters αk
dirichlet
effective number
observations
xk =
note
two-state quantities
binary variables
lejeune dirichlet
lejeune johann peter gustav dirichlet
re- served mathematician
contributions
number theory
me- chanics
astronomy
ﬁrst rigorous analysis
fourier series
family
richelet
belgium
name lejeune dirichlet
‘ le jeune
richelet ’
young person
richelet
dirichlet ’
ﬁrst paper
instant fame
fer- mat ’ s last theorem
positive integer solutions
+ yn = zn
dirichlet
partial proof
case n =
legendre
review
turn com-
proof
later
dirichlet
complete proof
n =
full proof
fermat ’
last theo- rem
arbitrary n
work
andrew wiles
closing years
20th century
� � �
probability distributions figure
plots
dirichlet distribution
variables
horizontal axes
coordinates
plane
simplex
vertical axis corresponds
value
density
left plot
αk
centre plot
right plot
binomial distribution
1-of-2 variables
multinomial distribution
k =
gaussian distribution
gaussian
normal distribution
model
distribution
continuous variables
case
single variable x
gaussian distribution
form
n
x|µ
σ2
=
x − µ
−
µ
mean
σ2
variance
d-dimensional vector x
multivariate gaussian distribution
form section
exercise
n
x|µ
σ
d/2 |σ|1/2
−
x − µ
tς−1
x − µ
µ
d-dimensional mean vector
σ
d × d covariance matrix
|σ|
determinant
σ
gaussian distribution arises
many different contexts
variety
different perspectives
example
single real variable
distribution
entropy
property
multivariate gaussian
situation
gaussian distribution arises
sum
multiple random variables
central limit theorem
laplace
certain mild conditions
sum
set
random variables
course
random variable
distribution
number
terms
sum increases
walker
gaussian distribution
n
n
n
figure
histogram plots
mean
n
numbers
various values
n.
n
distribution
gaussian
n variables x1
uniform distribution
interval [
]
distribution
mean
x1 + ··· + xn
/n
large n
distribution
figure
practice
convergence
n increases
consequence
result
binomial distribution
distribution
m
sum
n observations
random binary variable x
n → ∞
figure
case
n =
gaussian distribution
many important analytical properties
detail
result
section
sections
familiarity
various matrix identities
reader
pro- ﬁcient
gaussian distributions
techniques
complex models
later chapters
geometrical form
gaussian distribution
appendix c carl friedrich gauss
gauss
elementary school
age
teacher b¨uttner
class
pupils
integers
teacher ’ s amazement
gauss
answer
matter
moments
sum
pairs
an- swer
problem
same form
sequence
value
increment
gauss
german math- ematician
scientist
reputation
hard-working perfectionist
many contribu- tions
squares
assumption
errors
early formulation
non-euclidean geometry
self-consistent geometrical theory
axioms
euclid
dis- cuss
fear
reputation
geometry
point
gauss
geodetic survey
state
hanover
for- mulation
normal distribution
gaussian
death
study
di- aries
several impor- tant mathematical results years
decades
others
  i=1 i=1 d d σui = λiui i
= iij ut iij =
j
= λiuiut i
λi uiut i
d ∆2 = i=1 y2 i
probability distributions exercise
exercise
exercise
functional dependence
gaussian
x
quadratic form ∆2 =
x − µ
tς−1
x − µ
exponent
quantity ∆
mahalanobis distance
µ
reduces
euclidean distance
σ
identity matrix
gaussian distribution
surfaces
x-space
quadratic form
first
matrix σ
loss
generality
antisymmetric component
exponent
eigenvector equation
covariance matrix
i
d.
σ
symmetric matrix
eigenvalues
eigenvectors
orthonormal set
iij
i
j element
identity matrix
covariance matrix σ
expansion
terms
eigenvec- tors
form
inverse covariance matrix σ−1
substituting
quadratic form becomes
yi
new coordinate system
orthonormal vectors
respect
original xi coordinates
vector y =
y1
yd
t
x − µ
= ut y = u
x − µ
      appendix c x2 figure
red curve
ellip- tical surface
constant proba- bility density
two-dimensional space x =
x1
x2
density
value
−1/2
x = µ
major axes
ellipse
eigenvectors
covari- ance matrix
eigenvalues λi
gaussian distribution y2 u2 µ y1
x1
u
matrix
rows
ut i
u
orthogonal matrix
uut =
utu =
identity matrix
quadratic form
gaussian density
surfaces
eigenvalues
surfaces represent ellipsoids
centres
µ
axes
factors
directions
axes
λ1/2
figure
i
gaussian distribution
eigenvalues λi
covariance matrix
dis- tribution
matrix
eigenvalues
positive deﬁnite
chapter
gaussian distributions
eigenvalues
case
distribution
subspace
dimensionality
eigenvalues
covariance matrix
positive semideﬁnite
form
gaussian distribution
new coordinate system
yi
x
y coordinate system
jacobian matrix j
elements
jij = ∂xi ∂yj = uji
uji
elements
matrix ut
orthonormality property
matrix u
square
determinant
jacobian matrix
|j|
determinant |σ|
covariance matrix
= |i|
|j|2 = |u| = utu ut ut
=   − − j=1 d     j=1 d  d 
probability distributions
product
eigenvalues
|σ|1/2 = λ1/2 j
yj coordinate system
gaussian distribution
form p
y
= p
x
exp
y2 j
−
product
d independent univariate gaussian distributions
eigen- vectors
new set
rotated coordinates
respect
joint probability distribution
product
independent distributions
distribution
y coordinate system
dy = j=1 ∞
exp
y2 j
− dyj
result
normalization
univariate gaussian
multivariate gaussian
moments
gaussian distribution
interpretation
parameters
σ
expectation
x
gaussian distribution
e [ x ] =
d/2
|σ|1/2 |σ|1/2 exp exp
x − µ
tς−1
x − µ
ztς−1z
z + µ
dz x dx
variables
z = x − µ
exponent
even function
components
z
integrals
range
∞
term
z
factor
z + µ
symmetry
thus
e [ x ] = µ
mean
gaussian distribution
second order moments
gaussian
univariate case
second order moment
e [ x2 ]
multivariate gaus- sian
d2 second order moments
e [ xixj ]
group
matrix e [ xxt ]
matrix
e [ xxt ]
d/2
|σ|1/2 =
d/2 |σ|1/2
−
x − µ
tς−1
x − µ
− ztς−1z
z + µ
z + µ
t dz   d i=1   yjuj  d j=1  
d z = − j=1 i=1 d   k=1 d
gaussian distribution
variables
z = x − µ
note
cross-terms
µzt
µtz
symmetry
term µµt
gaussian distribution
term
zzt
again
use
eigenvector expansion
covariance matrix
completeness
set
eigenvectors
yj = ut j z
d/2
|σ|1/2
d/2
|σ|1/2 = = uiut i
= σ exp ztς−1z zzt dz uiut j exp − y2
yiyj dy
use
eigenvector equation
fact
right-hand side
middle line vanishes
symmetry
i = j
ﬁnal line
use
results
[ xxt ] = µµt + σ
single random variables
mean
second mo- ments
order
variance
multivariate case
mean
rise
covariance
random vector x
speciﬁc case
gaussian distribution
use
e [ x ] = µ
result
x − e [ x ]
x − e [ x ]
t cov [ x ] = e
cov [ x ] = σ
parameter matrix σ
covariance
x
gaussian distribution
covariance matrix
gaussian distribution
density model
signiﬁcant limitations
number
free parameters
distribution
general symmetric covariance matrix σ
d
d +
independent parameters
d independent parameters
µ
giv- ing d
d +
/2 parameters
large d
total number
parameters exercise
probability distributions figure
contours
constant probability density
gaussian distribution
dimensions
covariance matrix
general form
b
diagonal
elliptical contours
coordinate axes
c
proportional
identity matrix
contours
concentric circles
x2 x2 x1 x1 x1
b
c
therefore grows
d
computational task
large matrices
way
prob- lem
restricted forms
covariance matrix
covariance matrices
σ = diag
σ2 i
total
inde- pendent parameters
density model
corresponding contours
constant density
axis-aligned ellipsoids
covariance matrix
identity matrix
σ = σ2i
isotropic co- variance
d
independent parameters
model
spherical surfaces
constant density
possibilities
isotropic covari- ance matrices
figure
whereas such approaches
number
degrees
freedom
distribution
inversion
covariance
operation
form
probability density
limit
ability
interesting correlations
data
further limitation
gaussian distribution
uni- modal
i.e.
single maximum
good approximation
distributions
gaussian distribution
sense
many parameters
range
distributions
introduc- tion
latent variables
hidden variables
unobserved variables
problems
rich family
multimodal distributions
discrete latent variables
mixtures
gaussians
section
introduction
continuous latent variables
chapter
models
number
free parameters
dimensionality d
data space
model
dominant correlations
data set
approaches
rich set
hierarchical models
broad range
prac- tical applications
instance
gaussian version
markov random ﬁeld
probabilistic model
images
gaussian distribution
joint space
pixel intensities
imposition
considerable structure
spatial organization
pixels
linear dynamical system
time series data
applications
tracking
joint gaussian distribution
large number
latent variables
structure
distribution
powerful framework
form
properties
section
section
       
covariance matrix σ
µ = µa µb σ = σaa σab σba σbb
note
symmetry σt = σ
covariance matrix implies
σaa
σbb
σba = σt ab
many situations
inverse
covari- ance matrix λ ≡ σ−1
precision matrix
fact
properties
gaussian distributions
terms
covariance
whereas others
simpler form
terms
precision
partitioned form
precision matrix
gaussian distribution
such complex distributions
probabilistic graphical models
subject
chapter
conditional gaussian distributions
important property
multivariate gaussian distribution
sets
variables
gaussian
conditional distribution
set
marginal distribution
set
case
conditional distributions
suppose x
d-dimensional vector
gaussian distribution n
x|µ
σ
dis- joint subsets
xb
loss
generality
xa
ﬁrst m components
x
d − m components
x = xa xb
partitions
mean vector
exercise
λ = λaa λab λba λbb
corresponding
partitioning
vector x
inverse
symmetric matrix
λaa
λbb
ab = λba
point
instance
λaa
inverse
σaa
fact
relation
inverse
partitioned matrix
inverses
partitions
expression
conditional distribution p
xa|xb
product rule
probability
conditional distribution
probability distributions
joint distribution p
x
p
xb
xb
value
expression
valid probability distribution
xa
normalization
solution
quadratic form
exponent
gaussian distribution
normalization coefﬁcient
end
calculation
use
partitioning
−
x − µ
tς−1
x − µ
− −
xa − µa
tλaa
xa − µa
−
xb − µb
tλba
xa − µa
xa − µa
tλab
xb − µb
xb − µb
tλbb
xb − µb
function
xa
quadratic form
cor- responding conditional distribution p
xa|xb
distri- bution
mean
covariance
goal
expressions
mean
covariance
p
xa|xb
inspection
example
common operation
gaussian distributions
square ’
quadratic form
exponent terms
gaussian distribution
corresponding mean
covariance
such problems
exponent
general gaussian distribution n
x|µ
σ
xtς−1x + xtς−1µ + const
−
x − µ
tς−1
x − µ
‘ const ’
terms
x
use
symmetry
σ
general quadratic form
express
form
right-hand side
matrix
coefﬁcients
second order term
x
inverse covariance matrix σ−1
coefﬁcient
linear term
x
procedure
conditional gaussian distribution p
xa|xb
quadratic form
exponent
mean
covariance
distribution
µa|b
σa|b
functional dependence
xa
xb
constant
terms
second order
xa
−
λaaxa xt
covariance
inverse precision
p
xa|xb
σa|b = λ−1 aa
     
gaussian distribution
terms
xa
λaaµa − λab
xb − µb
coefﬁcient
xa
expression
σ−1 ba = λab
discussion
general form
µa|b = σa|b
λaaµa − λab
xb − µb
µa − λ−1 aa λab
xb − µb
use
results
terms
partitioned precision matrix
original joint distribution p
xb
results
terms
corresponding
covariance matrix
use
following identity
inverse
partitioned matrix a b c d
−1 = m −d−1cm d−1 + d−1cmbd−1 −mbd−1
quantity m−1
schur complement
matrix
left-hand side
respect
submatrix d. using
deﬁnition m =
a − bd−1c
−1
σab σba σbb −1 = λaa λab λba λbb
use
=
σaa − σabς−1 λab = −
σaa − σabς−1
σba
bb σba
−1σabς−1 bb
following expressions
mean
covariance
conditional distribution p
xa|xb
bb
xb − µb
bb σba
= µa + σabς−1 σa|b = σaa − σabς−1
comparing
conditional distribution p
xa|xb
simpler form
terms
partitioned precision matrix
terms
partitioned covariance matrix
note
mean
conditional distribution p
xa|xb
linear function
xb
covariance
xa
example
linear-gaussian model
exercise
section

probability distributions
marginal gaussian distributions
joint distribution p
xb
condi- tional distribution p
xa|xb
discussion
marginal distribution
p
xa
p
xb
dxb
strategy
distribution
quadratic form
exponent
joint distribution
thereby
mean
covariance
marginal distribution p
xa
quadratic form
joint distribution
par- titioned precision matrix
form
goal
xb
ﬁrst
terms
xb
square
order
integration
terms
b λbbxb+xt xt −
b
= −
xb−λ−1 bb m
tλbb
xb−λ−1 bb m
m = λbbµb − λba
xa − µa
mtλ−1 bb m
dependence
xb
standard quadratic form
gaussian distribution
ﬁrst term
right-hand side
term
xb
xa
exponential
quadratic form
integration
xb
form
−
xb − λ−1 bb m
tλbb
xb − λ−1 bb m
dxb
integration
unnor- malized gaussian
result
reciprocal
normalization co- efﬁcient
form
normalized gaussian
coefﬁcient
mean
determinant
covariance matrix
square
respect
xb
only term
contributions
left-hand side
xa
last term
right-hand side
m
term
terms
      
gaussian distribution
xa
[ λbbµb − λba
xa − µa
t λ−1 bb [ λbbµb − λba
xa − µa
λaaµa + λabµb
const − =
λaaxa + xt
λaa − λabλ−1 xt
λaa − λabλ−1 +xt
λba
bb λba
+ const
‘ const ’
quantities
xa
again
comparison
covariance
marginal distribution
p
xa
similarly
mean
σa =
λaa − λabλ−1
λba
−1
σa
λaa − λabλ−1
λba
= µa
covariance
terms
partitioned precision matrix
terms
corresponding partitioning
covariance matrix
conditional distribution
partitioned matrices
use
thus
satisfying result
marginal distribution p
xa
λaa λab λba λbb −1 = σaa σab σba σbb λaa − λabλ−1 bb λba −1 = σaa
e [ xa ] = µa cov [ xa ] = σaa
marginal distribution
mean
covariance
terms
partitioned covariance matrix
contrast
conditional distribution
precision matrix
rise
expres- sions
results
conditional distributions
partitioned gaus- sian
partitioned gaussians
joint gaussian distribution n
x|µ
σ
λ ≡ σ−1
x = xa xb
µ = µa µb

probability distributions xb
p
xa|xb =
p
xb
xa p
xa
figure
plot
left
contours
gaussian distribution p
xb
variables
plot
right
marginal distribution p
xa
blue curve
conditional distribution p
xa|xb
xb =
red curve
σ = σaa σab σba σbb
λ = λaa λab λba λbb
conditional distribution
p
xa|xb
n
x|µa|b
λ−1 aa
= µa − λ−1 aa λab
xb − µb
marginal distribution
p
xa
n
xa|µa
σaa
idea
marginal distributions
multivariate gaussian
example
variables
figure
bayes ’ theorem
gaussian variables
sections
gaussian p
x
vector x
subvectors
=
xa
xb
expressions
conditional distribution p
xa|xb
marginal distribution p
xa
mean
conditional distribution p
xa|xb
linear function
xb
gaussian marginal distribution p
x
gaussian conditional distribution p
y|x
p
y|x
mean
linear function
x
covariance
x
example
       

gaussian distribution
linear gaussian model
roweis
ghahramani
generality
section
marginal distribution p
y
conditional distribution p
x|y
problem
subsequent chapters
convenient
general results
conditional distributions
x
n p
y|x
n x|µ
λ−1 y|ax + b
l−1
µ
a
b
parameters
means
λ
l
precision matrices
x
m
dimensionality d
matrix a
size d × m. deﬁne first
expression
joint distribution
x
y
= x y
log
joint distribution ln p
z
ln p
x
ln p
y|x
− −
x − µ
tλ
x − µ
y − ax − b
tl
y − ax − b
+ const
‘ const ’
terms
x
y
quadratic function
components
z
p
z
gaussian distribution
precision
second order terms
xt
λ + atla
= − x
t
ytly
ytlax + xtatly λ + atla −atl l −la x
ztrz
= −
gaussian distribution
z
precision
inverse covariance
matrix
r = λ + atla −atl
l −la
exercise
covariance matrix
inverse
precision
matrix inversion formula
cov [ z ] = r−1 = λ−1 aλ−1 l−1 + aλ−1at λ−1at
        
probability distributions similarly
mean
gaussian distribution
z
linear terms
xtλµ − xtatlb + ytlb = t x y λµ − atlb lb
result
square
quadratic form
multivariate
mean
z
exercise
making use
e [ z ] = r−1 e [ z ] = λµ − atlb lb
aµ + b
section
section
expression
marginal distribution p
y
x
marginal distribution
subset
com- ponents
gaussian random vector
simple form
ex-
terms
partitioned covariance matrix
mean
covariance
use
mean
covariance
marginal distribution p
y
e [ y ] = aµ + b cov [ y ] = l−1 + aλ−1at
special case
result
a =
case
convolu- tion
gaussians
mean
convolution
sum
mean
gaussians
covariance
convolution
sum
covariances
expression
conditional p
x|y
results
conditional distribution
terms
partitioned precision matrix
results
conditional distribution p
x|y
e [ x|y ] =
λ + atla
cov [ x|y ] =
λ + atla
−1
atl
− b
+ λµ
evaluation
example
bayes ’ theorem
distribution p
prior distribution
x
variable y
conditional distribution p
x|y
corresponding posterior distribution
x
conditional distribu- tions
joint distribution p
z
= p
x
p
y|x
form p
x|y
p
y
results
 n   
n=1 n=1 n=1 n n n
gaussian distribution
marginal
conditional gaussians given
marginal gaussian distribution
x
conditional gaussian distri- bution
y
x
form p
x
n
x|µ
λ−1
p
y|x
= n
b
l−1
marginal distribution
y
conditional distribution
x
y
p
y
= n
b
l−1 + aλ−1at
p
x|y
n
x|σ
atl
− b
+ λµ
σ =
λ + atla
−1
maximum likelihood
gaussian given
data set x =
x1
xn
t
observations
xn
multivariate gaussian distribution
parameters
distribution
maximum likelihood
log likeli- hood function
ln p
x|µ
σ
− n d
ln
− n
ln|σ|−
xn−µ
tς−1
xn−µ
simple rearrangement
likelihood function
data set
quantities appendix c
sufﬁcient statistics
gaussian distribution
using
c.19
derivative
log likelihood
respect
xn
n=1 xnxt n.
∂ ∂µ ln p
x|µ
σ
σ−1
xn − µ
solution
maximum likelihood estimate
mean
n µml
n xn n=1
 
n  
probability distributions exercise
exercise
mean
observed set
data points
maximization
respect
approach
symmetry constraint
show
resulting solution
alternative derivations
result
symmetry
positive deﬁ- niteness constraints
magnus
neudecker
result
form σml
n
xn − µml
xn − µml
t
µml
result
joint maximization
respect
σ
note
solution
µml
σml
µml
σml
expectations
maximum likelihood solutions
true distribution
following results e [ µml ] = µ e [ σml ] = n
σ. n
expectation
maximum likelihood estimate
mean
true mean
maximum likelihood estimate
covariance
expectation
true value
hence
bias
different estimator
σ = n
n
n=1
xn − µml
xn − µml
t.
expectation
σ
sequential estimation
discussion
maximum likelihood solution
parameters
gaus- sian distribution
convenient opportunity
general discussion
topic
sequential estimation
maximum likelihood
sequential methods
data points
time
on-line applications
large data sets
batch processing
data points
result
maximum likelihood estimator
mean ml
n observations
µ
n
n  z  figure
a schematic illustration
correlated ran- dom variables
θ
regression function f
θ
robbins- ditional expectation e [ z|θ ]
monro
general sequen- tial procedure
root θ
such functions
gaussian distribution
θ f
θ
contribution
ﬁnal data point xn
µ
n
ml
n xn
= n−1
n xn
n
n ml + xn + n
n
n xn µ
n−1
ml = µ
n−1
xn − µ
n−1
ml
result
nice interpretation
n
data points
µ
µ
n−1
data point xn
revised estimate µ
n
ml
old estimate
small amount
direction
‘ error signal ’
xn − µ
n−1
note
n
contribution
successive data points
ml ml
result
same answer
batch result
formulae
de- rive
sequential algorithm
route
general formulation
sequential learning
robbins-monro algorithm
pair
random variables
z
joint distribution p
z
θ
con- ditional expectation
z
θ
deterministic function f
θ
f
θ
e [ z|θ ] = zp
z|θ
dz
figure
functions
way
regression functions
goal
root θ
f
θ
large data set
observations
z
θ
regression function
estimate
root
suppose
values
z
time
corresponding sequential estimation scheme
θ
following general procedure
such problems
     n n =1 ∞ n =1   n
probability distributions robbins
monro
conditional variance
z
ﬁnite
loss
generality
case
f
θ
θ > θ
f
θ
θ < θ
case
figure
robbins-monro procedure
sequence
successive estimates
root θ
z − f
| θ < ∞ e θ
n
θ
n−1
an−1z
θ
n−1
z
θ
n
observed value
z
θ
value θ
n
coefﬁcients
an
represent
sequence
positive numbers
conditions
lim n→∞ ∞
an = ∞ a2 n < ∞
robbins
monro
fukunaga
sequence
estimates
root
probability
note
ﬁrst condition
successive corrections decrease
magnitude
process
value
second con- dition
algorithm
root
third condition
accumulated noise
variance
hence
convergence
general maximum likelihood problem
robbins-monro algorithm
deﬁnition
maximum like- lihood solution θml
stationary point
log likelihood function
hence satisﬁes ∂
n ln p
xn|θ
θml
summation
limit n → ∞
n lim n→∞ ∂ ∂θ ln p
xn|θ
ex ∂ ∂θ ln p
x|θ
maximum likelihood solution corresponds
root
regression function
robbins-monro procedure
form θ
n
θ
n−1
an−1 ∂ ∂θ
n−1
ln p
xn|θ
n−1
 n z   n
gaussian distribution figure
case
gaussian distribution
mean µ
regression function
figure
form
straight line
case
random variable z corresponds
derivative
log likelihood function
x − µml
/σ2
expectation
regression function
straight line
µ − µml
/σ2
root
regres- sion function
maximum like- lihood estimator µml
p
z|µ
µ
speciﬁc example
sequential estimation
mean
gaussian distribution
case
parameter θ
n
estimate µ
n
ml
mean
gaussian
random variable z
z = ∂ ∂µml ln p
σ2
σ2
x − µml
distribution
z
mean µ − µml
fig- ure
univariate form
coefﬁcients
form an = σ2/n
note
case
single variable
same technique
same restrictions
–
coefﬁcients an
multivariate case
blum
bayesian inference
gaussian
maximum likelihood framework
point estimates
parameters
σ
bayesian treatment
prior distributions
parameters
simple example
single gaussian random variable x
variance σ2
task
mean µ
set
n observations x =
x1
likelihood function
probability
observed data
µ
function
µ
p
x|µ
p
xn|µ
n/2
−
xn − µ
n=1
likelihood function p
x|µ
probability distri- bution
µ
likelihood function
form
exponential
quad- ratic form
µ
prior p
µ
 n p
µ
n
posterior distribution
µ|µ0
p
µ|x
p
x|µ
p
µ
p
µ|x
n µ|µn
σ2 n µn
σ2 n
+ σ2 µ0 + n σ2 + n σ2 n σ2 n
+ σ2 µml
µml
maximum likelihood solution
µ
sample mean µml =
n xn
n=1
moment
form
posterior mean
variance
first
mean
posterior distribution
compromise
prior mean µ0
maximum likelihood solution µml
number
observed data points n
reduces
prior mean
n → ∞
posterior mean
maximum likelihood solution
result
variance
posterior distribution
terms
inverse variance
precision
precisions
precision
posterior
precision
contribution
data precision
data points
number
observed data points
precision
posterior distribution
variance
data points
prior variance
number
n
posterior distribution data points n → ∞
variance σ2
maximum likelihood solution
maximum likelihood result
point estimate
µ
bayesian formalism
limit
inﬁnite number
observations
note
ﬁnite n
limit
→ ∞
prior
variance
posterior mean
reduces
maximum n = σ2/n
likelihood result
posterior variance
probability distributions conjugate distribution
likelihood function
corresponding poste- rior
product
exponentials
quadratic functions
µ
hence
prior distribution
exercise
simple manipulation
square
exponent
posterior distribution
 n−1
n    n
gaussian distribution figure
illustration
bayesian inference
mean µ
gaussian distri- bution
variance
curves
prior distribution
µ
curve
n =
case
posterior distribution
numbers n
data points
data points
gaussian
variance
likelihood function
variance
true value
−1 n
n
n
n
exercise
section
analysis
bayesian inference
mean
gaussian distribution
figure
generalization
result
case
d- dimensional gaussian random variable x
known covariance
unknown mean
maximum likelihood expression
mean
sequential update formula
mean
n data points
terms
mean
n
data points
contribution
data point xn
fact
bayesian paradigm
sequential view
inference problem
context
inference
mean
posterior distribution
contribution
ﬁnal data point
p
µ|d
p
µ
p
xn|µ
p
xn|µ
term
square brackets
normalization coefﬁcient
posterior distribution
n
data points
prior distribution
bayes ’ theorem
likelihood function
data point xn
posterior distribution
n data points
sequential view
bayesian inference
applies
problem
data
variance
gaussian distribution
data
goal
mean
mean
variance
again
calculations
conjugate form
prior distribution
precision λ
likelihood function
λ
form p
x|λ
n=1 n
xn|µ
λ−1
λn/2 exp
xn − µ
n=1
−
n   n
probability distributions
=
b =
λ
λ
=
b =
=
b =
figure
plot
gamma distribution gam
λ|a
b
various values
parameters
b
corresponding conjugate
product
power
λ
exponential
linear function
λ
gamma distribution
gam
λ|a
b
γ
baλa−1 exp
−bλ
exercise
exercise
γ
gamma function
gamma distribution
>
distribution
various values
b
figure
mean
variance
gamma distribution
e [ λ ]
b var [ λ ]
b2
prior distribution gam
λ|a0
b0
likelihood function
posterior distribution p
λ|x
λa0−1λn/2 exp −b0λ −
xn − µ
n=1
gamma distribution
form gam
λ|an
bn
an = a0 + n
bn = b0 +
xn − µ
= b0 + n
σ2 ml
ml
maximum likelihood estimator
variance
note
σ2
need
track
normalization constants
likelihood function
correct coefﬁcient
end
normalized form
gamma distribution
  n
     
n  
n  section
gaussian distribution
from
effect
n data points
value
coefﬁcient
n/2
parameter a0
terms
‘ effective ’ prior observations
n data points
n σ2 ml
variance
parameter b0
‘ effective ’
observations
variance
= b0/a0
analogous interpretation
dirichlet
distributions
examples
exponential family
interpretation
conjugate
terms
effective ﬁctitious data points
exponential family
distributions
ml/2
parameter b
precision
variance
conjugate
case
inverse gamma distribution
precision
mean
precision
conjugate
dependence
likelihood function
µ
λ p
x|µ
λ
exp n λ
−
xn − µ
λ1/2 exp ∝
− exp λµ xn −
x2 n
prior distribution p
λ
same functional dependence
µ
λ
likelihood function
form p
λ
= exp λ1/2
β λµ2
−
cλµ − dλ
−
µ − c/β
λβ/2 exp − d −
λ
c
d
β
constants
p
λ
p
µ|λ
p
λ
p
µ|λ
p
λ
inspection
p
µ|λ
precision
linear function
λ
p
λ
gamma distri- bution
form p
λ
n
µ|µ0
βλ
−1
gam
λ|a
b
new constants
µ0 = c/β
+ β/2
b = d−c2/2β
distribution
gaussian-gamma distribution
figure
note
product
µ
gamma
λ
precision
µ
linear function
λ
prior
µ
λ
posterior distribution
coupling
precision
µ
value
λ
µ     d 
probability distributions figure
contour plot
normal-gamma distribution
parameter values
β =
−2
case
multivariate gaussian distribution n
d- dimensional variable x
conjugate prior distribution
mean µ
precision
mean
unknown precision matrix λ
conjugate prior
wishart distribution
x|µ
λ−1 exercise
w
λ|w
ν
b|λ|
ν−d−1
− tr
w−1λ
ν
number
degrees
freedom
distribution
w
d×d scale matrix
tr
·
trace
normalization constant b
b
w
ν
πd
d−1
γ i=1 ν
− i
−1
again
conjugate
covariance
precision matrix
inverse wishart distribu- tion
mean
precision
similar line
univariate case
conjugate prior
p
λ|µ0
β
w
ν
n
µ|µ0
βλ
−1
w
λ|w
ν
gaussian-wishart distribution
student ’ s t-distribution
conjugate
precision
gaussian
gamma distribution
univariate gaussian n
x|µ
τ−1
gamma prior gam
τ|a
b
precision
marginal distribution
x
form section
exercise
    
−5  
 ν → ∞ ν
ν
dτ
gaussian distribution
figure
plot
student ’
t-distribution
µ =
λ =
various values
ν
limit ν → ∞
gaussian distribution
mean µ
precision λ. p
x|µ
b
= =
n
x|µ
τ−1
gam
τ|a
b
dτ ∞ exp bae
−bτ
ba γ
= γ
τ
x − µ
b +
x − µ
−a−1/2 γ
change
variable z = τ [ b +
x − µ
]
convention
new parameters
ν =
λ = a/b
terms
distribution p
x|µ
b
form λ
+ λ
x − µ
st
x|µ
λ
ν
γ
ν/2 +
−ν/2−1/2 γ
ν/2
ν
student
s t-distribution
parameter λ
precision
t-distribution
inverse
variance
parameter ν
degrees
freedom
effect
figure
particular case
ν =
t-distribution reduces
cauchy distribution
limit ν →
t-distribution st
x|µ
λ
ν
gaussian n
x|µ
λ−1
mean µ
precision λ
from
student ’
t-distribution
inﬁnite number
gaussian distributions
same mean
different preci- sions
inﬁnite mixture
gaussians
gaussian mixtures
detail
section
result
distribution
gen- eral
‘ tails ’
figure
t- distribution
important property
robustness
presence
few data points
outliers
robustness
t-distribution
figure
maximum likelihood solutions
t-distribution
note
max- imum likelihood solution
t-distribution
expectation- maximization
em
algorithm
effect
small number
exercise
exercise
probability distributions
−5
−5 
b
figure
illustration
robustness
student ’
t-distribution
histogram distribution
data points
gaussian distribution
maximum likelihood ﬁt ob-
t-distribution
red curve
gaussian
green curve
red curve
t-distribution
gaussian
special case
same solution
gaussian
b
same data set
data points
gaussian
green curve
outliers
t-distribution
red curve
outliers
t-distribution
gaussian
outliers
practical applications
process
data corresponds
distribution
heavy tail
data
robustness
important property
regression problems
squares
regression
robustness
cor- responds
likelihood
gaussian distribution
regression model
heavy-tailed distribution
t-distribution
robust model
alternative parameters
λ = a/b
η = τ b/a
t-distribution
form st
x|µ
λ
ν
= x|µ
ηλ
−1 gam
η|ν/2
ν/2
dη
n
multivariate gaussian n
x|µ
λ
cor- responding multivariate student ’
t-distribution
form st
x|µ
λ
ν
n
x|µ
ηλ
−1
gam
η|ν/2
ν/2
dη
exercise
same technique
univariate case
gaussian distribution st
x|µ
λ
ν
γ
d/2 + ν/2
γ
ν/2
|λ|1/2
πν
d/2
+ ∆2 ν
d
dimensionality
x
∆2
squared mahalanobis distance
∆2 =
x − µ
tλ
x − µ
multivariate form
student ’
t-distribution
following properties e [ x ] = µ
cov [ x ] = mode [ x ] = µ ν
ν −
ν
ν
λ−1
results
univariate case
periodic variables
gaussian distributions
great practical signiﬁcance
own right
building blocks
complex probabilistic models
situations
density models
continuous vari- ables
important case
practical applications
periodic variables
example
periodic variable
wind direction
particular geographical location
instance
measure values
wind direction
number
days
parametric distribution
example
calendar time
quantities
hours
annual cycle
such quantities
angular
� θ <
periodic variables
direction
origin
conventional distribution
gaussian
approach
results
arbitrary choice
origin
suppose
instance
observations
θ1 =
θ2 =
standard univariate gaussian distribution
origin
sample mean
data set
standard deviation
origin
mean
standard deviation
special approach
treatment
periodic variables
problem
mean
set
observations d =
θ1
periodic variable
θ
radians
simple average
/n
coordinate dependent
invariant measure
mean
observations
points
unit circle
two-dimensional unit vectors
xn
n =
n
figure
vectors
xn
 
n n=1 n  n x2  n
probability distributions figure
illustration
representation
val- ues
two- dimensional vectors
unit circle
average x
vectors
xn x3 ¯x ¯r ¯θ x2 x1 x1
x =
corresponding angle θ
average
deﬁnition
location
mean
origin
angular coor- dinate
note
x
unit circle
cartesian coordinates
observations
xn =
cos θn
sin θn
carte- sian coordinates
sample mean
form x =
r cos θ
r sin θ
x1
x2 components
r cos θ
n cos θn
n=1 r sin θ
n sin θn
n=1
ratio
identity tan θ = sin θ/ cos θ
θ
θ = tan−1
n sin θn
cos θn shortly
result
maximum likelihood estimator
deﬁned distribution
periodic variable
periodic generalization
gaussian
von mises distribution
attention
univariate distributions
periodic distributions
hyperspheres
arbitrary dimension
extensive discussion
periodic distributions
mardia
jupp
convention
distributions
θ
period
probability density p
θ
θ
integrate    x2 
p
x
x1
gaussian distribution figure
von mises distribution
two-dimensional gaussian
form
density contours
blue
conditioning
unit circle
p
conditions
p
θ
p
θ
p
θ +
= p
θ
from
p
θ + m2π
p
θ
integer m.
gaussian-like distribution
prop- erties
gaussian distribution
variables
=
x1
x2
mean µ =
µ1
µ2
covariance matrix σ = σ2i
identity matrix
p
x2
exp
x1 − µ1
+
x2 − µ2
−
contours
constant p
x
circles
figure
value
distribution
circle
ﬁxed radius
con- struction
distribution
form
distribution
cartesian coordinates
x1
x2
coordinates
r
θ
x2 = r sin θ
mean µ
polar coordinates
x1 = r cos θ
µ1 = r0 cos θ0
µ2 = r0 sin θ0
transformations
two-dimensional gaussian distribu- tion
condition
unit circle r
dependence
θ.
exponent
gaussian distribution
= − = r0
r cos θ − r0 cos θ0
+
r sin θ − r0 sin θ0
σ2 cos
θ − θ0
cos θ cos θ0
sin θ sin
+ r2   n
probability distributions
θ0 = π/4
θ0
π/4
m =
θ0 = π/4
θ0 =
figure
von mises distribution
different parameter values
cartesian plot
left
corresponding polar plot
right
exercise
‘ const ’
terms
θ
use
following trigonometrical identities
+ sin2 a =
m = r0/σ2
ﬁnal expression
distribution
p
θ
unit circle r
form
a cos b +
a sin b = cos
a − b
p
m
m
m cos
θ − θ0
von mises distribution
param- eter θ0
mean
distribution
m
concentration parameter
inverse variance
precision
gaussian
normalization coefﬁcient
terms
i0
m
zeroth-order bessel function
ﬁrst kind
abramowitz
stegun
i0
m
exp
m cos θ
dθ
large m
distribution
von mises dis- tribution
figure
function i0
m
figure
maximum likelihood estimators
parameters
von mises distribution
log likelihood function
ln p
d|θ0
m
−n ln
− n ln i0
m
m cos
θn − θ0
n=1
gaussian distribution  
n
  
n=1 n
n
m    n i0
m
m a
m
figure
plot
bessel function i0
m
function a
m
respect
sin
θn − θ0
θ0
use
trigonometric identity exercise
sin
a − b
= cos b sin a −
a sin b
= tan−1 θml
sin θn
cos θn
result
mean
obser- vations
two-dimensional cartesian space
respect
use
i0
m
= i1
m
abramowitz
stegun
maximum likelihood solution
θml
joint optimization
θ
m
a
m
n cos
θn − θml
a
m
= i1
m
i0
m
function a
m
figure
use
trigonometric iden- tity
form a
mml
n cos θn cos
− n=1 sin θn sin
n
probability distributions
left figure
plots
‘ old faith- ful ’ data
blue
contours
constant proba- bility density
single gaussian distribution
data
maximum likelihood
note
distribution
clumps
data
places
probability mass
central region
clumps
data
right
distribution
linear combination
gaussians
data
maximum likelihood
techniques
chap- ter
rep- resentation
data
right-hand side
function a
m
completeness
alternative techniques
con- struction
periodic distributions
approach
histogram
observations
angular coordinate
ﬁxed bins
virtue
simplicity
ﬂexibility
suffers
signiﬁcant limitations
histogram methods
detail
section
approach
von mises distribution
gaussian distribution
euclidean space
unit circle
mardia
jupp
complex forms
distribution
valid distribution
real axis
periodic distribution
succes- sive intervals
periodic variable
real axis
unit circle
again
distribution
von mises distribution
limitation
von mises distribution
mixtures
von mises distributions
ﬂexible framework
periodic variables
multimodality
example
machine learn- ing application
use
von mises distributions
lawrence
al
extensions
conditional densities
regression problems
bishop
nabney
mixtures
gaussians
gaussian distribution
important analytical properties
suf- fers
signiﬁcant limitations
real data sets
example
figure
‘ old faithful ’ data set
measurements
eruption
old faithful geyser
yel- lowstone national park
usa
measurement
duration
appendix a   k=1 k k
eruption
minutes
horizontal axis
time
minutes
next erup- tion
vertical axis
data set forms
dominant clumps
simple gaussian distribution
structure
linear superposition
gaussians
characterization
data set
such superpositions
linear combinations
basic dis- tributions
gaussians
probabilistic models
mixture distributions
mclachlan
basford
mclachlan
peel
figure
linear combination
gaussians
rise
complex densities
sufﬁcient number
gaussians
means
covariances
coefﬁcients
linear combination
continuous density
arbitrary accuracy
superposition
k gaussian densities
form p
x
πkn
x|µk
σk
mixture
gaussians
gaussian density n
x|µk
σk
component
mixture
own mean µk
covariance σk
contour
surface plots
gaussian mixture
components
figure
section
gaussian components
frame- work
mixture models
mixture models
linear com- binations
other distributions
instance
section
mixtures
bernoulli distributions
example
mixture model
discrete variables
parameters
mixing coefﬁcients
sides
respect
note
x
individual gaussian components
k=1
requirement
p
x
n
x|µk
σk
implies
k.
condition
� πk
gaussian distribution figure
example
gaussian mixture distribution
dimension
gaussians
coefﬁcient
blue
sum
p
x
x section
    k=1 k n
  k 
probability distributions
b
figure
illustration
mixture
gaussians
two-dimensional space
contours
constant density
mixture components
components
values
mixing coefﬁcients
component
b
contours
marginal probability density p
x
mixture distribution
c
surface plot
distribution p
x
mixing coefﬁcients
requirements
probabil- ities
sum
product rules
marginal density
p
x
p
k
p
x|k
πk = p
k
prior prob- ability
kth component
density n
x|µk
σk
p
probability
x
k.
later chapters
impor- tant role
posterior probabilities
k|x
responsibilities
bayes ’
γk
x
p
k|x
= p
k
p
x|k
l p
l
p
x|l
πkn
σk
l πln
x|µl
σl
probabilistic interpretation
mixture distribution
detail
chapter
form
gaussian mixture distribution
parameters
µ
σ
notation π ≡
π1
µ ≡
µ1
σ ≡
σ1
way
values
parameters
maximum likelihood
from
log
likelihood function
ln p
x|π
µ
σ
ln n=1 k=1 πkn
xn|µk
σk
       
exponential family
x =
x1
situation
single gaussian
presence
summation
k
logarithm
result
maximum likelihood solution
parameters
closed-form analytical solution
approach
likelihood function
iterative numerical optimization techniques
fletcher
nocedal
wright
bishop
nabney
powerful framework
expectation maximization
length
chapter
exponential family
probability distributions
chapter
exception
gaussian mixture
speciﬁc examples
broad class
distri- butions
exponential family
duda
hart
bernardo
smith
members
exponential family
many important properties
com- mon
properties
generality
exponential family
distributions
x
parameters η
set
distributions
form p
x|η
= h
x
g
η
ηtu
x
x
vector
natural parameters
distribution
function
x
function g
coefﬁcient
distribu- tion
satisﬁes g
η
h
x
ηtu
x
integration
summation
x
discrete variable
examples
distributions
chapter
members
exponential family
bernoulli distribution p
x|µ
= bern
x|µ
µx
− µ
right-hand side
exponential
logarithm
x|µ
x ln µ +
− x
ln
− µ
ln x =
− µ
− µ comparison
η =
−
probability distributions   m  m k=1 u
x
x h
x
g
η
σ
−η
x
x h
x
g
η
µk
   k=1 m m−1
µ
µ = σ
η
σ
η
+ exp
−η
logistic sigmoid function
bernoulli distribution
standard representation
form p
x|η
σ
−η
exp
ηx
− σ
η
σ
−η
com- parison
multinomial distribution
single observation x
form p
x|µ
k=1 µxk k =
xk ln µk
x =
xn
t. again
standard representation
ηk = ln µk
η =
η1
ηm
t. again
x|η
= exp
parameters
parameters
sub- ject
constraint
m −
parameters
value
parameter
circumstances
constraint
distribution
terms
m
parameters
relationship
µm
terms
µk
k =
m −
m
parameters
parameters
constraints
� µk �
µk
   m          
            µk m−1 j=1 µj exp
ηk
exp
ηk
− m−1 m−1 m−1 j µj
+ µk k=1
−   k=1 
exponential family
making use
constraint
multinomial distribution
representa- tion
exp xk ln µk k=1 =
= exp m−1 k=1 m−1
xk ln
− xk
µk + xk
m−1 k=1 m−1 µk
− + ln = ηk
µk
sides
k
µk = j exp
ηj
softmax function
normalized exponential
represen- tation
multinomial distribution
form p
x|η
+ −1 exp
ηtx
standard form
exponential family
parameter vector η =
η1
ηm−1
t
u
x
x h
x
g
η
+ exp
ηk
−1
gaussian distribution
univariate gaussian
σ2
exp − −
x − µ
x2 + µ σ2 x
µ2
               dx ηt n 
n     n 
probability distributions
simple rearrangement
standard exponential family form
exercise
exercise
η = u
x
µ/σ2 −1/2σ2 x x2 h
x
=
−1/2 g
η
=
−2η2
exp
maximum likelihood
sufﬁcient statistics
problem
parameter vector η
gen- eral exponential family distribution
technique
maximum likeli- hood
gradient
sides
respect
x
ηtu
x
∇g
η
g
η
h
x
ηtu
x
u
x
rearranging
use
g
η
∇g
η
g
η
− h
x
ηtu
x
u
x
= e [ u
x
result −∇ ln g
η
e [ u
x
]
note
covariance
terms
second derivatives
g
η
order moments
distribution
exponential family
moments
simple differentiation
set
data
x =
x1
likelihood function
p
x|η
= h
xn
g
η
n exp u
xn
n=1
gradient
ln p
x|η
respect
following condition
maximum likelihood estimator ηml −∇ ln g
ηml
n u
xn
n=1
       n
exponential family
principle
solution
n u
xn
likelihood estimator
data
sufﬁcient statistic
distribution
entire data set
value
sufﬁcient statistic
bernoulli distribution
example
function u
x
x
sum
data points
xn
whereas
gaussian u
x
=
x
x2
t
sum
sum
x2 n
limit n → ∞
right-hand side
e [ u
x
]
limit ηml
true value η
fact
sufﬁciency property
bayesian inference
discussion
chapter
tools
graphical models
insight
important concepts
conjugate priors
concept
conjugate
several times
example
context
bernoulli distribution
conjugate prior
beta distribution
gaussian
conjugate
mean
conjugate
precision
wishart distribution
probability distribution p
x|η
prior p
η
likelihood function
posterior distribution
same functional form
prior
member
exponential family
conjugate
form p
ν
f
ν
g
η
exp
νηtχ
f
ν
normalization coefﬁcient
g
η
same function
ap- pears
likelihood function
posterior distribution
nor- malization coefﬁcient
form p
χ
ν
g
η
exp ηt n=1 u
xn
νχ
same functional form
conjugacy
parameter ν
effective number
pseudo-observations
value
sufﬁcient statistic u
x
χ
noninformative priors
applications
probabilistic inference
prior knowledge
prior distribution
example
prior assigns
probability
value
posterior dis- tribution
probability
value
irrespective
   dλ dη     
probability distributions
subsequent observations
data
many cases
little idea
distribution
form
prior distribution
noninformative prior
little inﬂu- ence
posterior distribution
jeffries
box
tao
bernardo
smith
data speak
’
distribution p
x|λ
parameter λ
prior distribution p
λ
= const
suitable prior
λ
discrete variable
k states
prior probability
state
case
continuous parameters
potential difﬁculties
approach
ﬁrst
domain
λ
prior distribution
λ diverges
such priors
practice
improper priors
corresponding posterior distribution
instance
uniform prior distribution
mean
gaussian
posterior distribution
mean
data point
second difﬁculty arises
transformation behaviour
probability density
nonlinear change
variables
function h
λ
variables
= η2
h
η
= h
η2
density pλ
λ
density
η
pη
η
pλ
λ
pλ
η2
∝ η
density
η
issue
maximum likelihood
likelihood function p
x|λ
simple function
λ
convenient parameterization
prior distribution
care
appropriate representation
parameters
simple examples
noninformative priors
berger
first
density
form p
x|µ
f
x − µ
parameter µ
location parameter
family
densities x = x + c
translation invariance
µ
f
µ = µ + c. thus
density
same form
density
choice
origin
prior distribution
translation invariance property
equal probability mass
− p
x| µ
          x   x σ
 σ   
exponential family
interval a � µ � b
interval a − c � µ � b − c.
b a p
µ
dµ = p
µ
dµ = b−c a−c b a p
µ − c
dµ
choices
a
b
µ − c
= p
µ
p
µ
example
location parameter
mean µ
gaussian distribution
conjugate prior distri- bution
µ
case
gaussian p
noninformative prior
limit
→ ∞
posterior distribution
µ
contributions
prior vanish
= n
µ|µ0
σ2
second example
density
form exercise
σ
note
normalized density
f
x
parameter σ
scale parameter
density
scale invariance
x = cx
p
x|σ
σ f σ
p
x| f
σ = cσ
transformation
change
scale
example
meters
kilometers
x
length
prior distribution
scale invariance
interval a � σ � b
scaled interval a/c � σ � b/c
prior
equal probability mass
intervals
a p
σ
dσ = b/c a/c p
σ
dσ = b a p
c
c dσ
choices
a
b
σ
c
c
p
σ
note
improper prior
integral
distribution
� σ � ∞
prior distribution
scale parameter
terms
density
log
parameter
transformation rule
densities
p
ln σ
= const
same probability mass
range
� σ
range
� σ
� σ �
  
section
probability distributions
example
scale parameter
standard deviation σ
gaussian distribution
account
location parameter µ
−
x/σ
n
x|µ
σ2
σ−1 exp
x = x − µ
terms
precision λ
transformation rule
densities
distribution p
σ
corresponds
distribution
λ
form p
λ
conjugate
λ
gamma distribution gam
λ|a0
b0
noninformative prior
special case a0 = b0
again
results
posterior distribution
λ
a0 = b0 =
posterior
terms
data
nonparametric methods throughout
chapter
use
probability distributions
speciﬁc functional forms
small number
parameters
values
data set
parametric approach
density modelling
important limitation
approach
chosen density
poor model
distribution
data
poor predictive performance
instance
process
data
aspect
distribution
gaussian
ﬁnal section
nonparametric approaches
es- timation
few assumptions
form
distribution
simple frequentist methods
reader
nonparametric bayesian methods
interest
walker
neal
m¨uller
quintana
teh
al.
discussion
histogram methods
density estimation
context
conditional distributions
figure
context
central limit theorem
figure
properties
histogram density models
detail
case
single continuous variable x
standard
partition x
distinct bins
width ∆i
number ni
observations
bin i
order
count
normalized probability density
total number n
observations
width ∆i
bins
probability values
bin
pi = ni n∆i
p
x
model
density
p
x
width
bin
bins
same width ∆i = ∆
figure
illustration
histogram approach
density estimation
data set
data points
distribution
green curve
histogram density estimates
common bin width ∆
various values
∆
nonparametric methods
∆
∆
∆
figure
example
histogram density estimation
data
distribution
green curve
mixture
gaussians
examples
his- togram density estimates
different choices
bin width ∆
top ﬁgure
density model
lot
structure
underlying distribution
data set
bottom ﬁgure
result
model
bimodal prop- erty
green curve
results
intermediate value
∆
middle ﬁgure
principle
histogram density model
choice
edge location
bins
value
∆
note
histogram method
property
methods
histogram
data
data set
histogram approach
data points
practice
histogram technique
quick visual- ization
data
dimensions
density estimation applications
obvious problem
density
discontinuities
bin
property
underlying distribution
data
major limitation
histogram approach
dimensionality
variable
d-dimensional space
m bins
total number
bins
m d.
exponential scaling
d
example
curse
dimensionality
space
high dimensional- ity
quantity
data
meaningful estimates
local probability density
histogram approach
estimation
im- portant lessons
first
probability density
particular location
data points
local neighbourhood
point
note
concept
locality
form
dis- tance measure
euclidean distance
histograms
section
distributions
neighbourhood property
bins
natural ‘ smooth-
’ parameter
spatial extent
local region
case
bin width
second
value
parameter
order
good results
choice
model complexity
polynomial curve
chapter
degree m
value α
regularization parameter
intermediate value
insights
discussion
nonparametric tech- niques
density estimation
kernel estimators
neighbours
dimensionality
simple histogram model
kernel density estimators
observations
unknown probabil- ity density p
x
d-dimensional space
euclidean
value
p
x
discussion
locality
small region r
x
probability mass
region
section
p = p
x
dx
r
data set
n observations
p
x
data point
probability p
r
total number k
points
r
binomial distribution
n
bin
k|n
p
k
n − k
p k
− p
using
mean fraction
points
region
e [ k/n ] = p
variance
mean
var [ k/n ] = p
− p
/n
large n
distribution
mean
region r
probability density p
x
region
k  n p. p  p
x
v
v
volume
r. combining
density estimate
form
p
x
k n v note
validity
contradictory assumptions
region r
density
region
relation
value
density
number k
points
region
binomial distribution
   n=1 n n
n
nonparametric methods
result
different ways
k
value
v
data
rise
k-nearest-neighbour technique
v
k
data
rise
kernel approach
k-nearest-neighbour density estimator
kernel density estimator converge
true probability density
limit n → ∞
v shrinks
n
k
n
duda
hart
kernel method
detail
region r
small hypercube
point x
probability density
order
number k
points
region
following function k
|ui| �
d
unit cube
origin
function k
example
kernel function
context
parzen window
quantity k
x − xn
/h
data point xn
cube
side h
x
total number
data points
cube
expression
following result
esti- mated density
x k = k x − xn h
p
x
n
hd k x − xn h
v = hd
volume
hypercube
side h
d di- mensions
symmetry
function k
equation
single cube
x
sum
n cubes
n data points
kernel density estimator
same problems
histogram method
presence
artiﬁcial discontinuities
case
boundaries
cubes
smoother density model
smoother kernel function
common choice
gaussian
rise
following kernel density model p
x
n
exp −x −
n=1
h
standard deviation
gaussian components
density model
data point
contributions
whole data set
n
den- sity
figure
model
data   
probability distributions figure
illustration
kernel density model
same data set
histogram approach
figure
h acts
smoothing parameter
top panel
result
noisy density model
bottom panel
bimodal nature
distribution
data
green curve
den- sity model
intermedi- ate value
h
middle panel
h
h
h
set
histogram technique
parameter h
role
smoothing parameter
trade-off
sensitivity
small h
over-smoothing
large h. again
optimization
h
problem
model complexity
choice
bin width
histogram density estimation
degree
polynomial
curve ﬁtting
other kernel function k
subject
condi- tions
k
du
probability distribution
class
density model
kernel density estimator
parzen estimator
great merit
compu- tation
‘ training ’ phase
storage
training set
great weaknesses
computa- tional cost
density
size
data set
nearest-neighbour methods
difﬁculties
kernel approach
density estimation
parameter h
kernel width
kernels
regions
high data density
large value
h
structure
data
h
estimates
data space
density
optimal choice
h
location
data space
issue
nearest-neighbour methods
density estimation
general result
local density estimation
v
value
k
data
ﬁxed value
k
data
appropriate value
v
point x
nonparametric methods figure
illustration
k-nearest-neighbour den- sity estimation
same data set
figures
parameter k
degree
smoothing
small value
k
noisy density model
top panel
large value
bot- tom panel
bimodal na- ture
true distribution
green curve
data set
k =
k =
k =

exercise
density p
x
radius
sphere
k data points
estimate
density p
x
v set
volume
technique
k
neighbours
figure
various choices
parameter k
same data set
figure
figure
value
k
degree
smoothing
optimum choice
k
note
model
k
neighbours
true density model
space diverges
chapter
k-nearest-neighbour technique
density estimation
problem
classiﬁcation
k-nearest-neighbour density estimation technique
class
use
bayes ’ theorem
data
com- k nk = n.
nk points
class ck
n points
new point x
x
k points irrespective
class
sphere
volume v
kk points
class ck
estimate
density
class
p
x|ck
kk nkv similarly
unconditional density
p
x
k n v
class priors
p
ck
nk n
bayes ’ theorem
posterior probability
class membership p
ck|x
p
x|ck
p
ck
p
x
kk k
probability distributions x2 x2 figure
k-nearest- neighbour classiﬁer
new point
black diamond
clas- siﬁed
majority class membership
k
train- ing data points
case k =
k =
approach
classiﬁcation
decision boundary
hyperplanes
perpendicular bisectors
pairs
points
different classes
b
x1 x1
b
probability
misclassiﬁcation
test point x
class
posterior probability
value
kk/k
new point
k
points
training data set
new point
class
number
representatives
set
ties
random
particular case
k =
nearest-neighbour rule
test point
same class
point
training set
concepts
figure
figure
results
k-nearest-neighbour algo- rithm
oil ﬂow data
chapter
various values
k. as
k
degree
smoothing
small k
many small regions
class
large k
regions
k
k =
x7
k =
x6
x6
figure
plot
data points
oil data
values
x6
x7
blue points correspond
‘ laminar ’
‘ annular ’
‘ homogeneous ’ classes
classiﬁcations
input space
k-nearest-neighbour algorithm
various values
k.   
        exercises
interesting property
k =
classiﬁer
limit n → ∞
error rate
minimum achievable error rate
optimal classiﬁer
true class distributions
cover
hart
k-nearest-neighbour method
kernel den- sity estimator
entire training data
expensive computation
data set
effect
expense
additional one-off computation
tree-based search
approximate
neighbours
exhaustive search
data set
nevertheless
nonparametric methods
other hand
simple parametric models
terms
forms
distribution
density models
complexity
models
size
training set
subsequent chapters
erties

www verify
bernoulli distribution
following prop-
p
x|µ
e [ x ] = µ var [ x ] = µ
− µ
show
entropy h [ x ]
bernoulli
random binary variable x
h [ x ] = −µ
µ −
− µ
ln
− µ
 
form
bernoulli distribution
symmetric be-
values
x
situations
equivalent formulation
x ∈
−1
case
distribution
x|µ
−
+ µ
/2
µ ∈ [ −1
]
show
distribution
mean
variance
entropy
 
www
exercise
binomial distribution
nor- malized
first
deﬁnition
number
combinations
m identical objects
total
n
n
+ n
= n
m
    n    n 
probability distributions use
result
induction
following result
+ x
n = xm
n m m=0
binomial theorem
real values
x
binomial distribution
n m µm
− µ
n−m
m=0
ﬁrst
factor
− µ
n
summation
use
binomial theorem
 
mean
binomial distribution
sides
normalization condition
respect
expression
mean
n. similarly
respect
use
result
mean
binomial distribution
result
variance
binomial
 
www
exercise
beta distribution
µa−1
− µ
b−1 dµ = γ
γ
b
γ
+ b
deﬁnition
gamma function
γ
b
exp
−x
exp
−y
yb−1 dy
use
expression
first
integrand
x
change
variable t = y + x
x
order
x
t integrations
change
variable x = tµ
t

make use
result
mean
variance
mode
beta distribution
e [ µ ] = var [ µ ] = mode [ µ ]
+ b ab
+ b
+ b +
+ b
    
 
binomial random variable x
prior distribution
µ
beta distribution
m occur- rences
x =
l occurrences
x =
show
posterior mean value
x
prior mean
maximum likelihood estimate
µ
posterior mean
λ
prior mean
− λ
maximum likelihood estimate
� λ �
con- cept
posterior distribution
compromise
prior distribution
maximum likelihood solution

variables
joint distribution p
follow- ing
results e [ x ] = ey [ ex [ x|y ] ] var [ x ] = ey [ varx [ x|y ] ] + vary [ ex [ x|y ] ]
ex [ x|y ]
expectation
x
conditional distribution p
x|y
similar notation
conditional variance
  
www
exercise
normalization
dirichlet dis- tribution
induction
exercise
beta distribution
special case
dirichlet
m =
dirichlet distribution
m −
variables
m variables
dirichlet k=1 µk
distribution
m variables
account
constraint
µm
dirichlet
m pm
µm−1
cm µαk−1
− m−1 k=1 αm−1 µj
m−1 j=1
goal
expression
cm
integrate
µm−1
care
limits
integration
change
limits
correct result
cm−1
use
expression
cm
 
property γ
x +
= xγ
x
gamma function
following results
mean
variance
covariance
dirichlet distribution
e [ µj ] = αj α0 var [ µj ] = αj
α0 − αj
α0 +
α2 αjαl cov [ µjµl ] =
α0 +
α2 j = l
α0
probability distributions   

www
expectation
ln µj
dirichlet distribution
respect
α0
e [
µj ] = ψ
αj
ψ
α0
ψ
≡ d da ln γ
digamma function

uniform distribution
continuous variable x
u
x|a
b
b −
� x � b
distribution
ﬁnd expressions
mean
variance
 
kullback-leibler divergence
gaussians
x
n
x|µ
σ
x
n
x|m
l
 
exercise
multivariate distribution
max- imum entropy
covariance
entropy
distribution p
x
h [ x ] = − p
x
p
x
dx
h [ x ]
distributions
x
subject
constraints
x
speciﬁc mean
covariance
p
x
p
x
dx = µ p
x
x − µ
x − µ
t dx = σ
variational maximization
lagrange multipliers
constraints
maximum likelihood distribution
gaussian
 
entropy
multivariate gaussian n
x|µ
σ
h [ x ]
ln|σ| + d
+ ln
d
dimensionality
x

  
consider
random variables x1
x2
gaussian distri- butions
means
µ2
precisions
τ2
derive
expression
differential entropy
variable x = x1 + x2
ﬁrst
distribution
x
relation p
x
∞ −∞ p
x|x2
p
x2
dx2
square
exponent
convolution
gaussian distributions
use
result
entropy
univariate gaussian

multivariate gaussian distribution
precision matrix
inverse covariance matrix
σ−1
sum
anti-symmetric matrix
anti-symmetric term
exponent
gaussian
precision matrix
loss
generality
inverse
symmetric matrix
exercise
covariance matrix
loss
generality
  
symmetric matrix σ
eigenvalue equation
complex conjugate
equation
original equation
inner product
eigenvector ui
eigenvalues λi
symmetry property
σ
eigenvectors
λj = λi
loss
generality
set
eigenvectors
eigenvalues
 
symmetric matrix σ
eigenvector equation
expansion
eigenvectors
coefﬁcients
eigenvalues
form
inverse matrix σ−1
representation
form
 
a positive deﬁnite matrix σ
quadratic form
real value
vector
show
sufﬁcient condition
σ
positive deﬁnite
eigenvalues λi
σ

symmetric matrix
size d× d
d
d +
independent parameters

www show
inverse
symmetric matrix
 
coordinate system
eigenvector expansion
volume
hyperellipsoid
constant       
probability distributions mahalanobis distance ∆
vd
volume
unit
d dimensions
mahalanobis distance
 
prove
identity
sides
matrix vd|σ|1/2∆d a b c d
use
deﬁnition
 
sections
marginal distri- butions
multivariate gaussian
partitioning
components
x
groups
xb
xc
corresponding par- titioning
mean vector µ
covariance matrix σ
form µ = µa µb µc
σ = σaa σab σac σba σbb σbc σca σcb σcc
use
results
section
expression
conditional distribution p
xa|xb
xc
 
useful result
linear algebra
woodbury matrix inversion formula
a + bcd
= a−1 − a−1b
c−1 + da−1b
−1da−1
sides
a + bcd
correctness
result

x
independent random vectors
p
z
= p
x
p
z
show
mean
sum y = x + z
sum
means
covariance matrix
y
sum
covariance matrices
x
z. conﬁrm
result
exercise
  
www
joint distribution
variable z = x y
mean
covariance
mak- ing use
results
show
marginal distribution p
x
use
results
show
conditional distribution p
y|x
 
partitioned matrix inversion formula
inverse
precision matrix
covariance matrix

use
result
result
 
consider
multidimensional random vectors
gaussian distributions
x
n
x|µx
σx
p
z
= n
z|µz
σz
sum y = x+z
results
expression
marginal distribution p
y
linear-gaussian model
product
marginal distribution p
x
conditional distribution p
y|x
  
exercise
next provide practice
quadratic forms
linear-gaussian models
indepen- dent check
results
main text
joint distribution p
conditional distributions
quadratic form
exponent
joint distribution
technique
‘
square ’
section
ﬁnd expressions
mean
covariance
marginal distribution p
y
variable x
use
woodbury matrix inversion formula
results
results
chapter
  
same joint distribution
exercise
technique
square
expressions
mean
covariance
conditional distribution p
x|y
again
verify
corre- sponding expressions
 
www
maximum likelihood solution
covariance matrix
multivariate
log likelihood function
respect
covariance matrix
positive deﬁnite
constraints
straightforward maximization
results
c.21
c.26
c.28
appendix c
covariance matrix σ
log likelihood function
sample covariance
ﬁnal result
positive deﬁnite
sample covariance
 
result
results
e [ xnxm ] = µµt + inmς
data point
gaussian distribution
mean µ
covariance σ
inm
m
element
identity matrix
hence
result
 
www
analogous procedure
expression
sequential estimation
variance
univariate gaussian  n
 
probability distributions distribution
maximum likelihood expression ml =
n
xn − µ
expression
gaussian distribution
robbins- monro sequential estimation formula
result
same form
expression
corresponding coefﬁcients
 
analogous procedure
ex- pression
sequential estimation
covariance
multivariate gaussian distribution
maximum likelihood expression
expression
gaussian distribution
robbins-monro se- quential estimation formula
result
same form
expression
corresponding coefﬁcients

technique
square
quadratic form
expo- nent
results
 
results
posterior distribution
mean
gaussian random
contributions
ﬁrst n
data points
expressions
sequential update
µn
σ2 n
same results
posterior distribution p
xn−1
= n
µ|µn−1
σ2 n−1
likelihood func- tion p
xn|µ
n
xn|µ
σ2
square
posterior distribution
n observations
 
www
d-dimensional gaussian random variable x
distribu- tion n
x|µ
σ
covariance σ
mean µ
set
observations x =
x1
prior distribution p
µ
n
µ|µ0
σ0
corresponding posterior distribution p
µ|x

deﬁnition
gamma function
gamma dis- tribution
 
mean
variance
mode
gamma distribution

following distribution p
q
exp
generalization
univariate gaussian distribution
show
distribution
∞ −∞ p
q
dx
q
regression model
target
t = y
+
random noise  n ln p
t|x
w
σ2
n=1
‘ const ’
terms
w
σ2
note
function
w
lq error function
section
 
univariate gaussian distribution n
x|µ
τ−1
data set x =
x1
i.i.d
observations
posterior distribution
gaussian-gamma distri- bution
same functional form
expressions
parameters
posterior distribution

wishart distribution
conjugate prior
precision matrix
multivariate gaussian

www verify
result

www show
limit ν → ∞
t-distribution
hint
normalization coefﬁcient
depen- dence
x

analogous steps
univariate student ’
t-distribution
result
multivariate form
stu- dent ’
t-distribution
variable η
deﬁnition
integration variables
multivariate t-distribution
 
deﬁnition
multivariate student ’
t-distribution
convolution
gamma distribution
properties
multivariate t-distribution

limit ν → ∞
multivariate student ’
t-distribution
reduces
mean µ
precision λ

various trigonometric identities
discussion
periodic variables
chapter
relation exp
ia
a + i
a
i
square root
minus
identity exp
ia
exp
−ia
result
identity cos
a − b
i
− b
variable drawn
distribution
show
log likelihood function
w
σ2
observed data set
input vectors
x1
target variables
=
t1
tn
t
|y
xn
w
tn|q − n q ln
+ const
probability distributions
 
real part
prove
sin
a − b
i
− b
imaginary part
result
large m
von mises distribution
mode θ0
ξ = m1/2
θ − θ0
taylor ex- pansion
cosine function
cos α
−
+ o
α4
show
m → ∞
von mises distribution
gaussian

trigonometric identity
solution
θ0

ﬁrst
second derivatives
von mises distribution
i0
m
m >
maximum
distribution
θ = θ0
minimum
θ = θ0 + π

use
result
trigonometric identity
maximum likelihood solution mml
concentra- tion
von mises distribution
a
mml
= r
r
radius
mean
observations
unit vectors
two-dimensional euclidean plane
figure
 
www express
beta distribution
gamma distribution
von mises distribution
members
exponential family
natural parameters

multivariate gaussian distribution
exponential family form
derive expressions
η
x
x
g
η
–

result
negative gradient
ln g
η
exponen- tial family
expectation
second derivatives
−∇∇
g
η
e [ u
x
u
x
t ] − e [ u
x
e [ u
x
t ] = cov [ u
x
]

variables
y = x/σ
density
f
x
 
www
histogram-like density model
space x
ﬁxed regions
density p
x
constant value hi
ith region
volume
region i
∆i
suppose
set
n observations
x
ni
observations
region i
lagrange multiplier
normalization constraint
density
expression
maximum likelihood estimator
hi

show
k-nearest-neighbour density model
improper distribu- tion
space
linear models
regression
focus
book
unsupervised learning
topics
density estimation
data
discussion
super- vised learning
regression
goal
regression
value
continuous target variables
value
d-dimensional vec- tor x
input variables
example
regression problem
polynomial curve
chapter
polynomial
speciﬁc example
broad class
functions
linear regression models
share
property
linear functions
adjustable parameters
focus
chapter
form
linear regression models
linear functions
input variables
useful class
functions
linear combinations
ﬁxed set
nonlinear functions
input variables
basis functions
such models
linear functions
parameters
simple analytical properties
respect
input variables
linear models for regression given
training data
n observations
xn
n =
n
target values
tn
goal
value
t
new value
x
approach
appropriate function y
x
values
new inputs
predictions
corresponding values
t. more
probabilistic perspective
predictive distribution p
t|x
uncertainty
value
t
value
x
conditional dis- tribution
predictions
t
new value
x
way
expected value
suitably chosen loss function
sec- tion
common choice
loss function
real-valued variables
squared loss
optimal solution
conditional expectation
t.
linear models
signiﬁcant limitations
practical techniques
pattern recognition
problems
input spaces
high dimen- sionality
nice analytical properties
foundation
so- phisticated models
later chapters
m−1 j=1
linear basis function models
linear model
regression
linear combination
input variables
w0 + w1x1 +
+ wdxd
x =
xd
t.
linear regression
key property
model
linear function
parameters
wd
linear function
input variables
signiﬁcant limitations
model
class
models
linear combinations
ﬁxed nonlinear functions
input variables
form y
w0 + wjφj
x
φj
x
basis functions
maximum value
index j
m −
total number
parameters
model
m.
parameter w0
ﬁxed offset
data
bias parameter
‘ bias ’
statistical sense
additional dummy ‘ basis function ’ φ0
x
m−1 y
x
w
= wjφj
x
wtφ
x
j=0
w =
w0
wm−1
t
φ =
φ0
φm−1
t.
many practical ap- plications
pattern recognition
form
ﬁxed pre-processing

linear basis function models
feature extraction
original data variables
original variables
vector x
features
terms
basis functions
φj
x
nonlinear basis functions
function y
non- linear function
input vector x
functions
form
linear models
function
w.
linearity
pa- rameters
analysis
class
models
signiﬁcant limitations
section
example
polynomial regression
chapter
particular example
model
single input variable x
basis func- tions
form
powers
x
φj
x
limitation
polynomial basis functions
global functions
input
changes
region
input space
other regions
input space
regions
different polynomial
region
functions
hastie
many other possible choices
basis functions
example φj
x
exp
x − µj
−
µj
locations
basis functions
input space
pa- rameter s
spatial scale
‘ gaussian ’ basis functions
prob- abilistic interpretation
normalization coefﬁcient
basis functions
adaptive parameters
possibility
sigmoidal basis function
form φj
x
σ x − µj s
σ
logistic sigmoid function
σ
+ exp
−a
‘ tanh ’ function
logistic sigmoid
tanh
=
−
general linear combination
logistic sigmoid functions
general linear combination
‘ tanh ’ functions
various choices
basis function
figure
possible choice
basis function
fourier basis
expansion
sinusoidal functions
basis function
speciﬁc fre- quency
spatial extent
contrast
basis functions
regions
input space
spectrum
different spatial frequencies
many signal processing applications
interest
ba- sis functions
space
frequency
class
functions
wavelets
application
wavelets
input values
linear models for regression
−0.5 −1
−1
−1
figure
examples
basis functions
polynomials
left
gaussians
form
centre
sigmoidal
form
right
regular lattice
successive time points
temporal sequence
pixels
image
useful texts
wavelets
ogden
mallat
vidakovic
discussion
chapter
particular choice
basis function
discussion
particular form
basis functions
purposes
numerical il- lustration
discussion
situation
vector φ
x
basis functions
identity φ
x
x. fur- thermore
order
notation simple
case
single target variable t.
section
modiﬁcations
multiple target variables
maximum likelihood
squares
chapter
polynomial functions
data sets
sum- of-squares error function
error function
maximum likelihood solution
assumed gaussian noise model
discussion
squares
relation
likelihood
detail
target variable t
deterministic func- tion y
additive gaussian noise
t = y
+
zero mean gaussian random
precision
inverse variance
β
section
recall
squared loss function
optimal prediction
new value
x
conditional mean
target
case
gaussian conditional distribution
form
conditional mean p
t|x
w
β
n
t|y
β−1
    
n=1 n=1 n n
n n
linear basis function models
e [ t|x ] = tp
t|x
dt = y
note
gaussian noise assumption implies
conditional distribution
t
x
applications
ex- tension
mixtures
conditional gaussian distributions
multimodal conditional distributions
section
data set
inputs x =
x1
target values
tn
group
target variables
tn
column vector
t
typeface
single observation
multivariate target
assumption
data points
distribution
following expression
likelihood function
function
adjustable parameters
β
form p
t|x
w
β
n
tn|wtφ
xn
β−1
note
problems
regres- sion
classiﬁcation
distribution
input variables
thus x
set
variables
explicit x
expressions
p
t|x
w
β
or- der
notation
logarithm
likelihood function
use
standard form
univariate gaussian
ln p
t|w
β
lnn
tn|wtφ
xn
β−1
n
ln β − n
ln
− βed
w
sum-of-squares error function
ed
w
tn − wtφ
xn
likelihood function
maximum likelihood
w
β
maximization
respect
section
maximization
likelihood function
conditional gaussian noise distribution
linear model
sum-of-squares error function
ed
w
gradient
log likelihood function
form ∇ ln p
t|w
β
n=1 tn − wtφ
xn
φ
xn
t.
 ⎞⎟⎟⎠    
n m−1 m−1
j=1 φj =
n  n  ⎛⎜⎜⎝ φ0
x1
 φ0
x2
n=1
n n  n
linear models for regression setting
gradient
w
= tnφ
xn
t − wt φ
xn
φ
xn
t
normal equations
squares problem
φ
n×m matrix
design matrix
elements
φnj = φj
xn
wml = φtφ −1 φtt φ
quantity φ1
x1
φ1
x2
φ0
xn
φ1
xn
φm−1
x1
φm−1
x2
···
··· φm−1
xn
φt φtφ φ† ≡
moore-penrose pseudo-inverse
matrix φ
rao
mitra
golub
van loan
generalization
notion
matrix inverse
matrices
property
ab
= b−1a−1
φ† ≡ φ−1
bias parameter explicit
error function
point
insight
role
bias parameter w0
ed
w
tn − w0 − wjφj
xn
respect
w0
w0 = t −
n n tn
n=1 φj
xn
bias w0 compensates
difference
averages
training
target values
weighted sum
averages
basis function values
log likelihood function
respect
noise precision parameter β
n
tn − wt mlφ
xn
n=1
linear basis function models
figure
geometrical interpretation
least-squares solution
n-dimensional space
axes
values
t1
tn
least-squares regression function
or- thogonal projection
data vector t
subspace
basis functions φj
x
basis function
vec- tor ϕj
length n
elements φj
xn
s ϕ1 y ϕ2 t
inverse
noise precision
residual variance
target
regression function
geometry
squares
point
geometrical interpretation
least-squares solution
n-dimensional space
axes
tn
t =
t1
tn
t
vector
space
basis function φj
xn
n data points
vector
same space
ϕj
figure
note
corresponds
jth column
φ
whereas φ
xn
nth row
φ
number m
basis functions
number n
data points
m vectors φj
xn
linear subspace s
dimensionality m.
n-dimensional vector
nth element
y
w
n =
n.
y
arbitrary linear combination
vectors
m-dimensional subspace
sum-of-squares error
factor
euclidean distance
y
t.
least-squares solution
w corresponds
choice
y
subspace s
figure
solution
orthogonal projection
t
subspace s.
case
solution
y
φwml
form
orthogonal projection
practice
direct solution
normal equations
numerical difﬁ- culties
φtφ
basis vectors
parameter values
large magnitudes
degeneracies
real data sets
numerical difﬁculties
technique
singular value decomposition
svd
press
bishop
nabney
note
addition
regularization term
matrix
non- singular
presence
degeneracies
batch techniques
maximum likelihood solution
in- volve
entire training
go
large data sets
chapter
data set
sequential algorithms
on-line algorithms
exercise
linear models for regression   n  n n=1
data points
time
model parameters
such presentation
sequential learning
real- time applications
data observations
continuous stream
predictions
data points
sequential learning algorithm
technique
stochastic gradient descent
sequential gradient descent
error function
sum
data points e =
en
presen- tation
pattern n
stochastic gradient descent algorithm
parameter vector w
τ
iteration number
η
rate parameter
choice
value
η
value
w
vector w
case
sum-of-squares error function
w
τ +1
w
τ
η∇en w
τ +1
w
τ
η
tn − w
τ
tφn
φn
φn = φ
xn
least-mean-squares
lms algorithm
value
η
care
algorithm converges
bishop
nabney
regularized
squares
section
idea
regularization term
error function
order
total error function
form ed
w
+ λew
w
λ
regularization coefﬁcient
relative importance
data-dependent error ed
w
regularization term ew
w
sim- plest forms
regularizer
sum-of-squares
weight vector ele- ments
sum-of-squares error function
ew
w
= wtw
total error function
e
w
tn − wtφ
xn
tn − wtφ
xn
wtw
particular choice
regularizer
machine
literature
weight decay
sequential learning algorithms
weight values
towards
data
statistics
ex- ample
parameter shrinkage method
parameter values towards
m  n
m
linear basis function models
q
q
q
q
figure
contours
regularization term
various values
parameter q. zero
advantage
error function
quadratic function
w
exact minimizer
closed form
gradient
respect
w
w = λi + φtφ −1 φtt
simple extension
least-squares solution
general regularizer
regularized error
form
tn − wtφ
xn
|wj|q j=1
corresponds
quadratic regularizer
figure
con- tours
regularization function
different values
q
case
q =
lasso
statistics literature
tibshirani
property
λ
coefﬁcients
sparse model
corresponding basis functions
role
unregularized sum-of-squares error
subject
constraint |wj|q � η j=1
appropriate value
parameter η
approaches
lagrange multipliers
origin
sparsity
figure
minimum
error function
constraint
λ
number
parameters
regularization
complex models
data sets
limited size
severe over-ﬁtting
effective model complexity
problem
optimal model complexity
appropriate number
basis functions
suitable value
regularization coefﬁcient λ
issue
model complexity
chapter
exercise
appendix e  n    n
linear models for regression
figure
plot
contours
unregularized error function
constraint re- gion
quadratic regular- izer q
left
lasso regularizer q
right
optimum value
pa- rameter vector w
w
sparse solution
w w2 w w1 w1
remainder
chapter
quadratic regularizer
practical importance
analytical tractability
multiple outputs
case
single target variable t.
applica- tions
k >
target variables
target vector t.
different set
basis func- tions
component
t
independent regression problems
approach
same set
basis functions
components
target vector
y
wtφ
x
y
k-dimensional column vector
w
m × k matrix
parameters
φ
x
m-dimensional column vector
elements φj
x
φ0
x
conditional distribution
target vector
isotropic gaussian
form p
t|x
w
β
n
t|wtφ
x
β−1i
set
observations t1
tn
matrix t
size n × k
nth row
tt n.
input vectors
matrix x
log likelihood function
ln p
t|x
w
β
n=1 lnn
tn|wtφ
xn
β−1i
n k
ln
− tn − wtφ
xn
bias-variance decomposition
function
respect
w
wml = φtφ −1 φtt
result
target variable tk
= φtφ −1 φttk = φ†tk exercise
tk
n-dimensional column vector
components
n =
n. thus
solution
regression problem decouples
different target variables
single pseudo-inverse matrix φ†
vectors
extension
general gaussian noise distributions
arbitrary covari- ance matrices
again
decoupling
k inde- pendent regression problems
result
parameters w
mean
gaussian noise distribution
sec- tion
maximum likelihood solution
mean
multivariate gaus- sian
covariance
single target variable t
simplicity
bias-variance decomposition so
discussion
linear models
regression
form
number
basis functions
ﬁxed
chapter
use
maximum likelihood
squares
complex models
data sets
limited size
number
basis functions
order
over-ﬁtting
side effect
ﬂexibility
model
important trends
data
introduction
regularization terms
models
many parameters
question
suitable value
regularization coefﬁcient λ
solution
regularized error function
respect
weight vector w
regularization coefﬁcient λ
right approach
unregularized solution
λ = 0
chapters
phenomenon
over-ﬁtting
unfortunate property
maximum likelihood
parameters
bayesian setting
chapter
bayesian view
model complexity
depth
frequentist viewpoint
model complexity issue
bias- variance trade-off
concept
context
linear basis function models
ideas
simple examples
discussion
general applicability
section
decision theory
regression problems
various loss functions
corresponding optimal prediction
conditional distribution p
t|x
popular choice
  
linear models for regression
squared loss function
optimal prediction
conditional expectation
h
x
h
x
e [ t|x ] = tp
t|x
dt
point
squared loss function
decision theory
sum-of-squares error function
maxi- mum likelihood estimation
model parameters
sophisticated techniques
squares
example regularization
bayesian ap- proach
conditional distribution p
t|x
squared loss function
purpose
predictions
section
squared loss
form e [ l ] =
y
x
h
x
p
x
h
x
x
t
dx dt
recall
second term
y
x
intrinsic noise
data
minimum achievable value
expected loss
ﬁrst term
choice
function y
x
so- lution
y
x
term
minimum
term
unlimited supply
data
unlimited computational resources
principle
regres- sion function h
x
desired degree
accuracy
optimal choice
y
x
practice
data set d
ﬁnite number n
data points
regression function h
x
h
x
parametric function y
pa- rameter vector w
bayesian perspective
uncertainty
model
posterior distribution
w. a frequentist treatment
point estimate
w
data set d
tries
uncertainty
estimate
following thought experi- ment
suppose
large number
data sets
size n
drawn
distribution p
t
x
data
d
algorithm
prediction function y
x
d
different data sets
ensemble
different functions
different values
squared loss
performance
particular learning algorithm
average
ensemble
data sets
integrand
ﬁrst term
particular data set d
form
quantity
particular data
d
aver- age
ensemble
data sets
quantity ed [ y
x
d
y
x
d
− h
x


+ 
   +
bias-variance decomposition
braces
y
x
d
ed [ y
x
d
+ ed [ y
x
d
− h
x
y
x
d
ed [ y
x
d
ed [ y
x
d
− h
x
y
x
d
ed [ y
x
d
ed [ y
x
d
− h
x
expectation
expression
respect
d
ﬁnal term
y
x
d
− h
x
ed =
ed [ y
x
d
− h
x
bias
variance + ed
y
x
d
ed [ y
x
d
difference
y
x
d
regression function h
sum
terms
ﬁrst term
squared bias
extent
average prediction
data sets differs
regression function
second term
variance
extent
solutions
individual data sets
average
extent
function y
x
d
particular choice
data
intuition
deﬁnitions
simple example
single input value x
expansion
following decomposition
squared loss
loss =
bias
+ variance + noise
bias
= variance = noise =
ed [ y
x
d
− h
x
x
y
x
d
ed [ y
x
d
ed
h
x
x
t
dx dt
p
x
dx
bias
variance terms
integrated quantities
goal
expected loss
sum
bias
variance
constant noise term
trade-off
bias
variance
ﬂexible models
low bias
high variance
rigid models
high bias
low variance
model
optimal predictive capability
one
balance
bias
variance
sinusoidal data set
chapter
data sets
n
data points
sinusoidal curve h
x
sin
data sets
l =
l
l
data
d
l
linear models for regression
−1
−1
−1
ln λ
t
x ln λ = −0.31
x ln λ = −2.4
x figure
illustration
dependence
bias
variance
model complexity
regulariza- tion parameter λ
sinusoidal data set
chapter
l =
data sets
n
data points
gaussian basis functions
model
total number
parameters
m =
bias parameter
left column
result
model
data sets
various values
ln λ
clarity
ﬁts
right column
corresponding average
ﬁts
sinusoidal function
data sets
  l=1 l
 
n n
figure
plot
squared bias
variance
sum
correspond- ing
results
fig- ure
average test set error
test data set size
points
minimum value
bias
+ variance
ln λ = −0.31
value
minimum error
test data
bias-variance decomposition
bias
variance
bias
+ variance test error −1
−3 −2
model
gaussian basis functions
regularized error function
prediction function y
l
x
figure
top row
large value
regularization coefﬁcient λ
low variance
red curves
left plot
high bias
curves
right plot
bottom row
λ
large variance
high variability
red curves
left plot
low bias
good ﬁt
average model ﬁt
original sinusoidal function
note
result
many solutions
complex model
m =
good ﬁt
regression function
beneﬁcial procedure
weighted averaging
multiple solutions
heart
bayesian approach
averaging
respect
posterior distribution
parameters
respect
data sets
bias-variance trade-off
example
average prediction
y
x
y
l
l
bias
variance
bias
= variance
n
n
y
xn
− h
xn
l l y
l
y
xn
n=1 l=1
distribution p
x
ﬁnite sum
data points drawn
distribution
quantities
sum
function
ln λ
figure
small values
λ
model
noise
linear models for regression data set
large variance
large value
λ
weight parameters
large bias
bias-variance decomposition
interesting in- sights
model complexity issue
frequentist perspective
lim- ited practical value
bias-variance decomposition
averages
respect
ensembles
data sets
whereas
practice
single observed data set
large number
sets
size
single large training set
course
level
over-ﬁtting
model complexity
limitations
next section
bayesian treatment
linear basis function models
powerful insights
issues
over-ﬁtting
practical techniques
question model complexity
bayesian linear regression
discussion
maximum likelihood
parameters
linear re- gression model
effective model complexity
number
basis functions
needs
size
data set
regularization term
log likelihood function
effective model complexity
value
regularization coefﬁ- cient
choice
number
form
basis functions
course
overall behaviour
model
issue
appropriate model complexity
par- ticular problem
likelihood func- tion
complex models
in- dependent hold-out data
model complexity
section
valu- able data
bayesian treatment
linear regression
over-ﬁtting problem
maximum likelihood
automatic methods
model complexity
training data
again
simplicity
case
single target variable t. ex- tension
target variables
discussion
section
parameter distribution
discussion
bayesian treatment
linear regression
prior probability distribution
model parameters
mo- ment
noise precision parameter β
constant
first note
likelihood function p
t|w
exponential
quadratic function
corresponding conjugate prior
gaussian distribution
form
mean m0
covariance s0
p
w
= n
w|m0
s0
 n
bayesian linear regression
next
posterior distribution
product
likelihood function
choice
conjugate gaus- sian prior distribution
posterior
distribution
usual procedure
square
normalization coefﬁcient
standard result
normalized gaussian
necessary work
gen- eral result
posterior distribution
form
p
w|t
= n
w|mn
sn
mn = sn s−1 n = s−1
+ βφtφ
s−1
m0 + βφtt note
posterior distribution
mode coincides
mean
maximum posterior weight vector
wmap = mn
broad prior s0 = α−1i
α →
mean mn
posterior distribution reduces
maximum likelihood value
n
posterior distribution reverts
data points
posterior distribution
stage
prior distribution
subsequent data point
new posterior distribution
remainder
chapter
particular form
gaus-
order
treatment
zero-mean isotropic gaussian
single precision parameter α
corresponding posterior distribution
w
p
w|α
= n
w|0
α−1i
mn = βsn φtt s−1 n = αi + βφtφ
exercise
exercise
log
posterior distribution
sum
log likelihood
log
function
w
form ln p
w|t
tn − wtφ
xn
wtw + const
maximization
posterior distribution
respect
equiva- lent
minimization
sum-of-squares error function
addition
quadratic regularization term
λ = α/β
bayesian learning
linear basis function model
sequential update
posterior distribution
simple example
straight-line ﬁtting
single input variable x
single target variable t
linear models for regression
linear model
form y
w0 + w1x
adap- tive parameters
posterior distributions
parameter space
synthetic data
function f
= a0 + a1x
param- eter values
= −0.3
a1
values
xn
uniform distribution u
x|−1
f
gaussian noise
standard deviation
target
tn
goal
values
a0
a1
such data
dependence
size
data set
noise variance
precision parameter
true value β =
parameter α
strategies
α
β
training data
figure
results
bayesian learning
model
size
data set
sequential nature
bayesian learning
current posterior distribution
new data point
time
ﬁgure
detail
several important aspects
bayesian inference
ﬁrst row
ﬁgure
situation
data points
plot
prior distribution
w space
samples
function y
values
w
second row
situation
single data point
location
data point
blue circle
right-hand column
left-hand column
plot
likelihood function p
t|x
w
data point
function
w. note
likelihood function
soft constraint
line
data point
close
noise precision β
comparison
true parameter values
= −0.3
a1
data set
white cross
plots
left column
figure
likelihood function
prior
top row
posterior distribution
middle plot
second row
sam- ples
regression function y
samples
w
posterior distribution
right-hand plot
note
sample lines
data point
third row
ﬁgure
effect
second data point
blue circle
plot
right-hand column
corresponding likelihood function
second data point
left plot
likelihood function
posterior distribution
second row
posterior distribution
middle plot
third row
note
same posterior distribution
original prior
likelihood function
data points
posterior
data points
points
line
compact posterior distribution
samples
posterior distribution
rise
functions
third column
functions
data points
fourth row
effect
total
data points
left-hand plot
likelihood function
data point
middle plot
posterior distribution
information
observations
posterior
third row
limit
inﬁnite number
data points
bayesian linear regression
figure
illustration
sequential bayesian learning
simple linear model
form y
w0 + w1x
detailed description
ﬁgure
text
 m   
linear models for regression posterior distribution
delta function
true parameter values
white cross
other forms
parameters
instance
gaussian prior
p
w|α
α
γ
m
− |wj|q j=1
corresponds
gaussian distribution
case
prior conjugate
likelihood function
maximum
poste- rior distribution
w corresponds
minimization
regularized error function
case
gaussian
mode
posterior distribution
mean
predictive distribution
practice
value
predictions
t
new values
x
predictive distribution
p
t|t
α
β
p
t|w
β
p
w|t
α
β
dw
t
vector
target values
training set
corresponding input vectors
right-hand side
conditioning statements
notation
conditional distribution p
t|x
w
β
target
posterior weight distribution
convolution
gaussian distributions
making use
result
section
predictive distribution
form
variance σ2 p
t|x
t
α
β
n
t|mt n
x
predictive distribution
n φ
x
σ2 n
x
n
x
β + φ
x
tsn φ
x
ﬁrst term
noise
data
second term
uncertainty
parameters
noise process
distribution
w
independent gaussians
variances
note
additional data points
posterior distribution
n +1
x
consequence
qazaz
σ2 n
x
limit n → ∞
second term
variance σ2
predictive distribution arises
additive noise
parameter β
illustration
predictive distribution
bayesian linear regression models
synthetic sinusoidal data set
section
figure
exercise
exercise
−1
−1
bayesian linear regression
x
x figure
examples
predictive distribution
model
gaussian basis functions
form
synthetic sinusoidal data set
section
text
detailed discussion
model
linear combination
gaussian basis functions
data sets
various sizes
corresponding posterior distributions
green curves correspond
function sin
data points
addition
gaussian noise
data sets
size n =
n =
n =
n =
plots
blue circles
plot
red curve
mean
corresponding gaussian predictive distribution
red shaded region
standard deviation
side
mean
note
predictive uncertainty
x
neighbourhood
data points
level
uncertainty decreases
data points
plots
figure
point-wise predictive variance
func- tion
x
order
insight
covariance
predictions
different values
x
samples
posterior distribution
w
corresponding functions
figure
linear models for regression
−1
−1
t
x
x figure
plots
function y
samples
posterior distributions
plots
figure
basis functions
gaussians
regions
basis function centres
contribution
second term
predic- tive variance
noise contribution β−1
thus
model
predictions
region
basis functions
undesirable behaviour
problem
alternative bayesian approach
re- gression
gaussian process
note
w
β
conjugate prior distribution p
w
β
discussion
section
gaussian-gamma distribution
denison
case
predictive distribution
student ’ s t-distribution
section
exercise
exercise
bayesian linear regression
figure
equivalent ker- nel k
x
gaussian basis functions
figure
plot
x versus x
slices
matrix cor- responding
different values
x
data set
kernel
values
x
interval
 n  n
equivalent
posterior mean solution
linear basis function model
in- teresting interpretation
stage
kernel methods
gaussian processes
expression
predictive mean
form chapter
y
n φ
x
βφ
x
tsn φtt = βφ
x
tsn φ
xn
tn
n=1
sn
mean
predictive distribution
point x
linear combination
training
target variables
function y
k
xn
tn n=1 k
x
βφ
x
tsn φ
x
smoother matrix
equivalent kernel
regression functions
predictions
linear combinations
training
target values
linear smoothers
note
equivalent kernel
input values
data set
deﬁnition
sn
equivalent kernel
case
gaussian basis functions
figure
kernel functions
function
x
different values
x
x
mean
predictive distribution
x
y
mn
weighted combination
target values
data points
x
weight
points
x
local evidence
distant evidence
note
localization property
localized gaussian basis functions
nonlocal polynomial
sigmoidal basis functions
figure
linear models for regression figure
examples
equiva- lent kernels
x
x =
function
x
corre- sponding
polynomial ba- sis functions
sig- moidal basis functions
fig- ure
note
local- ized functions
x
corresponding basis functions
 n
−1
further insight
role
equivalent kernel
covariance
y
x
x
cov [ y
x
x
= cov [ φ
x
tw
wtφ
x
= φ
x
tsn φ
x
β−1k
x
x
use
form
equivalent kernel
predictive mean
nearby points
whereas
distant pairs
points
correlation
predictive distribution
figure
point- wise uncertainty
predictions
sam- ples
posterior distribution
w
corresponding model functions
figure
joint uncertainty
posterior distribution
y values
x values
equivalent kernel
formulation
linear regression
terms
kernel function
alternative approach
regression
set
basis functions
equivalent kernel
localized kernel
predictions
new input vectors
observed training
practical framework
regression
classiﬁcation
gaussian processes
detail
section
effective kernel
weights
training
target values
order
prediction
new value
x
weights
other words
xn
n=1
exercise
values
x
result
x
summation
predictive mean
set
target data
n.
basis functions
data points
basis functions
basis functions
bias parameter
training data
predictive mean
bayesian model comparison
y
x
note
kernel function
summation constraint
corresponding predictions
convex combinations
training
target variables
chapter
equivalent kernel
important property
kernel functions
form
inner product
respect
vector ψ
x
nonlinear functions
k
z
= ψ
x
tψ
z
ψ
x
β1/2s1/2 n φ
x
bayesian model comparison
chapter
problem
over-ﬁtting
use
cross- validation
technique
values
regularization parameters
alternative models
problem
model se- lection
bayesian perspective
section
discussion
section
ideas
determination
regularization parameters
linear regression
over-ﬁtting
maximum likelihood
model parameters in- stead
point estimates
values
models
training data
need
validation
available data
training
multiple training runs
model
cross-validation
multiple complexity parame- ters
part
training process
example
chapter
relevance vector machine
bayesian model
complexity parameter
training data point
bayesian view
model comparison
use
probabilities
uncertainty
choice
model
consistent application
sum
product rules
probability
suppose
set
l models
mi
l.
model refers
probability distribution
data d.
case
polynomial curve-ﬁtting problem
distribution
set
target values
set
input values x
other types
model deﬁne
joint distributions
x
t.
data
models
one
uncertainty
prior probability distribution p
mi
training
d
posterior distribution
preference
different models
models
equal prior probability
interesting term
model evidence p
d|mi
preference
data
p
mi|d
p
mi
p
d|mi
section
  i=1 l
linear models for regression different models
term
detail
model evidence
marginal likelihood
likelihood function
space
models
parameters
ratio
model evidences
d|mi
/p
d|mj
models
bayes factor
kass
raftery
posterior distribution
models
predictive distribution
sum
product rules
p
t|x
d
p
t|x
mi
d
p
mi|d
example
mixture distribution
overall predictive distribu- tion
predictive distributions
t|x
mi
d
individual models
posterior probabilities
mi|d
models
in- stance
models
narrow distribution
t
narrow distribution
t = b
overall predictive distribution
bimodal distribution
modes
t
t = b
single model
t =
+ b
/2
simple approximation
probable model
predictions
model selection
model
set
parameters
model evidence
sum
product rules
probability
p
d|mi
p
d|w
mi
p
w|mi
dw
perspective
marginal likelihood
proba- bility
data set d
model
parameters
random
evidence
normalizing term
denominator
bayes ’ theorem
posterior distribution
parameters
p
w|d
mi
p
d|w
mi
p
w|mi
p
d|mi
insight
model evidence
simple approx- imation
parameters
case
model
single parameter
posterior distribution
parameters
d|w
p
w
dependence
model mi
notation
posterior distribution
probable value wmap
width ∆wposterior
in- tegral
value
integrand
maximum times
width
peak
prior
width ∆wprior
p
w
d
p
d|w
p
w
dw  p
d|wmap
∆wprior
chapter
   
bayesian model comparison ∆wposterior
w
figure
rough approximation
model evidence
posterior distribution
parame- ters
mode wmap
taking logs
ln p
d
ln p
d|wmap
ln wmap ∆wprior ∆wposterior ∆wprior
approximation
figure
ﬁrst term
ﬁt
data
probable parameter values
ﬂat prior
log likelihood
second term
model
complexity
∆wposterior < ∆wprior
term
magnitude
ratio ∆wposterior/∆wprior
parameters
data
posterior distribution
penalty term
model
set
m parameters
similar approximation
parameter
turn
parameters
same ratio
ln p
d
ln p
d|wmap
m ln ∆wposterior ∆wprior
simple approximation
size
complexity penalty
number m
adaptive parameters
model
complexity
model
ﬁrst term
complex model
data
second term
dependence
m.
optimal model complexity
maximum evidence
trade-off
terms
reﬁned version
approximation
gaussian approximation
posterior distribution
further insight
bayesian model comparison
marginal likelihood
models
intermediate complexity
con- sidering figure
horizontal axis
one-dimensional representation
space
possible data sets
point
speciﬁc data set
models m1
m2
m3
complexity
imagine
models
exam- ple data sets
distribution
data sets
result
section
linear models for regression figure
schematic illustration
distribution
data sets
models
different com-
m1
plexity
simplest
m3
note
dis- tributions
example
data
d0
model m2
intermedi- ate complexity
evidence
bayes factor
form p
d
m1 m2 d0 m3 d model
variety
different data sets
parameters
prior probability distribution
choice
parameters
random noise
target variables
particular data set
spe- ciﬁc model
values
parameters
prior distribution p
w
parameter
data
p
d|w
sim- ple model
example
ﬁrst order polynomial
little variability
data sets
distribution p
d
small region
horizontal axis
contrast
complex model
ninth order polynomial
great variety
different data sets
distribution p
d
large region
space
data sets
distributions
d|mi
particular data
d0
value
evidence
model
intermediate complexity
simpler model
data
complex model
predictive probability
range
data sets
small probability
implicit
bayesian model comparison framework
assumption
true distribution
data
set
models
consideration
bayesian model comparison
correct model
models m1
m2
truth
m1
ﬁnite data set
bayes factor
incorrect model
bayes factor
distribution
data sets
expected section
p
d|m1
ln p
d|m1
p
d|m2
dd
average
respect
true distribution
data
quantity
example
kullback-leibler divergence
prop- erty
distributions
case
bayes factor
correct model
bayesian framework
problem
over-ﬁtting
models
basis
training data
        
evidence approximation
bayesian approach
approach
recognition
as- sumptions
form
model
results
figure
model evidence
many aspects
behaviour
tails
evidence
prior
improper prior
arbitrary scaling factor
other words
normalization coefﬁcient
distribution
proper prior
suitable limit
order
improper prior
example
gaussian prior
limit
inﬁnite variance
evidence
evidence ratio
models
limit
meaningful answer
practical application
wise
independent test set
data
overall performance
ﬁnal system
evidence approximation
bayesian treatment
linear basis function model
duce prior distributions
hyperparameters
predictions
respect
hyperparameters
respect
parameters
w
hyperparameters
complete marginalization
variables
approximation
hyperparameters
values
marginal likeli- hood function
ﬁrst integrating
parameters
framework
statistics literature
empirical bayes
bernardo
smith
gelman
maximum likelihood
berger
maximum likelihood
wahba
machine learning literature
evidence approximation
gull
mackay
hyperpriors
α
β
predictive distribution
w
α
β
p
t|t
= p
t|w
β
p
w|t
α
β
p
β|t
dw dα dβ
p
t|w
β
p
w|t
α
β
mn
sn
dependence
input variable x
notation
posterior distribution p
β|t
values β
predictive distribution
w
α
β
values α
β
α
p
t|t
 p
t|t
α
β
p
t|w
β
p
w|t
α
β
dw
       
linear models for regression from bayes ’ theorem
posterior distribution
α
β
p
β|t
p
t|α
β
p
β
α
prior
evidence
values
β
marginal likelihood function p
t|α
β
marginal likelihood
linear basis function model
maxima
values
hyperpa- rameters
training data
recourse
cross-validation
ratio α/β
regularization parameter
aside
gamma
distri- butions
α
β
marginalization
hyperparameters
student ’
t-distribution
w
sec- tion
w
example
laplace approximation
section
local gaussian approxi- mation
mode
posterior distribution
practical alternative
evidence framework
buntine
weigend
integrand
function
w
skewed mode
laplace approximation
bulk
probability mass
re- sults
evidence
mackay
evidence framework
approaches
maximization
log evidence
evidence function
re-estimation equations
α
β
section
technique
expectation maximization
em
algorithm
section
approaches
same solution
evaluation
evidence function
marginal likelihood function p
t|α
β
weight parameters
p
t|α
β
p
t|w
β
p
w|α
dw
exercise
exercise
way
use
result
conditional distribution
linear-gaussian model
square
exponent
use
standard form
normalization coefﬁcient
from
evidence function
form p
t|α
β
n/2 m/2
exp
−e
w
dw

exercise
exercise
evidence approximation
m
dimensionality
w
e
w
= βed
w
+ αew
w
t − φw2 +
= β wtw
constant
proportionality
reg- ularized sum-of-squares error function
square
e
w
= e
mn
+
w − mn
ta
w − mn
= αi + βφtφ
e
mn
t − φmn2 +
mt n mn
note
a
matrix
second derivatives
error function
hessian matrix
a = ∇∇e
w
mn = βa−1φtt
using
a = s−1 deﬁnition
mean
posterior distribution
n
w
standard result
normalization coefﬁcient
multivariate gaussian
−e
w
dw = exp
−e
mn
=
−e
mn
m/2|a|−1/2
w − mn
ta
w − mn
dw
using
log
marginal likelihood
form ln p
t|α
β
m
ln α + n
ln β − e
mn
ln|a| − n
ln
required expression
evidence function
polynomial regression problem
model evidence
order
figure
prior
form
parameter α ﬁxed
α =
×
form
plot
figure
m =
polynomial
poor ﬁt
data
low value
linear models for regression figure
plot
model evidence
order m
polynomial re- gression model
evidence
model
m =
−20 −22 −24 −26  

m
evidence
m =
polynomial
data ﬁt
evidence
m =
data ﬁt
fact
sinusoidal function
data
odd function
terms
polynomial expansion
figure
residual data error
m
m =
model suffers
complexity penalty
evidence
m
m =
m =
signiﬁcant further improvement
data
figure
evidence
overall evidence
polynomials
further increases
value
m
small improvements
ﬁt
data
complexity penalty
decrease
evidence values
figure
generalization error
m =
m =
models
basis
plot
evidence values
clear preference
m =
model
good explanation
observed data
evidence function let
maximization
p
t|α
β
respect
ﬁrst
following eigenvector equation
a
α + λi
deriva- tive
term
ln|a|
respect
ui = λiui
βφtφ d dα ln|a| = d dα ln i
λi + α
= d dα ln
λi + α
λi + α i
stationary points
respect
= m
mt n mn
λi + α i
       α = γ
n=1 n i
n
  i
evidence approximation multiplying
αmt n mn = m −
= γ. λi + α i
m terms
sum
i
quantity γ
exercise
interpretation
quantity γ
from
value
α
marginal likelihood satisﬁes λi α + λi
mt n mn
note
implicit solution
α
γ
α
mode mn
posterior distribution
choice
α
iterative procedure
initial choice
α
mn
γ
values
re-estimate α
process
convergence
note
matrix φtφ
eigenvalues
start
β
λi
value
α
look- ing
training data
contrast
likelihood methods
independent data set
order
model complexity
log marginal likelihood
respect
eigenvalues
dλi/dβ = λi/β
d dβ ln|a| = d dβ ln
λi + α
β λi λi + α = γ β
stationary point
marginal likelihood
satisﬁes exercise
= n
− tn − mt n φ
xn
β = n − γ
−
n φ
xn
again
implicit solution
β
initial value
β
mn
γ
β
convergence
α
β
data
values
update
γ
linear models for regression figure
contours
likelihood function
axes
parameter space
eigenvectors
hessian
α =
mode
poste- rior
maximum likelihood solution wml
whereas
nonzero
mode
wmap = mn
direction
eigenvalue λ1
α
quantity λ1/
λ1 + α
corresponding map value
w1
zero
contrast
direction
eigenvalue λ2
α
quantity λ2/
λ2 +α
unity
map value
w2
maximum likelihood value
u2 wml wmap u1 w1
effective number
parameters
result
elegant interpretation
mackay
insight
bayesian solution
α
contours
like- lihood function
figure
set
axes
parameter space
eigenvec- tors
contours
likelihood function
axis-aligned ellipses
eigenvalues
measure
curvature
likelihood function
figure
eigenvalue λ1
λ2
curvature
elongation
contours
likelihood func- tion
βφtφ
positive deﬁnite matrix
positive eigenvalues
ratio λi/
λi + α
quantity γ
range
� γ � m.
directions
λi α
corresponding parameter wi
maximum likelihood value
ratio λi/
λi + α
such parameters
values
data
directions
λi α
corresponding parameters
ratios λi/
λi + α
directions
likelihood function
parameter value
parameter
small value
quantity γ
therefore
effective total number
parameters
insight
result
re-estimating β
maximum likelihood result
formulae
variance
inverse precision
average
squared differences
targets
model predictions
number
data points n
denominator
maximum like- lihood result
n − γ
bayesian result
maximum likelihood estimate
variance
gaussian distribution
  n=1 n
evidence approximation single variable x
ml =
n
xn − µml
estimate
maximum likelihood solution µml
mean
noise
data
effect
degree
freedom
model
corresponding unbiased estimate
form
map = σ2
xn − µml
n
n
n=1
section
result
bayesian treat- ment
unknown mean
factor
n −
denominator
bayesian result
account
fact
degree
free- dom
mean
bias
maximum likelihood
corresponding results
linear regression model
mean
target distribution
function wtφ
x
m parameters
parameters
data
effective number
parameters
data
m −γ parameters
small values
bayesian result
variance
factor n − γ
denominator
bias
maximum likelihood result
evidence framework
hyperparameters
sinusoidal synthetic data set
section
gaussian basis func- tion model
basis functions
total number
parameters
model
m =
bias
simplicity
illustra- tion
β
true value
evidence framework
α
figure
parameter α
magnitude
parameters
wi
individual parameters
effective number γ
param- eters
figure
limit n m
number
data points
relation
number
parameters
parameters
data
φtφ
implicit sum
data points
eigenvalues
increase
size
data set
case
γ = m
re-estimation equations
α
β become α = β = m
mn
n
mn
ew
ed
results
easy-to-compute approximation
full evidence re-estimation
linear models for regression −5
ln
figure
left plot shows
red curve
mn
blue curve
versus ln α
sinusoidal synthetic data set
intersection
curves
optimum value
α
evidence procedure
right plot
corresponding graph
log evidence ln p
t|α
β
versus ln α
red curve
showing
peak coincides
point
curves
left plot
test
error
blue curve
showing
evidence maximum
point
generalization
formulae
evaluation
eigenvalue spectrum
hessian
figure
plot
parameters
gaussian basis function model
effective num- ber
parameters γ
hyperparameter α
range
� α � ∞
γ
range
� γ � m.
−1
γ
limitations
fixed basis functions
chapter
models
linear combina- tion
ﬁxed
nonlinear basis functions
assumption
linearity
parameters
range
useful properties
closed-form solutions
least-squares problem
tractable bayesian treatment
furthermore
suitable choice
basis functions
arbitrary nonlinearities
  j=1 m m    
mapping
input variables
targets
next chapter
anal- ogous class
models
classiﬁcation
such linear models
general purpose framework
problems
pattern recognition
signiﬁcant shortcomings
linear models
later chapters
complex models
support vector machines
neural networks
difﬁculty
assumption
basis functions φj
x
training data set
manifestation
curse
dimen- sionality
section
consequence
number
basis functions
dimensionality d
input space
properties
real data sets
problem
first
data vectors
xn
non- linear manifold
intrinsic dimensionality
input space
result
strong correlations
input variables
example
images
handwritten digits
chapter
localized basis functions
input space
regions
data
approach
radial basis function networks
support vector
relevance vector machines
neural network models
adaptive basis functions
sigmoidal nonlinearities
parameters
regions
input space
basis functions
corresponds
data manifold
second property
target variables
signiﬁcant dependence
small number
possible directions
data manifold
neural networks
property
directions
input space
basis functions respond

www show
‘ tanh ’ function
logistic sigmoid function
hence show
general linear combination
logistic sigmoid functions
form tanh
=
−
linear combination
‘ tanh ’ functions
form y
w0 + wjσ x − µj s y
u0 + uj tanh j=1 x − µj s
ﬁnd expressions
new parameters
original pa- rameters
w1
   
n n  d
linear models for regression
 
matrix φ
φtφ
−1φt
vector v
projects
space
columns
φ
result
least-squares solution
corresponds
orthogonal projection
vector t
manifold s
figure

data set
data point tn
factor rn
sum-of-squares error function
ed
w
rn tn − wtφ
xn
expression
solution w
error function
alternative interpretations
weighted sum-of-squares error function
terms
i
data dependent noise variance
ii
data points

www
linear model
form y
w0 + wixi i=1
sum-of-squares error function
form ed
w
y
w
− tn
n=1
gaussian noise i
zero mean
variance σ2
input variables
use
e [ i
e [ i j ] = δijσ2
ed
noise distribution
sum-of-squares error
noise-free input variables
addition
weight-decay regularization term
bias parameter w0
regularizer

www
technique
lagrange multipliers
appendix e
minimization
regularized error function
unregularized sum-of-squares error
subject
constraint
relationship
parameters
λ

www
linear basis function regression model
multivariate target variable t
gaussian distribution
form p
t|w
σ
n
t|y
w
σ
y
w
wtφ
x
 n=1 n
training data
input basis vectors φ
xn
corre- sponding target vectors
n =
n. show
maximum likelihood solution wml
parameter matrix w
property
column
expression
form
solution
isotropic noise distribution
note
covariance matrix σ
show
maximum likelihood solution
σ
σ =
n tn − wt mlφ
xn
tn − wt mlφ
xn
t

technique
square
result
posterior distribution
parameters
linear basis function model
mn
sn
 
linear basis function model
section
n data points
posterior distribution
w
posterior
prior
next obser- vation
additional data point
xn +1
+1
square
posterior distribution
sn
sn +1
mn
mn +1
 
previous exercise
square
hand
use
general result
linear-gaussian models
 
www
use
result
predictive distribution
bayesian linear regression model
input-dependent variance
 
size
data set increases
uncertainty
posterior distribution
model parameters
make use
matrix identity
appendix c
uncertainty σ2
satisﬁes m + vvt
m−1v
+ vtm−1v −1 = m−1 − n
x
linear regression function
n +1
x
σ2 σ2 n
x
 
section
conjugate
gaussian distribution
unknown mean
unknown precision
inverse variance
normal-gamma distribution
property
case
conditional gaussian dis- tribution p
t|x
w
β
linear regression model
likelihood function
conjugate
w
β
p
w
β
n
w|m0
β−1s0
gam
β|a0
b0
 n
 n show
corresponding posterior distribution
same functional form
p
w
β|t
n
w|mn
β−1sn
gam
β|an
bn
ﬁnd expressions
posterior parameters
sn
an
bn
 
predictive distribution p
t|x
t
model
ex- ercise
student ’
t-distribution
form p
t|x
t
= st
t|µ
λ
ν
expressions
µ
λ
ν
 
exercise
detail
properties
equivalent kernel
sn
basis functions φj
x
number n
data points
number m
basis functions
basis functions
φ0
x
suitable linear combinations
basis functions
new basis
ψj
x
same space
linear models for regression ψj
xn
ψk
xn
ijk
ijk
j = k
x
show
α =
equivalent kernel
k
x
ψ
x
tψ
x
ψ =
ψm
t. use
result
kernel
summation constraint k
xn
n=1

www
linear basis function model
regression
pa- rameters
β
evidence framework
show
function e
mn
relation
mn
 
result
log evidence function p
t|α
β
linear regression model
use

show
evidence function
bayesian linear regression model
form
e
w
 
www
square
w
error function
bayesian linear regression
form
 
integration
w
bayesian linear regression model
result
hence show
log marginal likelihood
 
steps
maxi- mization
log marginal likelihood function
respect
leads
re-estimation equation
 
alternative way
result
optimal value
α
evidence framework
use
identity d dα ln|a| = tr a−1
a
identity
eigenvalue expansion
symmetric matrix a
use
standard results
determinant
trace
a
terms
eigenvalues
appendix c
use
 
steps
maximiza- tion
log marginal likelihood function
respect
leads
re-estimation equation
 
show
marginal probability
data
other words
model evidence
model
exercise
p
t
n/2
ban n γ
an
γ
a0
|sn|1/2
respect
respect
 
previous exercise
bayes ’ theorem
form p
t
= p
t|w
β
p
w
β
p
w
β|t
substitute
posterior distributions
likelihood func- tion
order
result
linear models
classiﬁcation
previous chapter
class
regression models
computational properties
analogous class
models
classiﬁcation problems
goal
classiﬁcation
input vector x
k discrete classes ck
k
k.
common scenario
classes
disjoint
input
class
input space
decision regions
boundaries
decision boundaries
decision surfaces
chapter
linear models
classiﬁcation
decision surfaces
linear functions
input vector x
hence
d −
-dimensional hyperplanes
d-dimensional input space
data sets
classes
linear decision surfaces
regression problems
target variable t
vector
real num- bers
values
case
classiﬁcation
linear models for classification ways
target values
class labels
probabilistic models
case
two-class problems
binary representation
single target variable t ∈
class c1
class c2
value
t
probability
class
c1
values
probability
extreme values
k >
classes
1-of-k coding scheme
t
vector
length k
class
cj
elements tk
t
element tj
value
instance
k =
classes
pattern
class
target vector t =
t.
again
value
tk
probability
class
ck
nonprobabilistic models
alternative choices
target variable representation
convenient
chapter
distinct approaches
classiﬁcation prob- lem
discriminant function
vector x
speciﬁc class
powerful approach
models
conditional probability distribution p
ck|x
inference stage
distribution
optimal decisions
inference
decision
numerous beneﬁts
section
different approaches
conditional probabilities
ck|x
technique
example
parametric models
parameters
training set
generative approach
class-conditional densities
p
x|ck
prior probabilities
ck
classes
required posterior probabilities
bayes ’ theorem p
ck|x
p
x|ck
p
ck
p
x
examples
approaches
chapter
linear regression models
chapter
model prediction y
linear function
parameters
case
model
input variables
form y
x
wtx + w0
y
real number
classiﬁcation problems
discrete class labels
posterior probabilities
range
generalization
model
linear function
w
nonlinear function f
·
y
x
f wtx + w0
machine learning literature f
·
activation function
inverse
link function
statistics literature
decision
correspond
x
=
wtx + w0 = constant
deci- sion surfaces
linear functions
x
function f
·
reason
class
models
generalized linear models
discriminant functions
mccullagh
nelder
note
contrast
models
regression
parameters
presence
nonlinear function f
·
computa- tional properties
linear regression models
models
general nonlinear models
subsequent chapters
algorithms
chapter
ﬁxed nonlinear transformation
input variables
vector
basis functions φ
x
regression models
chapter
classiﬁcation
original input space x
section
notation
basis functions
consistency
later chapters
discriminant functions
discriminant
function
input vector x
k classes
ck
chapter
attention
linear discriminants
decision surfaces
hyperplanes
dis- cussion
case
classes
extension
k >
classes
classes
representation
linear discriminant function
linear function
input vector
y
x
wtx + w0
w
weight vector
w0
bias
bias
statistical sense
negative
bias
threshold
input vector x
class c1
y
x
class c2
cor- responding decision boundary
relation y
x
d −
-dimensional hyperplane
d-dimensional input space
points
lie
decision surface
y
xa
y
xb
xa − xb
vector w
vector
decision surface
w determines
orientation
decision surface
point
decision surface
y
x
normal distance
origin
decision surface
bias parameter
location
decision surface
properties
case
d =
figure
value
y
x
signed measure
per- pendicular distance r
point x
decision surface
wtx w
= −
w
linear models for classification figure
illustration
geometry
linear discriminant function
dimensions
decision surface
perpen- dicular
displacement
origin
bias parameter w0
signed orthogonal distance
gen- eral point x
decision surface
y
x
/w
y
y
x2 r1 r2 x y
x
x1 x⊥ −w0 w    w  r = y
x
w
y
x
wt x
arbitrary point x
x⊥
orthogonal projection
decision surface
w w multiplying
sides
result
wt
w0
use
y
x
wtx + w0
y
x⊥
wtx⊥ + w0 =
= x⊥ + r
result
figure
linear regression models
chapter
compact notation
additional dummy ‘ input ’ value x0 =
x =
x0
x
w =
w0
w
case
decision surfaces
d-dimensional hyperplanes
origin
d +
input space
multiple classes
extension
linear discriminants
k >
classes
k-class discriminant
number
two-class discriminant functions
serious difﬁculties
duda
hart
use
k−1 classiﬁers
two-class problem
points
particular class ck
points
class
one-versus-the-rest classiﬁer
left-hand example
figure
 
discriminant functions
c1 r1 c3 r3
r2 c3 c2 c1 c2
r1
c1 r3
c2 r2 c2 c1 figure
attempting
k class discriminant
set
class discriminants
am- biguous regions
left
example
use
discriminants
points
class ck
points
class ck
right
example
discriminant functions
pair
classes ck
cj
example
classes
approach
regions
input space
alternative
k
k −
/2 binary discriminant functions
possible pair
classes
one-versus-one classiﬁer
point
majority vote
discriminant func- tions
problem
ambiguous regions
right-hand diagram
figure
difﬁculties
single k-class discriminant
k linear functions
form yk
x
wt
point x
class ck
yk
x
yj
x
j =
decision boundary
class ck
class cj
yk
x
yj
x
corresponds
d −
-dimensional hyperplane
k x + wk0
wk − wj
tx +
wk0 − wj0
same form
decision boundary
two-class case
section
analogous geometrical properties
decision regions
discriminant
convex
points
decision x
line
region rk
figure
point xa
xb
form x = λxa +
− λ
xb
b
linear models for classification figure
illustration
decision regions
mul- ticlass linear discriminant
decision boundaries
points
same decision re- gion rk
point x
line
points
rk
decision region
convex
ri xa rj rk ˆx xb
� λ �
linearity
discriminant functions
yk
x
λyk
xa
+
− λ
yk
xb
xa
xb
inside rk
yk
xa
yj
xa
yk
xb
yj
xb
j = k
yk
x
rk
thus rk
note
classes
formalism
discriminant functions
x
x
simpler
equivalent formulation
section
single discriminant function y
x
x
yj
approaches
parameters
linear discrimi- nant functions
squares
fisher ’
linear discriminant
percep- tron algorithm
least squares
classiﬁcation
chapter
models
linear functions
parame- ters
minimization
sum-of-squares error function
simple closed-form solution
parameter values
same formalism
classiﬁcation problems
general classiﬁcation problem
k classes
1-of-k binary coding scheme
target vector
justiﬁcation
squares
context
conditional expectation e [
]
target values
input vector
binary coding scheme
conditional expectation
vector
posterior class probabilities
probabilities
approximations
values
range
limited ﬂexibility
linear model
class ck
own linear model
=
group
vector nota- tion
yk
x
wt k x + wk0 y
x
wt x
  
   
w
form
ed
     
  
  t  

  x
discriminant functions
k
t
w
matrix
kth column
d + 1-dimensional vector
x
corresponding augmented input vector
xt
t
wk =
wk0
dummy input x0
representation
detail
section
new input x
class
output
= x
sum-of-squares error function
regression
chapter
training data
xn
tn
n =
n
matrix t
nth row
vector tt n
xt
sum-of-squares error function
matrix
parameter matrix x
nth row
wt k w
tr
w − t
t
x w − t
respect
solution
w
w =
xt x
xtt = x†t
x†
pseudo-inverse
matrix
discriminant function
form x
section
x
wt x = tt x† x
exercise
section
interesting property
least-squares solutions
multiple target variables
target vector
training
linear constraint attn + b =
constants
b
model prediction
value
x
same constraint
aty
x
b =
1-of-k coding scheme
k classes
predictions
model
property
elements
y
x
value
x
summation constraint
model
probabilities
interval
least-squares approach
exact closed-form solution
discrimi- nant function parameters
discriminant function
decisions
probabilistic interpretation
suf- fers
severe problems
least-squares solutions
robustness
outliers
classiﬁcation application
figure
additional data points
right- hand ﬁgure
signiﬁcant change
location
decision boundary
point
original decision bound- ary
left-hand ﬁgure
sum-of-squares error function
predictions
‘
correct ’
long way
correct side
decision
linear models for classification
−2 −4 −6
−2 −4 −6 −8 −4
−4
figure
left plot shows
classes
red crosses
blue circles
decision boundary
squares
curve
logistic regression model
green curve
section
right-hand plot
corresponding results
extra data points
bottom left
diagram
squares
outliers
logistic regression
section
several alternative error functions
classiﬁcation
difﬁculty
problems
squares
lack
robustness
figure
synthetic data
drawn
classes
two-dimensional input space
x2
property
lin- ear decision boundaries
excellent separation
classes
technique
logistic regression
chapter
satisfac- tory solution
right-hand plot
least-squares solution
poor results
small region
input space
green class
failure
squares
cor- responds
likelihood
assumption
gaussian conditional distribution
binary target vectors
distribution
appropriate probabilistic models
clas- siﬁcation techniques
properties
squares
moment
alternative nonprobabilistic methods
parameters
linear classiﬁcation models
fisher ’
way
linear classiﬁcation model
terms
dimensionality reduction
case
classes
d- 
discriminant functions
−2 −4 −6 −6 −4
−2 −4 −6 −6 −4 −2 figure
example
synthetic data set
classes
training data points
×
+
blue
lines
decision boundaries
background
respective classes
decision regions
left
result
least-squares discriminant
region
input space
green class
points
class
right
result
logistic regressions
section
correct classiﬁcation
training data
dimensional input vector x
project
dimension
y = wtx
threshold
y
� −w0
class c1
class c2
standard linear classiﬁer
previous section
projection
dimension
considerable loss
infor- mation
classes
original d-dimensional space
dimension
com- ponents
weight vector w
projection
class separation
two-class problem
n1 points
class c1
n2 points
class c2
mean vectors
classes
m1 = xn
xn
m2
n1 n ∈ c1
n2 n ∈ c2
measure
separation
classes
w
separation
projected class
w
m2 − m1 = wt
m2 − m1
mk = wtmk
 
linear models for classification
−2
−2 appendix e exercise
figure
left plot shows samples
classes
histograms
projection
line
class
note
considerable class overlap
projected space
right plot
projection
fisher linear discriminant
improved class separation
i w2
mean
data
class ck
expression
magnitude
w.
i
problem
unit length
lagrange multiplier
constrained maximization
w ∝
m2 − m1
problem
approach
figure
classes
original two- dimensional space
considerable overlap
line
means
difﬁculty
nondiagonal covariances
class distributions
idea
fisher
function
large separation
projected class means
small variance
class
class overlap
projection formula
set
labelled data points
x
labelled set
one-dimensional space y
within-class variance
data
class ck
yn − mk
k = s2
yn = wtxn
total within-class variance
fisher criterion
ratio
data
s2 between-class variance
within-class variance
+ s2 j
w
=
m2 − m1
+ s2
exercise
dependence
w explicit
fisher criterion
form 
discriminant functions j
w
wtsbw wtsww
sb
between-class covariance matrix
sb =
m2 − m1
m2 − m1
t
sw
total within-class covariance matrix
sw = n∈c1
xn − m1
xn − m1
t + n∈c2
xn − m2
xn − m2
t.
differentiating
respect
j
w
wtsbw
sww =
wtsww
sbw
sbw
direction
m2−m1
magnitude
w
direction
scalar factors
wtsbw
wtsww
sides
s−1 w
note
within-class covariance
sw
unit matrix
w
difference
class
w
m2 − m1
w ∝ s−1
result
fisher ’
linear discriminant
discriminant
speciﬁc choice
direction
projection
data
dimension
data
discriminant
threshold y0
new point
c1
y
x
y0
c2
example
class-conditional densities
y|ck
gaussian distributions
techniques
section
parameters
gaussian distributions
maximum likelihood
gaussian ap- proximations
classes
formalism
section
expression
optimal threshold
justiﬁcation
gaussian assumption
central limit theorem
y = wtx
sum
set
random variables
relation
squares
least-squares approach
determination
linear discriminant
goal
model predictions
set
target values
contrast
fisher criterion
maximum class separation
output space
relationship
approaches
two-class problem
fisher criterion
special case
squares
1-of-k coding
target values
different target
scheme
least-squares solution
     e
n=1 n=1 n n=1 n=1 n n n 
linear models for classification
weights
fisher solution
duda
hart
targets
class c1
n/n1
n1
number
patterns
class c1
n
total number
patterns
target value
reciprocal
prior probability
class c1
class c2
targets
n2
number
patterns
class c2
sum-of-squares error function
wtxn + w0 −
derivatives
e
respect
wtxn + w0 − tn
wtxn + w0 − tn xn
use
choice
target
scheme
tn
expression
bias
form
w0 = −wtm tn = n1 n n1 − n2
n n2 =
m
mean
total data set
m
n n xn =
n
n1m1 + n2m2
exercise
straightforward algebra
use
choice
tn
second equation
sw + n1n2 n sb
= n
m1 − m2
sw
sb
bias
using
sbw
direction
m2 − m1
w ∝ s−1 w
m2 − m1
irrelevant scale factors
weight vector coincides
found
fisher criterion
addition
expres- sion
bias value
new vector x
class c1
y
x
wt
x−m
class c2
k k=1     
nk
n n=1 n=1 n n k  k 
discriminant functions
fisher ’ s discriminant
multiple classes
generalization
fisher discriminant
k >
classes
dimensionality d
input space
number k
classes
d >
linear ‘ features
yk = wt k x
k
d
feature values
vector y
weight vectors
columns
matrix w
note
bias parameters
deﬁnition
y
generalization
within-class covariance matrix
case
k classes
y = wtx
sw = sk
− mk
xn − mk
t sk = mk = xn n∈ck
nk
number
patterns
class ck
order
generalization
between-class covariance matrix
duda
hart
ﬁrst
total covariance matrix
m
mean
total data set st =
xn − m
xn − m
t m = xn
n nkmk k=1
n = k nk
total number
data points
total covariance matrix
sum
within-class covariance matrix
additional matrix sb
measure
between-class covariance st = sw + sb
sb = k=1 nk
mk − m
mk − m
t.
    
k k n∈ck    k k=1
linear models for classification
covariance matrices
original x-space
similar matrices
projected d-dimensional y-space
sw =
yn − µk
yn − µk
t sb = nk
µk − µ
µk − µ
t µk
nk yn
µ
n nkµk
scalar
between-class covariance
within-class covariance
many possible choices
criterion
fukunaga
example
j
w
tr s−1 w sb
criterion
explicit function
projection matrix w
form j
w
= tr
wswwt
−1
wsbwt
maximization
such criteria
length
fukunaga
weight values
eigenvectors
s−1 w sb
correspond
d
eigenvalues
important result
such criteria
note
sb
sum
k ma- trices
outer product
vectors
therefore
rank
addition
k −
matrices
result
constraint
sb
k −
k −
nonzero eigenvalues
projection
k −
-dimensional subspace
eigenvectors
sb
value
j
w
k −
‘ features
means
fukunaga
perceptron algorithm
example
linear discriminant model
perceptron
rosenblatt
important place
history
pattern recognition al- gorithms
two-class model
input vector x
ﬁrst
ﬁxed nonlinear transformation
feature vector φ
x
generalized linear model
form y
x
f wtφ
x

discriminant functions
nonlinear activation function f
·
step function
form
f
= +1
−1
vector φ
x
bias component φ0
x
discussions
two-class classiﬁcation problems
target
scheme
t ∈
context
probabilistic models
perceptron
target values
= +1
class c1
= −1
class c2
choice
activation function
algorithm
parameters w
perceptron
error function minimization
natural choice
error func- tion
total number
misclassiﬁed patterns
simple learning algorithm
error
piecewise constant function
w
discontinuities
change
w
decision boundary
data points
methods
w
gradi- ent
error function
gradient
alternative error function
perceptron cri- terion
weight vector
xn
class c1
wtφ
xn
whereas
class c2
xn
t ∈
−1
+1
target
scheme
patterns
wtφ
xn
perceptron criterion
error
pattern
whereas
mis- classiﬁed pattern
quantity −wtφ
xn
tn
perceptron criterion
ep
w
− n∈m wtφntn
frank rosenblatt
rosenblatt ’
perceptron
important role
history
ma- chine learning
rosenblatt
perceptron
ibm
computer
cornell
early 1960s
special-purpose hardware
par- allel implementation
perceptron learning
ideas
“ principles
neuro- dynamics
perceptrons
theory
brain mech- anisms ”
rosenblatt ’
work
marvin minksy
objections
book “ perceptrons ”
seymour papert
book
time
neural networks
solutions
separable problems
fact
such limitations
case
single-layer networks
perceptron
general network models
book
substantial decline
research funding
neu- ral computing
situation
mid-1980s
today
many hundreds
thousands
applications
neural networks
widespread use
examples
areas
recognition
information retrieval be- ing
millions
people
linear models for classification section
m
set
misclassiﬁed patterns
contribution
error
particular misclassiﬁed pattern
linear function
w
regions
w space
pattern
zero
regions
total error function
piecewise linear
stochastic gradient descent algorithm
error function
change
weight vector w
w
τ +1
w
τ
η∇ep
w
= w
τ
ηφntn
η
rate parameter
τ
integer
steps
algorithm
perceptron function y
learning rate parameter η
generality
note
weight vector
training
set
patterns
perceptron
algorithm
simple interpretation
cycle
training patterns
turn
pattern
perceptron function
pattern
weight vector
class c1
vector φ
xn
current estimate
weight vector w
class c2
vector φ
xn
perceptron
algorithm
figure
effect
single update
perceptron
algorithm
contribution
error
misclassiﬁed pattern
τ +1
tφntn = −w
τ
tφntn −
φntn
tφntn < −w
τ
tφntn
η =
use
φntn2 >
course
contribution
error function
other misclassiﬁed patterns
change
weight vector
classiﬁed patterns
perceptron learning rule
total error function
stage
perceptron convergence theorem states
ex- act solution
other words
training data set
perceptron
algorithm
exact solution
ﬁnite num- ber
steps
proofs
theorem
example
rosenblatt
block
nilsson
minsky
papert
hertz
al
bishop
note
number
steps
con- vergence
practice
convergence
nonseparable problem
one
data set
many solutions
one
initialization
parameters
or- der
presentation
data points
furthermore
data sets
perceptron
algorithm
discriminant functions
−0.5 −1
−0.5 −1
−0.5 −1
−0.5 −1 −1
−0.5
−0.5
−0.5
figure
illustration
convergence
perceptron
algorithm
data points
classes
two-dimensional feature space
φ2
top left plot
initial parameter vector
black arrow
corresponding decision boundary
black line
arrow points
decision region
classiﬁed
belonging
red class
data point
feature vector
current weight vector
new decision
top right plot
bottom left plot
next misclassiﬁed point
green circle
feature vector
weight vector
decision
bottom right plot
data points
linear models for classification figure
illustration
mark
perceptron hardware
photograph
left
inputs
simple camera system
input scene
case
printed character
powerful lights
image
×
array
cadmium sulphide photocells
pixel image
perceptron
patch board
middle photograph
different conﬁgurations
input features
random
ability
perceptron
need
precise wiring
contrast
modern digital computer
photograph
right
racks
adaptive weights
weight
rotary variable resistor
potentiometer
electric motor thereby
value
weight
learning algorithm
difﬁculties
learning algorithm
perceptron
probabilistic outputs
k >
classes
important limitation
arises
fact
models
chapter
linear com- binations
ﬁxed basis functions
detailed discussions
limitations
perceptrons
minsky
papert
bishop
analogue hardware implementations
perceptron
rosenblatt
motor-driven variable resistors
adaptive parameters
figure
inputs
simple camera system
array
photo-sensors
basis functions φ
variety
ways
example
simple ﬁxed functions
subsets
pixels
input image
typical applications
simple shapes
characters
same time
perceptron
related system
adaline
‘ adaptive linear element ’
widrow
co-workers
functional form
model
perceptron
different approach
training
widrow
hoff
widrow
lehr
probabilistic generative models
probabilistic view
classiﬁcation
models
linear decision boundaries
simple assumptions
distribution
data
section
distinction
discriminative
generative approaches
classiﬁcation
probabilistic generative models
figure
plot
logistic sigmoid function σ
scaled pro- bit function φ
λa
λ2 = π/8
dashed blue
φ
scal- ing factor π/8
derivatives
curves
 
approach
class-conditional densities
x|ck
class priors
ck
posterior probabilities
ck|x
bayes ’ theorem
consider ﬁrst
case
classes
posterior probability
class c1
p
c1|x
= p
x|c1
p
c1
p
x|c1
p
c1
p
x|c2
p
c2
+ exp
−a
σ
= ln p
x|c1
p
c1
p
x|c2
p
c2
σ
logistic sigmoid function
σ
+ exp
−a
figure
term ‘ sigmoid ’
type
function
‘
function ’
whole real axis
ﬁnite interval
logistic sigmoid
chapters
important role
many classiﬁcation algorithms
following symmetry property
inverse
logistic sigmoid
σ
−a
− σ
= ln
− σ
logit function
log
ratio
probabilities
[ p
c1|x
/p
c2|x
]
classes
log odds

linear models for classification note
posterior probabilities
equivalent form
appearance
logistic sigmoid
signiﬁcance
x
simple functional form
situations
x
linear function
x
case
posterior probability
generalized linear model
case
k >
classes
ck|x
= p
x|ck
p
ck
j p
x|cj
p
cj
exp
ak
j exp
aj
ak = ln p
x|ck
p
ck
multiclass generalization
logistic sigmoid
quantities
normalized exponential
softmax function
smoothed version
‘ max ’ function
ak aj
j = k
p
ck|x
p
cj|x
consequences
speciﬁc forms
class- conditional densities
continuous input variables
case
discrete inputs
continuous inputs
class-conditional densities
resulting form
posterior probabilities
classes share
same covariance matrix
density
class ck
p
x|ck
=
x − µk
tς−1
x − µk
consider
case
classes
d/2
−
p
c1|x
σ
wtx + w0
w = σ−1
µ1 − µ2
σ−1µ1 + w0 =
µt
σ−1µ2 + ln p
c1
p
c2
quadratic terms
x
exponents
gaussian densities
assumption
common covariance matrices
linear function
x
argument
logistic sigmoid
result
case
two-dimensional input space x
figure
probabilistic generative models
figure
left-hand plot
class-conditional densities
classes
right
corresponding posterior probability p
c1|x
logistic sigmoid
linear function
x
surface
right-hand plot
proportion
red ink
p
c1|x
proportion
blue ink
p
c2|x
− p
c1|x
decision boundaries
surfaces
posterior probabilities
ck|x
linear functions
x
decision boundaries
input space
prior probabilities
ck
enter
bias parameter w0
changes
priors
effect
parallel shifts
decision boundary
parallel contours
constant posterior probability
general case
k
ak
x
wt k x +
wk = σ−1µk
wk0 =
µt k σ−1µk + ln p
ck
ak
x
linear functions
x
consequence
cancel- lation
quadratic terms
covariances
decision boundaries
minimum misclassiﬁcation rate
posterior probabilities
linear functions
x
linear model
assumption
covariance matrix
class- conditional density p
x|ck
own covariance matrix σk
cancellations
quadratic functions
x
rise
quadratic discriminant
quadratic decision boundaries
figure
linear models for classification
−0.5 −1 −1.5 −2 −2.5  n −2
figure
left-hand plot
class-conditional densities
classes
gaussian distribution
blue
red
green classes
same covariance matrix
right-hand plot
corresponding posterior probabilities
rgb colour vector
posterior probabilities
classes
decision boundaries
notice
boundary
green classes
same covariance matrix
other pairs
classes
maximum likelihood solution
parametric functional form
class-conditional densities
x|ck
values
parameters
prior class probabilities
ck
maximum likelihood
data set
observations
x
corresponding class labels
case
classes
gaussian class-conditional density
covariance matrix
data set
xn
tn
n =
n.
denotes class c1
denotes class c2
prior class probability p
c1
= π
p
c2
− π
data point xn
class c1
p
c1
p
c1
p
xn|c1
πn
xn|µ1
σ
class c2
p
c2
p
c2
p
xn|c2
=
− π
n
xn|µ2
σ
likelihood function
p
t|π
µ1
µ2
σ
n=1 [ πn
xn|µ1
σ
tn [
− π
n
xn|µ2
σ
t =
t1
tn
t. as
log
likelihood function
maximization
respect
terms
 n
n n=1    
n=1 n n n   
n1 n=1 n=1 n=1 n n n  
n n exercise
probabilistic generative models
log likelihood function
π
tn ln π +
− tn
ln
− π
respect
n tn = n1 n = n1 n1 + n2
n1
total number
data points
class c1
n2
total number
data points
class c2
maximum likelihood estimate
π
fraction
points
class c1
result
multiclass case
maximum likelihood estimate
prior probability
class ck
fraction
training
points
class
maximization
respect
log likelihood function
terms
µ1
tn lnn
σ
− tn
xn − µ1
tς−1
xn − µ1
+ const
respect
mean
input vectors
class c1
similar argument
corresponding result
µ2
µ1 = tnxn µ2
n2
− tn
xn
mean
input vectors
class c2
maximum likelihood solution
covariance matrix σ
terms
log likelihood function
σ
− − n=1 n
= − tn ln|σ|
− tn
ln|σ| − tn
xn − µ1
tς−1
xn − µ1
− tn
xn − µ2
tς−1
xn − µ2
− n
tr σ−1s
   d
 d
linear models for classification
s1 = s = n1 n
n1
n2 s2 = s2 s1 + n2 n
xn − µ1
xn − µ1
t
xn − µ2
xn − µ2
t.
exercise
section
section
exercise
standard result
maximum likelihood solution
gaussian distri- bution
σ = s
weighted average
covariance matrices
classes
result
k class problem
corresponding maximum likelihood solutions
parameters
class-conditional density
covariance matrix
note
approach
gaussian distributions
classes
outliers
maximum likelihood estimation
gaussian
discrete features
case
discrete feature values
simplicity
binary feature values
∈
extension
general discrete features
d inputs
general distribu- tion
table
numbers
class
independent variables
summation constraint
number
features
restricted representa- tion
naive bayes assumption
feature values
class ck
class-conditional distributions
form p
x|ck
i=1 ki
− µki
µxi
d independent parameters
class
ak
x
xi ln µki +
− xi
ln
− µki
ln p
ck
linear functions
input values
case
k =
classes
logistic sigmoid formulation
anal- ogous results
discrete variables
m >
states
exponential family
gaussian
discrete inputs
posterior class probabilities
generalized linear models
logistic sigmoid
k =   
probabilistic discriminative models
classes
softmax
k �
classes
activation functions
particular cases
general result
class-conditional densities
x|ck
members
exponential family
distributions
distribution
x
form
form
members
exponential family
p
x|λk
= h
x
g
λk
k u
x
λt
attention
subclass
such distributions
u
x
use
scaling parameter s
restricted set
exponential family class-conditional densities
form p
s h
s x g
λk
s λt k x
note
class
own parameter vector λk
classes share
same scale parameter s.
two-class problem
expression
class-conditional densities
posterior class probability
logistic sigmoid
linear function
x
x
=
λ1 − λ2
tx + ln g
λ1
ln g
λ2
ln p
c1
ln p
c2
k-class problem
class-conditional density ex- pression
ak
x
λt k x +
g
λk
ln p
ck
linear function
x
probabilistic discriminative models
two-class classiﬁcation problem
posterior probability
class c1
logistic sigmoid
linear function
x
wide choice
class-conditional distributions p
x|ck
multiclass case
posterior probability
class ck
softmax transformation
linear function
x
speciﬁc choices
class-conditional densities
x|ck
maximum likelihood
parameters
densities
class priors
ck
bayes ’ theorem
posterior class probabilities
alternative approach
functional form
generalized linear model
parameters
maximum likelihood
efﬁcient algorithm
such solutions
squares
irls
indirect approach
parameters
generalized linear model
class-conditional densities
class priors
linear models for classification x2
−1
φ2
figure
illustration
role
nonlinear basis functions
linear classiﬁcation models
left plot
original input space
x2
data points
classes
‘ gaussian ’ basis functions φ1
x
φ2
x
space
centres
green crosses
contours
green circles
right-hand plot
corresponding feature space
φ2
linear decision boundary
logistic regression model
form
section
nonlinear decision boundary
original input space
black curve
left-hand plot
bayes ’ theorem
example
generative modelling
model
generate synthetic data
values
x
marginal distribution p
x
direct approach
likelihood function
conditional distribution p
ck|x
form
discriminative training
advantage
discriminative approach
adaptive parameters
improved predictive performance
class-conditional density assumptions
poor approximation
true dis- tributions
fixed basis functions
chapter
classiﬁcation models
original input vector x
algorithms
ﬁxed nonlinear transformation
inputs
vector
basis functions φ
x
decision boundaries
feature space φ
correspond
decision boundaries
original x space
figure
classes
feature space φ
x
original observation space x
note
discussion
linear models
regression
probabilistic discriminative models
basis functions
φ0
x
correspond- ing parameter
role
bias
remainder
chapter
ﬁxed basis function transformation φ
x
useful similarities
regression models
chapter
many problems
practical interest
signiﬁcant overlap
class-conditional densities
x|ck
probabilities
ck|x
values
x
such cases
opti- mal solution
posterior probabilities
standard decision theory
chapter
note
nonlinear transformations
x
such class overlap
level
overlap
none
original observation space
suitable choices
nonlinearity
process
posterior probabilities
such ﬁxed basis function models
important limitations
later chapters
basis
data
limitations
models
ﬁxed nonlinear basis functions
important role
applications
discussion
such models
key concepts
understanding
complex counterparts
logistic regression
treatment
generalized linear models
problem
two-class classiﬁcation
discussion
generative approaches
section
general assumptions
posterior probability
class c1
logistic sigmoid
linear function
feature vector φ
p
c1|φ
y
φ
σ
p
c2|φ
− p
c1|φ
σ
·
logistic sigmoid function
terminology
statistics
model
logistic regression
model
classiﬁcation
regression
wtφ
m-dimensional feature space φ
model
m adjustable parameters
contrast
gaussian class conditional densities
maximum likelihood
parameters
means
m
m +
/2 parameters
covariance matrix
class
p
c1
total
m
m +5
/2+1 parameters
m
contrast
linear dependence
m
number
parameters
logistic regression
large values
m
clear advantage
logistic regression model
maximum likelihood
parameters
logistic regression model
use
derivative
logistic sig- moid function
terms
sigmoid function
dσ da = σ
− σ
section
exercise
  
n n
n exercise
section
exercise
e
w
− ln p
t|w
tn ln yn +
− tn
ln
− yn
yn = σ
= wtφn
gradient
error function
respect
∇e
w
=
yn − tn
φn n=1
use
factor
derivative
logistic sigmoid
simpliﬁed form
gradient
log likelihood
contribution
gradient
data point n
‘ error ’ yn − tn
target value
prediction
model
basis function vector φn
furthermore
comparison
same form
gradient
sum-of-squares error function
linear regression model
use
result
sequential algorithm
patterns
time
weight vectors
∇en
nth term
maximum likelihood
severe over-ﬁtting
data sets
maximum likelihood so- lution
hyperplane
classes
magnitude
w
inﬁnity
case
logistic sigmoid function
feature space
heaviside step function
training point
class k
posterior probability p
ck|x
continuum
such solutions
hyperplane
rise
same pos- terior probabilities
training data points
figure
maximum likelihood
way
such solution
solution
practice
choice
optimization algo- rithm
parameter initialization
note
problem
number
data points
number
parameters
model
training data set
singularity
inclusion
map solution
w
regularization term
error function
linear models for classification
data set
φn
tn
tn ∈
φn = φ
xn
n =
n
likelihood function
t|w
− yn
ytn
t =
t1
tn
t
yn = p
c1|φn
error function
negative logarithm
likelihood
cross- entropy error function
form    
n n
n  n
probabilistic discriminative models
iterative
squares
case
linear regression models
chapter
maxi- mum likelihood solution
assumption
gaussian noise model
closed-form solution
consequence
quadratic dependence
log likelihood function
parameter vector w.
logistic regression
closed-form solution
nonlinearity
logistic sigmoid function
departure
quadratic form
error function
unique minimum
error function
efﬁcient iterative technique
newton-raphson iterative optimization scheme
local quadratic approximation
log likelihood function
newton-raphson update
function e
w
form
fletcher
bishop
nabney
h
hessian matrix
elements
second derivatives
e
w
respect
components
w. w
= w
h−1∇e
w
newton-raphson method
linear regression model
sum-of-squares error function
gradient
hessian
error function
∇e
w
=
wtφn − tn
= φtφw − φtt h = ∇∇e
w
φnφt n = φtφ w
= w
−
φtφ
=
φtφ
φtφw
φtt
standard least-squares solution
note
error func- tion
case
newton-raphson formula
exact solution
step
newton-raphson update
cross-entropy error function
logistic regression model
from
gradient
hessian
error function
∇e
w
= n=1
yn − tn
= φt
y − t
h = ∇∇e
w
n=1 yn
− yn
n = φtrφ
section
φ
n × m design matrix
nth row
φt raphson
form
newton-  
linear models for classification rnn = yn
− yn
use
n × n diagonal matrix r
elements
hessian
w
weight- ing matrix r
fact
error function
property
< yn
form
logistic sigmoid function
arbitrary vector u
hessian matrix h
positive deﬁnite
error function
concave function
w
hence
unique minimum
newton-raphson update formula
logistic regression model
be- exercise
w
= w
−
φtrφ
−1φt
y − t
=
φtrφ
=
φtrφ
φtrφw
φt
y − t
z
n-dimensional vector
elements
= φw
r−1
y − t
update formula
form
set
normal equations
weighted least-squares problem
matrix r
parameter vector w
normal equations
time
new weight vector w
revised weighing matrix r.
reason
algorithm
squares
irls
rubin
weighted least-squares problem
elements
diagonal weighting matrix r
variances
mean
variance
t
logistic regression model
e [ t ] = σ
x
y var [ t ] = e [ t2 ] − e [
= σ
x
σ
x
= y
− y
property t2 = t
t ∈
fact
irls
solution
problem
space
= wtφ
quantity zn
nth element
z
simple interpretation
effective target value
space
local linear approximation
logistic sigmoid function
current operating point w
w
w
= φt w
dan dyn
yn − tn
yn
− yn
tn − yn
= zn
  n    
k n n k k=1  n  k
probabilistic discriminative models
multiclass logistic regression
discussion
generative models
multiclass classiﬁcation
large class
distributions
posterior probabilities
softmax transformation
linear functions
feature variables
p
ck|φ
yk
φ
exp
ak
j exp
aj
‘ activations
ak
ak = wt k φ
maximum likelihood
class-conditional densities
class priors
corresponding posterior probabilities
bayes ’ theorem
parameters
wk
use
maximum likelihood
parameters
wk
model
derivatives
yk
respect
activations
∂yk ∂aj = yk
ikj − yj
ikj
elements
identity matrix
likelihood function
1-of-k coding scheme
target vector tn
feature vector
class ck
binary vector
elements
element k
likelihood function
p
t|w1
wk
n=1 k=1 p
ck|φn
tnk = ytnk nk
ynk = yk
φn
t
n × k matrix
target variables
elements
negative logarithm
e
w1
wk
− ln p
t|w1
wk
− tnk ln ynk
n=1 k=1
cross-entropy error function
multiclass classiﬁcation problem
gradient
error function
respect
param- eter vectors
use
result
derivatives
softmax function
∇wj e
w1
wk
=
ynj − tnj
φn n=1
section
exercise
exercise
 n
linear models for classification k tnk
same form
use
gradient
sum-of-squares error function
linear model
cross-entropy error
logistic regression model
prod- uct
error
ynj − tnj
basis function φn
again
sequential algorithm
patterns
time
weight vectors
derivative
log likelihood function
linear regres- sion model
respect
parameter vector w
data point n
form
‘ error ’ yn − tn
feature vector φn
combination
logistic sigmoid activation function
cross-entropy error function
softmax activation function
multiclass cross-entropy error function
same simple form
example
general result
section
batch algorithm
newton-raphson update
corresponding irls algorithm
multiclass problem
evaluation
hessian matrix
blocks
size m × m
block j
k
exercise
∇wk∇wj e
w1
wk
− ynk
ikj − ynj
n. n=1
two-class problem
hessian matrix
multiclass logistic regres- sion model
positive deﬁnite
error function
unique minimum
practical details
irls
multiclass case
bishop
nabney
probit regression
broad range
class-conditional distributions
exponential family
posterior class probabilities
softmax
transformation
linear function
feature vari- ables
choices
class-conditional density
rise
simple form
posterior probabilities
instance
class-conditional densities
gaussian mixtures
other types
discriminative probabilistic model
purposes
chapter
two-class case
frame- work
generalized linear models
p
t =
= f
= wtφ
f
·
activation function
way
alternative choice
link function
noisy threshold model
input
= wtφn
target value
tn
� θ tn
 
probabilistic discriminative models
figure
schematic example
probability density p
θ
blue curve
example
mixture
gaussians
cumulative distribution function f
red curve
note
value
blue curve
point
vertical green line
slope
red curve
same point
value
red curve
point
area
blue curve
shaded green region
stochastic threshold model
class label
value t
value
= wtφ
threshold
oth- erwise
value t
activation function
cumulative distribution function f
value
θ
probability density p
θ
corresponding activation function
cumulative distribution function f
= p
θ
−∞
figure
speciﬁc example
density p
θ
zero mean
unit variance gaussian
corresponding cumulative distribution function
φ
= n
θ|0
dθ −∞
probit function
sigmoidal shape
logistic sigmoid function
figure
note
use
gen- eral gaussian distribution
model
re-scaling
linear coefﬁcients
many numerical packages
evaluation
related function
erf
√π
exp
−θ2/2
dθ
exercise
erf function
error function
error function
machine
model
probit function
φ
√2 erf
generalized linear model
probit activation function
probit regression
parameters
model
maximum likelihood
straightforward extension
ideas
practice
results
probit regression
logistic regression
linear models for classification    
use
probit model
bayesian treatments
logistic regression
section
issue
practical applications
outliers
instance
errors
input vector x
misla- belling
target value t.
such points
long way
wrong side
ideal decision boundary
classiﬁer
note
probit regression models
respect
tails
logistic sigmoid decay
exp
−x
x → ∞
whereas
probit activation function
exp
−x2
probit model
outliers
logistic
probit models
data
effect
mislabelling
probabilistic model
probability
target value t
wrong value
opper
winther
target value distribution
data point x
form p
t|x
=
−
σ
x
+
− σ
x
σ
x
activation function
input vector x
advance
hyperparameter
value
data
= +
σ
x
canonical link functions
linear regression model
gaussian noise distribution
error function
negative log likelihood
respect
parameter vector w
contribution
error function
data point n
form
‘ error ’ yn − tn
feature vector φn
yn = wtφn
combination
logistic sigmoid activation function
cross-entropy error function
softmax activation function
multiclass cross-entropy error function
same simple form
general result
conditional distribution
target
exponential family
corresponding choice
activation function
canonical link function
use
form
exponential family distribu- tions
note
assumption
exponential family distribu- tion
target variable t
contrast
section
input vector x
consider conditional distributions
target variable
form p
t|η
s
s h t s g
η
exp
ηt s
same line
argument
derivation
result
conditional mean
t
y
y ≡ e [ t|η ] = −s d dη ln g
η
  n  
n n   
n
dηn n
laplace approximation
thus y
η
relation
η = ψ
y
nelder
wedderburn
generalized linear model
y
nonlinear function
linear combination
input
feature
y = f
wtφ
f
·
activation function
machine learning literature
f−1
·
link function
statistics
log likelihood function
model
function
η
ln p
t|η
s
n=1 ln p
tn|η
s
ln g
ηn
ηntn s + const
observations share
common scale parameter
noise variance
gaussian distribution
instance
derivative
log likelihood
respect
model parameters
∇w ln p
t|η
s
ln g
ηn
tn s dηn dyn dyn dan∇an =
s
tn − yn
ψ
yn
f
φn
= wtφn
yn = f
result
e [ t|η ]
considerable simpliﬁcation
particular form
link function f−1
y
f−1
y
= ψ
y
f
ψ
y
hence f
ψ
ψ
y
= f−1
y
= ψ
hence f
ψ
y
case
gradient
error function reduces
ln e
w
s
yn − tn
φn
n=1
gaussian s = β−1
whereas
logistic model
laplace approximation
section
bayesian treatment
logistic regression
bayesian treatment
linear regression models
sections
   
chapter
chapter
linear models for classification
parameter vector w
posterior distribution
form
approximation
book
range
techniques
analytical approximations
numerical sampling
simple
framework
laplace ap- proximation
gaussian approximation
probability density
set
continuous variables
case
single contin- uous variable z
distribution p
z
f
z
dz
normalization coefﬁcient
z = value
z
laplace
goal
gaussian approx- imation q
z
mode
distribution p
z
ﬁrst step
mode
p
z
other words
point z0
p
z0
p
z
= f
z
z df
z
dz
z=z0 a gaussian distribution
property
logarithm
quadratic function
variables
taylor expansion
ln f
z
mode z0
note
ﬁrst-order term
taylor expansion
z0
local maximum
distribution
ln f
z
ln f
z0
a
z − z0
a = − d2 dz2 ln f
z
z=z0 f
z
 f
z0
z − z0
normalized distribution q
z
use
standard result
normalization
q
z
exp
−
z − z0
laplace approximation
figure
note
gaussian approximation
precision
other words
stationary point z0
local maximum
second derivative
f
z
point z0
laplace approximation
−2
−2
figure
illustration
laplace approximation
distribution p
z
∝ exp
−z2/2
σ
σ
z
logistic sigmoid function
σ
z
=
+ e−z
−1
left plot
normalized distribution p
z
yellow
laplace approximation
mode z0
p
z
right plot
negative logarithms
corresponding curves
laplace method
distribution p
z
= f
z
/z
m-dimensional space z
stationary point
gradient ∇f
z
stationary point
f
z
ln f
z0
−
z − z0
ta
− z0
m × m hessian matrix a
a = − ∇∇ ln f
z
|z=z0
gradient operator
exponential
sides
z
 f
z0
exp
z − z0
ta
− z0
distribution q
z
z
appropriate normalization coef- ﬁcient
inspection
standard result
normalized multivariate gaussian
q
z
= |a|1/2
m/2
−
z − z0
ta
− z0
= n
z|z0
a−1
|a|
determinant
a
gaussian distribution
precision matrix
a
positive deﬁnite
stationary point z0
local maximum
minimum
saddle point
order
laplace approximation
mode z0
hessian matrix
mode
practice
mode
form
numerical optimization algorithm
bishop   
+
linear models for classification
nabney
distributions
practice
mul- timodal
different laplace approximations
mode
note
normalization constant z
true distri- bution
order
laplace method
result
central limit theorem
posterior distribution
model
number
observed data points
laplace approximation
situations
number
data points
major weakness
laplace approximation
gaussian distribution
real variables
other cases
laplace approximation
transformation
instance
� τ < ∞
laplace approximation
ln τ
serious limitation
laplace framework
aspects
true distribution
speciﬁc value
important global properties
chapter
alternative approaches
global perspective
model comparison
bic as
distribution p
z
approxi- mation
normalization constant z
approximation
= f
z
dz  f
z0
= f
z0
m/2 |a|1/2
−
z − z0
ta
− z0
dz
integrand
use
standard result
normalized gaussian distribution
result
approximation
model evidence
section
central role
bayesian model comparison
data set d
set
models
mi
parameters
θi
model
likelihood function p
d|θi
mi
prior p
θi|mi
parameters
model evi- dence p
d|mi
various models
conditioning
mi
notation
bayes ’
model evidence
f
θ
p
d|θ
p
θ
z = p
d
result
d|θ
p
θ
dθ
p
d
ln p
d
ln p
d|θmap
ln p
θmap
m
ln|a|
ln
− occam factor exercise
exercise
section
bayesian logistic regression
θmap
value
θ
mode
posterior distribution
hessian matrix
second derivatives
negative log
= −∇∇ ln p
d|θmap
p
θmap
−∇∇ ln p
θmap|d
ﬁrst term
right hand side
log
optimized parameters
terms
‘ occam factor ’
model complexity
gaussian prior distribution
parameters
hessian
full rank
ln p
d
ln p
d|θmap
m ln n
n
number
data points
m
number
parameters
θ
additive constants
bayesian information criterion
bic
schwarz criterion
schwarz
note
aic
complexity
complexity
aic
bic
virtue
misleading results
assumption
hessian matrix
full rank
parameters
‘ well-determined ’
result
accurate estimate
model evidence
laplace approximation
context
neural networks
section
bayesian logistic regression
bayesian treatment
logistic regression
exact bayesian infer- ence
logistic regression
evaluation
posterior distribution
normalization
product
prior distribution
likelihood function
product
logistic sigmoid functions
data point
evaluation
predictive distribution
application
laplace approximation
problem
bayesian logistic regression
spiegelhalter
lauritzen
mackay
laplace approximation recall
section
laplace approximation
mode
posterior distribution
gaussian centred
mode
evaluation
second derivatives
log posterior
hessian matrix
gaussian representation
posterior distribution
gaussian prior
general form p
w
= n
w|m0
s0
n     
n
linear models for classification
m0
s0
ﬁxed hyperparameters
posterior distribution
w
t =
t1
tn
t.
log
sides
prior distribution
likelihood function
w|t
∝ p
w
p
t|w
ln p
w|t
= −
w − m0
w − m0
tn ln yn +
− tn
ln
− yn
const
yn = σ
wtφn
gaussian approximation
posterior dis- tribution
posterior distribution
map
maximum posterior
solution wmap
mean
gaussian
covariance
inverse
matrix
second derivatives
negative log likelihood
form sn = −∇∇ ln p
w|t
s−1
+ yn
− yn
φnφt n.
gaussian approximation
posterior distribution
form q
w
= n
w|wmap
sn
gaussian approximation
posterior distribution
task
respect
distribution
order
predictions
predictive distribution
predictive distribution
class c1
new feature vector φ
x
respect
posterior distribution p
w|t
gaussian distribution q
w
p
c1|φ
t
= p
c1|φ
w
p
w|t
dw  σ
wtφ
q
w
dw
corresponding probability
class c2
p
c2|φ
t
p
c1|φ
t
predictive distribution
function σ
wtφ
de- pends
w
projection
φ.
= wtφ
wtφ
= δ
− wtφ
σ
da
δ
·
dirac delta function
σ
wtφ
q
w
dw = σ
p
da
            
bayesian logistic regression
p
= δ
− wtφ
q
w
dw
p
delta function
linear constraint
w
marginal distribution
joint distribution q
w
directions
q
w
section
marginal distribution
mean
covariance
distribution
moments
order
integration
w
µa = e
] = p
da = q
w
wtφ dw = wt mapφ
result
variational posterior distribution q
w
= var
] = σ2 p
da a2 − e
n φ
= q
w
wtφ
−
mt dw = φtsn φ
note
distribution
same form
predictive distribution
linear regression model
noise variance
variational approximation
predictive distribution
p
c1|t
σ
p
da = σ
n
a|µa
da
result
use
results
marginal
gaussian distribution
section
convolution
logistic sig- moid
good approx- imation
spiegelhalter
lauritzen
mackay
barber
bishop
use
close similarity
logistic sigmoid function σ
probit function φ
order
approximation
logistic function
hori- zontal axis
σ
φ
λa
suitable value
λ
functions
same slope
origin
λ2 = π/8
similarity
logistic sigmoid
probit function
choice
λ
figure
advantage
probit function
convolution
terms
probit function
φ
λa
n
a|µ
σ2
da = φ µ
λ−2 + σ2
exercise
exercise
exercise
   

linear models for classification
approximation σ
 φ
λa
probit functions
sides
equation
following approximation
convo- lution
logistic sigmoid
gaussian σ
n
a|µ
σ2
da  σ κ
σ2
µ
result
approximate predictive distribution κ
σ2
=
+ πσ2/8
−1/2
form κ
µa p
c1|φ
t
= σ
µa
σ2
κ
σ2
de- note
decision boundary corresponding
c1|φ
t
µa =
decision boundary
map value
w.
decision criterion
misclassiﬁca- tion rate
equal prior probabilities
marginalization
w
ef- fect
complex decision criteria
important role
marginalization
logistic sigmoid model
gaussian approximation
posterior distribution
context
variational inference
figure
exercises
 
set
data points
xn
convex hull
set
points
x = αnxn
αn
n αn =
second set
points
yn
corresponding convex hull
deﬁnition
sets
points
wtxn + w0
vector wtyn +
yn
convex hulls
xn
sets
points
convex hulls
w
scalar w0
 
minimization
sum-of-squares error function
target vectors
training
linear constraint attn + b =
tn corresponds
nth row
matrix t
consequence
constraint
elements
model prediction y
x
least-squares solution
constraint
aty
x
b =
basis functions φ0
x
corresponding parameter w0
role
bias
 
result
exercise
multiple linear constraints
target vectors
same constraints
least-squares prediction
linear model

www show
maximization
class separation criterion
respect
lagrange multiplier
constraint wtw
result
w ∝
m2 − m1

use
fisher criterion
form

deﬁnitions
between-class
within-class covariance matrices
choice
target values
section
expression
sum-of-squares error function
form

www show
logistic sigmoid function
property σ
−a
− σ
inverse
σ−1
y
y/
− y

using
result
posterior class probability
two-class generative model
gaussian densities
results
parameters

www
generative classiﬁcation model
k classes
prior class probabilities
ck
πk
general class-conditional densities
φ|ck
φ
input feature vector
suppose
training data
φn
tn
n =
n
tn
binary target vector
length k
1-of- k
scheme
components
ijk
class ck
data points
model
maximum-likelihood solution
prior probabilities
 n πk = nk n
nk
number
data points
class ck
 
classiﬁcation model
exercise
class-conditional densities
gaussian distributions
covari- ance matrix
p
φ|ck
n
φ|µk
σ
show
maximum likelihood solution
mean
gaussian distribution
class ck
µk =
nk tnkφn n=1
 k k=1  n
linear models for classification
mean
feature vectors
class ck
maximum likelihood solution
covariance matrix
σ = nk n sk
sk
nk tnk
φn − µk
φn − µk
t. n=1 thus σ
weighted average
covariances
data
class
weighting coefﬁcients
prior probabilities
classes
 
classiﬁcation problem
k classes
feature vector φ
components
l discrete states
values
components
1-of-l binary coding scheme
class ck
m components
φ
class-conditional density
respect
feature vector components
quantities
argument
softmax function
posterior class probabilities
linear functions
components
φ
note
example
naive bayes model
section

verify
relation
derivative
logistic sigmoid func- tion

www
use
result
derivative
logistic sig- moid
derivative
error function
logistic regression model

linearly separable data set
maximum likelihood solution
logistic regression model
vector w
decision boundary wtφ
x
classes
magnitude
w
inﬁnity
 
hessian matrix h
logistic regression model
positive deﬁnite
r
diagonal matrix
elements yn
− yn
yn
output
logistic regression model
input vector xn
hence show
error function
concave function
w
unique minimum

binary classiﬁcation problem
observation xn
classes
procedure
training data
training points
data point xn
value t
class label
value πn
probability
probabilistic model p
t =
log likelihood function appropriate
data set

www show
derivatives
softmax activation function
ak

result
derivatives
softmax activation function
gradients
cross-entropy error

www write
expressions
gradient
log likelihood
corresponding hessian matrix
probit regression model
sec- tion
quantities
model
irls
 
hessian matrix
multiclass logistic regression problem
positive semideﬁnite
note
full hessian matrix
problem
size m k × m k
m
number
parameters
k
number
classes
positive semideﬁnite property
product uthu
u
arbitrary vector
length m k
jensen ’ s inequality

show
probit function
erf function

result
expression
log model evi- dence
laplace approximation
 
www
exercise
bic result
laplace approximation
model evidence
prior
parameters
form p
θ
n
θ|m
v0
log model evidence
laplace approximation
form
θmap − m
θmap − m
tv−1 ln p
d
ln p
d|θmap
ln|h| + const
h
matrix
second derivatives
log likelihood ln p
d|θ
θmap
prior
v−1
term
right-hand side
furthermore
case
data
h
sum
terms
data point
log model evidence
form
bic expression
 
results
section
result
marginal- ization
logistic regression model
respect
gaussian posterior distribu- tion
parameters
 
suppose
logistic sigmoid σ
scaled probit function φ
λa
φ
λ
derivatives
functions
λ2 = π/8
linear models for classification
 
exercise
relation
convolution
probit function
gaussian distribution
derivative
left- hand side
respect
derivative
right-hand side
sides
respect
constant
integration vanishes
left-hand side
change
= µ + σz
z
left-hand side
relation
gaussian integral
z
neural networks
chapters
models
regression
classiﬁcation
com-
linear combinations
ﬁxed basis functions
such models
computational properties
practical applicability
curse
dimensionality
order
such models
large- scale problems
basis functions
data
support vector machines
svms
chapter
address
basis functions
training data points
subset
training
advantage
svms
training
nonlinear optimization
objective function
solution
optimization problem
number
basis functions
resulting models
number
points
size
training set
relevance vector machine
section
subset
ﬁxed set
basis functions
results
neural networks sparser models
svm
probabilistic outputs
expense
nonconvex optimization
training
alternative approach
number
basis functions
advance
other words
parametric forms
basis func- tions
parameter values
training
successful model
type
context
pattern recognition
feed-forward neural network
multilayer perceptron
chapter
fact
‘ multilayer perceptron ’
misnomer
model
multi- ple layers
logistic regression models
continuous nonlinearities
multiple perceptrons
discontinuous nonlinearities
many applications
model
support vector machine
same generalization performance
price
compactness
relevance vector machine
like- lihood function
basis
network training
convex function
model parameters
practice
substantial computational resources
training phase
order
compact model
new data
term ‘ neural network ’
origins
attempts
mathematical rep- resentations
information processing
biological systems
mcculloch
pitts
widrow
hoff
rosenblatt
rumelhart
al.
wide range
different models
subject
exaggerated claims
biological plau- sibility
perspective
practical applications
pattern recognition
biological realism
unnecessary constraints
focus
chapter
neural networks
efﬁcient models
statistical pattern recognition
attention
speciﬁc class
neu- ral networks
practical value
multilayer perceptron
functional form
network model
speciﬁc parameterization
basis functions
prob- lem
network parameters
maximum likelihood frame- work
solution
nonlinear optimization problem
evaluation
derivatives
log likelihood function
respect
net- work parameters
technique
error backpropagation
backpropagation framework
other derivatives
jacobian
hessian matrices
various approaches
regulariza- tion
neural network training
relationships
extensions
neural network model
gen- eral framework
conditional probability distributions
mixture density networks
use
bayesian treatments
neural net- works
additional background
neural network models
bishop
   m   i=1 d m
feed-forward network functions
feed-forward network functions
linear models
regression
classiﬁcation
chapters
linear combinations
ﬁxed nonlinear basis functions φj
x
form y
f wjφj
x
j=1
f
·
nonlinear activation function
case
classiﬁcation
identity
case
regression
goal
model
basis functions φj
x
parameters
parameters
coefﬁcients
wj
training
course
many ways
parametric nonlinear basis functions
neural networks
basis functions
same form
basis function
nonlinear function
linear combination
inputs
coefﬁcients
linear combination
adaptive parameters
basic neural network model
series
functional transformations
first
m linear combinations
input variables
xd
form aj = w
ji xi + w
j0
j
m
superscript
corresponding param- eters
ﬁrst ‘ layer ’
network
parameters
ji
weights
parameters
j0
biases
nomenclature
chapter
quantities
activations
nonlinear activation function h
·
zj = h
aj
quantities
outputs
basis functions
context
neural networks
hidden units
nonlinear functions
·
sigmoidal functions
logistic sigmoid
‘ tanh ’ function
values
output unit activations
= w
kj zj + w
k0 j=1
k
k
k
total number
outputs
transformation cor- responds
second layer
network
w
k0
bias parameters
output unit activations
appropriate activation function
set
network outputs yk
choice
activation function
nature
data
assumed distribution
target variables
  m   d 
neural networks figure
network diagram
two- layer neural network corre- sponding
input
output variables
nodes
weight parameters
links
nodes
bias pa- rameters
links
additional input
hidden variables
z0
arrows
direc- tion
information ﬂow
network
forward propagation
inputs x1 x0
units w
m d w
km zm z1 z0 yk
y1 w
same considerations
linear models
chapters
standard regression problems
activation function
identity
yk = ak
multiple binary classiﬁcation problems
output unit activation
logistic sigmoid function
yk = σ
ak
σ
+ exp
−a
multiclass problems
softmax activation function
form
choice
output unit activation function
detail
sec- tion
various stages
overall network function
sigmoidal output unit activation functions
form yk
w
kj h w
ji xi + w
j0 + w
k0
j=1 i=1
set
weight
bias parameters
vector w. thus
neural network model
nonlinear function
set
input variables
xi
set
output variables
yk
vector w
adjustable parameters
function
form
network diagram
figure
process
forward propagation
information
network
diagrams
probabilistic graphical models
kind
chapter
internal nodes
deterministic variables
stochastic ones
reason
different graphical   i=0 d m    d
feed-forward network functions
notation
kinds
model
probabilistic interpretation
neural network
section
bias parameters
set
weight parameters
additional input variable x0
value
x0 =
form aj = w
ji xi
second-layer biases
second-layer weights
overall network function
w
kj h w
ji xi
j=0 i=0
figure
neural network model
stages
processing
perceptron model
section
reason
neural network
multilayer perceptron
mlp
a key difference
perceptron
neural net- work
continuous sigmoidal nonlinearities
hidden units
per- ceptron
step-function nonlinearities
neural network func- tion
respect
network parameters
property
central role
network training
activation functions
hidden units
network
such network
equivalent network
hidden units
fact
composition
successive linear transformations
linear transformation
number
hidden units
number
input
output units
transforma- tions
network
general possible linear trans- formations
inputs
information
dimensionality reduction
hidden units
section
networks
linear units
rise
component analysis
little interest
multilayer networks
linear units
network architecture
figure
practice
instance
additional layers
consisting
weighted linear combination
form
element-wise transformation
nonlinear activation func- tion
note
confusion
literature
terminology
number
layers
such networks
network
figure
3-layer network
number
layers
units
inputs
units
single-hidden-layer network
number
layers
hidden units
terminology
figure
two-layer network
number
layers
adap- tive weights
network properties
generalization
network architecture
skip-layer con- nections
corresponding adaptive parameter
 inputs
x1 
neural networks figure
example
neural network
general feed-forward topology
note
hidden
output unit
bias parameter
clarity
z1 z2 z3 y2
y1 instance
two-layer network
inputs
outputs
principle
network
sigmoidal hidden units
skip layer con- nections
input values
small ﬁrst-layer weight
operating range
hidden unit
large weight value
hidden unit
output
practice
skip-layer connections
network
possible connections
layer
example
sparse network architecture
convolutional neural networks
section
direct correspondence
network diagram
mathematical function
general network mappings
complex network diagrams
feed-forward architecture
other words
cycles
outputs
deterministic functions
inputs
simple example
figure
output
unit
network
function
zk = h wkjzj j
sum
units
connections
unit k
bias param- eter
summation
set
values
inputs
network
successive application
activations
units
network
output units
approximation properties
feed-forward networks
funahashi
cybenko
hornik
al.
stinchecombe
white
cotter
ito
hornik
kreinovich
ripley
neural networks
universal ap- proximators
example
two-layer network
linear outputs
continuous function
compact input domain
arbitrary accu- racy
network
large number
hidden units
result
wide range
hidden unit activation functions
polynomi- als
such theorems
key problem
suitable parameter values
set
training data
later sections
chapter
illustration
ca- pability
multilayer perceptron
different func- tions
f
x
= x2
b
f
x
sin
x
c
x
|x|
d
f
x
h
x
h
x
heaviside step function
case
n
data points
blue dots
sam- pled uniformly
x
interval
corresponding val- ues
f
x
data points
two- layer network
hidden units
‘ tanh ’ activation functions
linear output units
network functions
red curves
outputs
hidden units
curves
feed-forward network functions
c
b
d
effective solutions
problem
maximum likelihood
bayesian approaches
capability
two-layer network
broad range
functions
figure
ﬁgure
individual hidden units
ﬁnal function
role
hidden units
simple classiﬁcation problem
figure
synthetic classiﬁcation data
appendix a
weight-space symmetries
property
feed-forward networks
role
bayesian model comparison
multiple distinct choices
weight vector w
rise
same mapping function
inputs
outputs
chen
two-layer network
form
figure
m
units
‘ tanh ’ activation functions
full connectivity
layers
sign
weights
bias
particular hidden unit
input pattern
sign
activation
hidden unit
‘ tanh ’
odd function
tanh
−a
− tanh
transformation
sign
weights
hidden unit
signs
particular group
weights
bias
input–output
function
network
different weight vectors
rise
same mapping function
m hidden units
m such ‘ sign-ﬂip ’
neural networks figure
example
solution
simple two- class classiﬁcation problem
synthetic data
neural network
inputs
hidden units
‘ tanh ’ activation functions
single output
logistic sigmoid activa- tion function
dashed blue lines
z
contours
hidden units
red line
y
decision surface
net- work
comparison
green line
optimal decision boundary
distributions
data
−1 −2 −2
symmetries
weight vector
set
equivalent weight vectors
imagine
values
weights
bias
particular hidden unit
values
weights
bias
different hidden unit
again
network input–output
function
corre- sponds
different choice
weight vector
m hidden units
weight vector
set
m
equivalent weight vectors
inter- change symmetry
m
different orderings
hidden units
network
overall weight-space symmetry factor
m
networks
layers
weights
total level
symmetry
product
such factors
layer
hidden units
factors
symmetries
weight space
possible accidental symmetries
choices
weight val- ues
existence
symmetries
particular property
‘ tanh ’ function
applies
wide range
activation functions
k˙urkov´a
kainen
many cases
symmetries
weight space
little practi- cal consequence
section
situation
account
network training so
neural networks
general class
parametric nonlinear functions
vector x
input variables
vector y
output variables
simple approach
problem
network parameters
analogy
discussion
polynomial curve
section
sum-of-squares error function
training
set
input vectors
xn
n =
n
corresponding set
  
n n n  n
vectors
tn
error function e
w
y
w
− tn2
network
general view
network training
ﬁrst
probabilistic interpretation
network outputs
many advantages
probabilistic predictions
section
clearer motivation
choice
output unit nonlinearity
choice
error function
regression problems
moment
single target variable t
real value
discussions
section
t
gaussian distribution
x- dependent mean
output
neural network
p
t|x
w
n t|y
β−1
β
precision
inverse variance
gaussian noise
course
restrictive assumption
section
approach
general conditional distributions
conditional distribution
output unit activation function
identity
network
continuous function
x
data set
n
observations x =
x1
target values
=
t1
corresponding likelihood function p
t|x
w
β
p
tn|xn
w
β
negative logarithm
error function
y
w
− tn
− n
ln β + n
ln
parameters
β
section
cuss
bayesian treatment
neural networks
maximum likelihood approach
note
neural networks literature
con- sider
minimization
error function
maximization
log
likelihood
convention
determi- nation
w.
likelihood function
sum-of-squares error function
e
w
y
w
− tn
n=1
 
n n
neural networks
multiplicative constants
value
w
e
w
wml
maximum likelihood solution
practice
nonlinearity
network function y
w
error e
w
practice local maxima
likelihood
local minima
error function
section
wml
value
β
negative log likelihood
βml
n
y
wml
− tn
note
iterative optimization
wml
multiple target variables
inde- pendent conditional
x
w
noise precision β
conditional distribution
target values
p
t|x
w
n t|y
β−1i
same argument
single target
maximum likelihood weights
sum-of-squares error function
noise precision
exercise
exercise
βml
n k y
xn
wml
tn2 n=1
k
number
target variables
assumption
independence
expense
complex optimization problem
recall
section
natural pairing
error function
negative log likelihood
output unit activation function
regression case
network
output activation function
identity
yk = ak
corresponding sum-of-squares error function
property ∂e ∂ak = yk − tk
use
error backpropagation
section
case
binary classiﬁcation
single target variable t
denotes class c1
denotes class c2
discussion
canonical link functions
section
network
single output
activation function
logistic sigmoid y = σ
+ exp
−a
�
y
conditional probability p
c1|x
p
c2|x
−
conditional distribution
targets
inputs
bernoulli distribution
form p
t|x
w
= y
t
− y
  
k n k k=1  n  n  k
training set
independent observations
error function
negative log likelihood
cross-entropy error function
form e
w
tn ln yn +
− tn
ln
− yn
yn
w
note
analogue
noise precision β
target values
model
labelling errors
simard
al
cross-entropy error function
sum-of-squares
classiﬁcation problem
training
generalization
k separate binary classiﬁcations
net- work
k
logistic sigmoid activation function
output
binary class label tk ∈
k
k.
class labels
input vector
conditional distribution
targets
w
= yk
tk
− yk
negative logarithm
corresponding likelihood function
following error function e
w
tnk ln ynk +
− tnk
ln
− ynk
ynk
w
derivative
error function
re- spect
activation
particular output unit
form
regression case
neural network solution
problem
approach
linear classiﬁcation model
kind
chapter
standard two-layer network
kind
figure
weight parameters
ﬁrst layer
network
various outputs
whereas
linear model
classiﬁcation problem
ﬁrst layer
network
nonlinear feature extraction
sharing
features
different outputs
computation
improved generalization
standard multiclass classiﬁcation problem
input
k
exclusive classes
binary target variables
∈
1-of-k coding scheme
class
network outputs
yk
p
tk =
following error function e
w
− n=1 k=1 tkn ln yk
w
network
exercise
exercise
exercise

neural networks figure
geometrical view
error function e
w
surface
weight space
point wa
local minimum
wb
global minimum
point wc
local gradient
error surface
vector ∇e
e
w
wa wb wc w1
∇e
discussion
section
output unit activation function
canonical link
softmax function yk
=
exp
exp
j
� yk
yk =
note
yk
constant
ak
error function
directions
weight space
degeneracy
appropriate regularization term
section
error function
derivative
error function
respect
activation
exercise
particular output unit
familiar form
natural choice
output unit activation function
error function
type
problem
re- gression
linear outputs
sum-of-squares error
binary classiﬁcations
logistic sigmoid outputs
cross-entropy error func- tion
multiclass classiﬁcation
softmax outputs
corresponding multiclass cross-entropy error function
classiﬁcation problems
classes
single logistic sigmoid output
network
outputs
softmax output activation function
parameter optimization
task
weight vector w
chosen function e
w
point
geometrical picture
error function
surface
weight space
figure
first note
small step
weight space
w
δw
change
error function
δe  δwt∇e
w
vector ∇e
w
points
direction
rate
increase
error function
error e
w
smooth continuous function
w
value
section
 
network
point
weight space
gradient
error function vanishes
otherwise
small step
direction
−∇e
w
error
points
gradient vanishes
stationary points
further classiﬁed
minima
maxima
saddle points
∇e
w
goal
vector w
e
w
value
error function
nonlinear dependence
weights
bias parameters
many points
weight space
gradient vanishes
discussion
sec- tion
point w
local minimum
other points
weight space
equivalent minima
instance
two-layer net- work
kind
figure
m hidden units
point
weight space
member
family
m
equivalent points
multiple inequivalent stationary points
particular multiple inequivalent minima
minimum
value
error function
weight vector
global minimum
other minima
values
error function
local minima
successful application
neural networks
global minimum
global minimum
several local minima
order
good solution
hope
analytical solution
equa- tion ∇e
w
numerical procedures
optimization
continuous nonlinear functions
problem
ex- tensive literature
techniques
initial value w
weight vector
weight space
succession
steps
form w
τ +1
w
τ
∆w
τ
τ
iteration step
different
different choices
weight vector update ∆w
τ
many algorithms
use
gradient information
therefore require
update
value
∇e
w
new weight vector w
τ +1
order
importance
gradient information
local approximation
error function
taylor expansion
local quadratic approximation insight
optimization problem
various techniques
solv- ing
local quadratic approximation
error function
taylor expansion
e
w
point w
weight space e
w
 e
w
+
w − w
tb
w − w
th
w − w
 
neural networks     b b  w=
cubic
terms
b
gradient
e
w b ≡ ∇e|w=
hessian matrix h = ∇∇e
elements w
h
≡ ∂e ∂wi∂wj w
corresponding local approximation
gradient
∇e  b + h
w − w
points
approximations
error
gradient
w
expressions
particular case
local quadratic approximation
point w
minimum
error function
case
linear term
∇e
w
becomes e
w
= e
w
w − w
th
w − w
hessian h
w
order
geometrically
eigenvalue equation
hessian matrix
hui = λiui
eigenvectors
complete orthonormal
appendix c
w − w
linear combination
eigenvectors
form
i
= δij
ut αiui
w − w =
transformation
coordinate system
origin
point w
axes
eigenvectors
orthogonal matrix
columns
ui
detail
appendix c. substituting
error function
form e
w
= e
w
i λiα2 i
matrix h
positive deﬁnite
v.
   i
error figure
neighbourhood
min- imum w
function
contours
con- stant error
ellipses
axes
eigenvectors
hes- sian matrix
lengths
square roots
correspond- ing eigenvectors
network
u2 w u1 w1
eigenvectors
complete set
arbitrary vector v
form
v = ciui
from
= c2 i λi
exercise
exercise
h
positive deﬁnite
eigenvalues
new coordinate system
basis vectors
eigenvectors
contours
constant e
origin
figure
one-dimensional weight space
stationary point w
∂2e ∂w2 w
exercise
corresponding result
d-dimensions
hessian matrix
w
positive deﬁnite
use
gradient information
section
gradient
error function
means
backpropagation procedure
use
gradient information
signiﬁcant improvements
speed
minima
error function
quadratic approximation
error function
error surface
quantities b
h
total
w
w +
independent elements
matrix h
w
dimensionality
w
total number
adaptive parameters
network
location
minimum
quadratic approximation
o
w
parameters
minimum
o
w
independent pieces
information
use
gradient information
o
w
function exercise
neural networks evaluations
o
w
steps
computational effort
approach
o
w
algorithm
use
gradient information
evaluation
∇e brings w items
information
minimum
function
o
w
gradient evaluations
error backpropagation
such evaluation
o
w
steps
minimum
o
w
steps
reason
use
gradient information
basis
practical algorithms
neural networks
n
gradient descent optimization
approach
gradient information
weight update
small step
direction
negative gradient
w
τ +1
w
τ
η∇e
w
τ
parameter η >
learning rate
such update
gradient
new weight vector
process
note
error function
respect
training set
step
entire training set
order
∇e
techniques
whole data set
once
batch methods
step
weight vector
direction
rate
decrease
error function
approach
gradient descent
steepest descent
approach
fact
poor algorithm
reasons
bishop
nabney
batch optimization
efﬁcient methods
conjugate gra- dients
quasi-newton methods
simple gradient descent
gill
fletcher
nocedal
wright
gradient descent
algorithms
property
error function
iteration
weight vector
global minimum
order
good minimum
gradient-based algorithm multiple times
time
cho- sen
point
performance
independent vali- dation
on-line version
gradient descent
practice
neural networks
large data sets
le cun
al.
error functions
maximum likelihood
set
independent observations
sum
terms
data point e
w
= en
w
n=1
gradient descent
sequential gradient descent
stochastic gradient descent
weight vector
data point
time
w
τ +1
w
τ
η∇en
w
τ
error backpropagation
update
data either
sequence
points
random
replacement
course intermediate scenarios
updates
batches
data points
advantage
on-line methods
methods
former handle redundancy
data
ex- treme example
data set
size
data point
note
error function
factor
original error function
batch methods
computational effort
batch error function gradient
on- line methods
property
on-line gradient descent
possibility
local minima
stationary point
respect
error function
whole data set
stationary point
data point
nonlinear optimization algorithms
practical application
neural net- work training
detail
bishop
nabney
error backpropagation
goal
section
efﬁcient technique
gradient
error function e
w
feed-forward neural network
local message
scheme
information
forwards
backwards
network
error backpropagation
backprop
term backpropagation
neural com- puting literature
variety
different things
instance
multilayer perceptron architecture
backpropagation network
term backpropagation
training
multilayer perceptron us- ing gradient descent
sum-of-squares error function
order
terminology
nature
training process
training algorithms
iterative procedure
minimization
error function
adjustments
weights
sequence
steps
such step
distinct stages
ﬁrst stage
derivatives
error function
respect
weights
important contribution
backpropagation technique
efﬁcient method
such derivatives
stage
errors
backwards
network
term backpropagation
evaluation
derivatives
second stage
derivatives
adjustments
weights
such technique
rumelhart
al
gradient descent
stages
ﬁrst stage
propagation
er- rors backwards
network
order
derivatives
many other kinds
network
multilayer perceptron
functions
simple sum-of-squares
eval-    
n
i yk
wkixi en =
ynk − tnk
∂en ∂wji =
ynj − tnj
xni
ynk = yk
w
gradient
error function
respect
weight wji
‘ local ’ computation
product
‘ error signal ’ ynj − tnj
output end
link wji
variable xni
input end
link
section
similar formula arises
logistic sigmoid activation function
cross entropy error function
softmax activation function
cross-entropy error function
simple result extends
complex setting
multilayer feed-forward networks
general feed-forward network
unit
weighted sum
inputs
form aj = wjizi i
neural networks uation
other derivatives
jacobian
hessian matrices
chapter
second stage
weight adjustment
calculated derivatives
variety
optimization schemes
simple gradient descent
evaluation
error-function derivatives
backpropagation algorithm
general network
ar- bitrary feed-forward topology
arbitrary differentiable nonlinear activation functions
broad class
error function
formulae
simple layered network structure
single layer
sigmoidal hidden units
sum-of-squares error
many error functions
practical interest
instance
maxi- mum likelihood
set
i.i.d
data
sum
terms
data point
training set
e
w
= en
w
problem
∇en
w
such term
error function
sequential optimization
results
training
case
batch methods
simple linear model
outputs
linear combina- tions
input variables
error function
particular input pattern n
form
error backpropagation
zi
activation
unit
input
connection
unit j
wji
weight
connection
section
biases
sum
extra unit
input
activation
+1
biases
sum
nonlinear activation function h
·
activation zj
unit j
form
note
variables
sum
input
unit j
output
zj = h
aj
pattern
training set
corresponding input vector
network
activations
hidden
output units
network
successive application
process
propagation
forward ﬂow
information
network
evaluation
derivative
en
respect
weight wji
outputs
various units
particular input pattern
order
notation
subscript n
network variables
first
en
weight
summed input aj
unit j
chain rule
partial derivatives
∂en ∂wji = ∂en ∂aj ∂aj ∂wji
useful notation δj ≡ ∂en ∂aj
δ ’ s
errors
reasons
using
∂aj ∂wji = zi
substituting
∂en ∂wji = δjzi
equation
tells
required derivative
value
δ
unit
output end
weight
value
z
unit
input end
weight
z
case
bias
note
same form
simple linear model
start
section
order
derivatives
value
δj
hidden
output unit
network
output units
=
  k δj ≡ ∂en ∂ak ∂ak ∂aj ∂en ∂aj =
sum
units
unit j
connections
arrange- ment
units
weights
figure
note
units
k
other hidden units
output units
use
fact
variations
aj
rise
variations
error func- tion
variations
variables
deﬁnition
δ
use
following backpropagation formula δj = h
aj
wkjδk k
value
δ
particular hidden unit
δ ’ s backwards
units
network
figure
note
summation
ﬁrst index
wkj
propagation
information
network
forward propagation equation
second index
values
δ ’ s
output units
δ ’ s
hidden units
feed-forward network
topology
backpropagation procedure
error backpropagation
input vector xn
network
forward propagate
network
activations
hidden
output units
δk
output units
δ ’ s
δj
hidden unit
network
use
required derivatives
neural networks figure
illustration
calculation
δj
hidden unit j
backpropagation
δ ’ s
units
unit j
connections
blue arrow
direction
information ﬂow
forward propagation
red arrows
backward propagation
error information
zi wji δj zj
δk δ1
canonical link
output-unit activation function
δ ’ s
hidden units
use
chain rule
partial derivatives
    k=1 i=0 k n d m
error backpropagation
batch methods
derivative
total error e
above steps
pattern
training set
patterns
∂e ∂wji = ∂en ∂wji
above derivation
hidden
output unit
network
same activation function h
·
derivation
general- ized
different units
individual activation functions
track
form
h
·
unit
a simple example
above derivation
backpropagation procedure
general forms
error function
activation functions
network topology
order
application
algorithm
particular example
simplicity
practical importance
be- cause many applications
neural networks
literature
use
type
network
two-layer network
form
figure
sum-of-squares error
output units
linear activation functions
yk = ak
hidden units
logistic sigmoid activation functions
useful feature
function
par- ticularly simple form
standard sum-of-squares error function
pattern
error
h
− h
h
≡ tanh
tanh
= ea − e−a ea + e−a
en
yk − tk
w
ji xi zj = tanh
aj
yk = w
kj zj
j=0
yk
activation
output unit k
tk
corresponding target
particular input pattern xn
pattern
training
turn
forward propagation
aj =  k next
δ ’ s
output unit
δk = yk − tk
δs
hidden units
neural networks δj =
− z2 j
k=1 wkjδk
derivatives
respect
second-layer weights
∂en ∂w
ji = δjxi
∂en ∂w
kj = δkzj
efﬁciency
backpropagation
important aspects
backpropagation
computational efﬁ- ciency
number
computer operations
derivatives
error function scales
total number w
weights
biases
network
single evaluation
error function
input pattern
o
w
operations
large w
fact
network
sparse connections
number
weights
number
units
bulk
computational effort
forward propagation
sums
evaluation
activation functions
small overhead
term
sum
multiplication
addition
overall computational cost
o
w
alternative approach
backpropagation
derivatives
error function
ﬁnite differences
weight
turn
derivatives
expression ∂en ∂wji = en
wji +
− en
wji
+ o
software simulation
accuracy
approximation
derivatives
numerical roundoff problems
accuracy
ﬁnite differences method
symmetrical central differences
form ∂en ∂wji = en
wji +
− en
wji −
+ o
exercise
case
o
corrections
taylor expansion
right-hand side
residual corrections
o
number
computational steps
main problem
numerical differentiation
desirable o
w
scaling
forward propagation
o
w
steps
figure
illustration
modular pattern recognition system
jacobian matrix
error signals
outputs
ear- lier modules
system
error backpropagation
v z w y
w weights
network
overall scaling
o
w
numerical differentiation
important role
practice
comparison
derivatives
backpropagation
us- ing central differences
powerful check
correctness
software implementation
backpropagation algorithm
networks
prac- tice
derivatives
backpropagation
accuracy
numerical efﬁciency
results
numerical differentiation
test cases
order
correctness
implementation
jacobian matrix
derivatives
error function
respect
weights
propagation
errors backwards
network
technique
backpropagation
calculation
other deriva- tives
evaluation
jacobian matrix
elements
derivatives
network
respect
inputs jki ≡ ∂yk ∂xi
other inputs
ﬁxed
jacobian matrices
useful role
systems
number
distinct modules
figure
module
ﬁxed
adaptive function
suppose
error function e
respect
parameter w
figure
derivative
error function
∂e ∂w = k
j ∂e ∂yk ∂yk ∂zj ∂zj ∂w
jacobian matrix
red module
figure
middle term
jacobian matrix
measure
local sensitivity
outputs
changes
input variables
known errors
  = = i
  j j
inputs
trained network
order
contribution ∆yk
errors
outputs
relation ∆yk  ∂yk ∂xi ∆xi
|∆xi|
network
rep-
trained neural network
elements
jacobian matrix
constants
particular input vector
thus
small perturbations
inputs
new input vector
jacobian matrix
backpropagation procedure
one
derivatives
error function
respect
weights
element jki
form jki = ∂yk ∂xi ∂yk ∂aj ∂aj ∂xi
∂yk ∂aj
use
sum
units
input unit i sends connections
example
units
ﬁrst hidden layer
layered topology
recursive backpropagation formula
derivatives
neural networks ∂yk ∂aj = l ∂yk ∂al ∂al ∂aj = h
aj
wlj ∂yk ∂al l
sum
units
unit j
connections
ﬁrst index
wlj
again
use
backpropagation
output units
required derivatives
functional form
output-unit activation function
instance
individual sigmoidal activation functions
output unit
∂yk ∂aj whereas
softmax outputs
δkjσ
aj
∂yk ∂aj = δkjyk − ykyj
procedure
jacobian matrix
input vector
point
input space
ja- cobian matrix
propagate
usual way
hessian matrix
activations
hidden
output units
network
next
row k
jacobian matrix
output unit k
backpropagate
recursive relation
hidden units
network
use
backpropagation
inputs
alternative forward propagation formalism
analogous way
backpropagation approach
again
implementation
such algorithms
numeri- cal differentiation
form ∂yk ∂xi = yk
xi +
yk
xi −
+ o
forward propagations
network
d inputs
exercise
hessian matrix
technique
backpropagation
ﬁrst derivatives
error function
respect
weights
network
back- propagation
second derivatives
error
∂2e ∂wji∂wlk
note
weight
bias parameters
elements
single vector
w
case
second derivatives
elements hij
hessian matrix h
i
j ∈
w
w
total number
weights
biases
hessian
important role
many aspects
neural computing
following
several nonlinear optimization algorithms
neural networks
considerations
second-order properties
error surface
hessian matrix
bishop
nabney
basis
fast procedure
feed-forward network
small change
training data
bishop
inverse
hessian
signiﬁcant weights
network
part
network ‘ pruning ’ algorithms
le cun
al.
hessian
central role
laplace approximation
bayesian neural network
section
inverse
predic- tive distribution
trained network
eigenvalues
values
hyperparameters
determinant
model evidence
various approximation schemes
hessian matrix
neural network
hessian
extension
backpropagation technique
   k  
neural networks
important consideration
many applications
hessian
efﬁciency
w parameters
weights
biases
network
hessian matrix
dimensions w × w
computational effort
hessian
o
w
pattern
data set
efﬁcient methods
scaling
o
w
diagonal approximation
applications
hessian matrix
require
inverse
hessian
reason
interest
diagonal approximation
hessian
other words
off-diagonal elements
zeros
inverse
again
error function
sum
terms
pattern
data set
e =
en
pattern
time
results
patterns
from
diagonal elements
hessian
pattern n
∂2en ∂w2 ji = ∂2en ∂a2 j z2 i
using
second derivatives
right-hand side
chain rule
differential calculus
backprop- agation equation
form ∂2en ∂a2 j = h
aj
k k wkjwkj ∂2en ∂ak∂ak + h
aj
wkj ∂en ∂ak
off-diagonal elements
second-derivative terms
becker
le cun
le cun
al.
∂a2 j = h
aj
w2 kj ∂2en ∂a2 k k + h
aj
wkj ∂en ∂ak
k
note
number
computational steps
approximation
o
w
w
total number
weight
bias parameters
network
o
w
full hessian
ricotti
al
diagonal approximation
hessian
terms
evaluation
∂2en/∂a2 j
obtained exact expres- sions
diagonal terms
note
o
w
scaling
major problem
diagonal approximations
practice
hessian
approximations
computational convenience
care
n  
n n   e =
n n
hessian matrix
outer product approximation
neural networks
problems
sum-of-squares error function
form
yn − tn
case
single output
order
notation simple
extension
several outputs
hessian matrix
form h = ∇∇e = ∇yn∇yn +
yn − tn
∇∇yn
n=1
network
data set
outputs
target values
second term
term
following argument
section
optimal function
sum-of-squares loss
conditional average
target data
quantity
yn − tn
random
mean
value
value
second derivative term
right-hand side
whole term
summation
second term
levenberg–marquardt approximation
outer product approximation
hessian matrix
sum
outer products
vectors
h  bnbt n n=1
bn = ∇yn = ∇an
activation function
output units
identity
evaluation
outer product approximation
hessian
ﬁrst derivatives
error function
o
w
steps
standard backpropagation
elements
matrix
o
w
steps
simple multiplication
approximation
network
general network
second derivative terms
right-hand side
case
cross-entropy error function
network
logistic sigmoid output-unit activation functions
corresponding approximation
exercise
exercise
exercise
exercise
analogous result
multiclass networks
softmax output- unit activation functions
h 
yn
− yn
bnbt n.
 n
neural networks
inverse hessian
outer-product approximation
ef- ﬁcient procedure
inverse
hessian
hassibi
stork
first
outer-product approximation
matrix notation
hn = bnbt n n=1
bn ≡ ∇wan
contribution
gradient
output unit activation
data point
sequential procedure
data points
time
suppose
inverse hessian
ﬁrst l data points
contribution
data point l +
order
inverse
hessian
matrix identity hl+1 = hl + bl+1bt l+1
m + vvt −1 = m−1 −
m−1v
+ vtm−1v
unit matrix
special case
woodbury identity
c.7
hl
m
bl+1
v
h−1 l+1 = h−1 l − h−1
+ bt l bl+1bt l+1h−1 l+1h−1 l bl+1 l
exercise
way
data points
l+1 = n
whole data set
result therefore
procedure
inverse
hessian
single pass
data set
initial matrix h0
α
small quantity
algorithm
inverse
h + αi
results
precise value
α
extension
algorithm
output
hessian matrix
part
network training algorithm
quasi-newton nonlinear opti- mization algorithms
approximation
inverse
hes- sian
such algorithms
detail
bishop
nabney
finite differences
case
ﬁrst derivatives
error function
second derivatives
ﬁnite differences
accuracy
numerical precision
possible pair
weights
turn
∂wji∂wlk ∂2e =
e
wji +
+
− e
wji +
−
−e
wji −
+
+ e
wji −
−
o
hessian matrix
again
symmetrical central differences formulation
residual errors
o
o
w
elements
hessian matrix
evaluation
element
forward propagations
o
w
operations
pattern
approach
o
w
operations
complete hessian
properties
practice
check
soft- ware implementation
backpropagation methods
efﬁcient version
numerical differentiation
central differences
ﬁrst derivatives
error function
backpropagation
∂2e ∂wji∂wlk =
∂e ∂wji
wlk +
∂e ∂wji
wlk −
+ o
w weights
gradients
o
w
steps
method
o
w
operations
exact evaluation
hessian so
various approximation schemes
hessian matrix
inverse
net- work
arbitrary feed-forward topology
extension
technique
back- propagation
ﬁrst derivatives
shares
desirable features
computational efﬁciency
bishop
bishop
differentiable error function
function
network outputs
networks
arbitrary differentiable activation func- tions
number
computational steps
hessian scales
o
w
similar algorithms
buntine
weigend
speciﬁc case
network
layers
weights
required equations
indices
inputs
indices j
j
hidden units
indices k
k
outputs
deﬁne δk = ∂en ∂ak
mkk ≡ ∂2en ∂ak∂ak
en
contribution
error
data point
hessian matrix
network
separate blocks
weights
second layer
∂2en kj ∂w
kj ∂w
= zjzjmkk
exercise
 k    k 
neural networks
weights
ﬁrst layer
∂w
= xixih
aj
ijj ∂2en ji ∂w
ji +xixih
aj
h
aj
w
kjδk k w
kjw
kj mkk
weight
layer
∂2en ji ∂w
kj ∂w
= xih
aj
+ zj w
kjhkk
k exercise
ijj
j
j element
identity matrix
weights
bias term
corresponding expressions
appropriate activation
inclusion
skip-layer connections
fast multiplication
hessian
many applications
hessian
quantity
interest
hessian matrix h
product
h
vector
evaluation
hessian
o
w
operations
storage
o
w
vector vth
w elements
hessian
intermediate step
efﬁcient approach
vth
way
o
w
operations
vth = vt∇
∇e
∇
gradient operator
weight space
standard forward-propagation
backpropagation equations
evaluation
∇e
equations
set
forward-propagation
backpropagation equations
evaluation
vth
møller
pearlmutter
original forward-propagation
back- propagation equations
differential operator vt∇
pearlmutter
notation r
operator vt∇
convention
analysis
use
usual rules
differential calculus
result r
w
= v.
technique
simple example
two-layer network
form
figure
linear output units
sum-of-squares error function
contribution
error function
pattern
data set
required vector
    k j i
  j i
 j
hessian matrix
contributions
patterns
two-layer network
forward-propagation equations
equations
r
·
operator
set
forward propagation equations
form aj = wjixi zj = h
aj
yk = wkjzj
vjixi r
aj
= r
zj
= h
aj
r
aj
r
yk
=
zj
+ vkjzj
vji
element
vector v
weight wji
quan- tities
form r
zj
r
aj
r
yk
new variables
values
above equations
sum-of-squares error function
fol- lowing standard backpropagation expressions
δk = yk − tk δj = h
aj
wkjδk
again
equations
r
·
operator
set
backprop- agation equations
form r
δk
= r
yk
r
δj
= h
aj
r
aj
wkjδk k + h
aj
vkjδk + h
aj
k
δk
k
usual equations
ﬁrst derivatives
error ∂e ∂wkj ∂e ∂wji = δkzj = δjxi
r
·
operator
expressions
elements
vector vth = r
δk
zj + δkr
zj
= xir
δj
r r ∂e ∂wkj ∂e
networks
implementation
algorithm
introduction
additional variables r
aj
r
zj
r
δj
hidden units
r
δk
r
yk
output units
input pattern
values
quantities
above results
elements
vth
elegant aspect
technique
equations
vth mirror
standard forward
backward propagation
extension
software
product
technique
full hessian matrix
vector v
series
unit vectors
form
column
hessian
formalism
backpropagation procedure
bishop
section
loss
efﬁciency
redundant calculations
regularization
neural networks
number
input
units
neural network
dimensionality
data set
number m
hidden units
free parameter
predictive performance
note
m
number
parameters
weights
biases
network
maximum likelihood
optimum value
m
generalization performance
optimum balance
figure
example
effect
different values
m
sinusoidal regression problem
generalization error
simple function
m
presence
local minima
error function
figure
effect
multiple random initializations
weight vector
range
values
m.
validation
performance
case
particular solution
m
practice
approach
m
fact
graph
kind
figure
speciﬁc solution
validation
error
other ways
complexity
neural network model
order
discussion
polynomial curve
chapter
alternative approach
large value
m
complexity
addition
regularization term
error function
regularizer
regularized error 
form e
w
= e
w
wtw
regularizer
weight decay
length
chapter
effective model complexity
choice
regularization coefﬁcient λ
regularizer
negative logarithm
zero-mean gaussian prior distribution
weight vector
consistent gaussian priors
limitations
simple weight decay
form
properties
network mappings
multilayer perceptron network
layers
weights
linear output units
mapping
set
input variables
xi
set
output variables
yk
activations
hidden units
ﬁrst hidden layer figure
plot
sum-of-squares test-set error
polynomial data
ver- sus
number
hidden units
network
random starts
network size
ef- fect
local minima
new start
weight vector
isotropic gaussian distribution
mean
zero
variance
regularization
neural networks
−1 m =
−1
m
m
−1
figure
examples
two-layer networks
data points drawn
sinusoidal data set
graphs
result
networks
m =
hidden units
sum-of-squares error function
scaled conjugate-gradient algorithm
       
= yk → xi → j i
 
i
neural networks
form
activations
output units
zj = h wjixi + wj0 yk = wkjzj + wk0
suppose
linear transformation
input data
form xi = axi + b
exercise
mapping
network
corresponding linear transformation
weights
biases
inputs
units
hidden layer
form similarly
linear transformation
output variables
network
form
transformation
second-layer weights
biases
wji wji → wj0 → wj0 = wj0 − wji
= cyk + d wkj → wk0 → wkj = cwkj wk0 = cwk0 + d.
network
original data
network
data
input
target variables
above linear transfor- mations
consistency
equivalent networks
linear transformation
weights
regularizer
property
solution
simple weight decay
weights
biases
equal footing
property
regularizer
linear trans- formations
regularizer
re-scaling
weights
shifts
biases
regularizer
w2 +
w∈w1 w∈w2 w2
w1
set
weights
ﬁrst layer
w2
set
weights
second layer
biases
summations
regularizer
   
−  
regularization
neural networks
weight transformations
regularization parameters
λ1 → a1/2λ1
λ2 → c−1/2λ2
regularizer
corresponds
prior
form p
w|α1
α2
− w2 −
w2
w∈w2 note
priors
form
bias parameters
use
improper priors
difﬁculties
regularization coefﬁcients
model comparison
bayesian framework
corresponding evidence
separate priors
biases
shift invariance
own hyperparameters
effect
hyperpa- rameters
samples
corresponding network functions
figure
priors
weights
number
groups wk
p
w
exp αkw2 k
w2 k = w2 j
j∈wk
special case
groups
sets
weights
input units
marginal likelihood
respect
corresponding parameters
automatic relevance determination
section 7.2.2
alternative
regularization
way
effective complexity
network
procedure
early stopping
training
nonlinear network models
iterative reduction
error function
re- spect
set
training data
optimization algorithms
network training
conjugate gradients
error
nonincreasing function
iteration index
error
respect
independent data
validation set
decrease
ﬁrst
in- crease
network
training
point
error
respect
validation data set
figure
order
network
good generalization performance
behaviour
network
case
terms
effective number
degrees
freedom
network
number
grows
training process
steady increase
effective complexity
model

=
αb
=
=
−0.5
=
αb
=
=
neural networks
=
αb
=
=
=
αb
=
=
−2 −4 −6
−5 −10
−20 −40 −60
−10 −1 −0.5
figure
illustration
effect
hyperparameters
prior distribution
weights
biases
two-layer network
single input
single linear output
hidden units
‘ tanh
activation functions
priors
hyperparameters
precisions
gaussian distributions
ﬁrst-layer biases
ﬁrst-layer weights
second-layer biases
governs
vertical scale
functions
second-layer weights
parameter
different vertical axis
diagrams
horizontal scale
variations
function values
αb effect
range
vertical offsets
functions
horizontal range
variations
parameter
minimum
training error
way
effective network complexity
case
quadratic error function
insight
early stopping
similar behaviour
sim- ple weight-decay term
figure
axes
weight space
eigenvectors
hessian matrix
absence
weight decay
weight vector
origin
proceeds
training
path
local negative gradient vec- tor
weight vector
w2
point w
minimum
error
tion wml
shape
error surface
w
eigenvalues
hessian
point
decay
relationship
early stopping
weight decay
quantity τ η
τ
iteration index
η
rate parameter
role
reciprocal
regularization exercise
regularization
neural networks
figure
illustration
behaviour
training
error
validation
error
typical training session
function
iteration step
sinusoidal data set
goal
generalization performance
training
point
vertical dashed lines
minimum
validation
error
parameter λ
effective number
parameters
network
course
training
invariances
many applications
pattern recognition
predictions
invariant
transformations
input vari- ables
example
classiﬁcation
objects
two-dimensional images
handwritten digits
particular object
same classiﬁcation irrespective
position
image
translation invariance
size
scale invariance
such transformations
signiﬁcant changes
raw data
terms
intensities
pixels
image
rise
same output
classiﬁcation system
speech recognition
small levels
time axis
temporal ordering
interpretation
signal
large numbers
training patterns
adaptive model
neural network
invariance
training
large number
examples
effects
various transformations
translation invariance
im- age
training set
examples
objects
many different positions
approach
number
training examples
several invariants
number
combinations
transformations grows
number
such transformations
seek alternative approaches
adaptive model
required invariances
categories
training set
replicas
training patterns
trans-
desired invariances
instance
digit recog- nition example
multiple copies
example
 w w2 wml
neural networks figure
a schematic illustration
early stopping
similar results
decay
case
quadratic error func- tion
ellipse
con- tour
constant error
wml
minimum
er- ror function
weight vector
origin
ac- cording
local negative gra- dient direction
path
curve
weight vector w
simple weight-decay reg- ularizer
training
mini- mum
regularized error
figure
digit
different position
image
regularization term
error function
changes
model output
input
technique
tangent propagation
section
invariance
features
required transformations
subsequent regression
classi- ﬁcation system
such features
inputs
invariances
ﬁnal option
invariance properties
structure
neu- ral network
deﬁnition
kernel function
case
techniques
relevance vector machine
way
use
local receptive ﬁelds
weights
context
convolutional neural networks
section
approach
com- plex invariances
figure
sequential training algorithms
input pattern
model
patterns
different transformation
appropriate distribution
time
batch methods
similar effect
data point
number
times
copy
use
data
signiﬁcant improvements
generalization
simard
data set
error function
addition
regularizer
section
approach
regularization
neural networks
figure
illustration
synthetic warping
handwritten digit
original image
left
right
top row shows
examples
warped digits
corresponding displacement ﬁelds
bottom row
displacement ﬁelds
random displacements
∆y ∈
pixel
convolution
gaussians
advantage
approach
range
transformations
training set
hand-crafted features
required invariances
discard information
discrimination
tangent propagation
regularization
models
transformations
input
technique
tangent propagation
simard
effect
transformation
particular input vector xn
transformation
translation
rotation
mirror reﬂection
instance
transformed pattern
manifold m
d-dimensional input space
figure
case
d =
simplicity
transformation
single parameter ξ
rotation angle
instance
subspace m
xn figure
illustration
two-dimensional input space
effect
continuous transforma- tion
particular input vector xn
one- dimensional transformation
continuous variable ξ
causes
one-dimensional manifold m. locally
effect
transformation
tangent vector τ
x2 τ
m ξ x1       i=1 d d 
  i=1 d     
neural networks
ξ
vector
results
xn
transformation
s
ξ
s
= x
tangent
curve m
directional derivative τ = ∂s/∂ξ
tangent vector
point xn
τ
= ∂s
xn
ξ
ξ=0
transformation
input vector
network output vector
change
derivative
output k
respect
∂yk ∂ξ = ξ=0 ∂yk ∂xi ∂xi ∂ξ ξ=0 = jkiτi
jki
k
i
element
jacobian matrix j
section
result
standard error function
encour- age local invariance
neighbourhood
data points
addition
original error function e
regularization function ω
total error function
form e = e + λω
λ
regularization coefﬁcient
ω =
n k ∂ynk
=
ξ=0 n k i=1
jnkiτni
exercise
regularization function
network mapping function
in- variant
transformation
neighbourhood
pattern vector
value
parameter λ
balance
training data
invariance property
practical implementation
tangent vector
n
ﬁnite differences
original vector xn
vector
transformation
small value
ξ
ξ
figure
regularization function
network weights
jaco- bian j
backpropagation formalism
derivatives
regu- larizer
respect
network weights
extension
techniques
section
transformation
l parameters
e.g.
l
case
translations
in-plane rotations
two-dimensional image
manifold m
dimensionality l
corresponding regularizer
sum
terms
form
transformation
several transformations
same time
network mapping
invariant
combinations
transformations
simard
regularization
neural networks
figure
illustration showing
original image x
digit
b
tangent vector
inﬁnitesimal clockwise rotation
c
result
small contribution
tangent vector
original image
x + τ
 =
degrees
d
true image
comparison
c
b
d
a related technique
tangent distance
invariance properties
distance-based methods
nearest-neighbour classiﬁers
simard
data
way
invariance
model
set
trans- formations
training
transformed versions
original input patterns
approach
technique
tangent propagation
bishop
leen
section
transformation
single parameter ξ
function s
ξ
s
= x
sum-of-squares error function
error function
untransformed inputs
inﬁnite data
limit
form e
y
x
t|x
p
x
dx dt
section
network
single output
order
notation
inﬁnite number
copies
data point
transformation       
 
  
neural networks
parameter ξ
distribution p
ξ
error function
data set
e
y
ξ
t|x
p
x
p
ξ
dx dt dξ
distribution p
ξ
mean
small variance
small transformations
original input vectors
transformation function
taylor series
powers
ξ
s
ξ
s
+ ξ s
ξ
∂2 ∂ξ2 s
ξ
+ o
ξ3
∂ξ ξ=0
ξ2τ  + o
ξ3
x + ξτ +
τ 
second derivative
s
respect
ξ =
model function
y
ξ
y
x
ξτ t∇y
x
τ 
t ∇y
x
τ t∇∇y
x
+ o
ξ3
mean error function
e =
y
x
t|x
p
x
dt + e [ ξ ]
y
x
τ t∇y
x
p
t|x
p
x
dt + e [ ξ2 ]
y
x
τ 
t ∇y
x
τ t∇∇y
x
+ τ t∇y
x
p
t|x
p
x
dt + o
ξ3
distribution
transformations
mean
[ ξ ]
e [ ξ2 ]
λ. omitting terms
o
ξ3
average error function
e
original sum-of-squares error
regularization term ω
form e = e + λω ω =
y
x
e [ t|x ]
τ t∇y
x
+ p
x
τ 
t ∇y
x
τ t∇∇y
x
τ
integration
regularization term
section
function
sum-of-squares error
condi- tional average e [
]
target values
from
regularized error
unregularized sum-of-squares
terms
o
ξ
network function
total error
form y
x
e [ t|x ] + o
ξ
order
ξ
ﬁrst term
regularizer vanishes
ω =
τ t∇y
x
p
x
dx
tangent propagation regularizer
special case
transformation
inputs
addition
random noise
x → x + ξ
regularizer
form
ω
∇y
x
p
x
dx
tikhonov regularization
tikhonov
arsenin
bishop
derivatives
regularizer
respect
network weights
extended backpropagation algorithm
bishop
small noise amplitudes
tikhonov regularization
addition
random noise
inputs
generalization
appropriate circumstances
sietsma
dow
convolutional networks
approach
models
certain transformation
inputs
invariance properties
structure
neural net- work
basis
convolutional neural network
le cun
al.
lecun
al.
image data
speciﬁc task
handwritten digits
input image
set
pixel intensity values
desired output
posterior proba- bility distribution
ten digit classes
identity
digit
translations
scaling
rotations
network
invariance
subtle transformations
elastic deformations
kind
figure
simple approach
image
input
network
kind
figure
large training
network
principle yield
good solution
problem
appropriate invariances
example
approach
key property
images
nearby pixels
distant pixels
many
modern approaches
computer vision
property
local features
small subregions
image
information
such features
later stages
processing
order
higher-order features
regularization
neural networks
exercise
neural networks input image convolutional
sub-sampling layer figure
diagram
part
convolutional neural network
layer
convolu- tional units
layer
units
several successive pairs
such layers
information
image
local features
region
image
other regions
image
instance
object
interest
notions
convolutional neural networks
mechanisms
i
local receptive ﬁelds
ii
weight sharing
iii
subsampling
structure
convolutional network
figure
convolutional layer
units
planes
feature map
units
feature
take
small subregion
image
units
feature map
share
same weight values
instance
feature map
units
×
grid
unit
inputs
pixel patch
image
whole feature map therefore
adjustable weight parameters
adjustable bias parameter
input values
patch
weights
bias
result
sigmoidal nonlinearity
units
feature detectors
units
feature map
same pattern
different locations
input image
due
weight sharing
evaluation
activations
units
convolution
image pixel intensities
‘ kernel ’
weight parameters
input image
activations
feature map
same amount
basis
approximate
invariance
regularization
neural networks
network
translations
distortions
input image
multiple features
order
effective model
multiple feature maps
convolutional layer
own set
weight
bias parameters
outputs
convolutional units
inputs
layer
network
feature map
convolutional layer
plane
units
subsampling layer
unit
inputs
small receptive ﬁeld
corresponding feature map
convolutional layer
units
instance
unit
inputs
unit region
corresponding feature map
average
inputs
adaptive weight
addition
adaptive bias parameter
sigmoidal nonlinear activation function
receptive ﬁelds
number
rows
columns
layer
convolutional layer
way
response
unit
subsampling layer
small shifts
image
corresponding regions
input space
practical architecture
several pairs
sub- sampling layers
stage
degree
invariance
trans- formations
previous layer
several feature maps
convolutional layer
plane
units
previous subsampling layer
gradual reduction
spatial resolution
increas- ing number
features
ﬁnal layer
network
adaptive layer
softmax output nonlinearity
case
multiclass classiﬁcation
whole network
error minimization
backpropagation
gradient
error function
slight modiﬁcation
usual backpropagation algorithm
shared-weight constraints
use
local receptive ﬁelds
number
weights
network
network
number
independent parameters
data
substantial numbers
constraints
weights
soft weight
way
effective complexity
network
large number
weights
weights
certain groups
technique
weight
section
way
building translation invariance
networks
image interpretation
appli- cable
problems
form
constraints
advance
form
soft weight
nowlan
hinton
hard constraint
equal weights
form
regularization
groups
weights
similar values
division
weights
groups
mean
value
group
spread
values
groups
part
learning process
exercise
         j=1 j=1 m m i
neural networks section
exercise
recall
simple weight decay regularizer
negative log
gaussian prior distribution
weights
age
weight values
several groups
group
probability distribution
mixture
gaussians
centres
variances
gaussian components
mixing coefﬁcients
adjustable parameters
part
learning process
probability density
form
p
w
= p
wi
p
wi
= πjn
wi|µj
σ2 j
mixing coefﬁcients
negative logarithm
regularization function
form ω
w
− ln i πjn
wi|µj
σ2 j
total error function
e
w
= e
w
+ λω
w
λ
regularization coefﬁcient
error
respect
weights wi
respect
parameters
πj
µj
σj
mixture model
weights
parameters
mixture model
em algorithm
chapter
dis- tribution
weights
learning process
nu- merical instability
joint optimization
weights
mixture-model parameters
standard optimization algorithm
conjugate gradients
quasi-newton methods
order
total error function
derivatives
respect
various adjustable parameters
con- venient
πj
prior probabilities
corresponding posterior probabilities
bayes ’ theorem
form γj
w
= πjn
w|µj
σ2 j
k
πkn
w|µk
σ2
derivatives
total error function
respect
weights
∂ e ∂wi = ∂e ∂wi + λ γj
wi
j
wi − µj
σ2 j
    account
constraints
e ∂ ∂µj e ∂ ∂σj i
 i
exercise
regularization
neural networks
effect
regularization term
weight
centre
jth gaussian
force proportional
posterior probability
gaussian
weight
kind
effect
derivatives
error
respect
centres
gaussians
= λ γj
wi
µi − wj
j
simple intuitive interpretation
µj towards
aver- age
weight values
posterior probabilities
respective weight parameters
component j
derivatives
respect
variances
= λ γj
wi
σj −
wi − µj
σ3 j
weighted average
squared deviations
weights
corresponding centre µj
weighting coefﬁcients
posterior probability
weight
component j
note
practical implementation
new variables
j = exp
ηj
σ2
minimization
respect
ηj
en- sures
parameters
effect
pathological solutions
σj
gaussian component
weight parameter values
such solutions
detail
context
gaussian mixture models
section
derivatives
respect
mixing coefﬁcients
πj =
� πi
j
interpretation
πj
prior probabilities
mixing coefﬁcients
terms
set
auxiliary variables
ηj
softmax function
πj = exp
ηj
m k=1 exp
ηk
exercise
derivatives
regularized error function
respect
ηj
form
neural networks figure
left ﬁgure
two-link robot arm
cartesian
x2
end ef- fector
joint angles
θ2
ﬁxed
l1
l2
arms
forward kinematics
arm
prac- tice
joint angles
rise
desired end effector position
right ﬁg- ure
solutions
’
‘
’
l2 θ2 l1 ∂ e ∂ηj = i
πj − γj
wi
πj
average posterior probability
com- ponent j
mixture density networks
goal
supervised learning
conditional distribution p
t|x
many simple regression problems
practical machine
problems
non-gaussian distributions
example
inverse problems
distribution
case
gaussian assumption
poor predic- tions
simple example
inverse problem
kinematics
robot arm
figure
forward problem
end ef- fector position
joint angles
unique solution
practice
end effector
robot
speciﬁc position
appropriate joint angles
inverse problem
solutions
figure
forward problems
causality
physical system
gen-
unique solution
instance
speciﬁc pattern
symptoms
human body
presence
particular disease
pattern recog- nition
inverse problem
presence
disease
set
symptoms
forward problem
many-to-one mapping
inverse problem
multiple solu- tions
instance
several different diseases
same symptoms
robotics example
kinematics
geometrical equations
multimodality
many machine
problems
presence
multimodality
problems
spaces
high di- mensionality
tutorial purposes
simple toy problem
multimodality
data
problem
variable x
interval
set
values
xn
corresponding target values tn
x2
θ1
 k
mixture density networks figure
left
data set
simple ‘ forward problem ’
red curve
result
two-layer neural network
sum-of-squares error function
corresponding inverse problem
right
roles
x
t.
same net- work
sum-of-squares error function
poor ﬁt
data
multimodality
data set
function xn
sin
uniform noise
interval
inverse problem
same data points
roles
x
t. figure
data sets
forward
inverse problems
results
two-layer neural networks
hidden units
single linear output unit
sum- of-squares error function
least
corresponds
likelihood
gaussian assumption
poor model
non-gaussian inverse problem
general framework
conditional probability distributions
mixture model
p
t|x
mixing coefﬁcients
component densities
ﬂexible functions
input vector x
rise
mixture density network
value
x
mixture model
general formalism
arbitrary conditional density function p
t|x
ﬂexible network
framework
arbitrary conditional distri- butions
model
gaussian components
p
t|x
k=1 πk
x
n t|µk
x
σ2 k
x
example
heteroscedastic model
noise variance
data
function
input vector x
gaussians
other distribu- tions
components
bernoulli distributions
target variables
case
isotropic co- variances
components
mixture density network
general covariance matrices
covariances
cholesky factorization
williams
isotropic components
conditional distribution p
t|x
factorization
respect
components
t
contrast
standard sum-of-squares regression model
consequence
mixture distribution
various parameters
mixture model
mixing k
x
coefﬁcients πk
x
means
x
variances
 k θ
neural networks xd x1 θm θ1 p
t|x
t figure
mixturedensitynetwork
general conditional probability densities
t|x
parametric mixture model
distribution
t
parameters
outputs
neural network
input vector
outputs
conventional neural network
input
structure
mixture density network
figure
mixture density network
mixture
experts
section
principle difference
mixture density network
same function
parameters
component densities
co- efﬁcients
nonlinear hidden units
input-dependent functions
neural network
figure
example
two-layer network
sigmoidal
‘ tanh ’
units
l components
mixture model
t
k components
network
l output unit k
mixing coefﬁcients
x
k
activations
aπ k
kernel widths σk
x
l × k outputs
aσ
aµ kj
components µkj
x
kernel
µk
x
total number
network outputs
k +
l
usual k outputs
network
conditional means
target variables
mixing coefﬁcients
constraints πk
x
� πk
x
set
softmax outputs πk
x
exp
aπ k
k
exp
aπ l
variances
σ2
exponentials
corresponding network activations
k
x
terms
means
x
real components
σk
x
exp
aσ k
 n  
mixture density networks
network output activations
x
aµ kj
adaptive parameters
mixture density network
vector w
weights
biases
neural network
maximum likelihood
error function
negative logarithm
likelihood
independent data
error function
form e
w
− ln n=1 k=1 πk
xn
w
n tn|µk
w
σ2 k
w
dependencies
w explicit
order
error function
derivatives
error e
w
respect
components
standard backpropagation procedure
suitable expres- sions
derivatives
error
respect
output-unit activations
represent error signals
pattern
output unit
hidden units
error function derivatives
usual way
error function
sum
terms
training data point
derivatives
particular pattern n
derivatives
e
patterns
mixture distributions
mixing coefﬁcients
x
x-dependent prior probabilities
corresponding posterior probabilities
nnk
n
tn|µk
xn
σ2 ing coefﬁcients
k
xn
derivatives
respect
network output activations
mix- similarly
derivatives
respect
output activations
com- ponent means
γk
t|x
πknnk l=1 πlnnl k ∂en ∂aπ k = πk − γk
∂aµ kl = γk µkl − tl σ2 k
derivatives
respect
output activations
compo- nent variances
∂en ∂aσ k = −γk t − µk2 σ3
σk −
exercise
exercise
exercise

neural networks figure
plot
mixing coefﬁcients
x
function
x
kernel functions
mixture density network
data
figure
model
gaussian compo- nents
two-layer multi- layer perceptron
ﬁve ‘ tanh ’ sig- moidal units
hidden layer
outputs
means
variances
gaus- sian components
mixing coefﬁcients
large values
x
conditional probability density
target data
ker- nels
high value
prior probability
intermediate val- ues
x
conditional den- sity
co- efﬁcients
comparable values
b
plots
means
x
same colour coding
mixing coefﬁcients
c
plot
contours
corresponding con- ditional probability density
tar- get data
same mixture den- sity network
ap- proximate conditional mode
red points
conditional density
d
plot
b
d

c
k
use
mixture density network
toy ex- ample
inverse problem
figure
plots
coefﬁ- cients
x
means
x
conditional density contours
t|x
figure
outputs
neural network
parameters
mixture model
continuous single-valued functions
input variables
figure
c
model
conditional density
values
x
trimodal
other values
amplitudes
mixing components
x
mixture density network
conditional density function
target data
value
input vector
conditional density
complete description
generator
data
problem
value
output vector
density function
speciﬁc quantities
interest
different applications
mean
conditional average
target data
e [ t|x ] = tp
t|x
dt = k=1 πk
x
µk
x
  k=1 ⎧⎨⎩σ2
''µk
x
 k
⎫⎬⎭
bayesian neural networks
standard network
squares
conditional mean
mixture density network
conventional least-squares result
special case
course
multimodal distribution
conditional mean
limited value
variance
density function
condi- exercise
tional average
s2
x
e k = t − e [ t|x ] 2 |x k
x
πk
x
πl
x
µl
x
corresponding least-squares result
variance
function
x
multimodal distributions
conditional mean
poor representation
data
instance
simple robot arm
figure
possible joint angle settings
order
desired end-effector location
average
solutions
solution
such cases
conditional mode
value
conditional mode
mixture density network
simple analytical solution
numerical iteration
simple alternative
mean
probable component
i.e.
one
mixing coefﬁcient
value
x
toy data
figure
d
bayesian neural networks so
discussion
neural networks
use
maximum like- lihood
network parameters
weights
biases
max- imum likelihood
map
maximum posterior
approach
regularizer
logarithm
prior parameter distribu- tion
bayesian treatment
distribution
parameters
order
predictions
section
bayesian solution
simple linear regression model
assumption
gaussian noise
posterior distribu- tion
predictive distribu- tion
closed form
case
multilayered network
nonlinear dependence
network function
parameter values
exact bayesian treatment
fact
log
pos- terior distribution
multiple local minima
error function
technique
variational inference
chapter
bayesian neural networks
factorized gaussian approximation  n
neural networks
posterior distribution
hinton
van camp
full- covariance gaussian
barber
bishop
barber
bishop
complete treatment
laplace approximation
mackay
mackay
basis
discussion
posterior distribution
mode
true posterior
covariance
gaus- sian
network function
respect
parameters
region
parameter space
posterior probability
approximations
models
linear regression
classiﬁcation models
chapters
results
use
evidence framework
point estimates
hyperparameters
alternative models
example
networks
different numbers
hid- den units
regression case
modiﬁcations
classiﬁcation tasks
posterior parameter distribution consider
problem
single continuous target variable t
vector x
inputs
extension
multiple targets
conditional distribution p
t|x
x-dependent mean
output
neural network model y
precision
inverse variance
β p
t|x
w
β
n
t|y
β−1
prior distribution
weights
form
i.i.d
data set
n observations x1
corresponding set
target values d =
t1
likelihood function
p
w|α
= n
w|0
α−1i
p
d|w
β
n=1 n
tn|y
w
β−1
posterior distribution
α
β
p
w|α
p
d|w
β
consequence
nonlinear dependence
y
w
non- gaussian
gaussian approximation
posterior distribution
laplace approximation
maximum
posterior
iterative numerical optimization
logarithm
posterior

n  exercise
use
general result
marginal p
t
p
t|x
w
β
n t|y
gt
w − wmap
β−1
p
t|x
d
α
β
n t|y
σ2
x
bayesian neural networks form ln p
w|d
wtw − β
y
w
− tn
+ const
sum-of-squares
function
moment
α
β
maximum
posterior
standard nonlinear optimization algorithms
conjugate gradients
error backpropagation
required derivatives
mode wmap
local gaussian approximation
matrix
second derivatives
negative log posterior distribu- tion
from
a = −∇∇ ln p
w|d
α
β
αi + βh
h
hessian matrix
second derivatives
sum-of- squares
function
respect
components
w. algorithms
comput- ing
section
corresponding gaussian approximation
posterior
q
w|d
= n
w|wmap
a−1
predictive distribution
respect
posterior distribution p
t|x
d
p
t|x
w
q
w|d
dw
gaussian approximation
posterior
integration
nonlinearity
network function y
function
w.
progress
posterior distribution
small variance
characteristic scales
w
y
taylor series expansion
network function
wmap
retain
linear terms
y
gt
w − wmap
approximation
linear-gaussian model
gaussian distribution
p
w
p
t|w
mean
linear function
w
form g = ∇wy
x
w
|w=wmap
neural networks   n
use
laplace approximation result
p
d|α
β
p
d|w
β
p
w|α
dw
input-dependent variance
σ2
x
β−1 + gta−1g
predictive distribution p
t|x
d
mean
network function y
parameter
map value
variance
terms
ﬁrst
intrinsic noise
target
second
x-dependent term
uncertainty
interpolant
uncertainty
model parameters
corresponding predictive distribution
linear regression model
hyperparameter optimization so
hyperparameters
β
use
evidence framework
section
gaussian approximation
posterior
laplace approxima- tion
practical procedure
values
such hyperparameters
marginal likelihood
evidence
hyperparameters
network weights ln p
d|α
β
−e
wmap
−
w
total number
parameters
w
regularized error function
ln
ln β −
+ w
ln α + n
n
e
wmap
y
wmap
− tn
wt mapwmap
same form
corresponding result
linear regression model
evidence framework
point estimates
α
β
ln p
d|α
β
maximization
respect
α
analogy
linear regression case
section
eigenvalue equation
h
hessian matrix
second derivatives
sum-of- squares
function
w = wmap
analogy
βhui = λiui α = γ wt mapwmap
  i=1 w
bayesian neural networks
γ
effective number
parameters
γ = λi α + λi
section
section
note
result
linear regression case
nonlinear neural network
fact
changes
α
changes
hessian h
turn
eigenvalues
terms
derivatives
λi
respect
evidence
respect
re-estimation formula
β = n
n − γ
y
wmap
− tn
linear model
re-estimation
hyper- parameters
β
updating
posterior distribution
situation
neural network model
multimodality
posterior distribution
consequence
solution
wmap
log posterior
initialization
w. solutions
consequence
interchange
sign reversal symmetries
hidden units
predictions
equivalent solutions
inequivalent solutions
different values
optimized hyperparameters
order
different models
example neural networks
differ- ent numbers
hidden units
model evidence p
d
values
α
β
iterative optimization
hyperparameters
careful evaluation
α
β
gaussian approxima- tion
mackay
bishop
case
determinant |a|
hessian matrix
practice
determinant
trace
small eigenvalues
laplace approximation
local quadratic expansion
mode
posterior distribution
weights
section
mode
two-layer network
member
set
m
equivalent modes
interchange
sign-change symmetries
m
num- ber
hidden units
networks
different numbers
hid- den units
account
evidence
factor
m
bayesian neural networks
classiﬁcation so
laplace approximation
bayesian treat- ment
neural network regression models
modiﬁcations
 n
framework
arise
classiﬁcation
sider
network
single logistic sigmoid output
two-class classiﬁcation problem
extension
networks
multiclass softmax outputs
analogous results
linear classiﬁcation models
section
reader
material
section
log likelihood function
model
ln p
d|w
tn ln yn +
− tn
ln
− yn
tn ∈
target values
≡ y
w
note
hyperparameter
data points
prior
isotropic gaussian
form
ﬁrst stage
laplace framework
model
hyperparameter α
parameter vector w
log posterior distribution
regularized error function e
w
− ln p
d|w
wtw
error backpropagation
standard optimiza- tion algorithms
section
solution wmap
weight vector
next step
hessian matrix h
second derivatives
negative log likelihood function
instance
exact method
sec- tion
outer product approximation
second derivatives
negative log posterior
form
gaussian approximation
posterior
hyperparameter α
marginal likelihood
form ln p
d|α
−e
wmap
ln|a| + w
ln α + const
regularized error function
e
wmap
tn ln yn +
− tn
ln
− yn
+
wt mapwmap
yn ≡ y
wmap
evidence function
respect
re-estimation equation
use
evidence procedure
α
figure
synthetic two-dimensional data
appendix a
predictive distribution
integration
nonlinearity
network function
neural networks exercise
exercise

bayesian neural networks
figure
illustration
evidence framework
synthetic two-class data set
green curve
optimal de- cision boundary
black curve
result
two-layer network
hidden units
maximum likeli- hood
red curve
re- sult
regularizer
α
evidence pro- cedure
initial value α
note
evidence proce- dure
over-ﬁtting
network
−1 −2 −2
simplest approximation
posterior distribution
approximation p
t|x
d
p
t|x
wmap
account
variance
posterior distribution
case
linear approximation
network
case
regression
logistic sigmoid output- unit activation function
output
range
linear approximation
output unit activation
form
x
w
 amap
x
bt
w − wmap
amap
x
x
wmap
vector b ≡ ∇a
x
wmap
backpropagation
gaussian approximation
posterior distribution
w
model
linear function
w
results
section
distribution
output unit activation values
distribution
network weights
δ p
d
− amap
x
bt
x
w − wmap
q
w|d
dw
q
w|d
gaussian approximation
posterior distribution
section
distribution
mean amap
x
wmap
variance
x
bt
x
a−1b
x
σ2
predictive distribution
p
t =
d
σ
p
d
da
neural networks
−1 −2 −2
−1 −2 −2
figure
illustration
laplace approximation
bayesian neural network
hidden units
‘ tanh ’ activation functions
single logistic-sigmoid output unit
weight parameters
scaled conjugate gradients
hyperparameter α
evidence framework
left
result
simple approximation
point estimate wmap
parameters
green curve
y
decision boundary
other contours correspond
output probabilities
y =
right
corresponding result
note
effect
marginalization
contours
predictions
input point x
posterior probabilities
towards
y
contour
convolution
logistic sigmoid
approximation
p
t =
d
σ κ
btwmap
κ
·
σ2 ﬁcation data
appendix a
figure
example
framework
synthetic classi-
b
functions
x
 
two-layer network function
form
hidden- unit nonlinear activation functions
·
logistic sigmoid functions
form
σ
=
+ exp
−a
equivalent network
same func- tion
hidden unit activation functions
tanh
tanh func- tion
hint
relation
σ
parameters
networks
linear transformations

www show
likelihood function
conditional distribution
multioutput neural network
sum-of-squares error function
 
regression problem
multiple target variables
distribution
targets
input vector x
form p
t|x
w
= n
t|y
σ
y
output
neural network
input vector x
weight vector w
σ
covariance
assumed gaussian noise
targets
set
independent observations
x
t
error function
order
maximum likelihood solution
w
σ
σ
data
expression
maximum likelihood solution
σ
note
optimizations
w
σ
contrast
case
independent target variables
section
 
binary classiﬁcation problem
target values
t ∈
network output y
p
t =
probability
class label
training data point
data
error function
negative log likelihood
error function
note
error function
model robust
data
contrast
usual error function

www show
likelihood
multiclass neural network model
network outputs
interpretation yk
p
tk =
minimization
cross-entropy error function

derivative
error function
respect
activation ak
output unit
logistic sigmoid activation function satisﬁes

derivative
error function
respect
activation ak
output units
softmax activation function satisﬁes

derivative
logistic sigmoid activation function
terms
function value
corresponding result
‘ tanh ’ activation function

error function
binary classiﬁcation problems
network
logistic-sigmoid output activation function
�
data
target values
∈
correspond- ing error function
network
output −1 � y
x
w
values
class c1
= −1
class c2
appropriate choice
output unit activation function

www
hessian matrix h
eigenvector equation
vector v
eigenvectors
turn
h
positive deﬁnite
eigenvalues
  
neural networks
 
www
quadratic error function
hessian matrix h
eigenvalue equation
show
con- tours
constant error
ellipses
axes
eigenvectors
lengths
square root
corresponding eigenvalues
 
www
local taylor expansion
error function
stationary point w
sufﬁcient condition
stationary point
local minimum
error function
hessian matrix h
w = w
positive deﬁnite

consequence
symmetry
hessian matrix h
number
independent elements
quadratic error function
w
w +
/2

taylor expansion
verify
terms
o
cancel
right-hand side
 
section
procedure
jacobian matrix
neural network
backpropagation procedure
derive
alternative formalism
jacobian
forward propagation equations

outer product approximation
hessian matrix
neural network
sum-of-squares error function
result
case
multiple outputs

squared loss function
form e
y
t
p
dx dt
y
parametric function
neural network
result
function y
error
conditional expectation
t
x
result
second derivative
e
respect
elements
ws
vector w
∂2e ∂wr∂ws = ∂y ∂wr ∂y ∂ws p
x
dx
note
ﬁnite sample
p
x

two-layer network
form
figure
addition
extra parameters
skip-layer connections
inputs
outputs
discussion
section
equations
derivatives
error function
respect
additional parameters

expression
outer product approximation
hessian matrix
network
single output
logistic sigmoid output-unit activation function
cross-entropy error function
result
sum-of-squares error function

derive
expression
outer product approximation
hessian matrix
network
k outputs
softmax output-unit activation function
cross-entropy error function
result
sum-of- squares
function
  
expression
outer product approximation
hes- sian matrix
case
k >
output units
hence
recursive expression
number n
patterns
similar expres- sion
number k
outputs
results
identity
sequential update expressions
inverse
extra patterns
extra outputs
 
results
elements
hessian matrix
two-layer feed-forward network
application
chain rule
cal- culus
 
results
section
exact hessian
two-layer network
skip-layer connections
inputs
outputs

network function
invariant un- der
transformation
inputs
weights
biases
network outputs
transforma- tion
second-layer weights
biases
  
www
quadratic error function
form e = e0
w − w
th
w − w
minimum
hessian matrix h
positive deﬁnite
initial weight vector w
origin
simple gradient descent w
τ
w
τ−1
ρ∇e
τ
step number
ρ
rate
show
τ steps
components
weight vector parallel
eigenvectors
h
τ
j =
−
− ρηj
τ
w j
wj = wtuj
ηj
eigenvectors
eigenvalues
h
show
τ → ∞
w
τ
w
|1 − ρηj|
training
ﬁnite number τ
steps
huj = ηjuj

   i
neural networks components
weight vector parallel
eigenvectors
hessian satisfy w
τ
j  w | |w j
ηj
ρτ
j|
ηj
ρτ
−1
|w
τ
j
compare
result
discussion
section
regularization
simple weight decay
show
ρτ
−1
regularization param- eter λ
above results
effective number
parameters
network
training
 
multilayer perceptron
arbitrary feed-forward topology
tangent propagation error function
regularizing function
show
regularization term ω
sum
patterns
terms
form ωn =
gyk
g
differential operator
τi g ≡ i ∂ ∂xi
forward propagation equations
= h
aj
aj = wjizi
operator g
ωn
forward propagation
following equations
αj = h
aj
βj
βj = wjiαi
new variables
≡ gzj
βj ≡ gaj
derivatives
ωn
respect
weight wrs
network
form
∂ωn ∂wrs = k αk
φkrzs + δkrαs
δkr ≡ ∂yk ∂ar
φkr ≡ gδkr
write
backpropagation equations
δkr
set
back- propagation equations
evaluation
φkr
 
framework
data
special case
transformation
addition
random noise x → x + ξ
ξ
gaussian distribution
zero mean
unit covariance
argument
section
regularizer reduces
tikhonov form

www
neural network
convolutional network
section
multiple weights
same value
discuss
standard backpropagation algorithm
order
such constraints
derivatives
error function
respect
adjustable parameters
network

verify
result

result

result
 
derivatives
mixing coefﬁcients
πk
respect
auxiliary parameters
ηj
exercises
∂πk ∂ηj = δjkπj − πjπk
hence
use
constraint k πk =
result

write
pair
equations
cartesian coordinates
x1
x2
robot arm
figure
terms
joint angles
θ2
lengths l1
l2
links
origin
coordinate system
attachment point
arm
equations
‘ forward kinematics ’
robot arm

result
derivative
error function
respect
network output activations
mixing coefﬁcients
mixture density network

result
derivative
error function
respect
network output activations
component
mixture density network

result
derivative
error function
respect
network output activations
component variances
mixture density network

results
conditional mean
variance
mixture density network model

general result
predictive distribution
laplace approximation
bayesian neural network model
neural networks

make use
laplace approximation result
evidence function
hyperparameters
bayesian neural network model

modiﬁcations
framework
bayesian neural networks
section
multiclass problems
networks
softmax output-unit activation functions
 
analogous steps
section
regression networks
result
marginal likelihood
case
net- work
cross-entropy error function
logistic-sigmoid output-unit activa- tion function
kernel methods
chapters
linear parametric models
regression
classiﬁcation
form
mapping y
input
output y
vector w
adaptive parameters
learning phase
set
training data
point estimate
parameter vector
posterior distribution
vector
training data
predictions
new inputs
learned parameter vector w.
approach
nonlinear parametric models
neural networks
class
pattern recognition techniques
training data points
subset
prediction phase
instance
parzen probability density model
linear combination
‘ kernel ’ functions
training data points
section
simple technique
classiﬁcation
neighbours
new test vector
same label
chapter
section
kernel methods
example
training set
examples
memory-based methods
entire training
order
predictions
future data points
similarity
vectors
input space
train ’
predictions
test data points
many linear parametric models
equivalent ‘ dual represen- tation ’
predictions
linear combinations
kernel function
training data points
models
ﬁxed nonlinear feature space mapping φ
x
kernel function
relation k
x
φ
x
tφ
x
deﬁnition
kernel
symmetric function
arguments
k
x
k
x
kernel concept
ﬁeld
pat- tern recognition
aizerman
al
context
method
potential functions
analogy
electrostatics
many years
machine learning
context
large- margin classiﬁers
boser
al
rise
technique
support vector machines
considerable interest
topic
terms
theory
applications
signiﬁcant developments
extension
kernels
symbolic objects
range
problems
example
kernel function
identity mapping
feature space
φ
x
= x
case k
x
linear kernel
concept
kernel
inner product
feature space
interesting extensions
many well-known algorithms
use
kernel trick
kernel substitution
general idea
algorithm
way
input vector x enters
form
scalar products
scalar product
other choice
kernel
instance
technique
kernel substitution
component analysis
order
nonlinear variant
pca
sch¨olkopf
other examples
kernel substitution
nearest-neighbour classiﬁers
kernel fisher discriminant
mika
roth
steinhage
baudat
anouar
numerous forms
kernel functions
common use
several examples
chapter
property
function
difference
arguments
k
x
k
x − x
stationary kernels
translations
input space
further specialization
homogeneous kernels
ra- dial basis functions
magnitude
distance
euclidean
arguments
k
x
k
x − x
brich
shawe-taylor
cristianini
recent textbooks
kernel methods
sch¨olkopf
smola
her- chapter
section
section
  n
   n
 
n
dual representations
dual representations many
models
regression
classiﬁcation
terms
dual representation
kernel function
concept
important role
support vector machines
next chapter
linear regression model
parameters
regularized sum-of-squares error function
j
w
wtφ
xn
+
wtw
λ �
gradient
j
w
respect
solution
w
form
linear combination
vectors φ
xn
coefﬁcients
functions
w
form w =
λ wtφ
xn
tn φ
xn
anφ
xn
φta
φ
design matrix
nth row
φ
xn
t.
vector
=
an
t
= −
λ wtφ
xn
tn
parameter vector w
least- squares
terms
parameter vector
rise
dual represen- tation
w = φta
j
w
j
atφφtφφta −
ttt +
atφφta
t =
t1
tn
gram matrix k = φφt
n × n symmetric matrix
elements knm = φ
xn
tφ
xm
k
kernel function k
x
terms
gram matrix
sum-of-squares error function
j
atkka −
ttt +
atka
gradient
j
respect
following solu- tion
=
k + λin
−1 t.
linear regression model
following prediction
new input x y
x
wtφ
x
atφφ
x
k
x
t
k + λin
t
vector k
x
elements
x
k
x
dual formulation
solution
least-squares problem
terms
kernel function k
dual formulation
solution
linear combination
elements
φ
x
original formulation
terms
parameter vector w. note
prediction
x
linear combination
target values
training set
fact
result
different notation
section
dual formulation
parameter vector
n × n matrix
whereas
original parameter space formulation
m × m matrix
order
w.
n
m
dual formulation
advantage
dual formulation
terms
kernel function k
work
terms
kernels
explicit introduction
feature vector φ
x
feature spaces
dimensionality
existence
dual representation
gram matrix
property
many linear models
perceptron
section
dual- ity
probabilistic linear models
regression
technique
gaussian processes
duality
important role
support vector machines
chapter
exercise
exercise
 i=1 m
kernel methods
kernels
order
kernel substitution
valid kernel functions
approach
feature space mapping φ
x
corresponding kernel
figure
kernel function
one-dimensional input space
k
x
φ
x
tφ
x
φi
x
φi
x
φi
x
basis functions
alternative approach
kernel functions
case
function
valid kernel
other words
scalar product
feature space
simple example
kernel function
k
z
−0.5 −1
−1
−1
−1
kernels
−1
−1
figure
illustration
construction
kernel functions
corresponding set
basis func- tions
column
plot
kernel function k
x
function
x
x =
upper plot
corresponding basis functions
polynomials
left column
‘ gaussians
centre column
logistic sigmoids
right column
particular case
two-dimensional input space x =
x1
x2
terms
corresponding nonlinear feature
k
z
=
x1z1 + x2z2
xtz =
+
+
√2x1x2
x2 =
x2 = φ
x
tφ
z
√2z1z2
z2
t
z2
t
feature mapping
form φ
x
=
x2
possible second order terms
√2x1x2
x2 more
simple way
function con-
valid kernel
function φ
x
sufﬁcient condition
function k
valid kernel
shawe- taylor
cristianini
gram matrix k
elements
k
xm
positive semideﬁnite
possible choices
set
positive semideﬁnite matrix
same thing
matrix
elements
powerful technique
new kernels
simpler kernels
building blocks
following properties
appendix c
kernel methods techniques
new kernels
given valid kernels
x
x
x
new kernels
k
x
ck1
x
k
x
f
x
k1
x
f
x
k
x
q
x
k
x
exp
x
k
x
k1
x
k2
x
k
x
k1
x
k2
x
k
x
k3
φ
x
φ
x
k
x
xtax k
x
ka
xa
kb
xb
k
x
ka
xa
kb
c >
function
q
·
nonneg- ative coefﬁcients
φ
x
function
x
rm
k3
·
valid kernel
rm
a
symmetric positive semideﬁnite matrix
xa
xb
variables
disjoint
x =
xa
xb
ka
kb
valid kernel functions
respective spaces
properties
construction
complex kernels
applications
kernel k
positive semideﬁnite
appropriate form
similarity
x
x
application
few common examples
kernel functions
extensive discus- sion
‘ kernel engineering ’
shawe-taylor
cristianini
simple polynomial kernel k
x
contains
kernel k
x
c
corresponding feature
φ
x
con- m terms
degree
+ c stant
linear terms
terms
order
k
x
=
monomials
order m.
instance
images
kernel
particular weighted sum
possible products
m pixels
ﬁrst image
m pixels
second image
gener- m
terms
m
k
x
=
results
kernels
valid kernel functions
+ c xtx
kernel
form k
x
exp −x − x2/2σ2
‘ gaussian ’ kernel
note
context
probability density
normalization coefﬁcient
exercise
exercise
kernels
valid kernel
square x − x2 = xtx +
x
tx
k
x
exp −xtx/2σ2 exp xtx/σ2 exp −
x
tx/2σ2
use
validity
linear kernel k
x
note
feature vector
gaussian kernel
inﬁnite dimensionality
gaussian kernel
use
euclidean distance
kernel substitution
xtx
nonlinear kernel κ
x
x
exp −
κ
x
x
κ
x
x
x
x
important contribution
kernel viewpoint
exten- sion
vectors
real numbers
kernel functions
objects
graphs
sets
strings
text doc- uments
instance
ﬁxed set
nonvectorial space consisting
possible subsets
set
a1
a2
such subsets
simple choice
kernel
a1
a2
a1 ∩ a2
intersection
sets a1
a2
|a|
number
subsets
valid kernel function
inner product
feature space
powerful approach
construction
kernels starts
probabilistic generative model
haussler
generative models
discriminative setting
generative models
data
case
hidden markov models
sequences
length
contrast
discriminative models
performance
discriminative tasks
generative models
interest
approaches
lasserre
way
generative model
kernel
kernel
discriminative approach
generative model p
kernel
k
x
p
x
p
x
valid kernel function
inner product
one-dimensional feature space
mapping p
x
inputs
x
high probabilities
class
kernels
sums
products
different probability distributions
coefﬁcients p
i
form k
x
p
x|i
p
x|i
p
i
i
     z n
kernel methods
overall multiplicative constant
mixture distribution
components
index i
role
‘ latent ’
inputs
x
large value
kernel function
signiﬁcant probability
range
different components
limit
inﬁnite sum
kernels
form k
x
p
x|z
p
x|z
p
z
dz
z
continuous latent
data consists
ordered sequences
length l
observation
x =
x1
popular generative model
sequences
hidden markov model
distribution p
x
marginalization
corresponding sequence
hidden states z =
z1
zl
approach
kernel function
similarity
sequences x
x
mixture representation
k
x
x
p
x|z
p
x|z
p
z
observed sequences
same hidden sequence z
model
sequences
length
alternative technique
generative models
kernel functions
fisher kernel
jaakkola
haussler
parametric generative model p
x|θ
θ
vector
parameters
goal
kernel
similarity
input vectors
x
generative model
jaakkola
haussler
gradient
respect
θ
vector
‘ feature ’ space
same dimensionality
θ
fisher score section
section
exercise
fisher kernel
g
x
∇θ ln p
x|θ
f
fisher information matrix
k
x
g
x
tf−1g
θ
x
f = ex g
x
g
x
t
expectation
respect
distribution p
x|θ
perspective
information geometry
amari
differential geometry
space
model parameters
ply note
presence
fisher information matrix
kernel
nonlinear re-parameterization
density model θ → ψ
θ
practice
fisher information matrix
approach
expectation
deﬁnition
fisher informa- tion
sample average
n f  g
xn
g
xn
t.
n=1  n
radial basis function networks
section
section
covariance matrix
fisher scores
fisher
corre- sponds
whitening
scores
fisher information matrix
noninvariant kernel k
x
g
x
tg
θ
x
application
fisher kernels
retrieval
hofmann
ﬁnal example
kernel function
sigmoidal kernel
k
x
tanh axtx + b
gram matrix
positive semideﬁnite
form
kernel
practice
vapnik
kernel expansions
support vector machine
superﬁcial resemblance
neural network models
limit
inﬁnite number
basis functions
bayesian neural network
appropriate prior reduces
gaussian process
link
neural networks
kernel methods
radial basis function networks
chapter
regression models
linear combinations
ﬁxed basis functions
detail
basis functions
choice
radial basis functions
property
basis function
radial distance
euclidean
centre µj
φj
x
h
x − µj
historically
radial basis functions
purpose
exact func- tion interpolation
powell
set
input vectors
x1
target values
t1
goal
smooth function f
x
target value
f
xn
tn
n =
n.
f
linear combination
radial basis functions
data point f
x
wnh
x − xn
n=1
values
coefﬁcients
wn
squares
same number
coefﬁcients
constraints
result
function
target value
pattern recognition applications
target values
exact interpolation
over-ﬁtted solution
expansions
radial basis functions
regularization theory
pog- gio
girosi
bishop
sum-of-squares error function
regularizer
terms
differential operator
optimal solution
expansion
green ’
functions
operator
eigenvectors
discrete matrix
basis function
data   n
 n=1 n 
kernel methods point
differential operator
green ’ s functions
radial distance
corresponding data point
due
presence
regularizer
solution
training data
motivation
radial basis functions
consideration
interpolation problem
input
target
variables
noise
input variable x
webb
bishop
variable ξ
distribution ν
ξ
sum-of-squares error function
e =
y
xn + ξ
ν
ξ
dξ
appendix d exercise
calculus
variations
respect
function f
x
basis functions
y
xn
tnh
x − xn
h
x − xn
ν
x − xn
ν
x − xn
n
n=1
basis function
data point
nadaraya-watson model
different perspective
section
noise distribution ν
ξ
function
ξ
basis functions
n h
x − xn
value
x
effect
such normalization
figure
normal- ization
practice
regions
input space
basis functions
small values
predic- tions
such regions
bias parameter
note
basis functions
situation
expansions
normalized radial basis functions
application
kernel density estimation
problem
regression
section
basis function
data point
corre- sponding model
predictions
new data points
models
broomhead
lowe
moody
darken
poggio
girosi
expan- sion
radial basis functions
number m
basis functions
number n
data points
number
basis functions
locations µi
centres
input data
basis functions
ﬁxed
coefﬁcients
wi
squares
usual set
linear equations
section
radial basis function networks
−1
−1 figure
plot
set
gaussian basis functions
left
normalized basis functions
right
 n section
section
ways
basis function centres
randomly chosen subset
data points
systematic approach
orthogonal least squares
chen
sequential selection process
step
next data point
basis function centre
one
reduction
sum-of-squares error
values
expansion coefﬁcients
part
algorithm
k-means
set
basis function
training data points
nadaraya-watson model
section
prediction
linear regression model
new input x
form
linear combination
training
target values
coefﬁcients
‘ equivalent kernel
equivalent kernel
summation constraint
kernel regression model
different perspective
kernel density estimation
suppose
training
xn
tn
parzen density estimator
joint distribution p
p
n f
x − xn
t − tn
n=1
f
component density function
such component
data point
expression
regression function y
x
conditional average
target
        m m n n −∞       −∞ ∞ = m n n y
x
g
x − xn
tn g
x − xm
k
xn
tn k
xn
g
x − xn
g
x − xm
g
x
∞ f
dt
k
xn
n=1
n
m
n
kernel function k
result
nadaraya-watson model
regression
nadaraya
watson
localized kernel function
prop- erty
weight
data points
kernel
summation constraint
kernel methods
input variable
y
x
e [ t|x ] = tp
t|x
dt ∞ −∞ tp
dt p
dt = = tf
x − xn
t − tn
dt f
x − xm
t − tm
dt
simplicity
component density functions
f
t dt
values
x
simple change
  
figure
illustration
nadaraya-watson kernel regression model
isotropic gaussian kernels
sinusoidal data set
original sine function
green curve
data points
blue
centre
isotropic gaussian kernel
regression function
condi- tional mean
red line
two- standard-deviation region
conditional distribution p
t|x
red shading
blue ellipse
data point
standard deviation contour
corresponding kernel
different scales
horizontal
vertical axes
−0.5 −1
fact
model
conditional expectation
full conditional distribution
p
t|x
= p
t
x
p
t
x
= f
x − xn
t − tn
n f
x − xm
t − tm
m exercise
other expectations
illustration
case
single input variable x
f
zero-mean isotropic gaussian
variable z =
x
t
variance σ2
corresponding conditional distribution
gaus- sian mixture
conditional mean
sinusoidal synthetic data
figure
obvious extension
model
ﬂexible forms
gaus- sian components
instance
different variance parameters
input
target variables
joint distribution p
t
x
gaussian mixture model
techniques
chapter
ghahra- mani
jordan
corresponding conditional distribution p
t|x
latter case
representation
terms
kernel func- tions
training
data points
number
components
mixture model
number
training
points
model
test data points
computational cost
training phase
order
model
predictions
gaussian processes
section
kernels
concept
duality
non- probabilistic model
regression
role
kernels
kernel methods tic discriminative models
framework
gaussian processes
kernels
bayesian setting
chapter
linear regression models
form y
wtφ
x
w
vector
parameters
φ
x
vector
ﬁxed nonlinear basis functions
input vector x
prior distribution
w
corresponding prior distribution
functions y
x
w
training data set
posterior distribution
w
corresponding posterior distribution
regression functions
turn
addition
noise
predictive distribution p
t|x
new input vectors
gaussian process viewpoint
parametric model
prior probability distribution
functions
ﬁrst sight
difﬁcult
distribution
uncountably inﬁnite space
functions
ﬁnite training set
values
function
discrete set
input values
training set
test
data points
practice
ﬁnite space
models equivalent
gaussian processes
many dif- ferent ﬁelds
instance
geostatistics literature gaussian process regression
kriging
cressie
arma
aver- age
models
kalman ﬁlters
radial basis function networks
forms
gaussian process models
reviews
gaussian processes
machine
perspective
mackay
williams
mackay
comparison
gaussian process models
alternative approaches
rasmussen
see
rasmussen
williams
recent textbook
gaussian processes
linear regression
order
gaussian process viewpoint
linear regression example
predictive distribution
terms
distributions
functions y
x
w
speciﬁc example
gaussian process
model
terms
linear combination
m ﬁxed basis functions
elements
vector φ
x
y
x
wtφ
x
x
input vector
w
m-dimensional weight vector
prior distribution
w
isotropic gaussian
form p
w
= n
w|0
α−1i
hyperparameter α
precision
inverse variance
distribution
value
w
deﬁnition
partic- ular function
x
probability distribution
w
therefore
probability distribution
functions y
x
practice
eval- uate
function
speciﬁc values
x
example
training data points  
x1
joint distribution
function val- ues
x1
y
xn
vector y
elements
= y
xn
n =
n. from
vector
y = φw
φ
design matrix
elements
= φk
xn
proba- bility distribution
y
first
y
linear combination
gaussian
variables
elements
w
hence
mean
covariance
e [ y ] = φe [
] =
cov [ y ] = e = φe
k
gram matrix
elements
wwt φt =
α φφt = k knm = k
xm
α φ
xn
tφ
xm
x
x
kernel function
model
particular example
gaussian process
gaussian process
probability distribution
functions y
x
set
values
y
x
arbitrary set
points x1
gaussian distribution
cases
input vector x
di- mensional
gaussian random ﬁeld
stochastic process y
x
joint probability distribution
ﬁnite set
values y
x1
y
xn
consistent manner
key point
gaussian stochastic processes
joint distribution
n variables
second-order statistics
mean
covariance
applications
prior knowledge
mean
y
x
symmetry
mean
weight values
w|α
basis function viewpoint
speciﬁcation
gaussian process
covariance
y
x
values
x
kernel function e [ y
xn
y
xm
= k
xm
speciﬁc case
gaussian process
linear regression model
weight
kernel function
kernel function
choice
basis function
figure
samples
functions
gaus- sian processes
different choices
kernel function
ﬁrst
‘ gaussian ’ kernel
form
exponential kernel
k
x
exp
−θ |x − x|
ornstein-uhlenbeck process
uh- lenbeck
ornstein
brownian motion
exercise
kernel methods figure
samples
gaus- sian processes
‘ gaussian ’ ker- nel
exponential kernel
−1.5 −3 −1
−1.5 −3
−0.5
gaussian processes
regression
order
gaussian process models
problem
regression
account
noise
observed target values
tn = yn + n
yn = y
xn
n
random noise variable
value
observation
noise
gaussian distribution
p
tn|yn
= n
tn|yn
β−1
β
hyperparameter
precision
noise
noise
data point
joint distribution
target values
=
t1
tn
t
values
y =
y1
yn
t
isotropic gaussian
form
in
n × n unit matrix
deﬁnition
gaussian process
marginal distribution p
y
mean
covariance
gram matrix k
p
t|y
= n
t|y
β−1in
p
y
= n
k
kernel function
k
property
points
corresponding values
xn
xm
dissimilar points
notion
similarity
application
order
marginal distribution p
t
input values x1
y
use
results
section
linear-gaussian model
using
marginal distribution
t
p
t
= p
t|y
p
y
dy = n
t|0
c

covariance matrix c
elements c
xn
xm
k
xm
result
fact
gaussian sources
randomness
y
x
covariances
kernel function
gaussian process regression
exponential
quadratic form
addition
linear terms
k
xm
θ0 exp −
xn − xm2 + θ2 + θ3xt nxm
note
term
θ3 corresponds
parametric model
linear function
input variables
samples
various values
parameters θ0
figure
figure
set
points
joint distribution
values
gaussian process viewpoint
model
joint distribution
sets
data points
goal
regression
predictions
target variables
new inputs
set
training data
tn =
t1
tn
t
values x1
observed training
goal
target variable tn +1
new input vector xn +1
predictive distri- bution p
tn +1|tn
note
distribution
variables x1
xn +1
notation simple
variables
conditional distribution p
tn +1|t
joint distribution p
tn +1
tn +1
vector
t1
tn
+1
t.
results
section
required conditional distribu- tion
figure
from
joint distribution
t1
tn +1
p
tn +1
= n
tn +1|0
cn +1
cn +1
n +
×
n +
covariance matrix
elements
joint distribution
results
section
conditional gaussian distribution
covariance matrix
cn +1 = cn k kt c
cn
n × n covariance matrix
elements
n
m
n
vector k
elements
xn +1
n =
n
kernel methods
−1.5 −3
−1.5 −3
−4.5 −9 −1
−1.5 −3
−2 −4 −1
−0.5
figure
samples
gaussian process
covariance function
title
plot
θ1
θ2
θ3
c = k
xn +1
xn +1
β−1
results
con- ditional distribution p
tn +1|t
gaussian distribution
mean
covariance
m
xn +1
ktc−1 n t σ2
xn +1
c − ktc−1 n k.
key results
gaussian process regression
vector k
function
test point input value xn +1
predictive distribu- tion
mean
variance
xn +1
example
gaussian process regression
figure
only restriction
kernel function
covariance matrix
positive deﬁnite
λi
eigenvalue
k
eigenvalue
c
λi + β−1
kernel matrix k
positive semideﬁnite
pair
points xn
xm
λi
eigenvalue λi
rise
positive eigenvalue
c
β >
same restriction
kernel function
techniques
section
 −1 n −3 figure
illustration
sampling
data points
tn
gaussian process
blue curve
sample func- tion
gaussian process
functions
red points
values
yn
function
set
in- put values
xn
correspond- ing values
tn
independent gaussian noise
yn
suitable kernels
tion
xn +1
form note
mean
predictive distribution
func- exercise
m
xn +1
= ank
xn +1
n=1
nth component
c−1 n t. thus
kernel function k
xm
distance xn − xm
expansion
radial basis functions
results
predictive distribution
gaussian pro- cess regression
arbitrary kernel function k
xm
particular case
kernel function k
terms
ﬁnite set
basis functions
results
section
linear regression
gaussian process viewpoint
such models
predictive distribution
parameter space viewpoint
linear regression result
function space viewpoint
gaussian process result
central computational operation
gaussian processes
inversion
matrix
size n × n
standard methods
o
n
computations
contrast
basis function model
matrix sn
size m × m
o
m
computational complexity
note
viewpoints
matrix inversion
training set
new test point
methods
vector-matrix multiply
cost o
n
gaussian process case
o
m
linear basis func- tion model
number m
basis functions
number n
data points
basis function
kernel methods figure
illustration
mechanism
gaussian process regression
case
training point
test point
red el- lipses
contours
joint dis- tribution p
t1
t2
t1
training data point
condition- ing
value
t1
correspond- ing
vertical blue line
tain p
t2|t1
function
t2
green curve
−1 t2 m
x2
framework
advantage
gaussian processes viewpoint
covariance functions
terms
inﬁnite number
basis functions
data sets
direct application
gaussian process methods
range
approximation schemes
training
size
exact approach
gibbs
tresp
smola
bartlett
williams
seeger
csat´o
opper
seeger
al.
practical issues
application
gaussian processes
bishop
nabney
gaussian process regression
case
single tar- get
extension
formalism
target variables
co-kriging
cressie
various other extensions
gaus- exercise
figure
illustration
gaussian process re- gression
sinusoidal data set
figure a.6
right-most data points
green curve
sinusoidal function
data points
blue
addition
gaussian noise
red line
mean
gaussian process predictive distri- bution
shaded region cor- responds
standard deviations
notice
uncertainty
re- gion
right
data points
−0.5


sian process regression
purposes
distribution
low-dimensional manifolds
unsupervised learning
bishop
solution
stochastic differential equations
graepel
hyperparameters
predictions
gaussian process model
part
choice
covariance function
practice
covariance function
parametric family
functions
parameter values
data
parameters
such things
length scale
correlations
precision
noise
correspond
hyperparameters
standard parametric model
techniques
hyperparameters
evaluation
likelihood function p
t|θ
θ
hyperparameters
gaussian pro- cess model
approach
point estimate
θ
log likelihood function
set
hyperparameters
regression problem
type
maximum like- lihood procedure
linear regression models
maximization
log likelihood
efﬁcient gradient-based optimization algorithms
conjugate gradients
fletcher
nocedal
wright
bishop
nabney
log likelihood function
gaussian process regression model
standard form
multivariate gaussian distribution
ln p
t|θ
− ln|cn| − ttc−1 n t − n
ln
nonlinear optimization
gradient
log
func- tion
respect
parameter vector θ
evaluation
derivatives
cn
case
covariance func- tions
chapter
use
result
c.21
derivative
c−1 n
result
c.22
derivative
ln|cn|
∂ ∂θi ln p
t|θ
tr c−1 n ∂cn ∂θi
ttc−1 n ∂cn ∂θi c−1 n t.
ln p
t|θ
nonconvex function
multiple max- ima
prior
θ
log
gradient-based methods
bayesian treatment
marginals
θ
product
prior p
θ
likelihood func- tion p
t|θ
exact marginalization
approximations
gaussian process regression model
predictive distribution
mean
variance
functions
input vector x
contribution
predictive variance
additive noise
gov-
parameter β
problems
heteroscedas- tic
noise variance
x
section

kernel methods
gaussian processes
figure
samples
ard prior
kernel function
left plot corresponds
= η2 =
right plot cor- responds
η2
gaussian process framework
second gaussian process
dependence
β
input x
goldberg
β
variance
gaussian process
ln β
x
automatic relevance determination
previous section
maximum likelihood
de- termine
value
correlation length-scale parameter
gaussian process
technique
separate parameter
input
rasmussen
williams
result
optimization
parameters
maximum likelihood
relative importance
different inputs
data
exam- ple
gaussian process context
automatic relevance determination
ard
framework
neural networks
mackay
neal
mechanism
appropriate inputs
section
gaussian process
two-dimensional input space x =
x1
x2
kernel function
form k
x
− ηi
xi − xi
i=1
samples
functions y
x
different settings
precision parameters
figure
particu- lar parameter ηi
function
corresponding input variable xi
parameters
data set
maximum likelihood
input variables
little effect
predictive distribution
corresponding values
ηi
practice
such inputs
ard
simple synthetic data
inputs
x2
x3
nabney
figure
target variable t
values
x1
function sin
  d   d
figure
illustration
automatic rele- vance determination
gaus- sian process
synthetic prob- lem
inputs
x2
x3
curves
corresponding values
hyperparameters η1
η2
η3
blue
func- tion
number
iterations
marginal likelihood
details
text
logarithmic scale
vertical axis
gaussian noise
values
x2
corresponding values
x1
noise
values
x3
independent gaussian dis- tribution
thus x1
good predictor
t
x2
noisy predictor
t
x3
chance correlations
marginal likelihood
gaussian process
ard parameters η1
η2
η3
scaled conjugate gradients
figure
η1 converges
large value
η2
value
η3
small indicating
x3
ard framework
exponential-quadratic kernel
following form
kernel function
applications
gaussian processes
range
regression problems
xm
− ηi
xni − xmi
i=1 + θ2 + θ3 xnixmi
i=1
d
dimensionality
input space
gaussian processes
classiﬁcation
probabilistic approach
classiﬁcation
goal
posterior probabilities
target
new input vector
set
training data
probabilities
interval
gaussian process model
predictions
entire real axis
gaussian processes
problems
output
gaussian process
appropriate nonlinear activation function
two-class problem
target variable t ∈
gaussian process
function
x
function
logistic sigmoid y = σ
non-gaussian stochastic process
functions y
x
y ∈
case
one-dimensional input space
figure
probability
kernel methods
−5 −10 −1
−1
figure
left plot
sample
gaussian process
functions
x
right plot
result
sample
logistic sigmoid function
bution
target variable t
bernoulli distribution p
t|a
= σ
t
− σ
training
inputs
x1
observed target variables
=
t1
tn
t.
single test point xn +1
target value tn +1
goal
predictive distribution p
tn +1|t
conditioning
input variables
gaussian process
vector an +1
compo- nents
x1
xn +1
turn deﬁnes
non-gaussian process
tn +1
training data
required predictive distri- bution
gaussian process
an +1
form p
an +1
= n
an +1|0
cn +1
regression case
covariance matrix
noise term
training data points
numerical reasons
noise-like term
parameter ν
covariance matrix
positive deﬁnite
covariance matrix cn +1
elements
c
xn
xm
k
xm
νδnm
k
xm
positive semideﬁnite kernel function
kind
section
value
ν
advance
kernel function k
vector θ
parameters
θ
training data
two-class problems
p
tn +1 =
value
p
tn +1 =
− p
tn +1 =
required      predictive distribution
p
tn +1 =
= p
tn +1 =
+1
p
an +1|tn
dan +1
p
tn +1 =
+1
σ
an +1
sam- pling methods
neal
techniques
analytical approximation
section
approximate formula
convolution
logistic sigmoid
gaussian distribution
result
gaussian approximation
posterior distribution p
an +1|tn
usual justiﬁcation
gaussian approximation
posterior distribution
true posterior
number
data points
consequence
central limit theorem
case
gaussian processes
number
variables grows
number
data points
argument
number
data points
ﬁxed region
x space
corresponding uncertainty
function
x
gaussian
williams
barber
different approaches
gaussian approximation
technique
variational inference
gibbs
mackay
use
local variational bound
logistic sigmoid
product
sigmoid functions
product
gaussians
marginalization
an
approach
bound
likelihood function p
tn|θ
variational framework
gaussian process classiﬁcation
k >
problems
gaussian approximation
softmax function
gibbs
second approach
expectation propagation
opper
winther
minka
seeger
true posterior distribution
expectation propagation approach
good results
laplace approximation
third approach
gaussian process classiﬁcation
laplace approximation
detail
order
predictive distribution
gaussian approximation
posterior distribution
an +1
bayes ’ theorem
p
an +1|tn
= = = p
an +1
an|tn
p
tn
p
tn
p
an +1
an
p
tn|an +1
an
dan p
an +1|an
p
an
p
tn|an
dan p
an +1|an
p
an|tn
dan
section
section
section
section
 n  n
n
kernel methods
p
tn|an +1
an
= p
tn|an
conditional distribution p
an +1|an
results
gaussian pro- cess regression
p
an +1|an
= n
an +1|ktc−1 n an
c −
n k
laplace approximation
posterior distribution p
an|tn
standard result
convolution
gaussian distributions
prior p
an
zero-mean gaussian process
covariance ma- trix cn
data term
independence
data points
p
tn|an
n=1 σ
tn
− σ
= eantnσ
−an
n=1
laplace approximation
taylor
logarithm
p
an|tn
additive normalization constant
quantity ψ
an
= ln p
an
+ ln p
tn|an
− n c−1 at n an − n
ln
ln|cn| + tt n an ln
+ ean
+ const
− n=1
first
mode
posterior distribution
gradient
ψ
an
∇ψ
an
= tn − σn − c−1 n an
σn
vector
elements σ
mode
gradient
σn
an
iterative scheme
newton-raphson method
rise
squares
irls
algorithm
second derivatives
ψ
an
laplace approximation
n ∇∇ψ
an
= −wn − c−1
wn
diagonal matrix
elements σ
σ
result
derivative
logistic sigmoid function
note
diagonal elements
range
wn
positive deﬁnite matrix
cn
inverse
positive deﬁnite
construction
sum
positive deﬁnite matrices
positive deﬁnite
hessian matrix a = −∇∇ψ
an
positive deﬁnite
posterior distribution p
an|tn
log convex
therefore
single mode
global section
exercise
maximum
posterior distribution
hessian
function
an
newton-raphson formula
iterative update equation
an
n = cn
wn cn
tn − σn + wn an
anew
equations
mode
n
mode
gradient ∇ψ
an
a a n
n = cn
tn − σn
a
mode a n
posterior
hessian matrix
h = −∇∇ψ
an
= wn + c−1
elements
wn
a proximation
posterior distribution p
an|tn
n
n
gaussian ap- q
an
= n
an|a n
h−1
 exercise
exercise
linear-gaussian model
general result
n + cn
−1k
e
an +1|tn ] = kt
tn − σn
var
an +1|tn ] =
− kt
w−1
gaussian distribution
p
an +1|tn
result
bayesian logistic regression model
section
decision boundary correspond- ing
+1|tn
mean
effect
variance
parameters θ
covariance function
approach
likelihood function
p
tn|θ
expressions
log likelihood
gradient
suitable regularization terms
maximum likelihood solution
likelihood function
p
tn|θ
= p
tn|an
p
an|θ
dan
use
laplace approx- imation
result
following approximation
log
likelihood function ln p
tn|θ
= ψ
a n
ln|wn + c−1 n | + n
ln
 n  ∂θj n
kernel methods n
ln p
a n|θ
ln p
tn|a n
gradient
ψ
a
ln p
tn|θ
respect
parameter vector θ
note
changes
θ
changes
a n
additional terms
gradient
respect
sets
terms
ﬁrst arising
dependence
covariance matrix cn
θ
rest
dependence
terms
explicit dependence
θ
n
θ
results
c.21
c.22
∂ ln p
tn|θ
n c−1
n c−1 n
n ∂cn
− tr
cn wn
∂cn ∂θj
terms
dependence
a n
θ
laplace approximation
ψ
an
gradient
an = a n
contribution
gradient
result
dependence
a n
following contribution
respect
component θj
θ n
− ∂
+ c−1 n | ∂a
n ∂θj
= −
cn wn
nn n=1 n
− σ σ n
n
n ∂θj
n = σ
σ deﬁnition
wn
derivative
a
relation
respect
n
result
c.22
n
respect
differ- ∂a
= ∂cn ∂θj
tn − σn
cn wn ∂a n ∂θj
∂a n ∂θj =
wn cn
∂cn ∂θj
tn − σn
combining
gradient
log likelihood function
standard nonlinear optimization algo- rithms
order
value
θ
application
laplace approximation
gaussian pro- cesses
synthetic two-class data
figure
extension
laplace approximation
gaussian processes
k >
classes
softmax activation function
williams
barber
−2
figure
illustration
use
gaussian process
classiﬁcation
data
left
optimal decision boundary
true distribution
decision boundary
gaussian process classiﬁer
right
predicted posterior probability
blue
red classes
gaussian process decision boundary
connection
neural networks
range
functions
neural network
number m
hidden units
large m
two-layer network
function
arbitrary accuracy
framework
maximum likelihood
number
hidden units needs
level dependent
size
training set
order
over-ﬁtting
bayesian perspective
little sense
number
parameters
network
size
training set
bayesian neural network
prior distribution
parameter vector w
conjunction
network function f
prior distribution
functions
y
x
y
vector
network outputs
neal
broad class
prior distributions
w
distribution
functions
neural network
gaussian process
limit m → ∞
limit
output variables
neural network
great merits
neural networks
outputs share
hidden units
statistical strength ’
weights
hidden unit
output
property
gaussian process limit
gaussian process
covariance
kernel
function
williams
forms
covariance
case
speciﬁc choices
hidden unit activation function
probit
gaussian
kernel functions
function
difference x − x
consequence
gaussian weight
translation invariance
weight space
kernel methods exercises
covariance function
distribution
weights
weight
hyperpa- rameters
values
length scales
distribution
functions
examples
figure
case
ﬁnite number
hidden units
note
hyperpa- rameters
techniques
kind
section
 
dual formulation
squares linear regression problem
section
show
solution
components
vector
linear combination
elements
vector φ
xn
coefﬁcients
vector w
dual
dual formulation
original representation
terms
parameter vector w.
 
exercise
dual formulation
perceptron
algorithm
perceptron learning rule
learned weight vector w
linear combination
vectors
xn
tn ∈
−1
+1
coefﬁcients
linear combination
αn
formulation
perceptron
algorithm
predictive function
perceptron
terms
αn
show
feature vector φ
x
form
kernel function k
x
φ
x
tφ
x

nearest-neighbour classiﬁer
section
new input vector x
same class
input vector xn
training set
case
distance
euclidean metric x − xn2
rule
terms
scalar products
use
kernel sub- stitution
nearest-neighbour classiﬁer
general nonlinear kernel

appendix c
example
matrix
positive elements
negative eigenvalue
hence
positive deﬁnite
example
converse property
×
matrix
positive eigenvalues
negative element

results
valid kernels

results
valid kernels

results
valid kernels

results
valid kernels

results
valid kernels

show
excellent choice
kernel
function f
x
k
x
f
x
f
x
linear learning machine
kernel
solution proportional
x
 n
w = n=1 αnφ
xn
w⊥ show
value
w
j
w
form
linear combination
basis functions φ
xn
n =
n.
 
sum-of-squares error function
data
noisy inputs
ν
ξ
distribution
noise
calculus
vari- ations
error function
respect
function y
x
show
optimal solution
expansion
form
basis functions

use
expansion
middle factor
power series
gaussian kernel
inner product
inﬁnite-dimensional feature vector
 
space
possible subsets a
ﬁxed
d. show
kernel function
corresponds
inner product
feature space
dimensionality
mapping φ
a
subset
d
element φu
a
subset u
φu
a
=
u
a
u
a
u
subset
a
a

show
fisher kernel
nonlinear transformation
parameter vector θ → ψ
θ
function ψ
·

www write
form
fisher kernel
case
distribution p
x|µ
n
x|µ
s
mean µ
ﬁxed covariance s.

determinant
×
gram matrix
positive- deﬁnite kernel function k
x
cauchy-schwartz inequality k
x2
� k
x1
k
x2
 
parametric model
parameter vector
data set
input values x1
nonlinear feature
φ
x
dependence
error function
w
form j
w
= f
wtφ
x1
wtφ
xn
g
wtw
g
·
function
w
form   n e
y
xn − ξn
g
ξn
dξn
n=1
e
respect
function y
z
calculus
variations
appendix d
optimal solution
y
x
nadaraya-watson kernel regression solution
form
kernel
form
 
results
 
www
gaussian process regression model
kernel function
terms
ﬁxed set
nonlinear basis functions
predictive distribution
result
section
bayesian linear regression model
note
models
gaussian predictive distributions
conditional mean
variance
mean
use
matrix identity
c.6
variance
use
matrix identity
c.7
 
regression problem
n
input vectors
l test
input vectors
+1
xn +l
gaussian process
functions t
x
derive
expression
joint predictive dis- tribution
t
xn +1
t
xn +l
values
t
x1
t
xn
marginal
distribution
test observations
n
� j � n + l
usual gaussian process regression result
 
www
gaussian process regression model
target variable t
dimensionality d. write
conditional distribution
tn +1
test input vector xn +1
training set
input vectors
xn +1
target observations t1
tn

diagonal matrix w
elements
< wii <
positive deﬁnite
show
sum
positive deﬁnite matrices
positive deﬁnite
kernel methods

nadaraya-watson model
input variable x
target variable t
gaussian components
isotropic covariances
co- variance matrix
σ2i
unit matrix
write
expressions
conditional density p
t|x
conditional mean e [ t|x ]
variance var [ t|x ]
terms
kernel function k
 
viewpoint
kernel regression
consideration
re- gression problems
input variables
target variables
additive noise
suppose
target value tn
function y
zn
point zn
gaussian noise
value
zn
noise
version xn = zn + ξn
random variable ξ
distribution g
ξ
set
observations
xn
tn
n =
n
sum-of-squares error function
distribution
input noise
exercises

www
newton-raphson formula
iterative update n
posterior distribution
gaussian formula
mode a process classiﬁcation model

result
expressions
mean
variance
posterior distribution p
an +1|tn
gaussian process clas- siﬁcation model
  
result
log likelihood function
laplace approx- imation framework
gaussian process classiﬁcation
results
terms
gradient
log likelihood
sparse kernel machines
previous chapter
variety
algorithms
non- linear kernels
signiﬁcant limitations
many such algorithms
kernel function k
possible pairs xn
xm
training points
training
excessive computation times
predictions
new data points
chapter
kernel-based algorithms
solutions
predictions
new inputs
kernel function
subset
training data points
detail
support vector machine
svm
years
problems
classiﬁcation
regression
detection
important property
support vector machines
determination
model parameters
convex optimization prob- lem
local solution
global optimum
discussion
support vector machines
extensive use
lagrange multipliers
reader
sparse kernel machines
key concepts
appendix e. additional infor- mation
support vector machines
vapnik
burges
cristianini
shawe-taylor
m¨uller
al
sch¨olkopf
smola
herbrich
svm
decision machine
posterior probabilities
beneﬁts
probabilities
sec- tion
alternative sparse kernel technique
relevance vector machine
rvm
bayesian formulation
posterior proba- bilistic outputs
solutions
svm
section
maximum margin classiﬁers
discussion
support vector machines
two-class classiﬁcation problem
linear models
form y
x
wtφ
x
+ b
φ
x
ﬁxed feature-space transformation
bias parameter b explicit
note
dual representation
terms
kernel functions
feature space
training data set comprises n input vectors
target values
tn ∈
−1
new data points x
sign
y
x
moment
training data set
feature space
deﬁnition
choice
parameters
function
form
satisﬁes y
xn
points
tn = +1
y
xn
points
tn = −1
tny
xn
training data points
course
many such solutions
classes
section
perceptron algorithm
solution
ﬁnite number
steps
solution
initial values
w
b
order
data points
multiple solutions
training data set
one
generalization error
support vector machine
problem
concept
margin
distance
decision boundary
samples
figure
support vector machines
decision boundary
margin
maximum margin solution
computational learning theory
theory
simple insight
origins
maximum margin
tong
koller
framework
classiﬁcation
hybrid
discriminative approaches
distribution
in- put vectors
class
parzen density estimator
gaussian kernels section

maximum margin classiﬁers
y = −1 y
y
y
y
y = −1 margin figure
margin
perpendicular distance
decision boundary
data points
left ﬁgure
margin
particular choice
decision boundary
right
location
boundary
subset
data points
support vectors
circles
common parameter σ2
class priors
opti- mal misclassiﬁcation-rate decision boundary
optimal boundary
hyperplane
probability
error relative
density model
limit σ2 →
optimal hyperplane
maximum margin
intuition
result
σ2
hyperplane
nearby data points
distant ones
limit
hyperplane
data points
support vectors
figure
marginalization
respect
prior distri- bution
parameters
bayesian approach
simple linearly separable data
leads
decision boundary
middle
region
data points
large margin solution
similar behaviour
figure
perpendicular distance
point x
hyper- plane
y
x
y
x
form
|y
x
|/w
solutions
data points
tny
xn
n. thus
distance
point xn
decision surface
tny
xn
= tn
wtφ
xn
+ b
w
margin
perpendicular distance
point xn
data set
parameters
order
distance
maximum margin solution
arg max w
w min n tn wtφ
xn
+ b
factor
optimization
w    
n n n
appendix e
sparse kernel machines
n. direct solution
optimization problem
equivalent problem
rescaling w → κw
→ κb
distance
point xn
decision surface
tny
xn
/w
freedom
tn wtφ
xn
+ b =
tn
point
surface
case
data points
constraints
n. wtφ
xn
+ b
canonical representation
decision hyperplane
case
data points
equality
constraints
whereas
remainder
deﬁnition
active constraint
closest point
margin
active constraints
optimization problem
w−1
w2
optimization problem
arg
w
b subject
constraints
factor
convenience
example
quadratic programming problem
quadratic function subject
set
linear inequality constraints
bias parameter b
optimiza- tion
constraints
changes
order
constrained optimization problem
lagrange multipliers
�
constraints
lagrangian function l
w
b
−
tn
wtφ
xn
+ b
=
an
t. note
minus sign
front
lagrange
term
respect
respect
derivatives
l
w
b
respect
conditions
= antnφ
xn
= antn
n=1
  
n n
n  
n n
maximum margin classiﬁers
eliminating w
b
l
w
b
conditions
dual representation
maximum margin problem
l
− n=1
respect
subject
constraints
anamtntmk
n
n
kernel function
k
x
φ
x
tφ
x
form
quadratic programming problem
quadratic function
subject
set
inequality constraints
techniques
such quadratic programming problems
section
solution
quadratic programming problem
m variables
computational complexity
o
m
dual formulation
original optimization problem
minimizing
m variables
dual problem
n variables
ﬁxed set
basis functions
number m
number n
data points
move
dual problem
model
kernels
maximum margin classiﬁer
spaces
dimensionality
number
data points
inﬁnite feature spaces
kernel formulation
role
constraint
kernel function k
positive deﬁnite
l
rise
well- ensures
lagrangian function
optimization problem
order
new data points
trained model
sign
y
x
terms
parameters
kernel function
w
y
x
antnk
xn
+ b
joseph-louis lagrange
french mathematician
lagrange
turin
italy
age
important contributions mathemat- ics
pro- fessor
royal artillery school
turin
many years
euler
lagrange
berlin
euler
director
mathematics
berlin academy
paris
nar- rowly
life
french revo- lution thanks
personal intervention
lavoisier
french chemist
oxygen
self
guillotine
lagrange
key contributions
calculus
variations
foundations
dynamics
     m∈s n∈s n
sparse kernel machines
appendix e
constrained optimization
form
karush-kuhn-tucker
kkt
conditions
case
properties
tny
xn
�
tny
xn
data point
tny
xn
data point
=
sum
hence
role
predictions
new data points
data points
support vectors
tny
xn
points
maximum margin hyperplanes
feature space
figure
property
practical applicability
support vector machines
model
signiﬁcant proportion
data points
support vectors
quadratic programming problem
value
value
threshold parameter b
support vector xn satisﬁes tny
xn
using
tn amtmk
xm
+ b =
s
set
indices
support vectors
equation
b
support vector xn
n =
stable solution
tn
use
t2
equations
support vectors
b
b =
ns amtmk
tn − m∈s
ns
total number
support vectors
comparison
alternative models
maximum- margin classiﬁer
terms
minimization
error function
simple quadratic regularizer
form e∞
y
xn
+ λw2 n=1
e∞
z
function
∞
constraints
note
regularization parameter satisﬁes λ >
precise value
role
figure
example
classiﬁcation
sup- port vector machine
simple synthetic data
gaussian kernel
maximum margin classiﬁers
figure
example
synthetic data
classes
dimensions
contours
constant y
x
support vector machine
gaus- sian kernel function
decision boundary
margin boundaries
sup- port vectors
form
data set
two-dimensional data space x
nonlinear feature space
nonlinear kernel function
training data points
original data space
example
geometrical insight
origin
sparsity
svm
maximum margin hyperplane
location
support vectors
other data points
out- side
margin region
decision boundary
solution
such data points
class distributions
training data points
feature space φ
x
support vector machine
exact separation
training data
original input space x
corresponding decision boundary
practice
class-conditional distributions
case exact separation
training data
poor generalization
way
support vector machine
training points
from
case
separable classes
error function
inﬁnite error
data point
error
model parameters
margin
approach
data points
‘ wrong side ’
margin boundary
penalty
distance
boundary
subsequent optimization problem
penalty
linear function
distance
slack variables
ξn
n =
n
slack variable
training data point
bennett
cortes
vapnik
ξn =
data points
correct margin boundary
ξn = |tn − y
xn
|
other points
data point
decision boundary y
xn
ξn =
points
sparse kernel machines figure
illustration
slack
ξn �
data points
circles
support vectors
  n=1 n n  n y = −1 y
y
ξ
ξ
ξ
ξ
 n
ξn >
exact classiﬁcation constraints
n =
n tny
xn
− ξn
slack variables
ξn �
data points
ξn
margin
correct side
margin
points
< ξn
lie
margin
cor- rect side
decision boundary
data points
ξn
lie
wrong side
decision boundary
fig- ure
hard margin constraint
soft margin
training
data points
note
slack variables
class distributions
framework
outliers
penalty
misclassiﬁcation increases
ξ
goal
margin
points
wrong side
margin boundary
minimize c ξn
parameter c
controls
trade-off
slack variable penalty
margin
point
n ξn
upper bound
number
misclassiﬁed points
parameter c
inverse
regularization coefﬁcient
trade-off
errors
model complexity
limit c → ∞
support vector machine
separable data
subject
constraints
ξn �
l
w
b
+ c ξn−
tny
xn
+ ξn
− n=1 µnξn
  n ∂l ∂w ∂l ∂b ∂l ∂ξn    
n n n n
n
results
w
b
dual lagrangian
form l
=
− anamtntmk
n=1 m=1
separable case
constraints
constraints
lagrange multipliers
µn �
implies
� c.
respect
dual variables
subject
�
� c antn
n=1
n =
n
box constraints
quadratic programming problem
predictions
new data points
resulting solution
subset
data points
case
predictive appendix e
�
lagrange multipliers
corresponding set
kkt conditions
maximum margin classiﬁers
n =
n.
w
b
use
deﬁnition
y
x
�
tny
xn
+ ξn �
tny
xn
+ ξn
µn
ξn
µnξn
⇒ w = antnφ
xn
⇒ antn
= c − µn
         tn −
m∈s n n
n n=1 n n=1 m=1
�
antn
� ν.
sparse kernel machines model
data points
support vectors
tny
xn
− ξn
< c
implies
such points
margin
points
= c
margin
parameter b
support vectors
< c
tny
xn
tn amtmk
xm
+ b
again
stable solution
b =
nm amtmk
m∈s
m
set
indices
data points
< c.
equivalent formulation
support vector machine
ν-svm
sch¨olkopf
al
l
= − anamtntmk
subject
constraints
approach
advantage
parameter ν
c
upper bound
fraction
margin errors
points
ξn
wrong side
margin boundary
bound
fraction
support vectors
example
synthetic data set
figure
gaussian kernels
form exp
−γx − x2
γ =
predictions
new inputs
support vectors
training phase
determination
parameters
b
use
whole data set
efﬁcient algorithms
maximum margin classiﬁers figure
illustration
nonseparable data
dimensions
support vectors
circles
−2
l
quadratic programming problem
objective function
local optimum
global optimum
constraints
convex region
conse- quence
direct solution
quadratic programming problem us- ing traditional techniques
computation
memory requirements
practical approaches
tech- nique
vapnik
fact
value
lagrangian
rows
columns
kernel matrix
lagrange multipliers
value zero
full quadratic pro- gramming problem
series
ones
goal
nonzero lagrange multipliers
others
protected conjugate gradients
burges
size
matrix
quadratic function
number
data points
number
nonzero lagrange multipliers
memory
large-scale appli- cations
decomposition methods
osuna
series
quadratic programming problems
ﬁxed size
technique
large data sets
numerical solution
quadratic programming subproblems
popular approaches
support vector machines
sequential minimal optimization
smo
platt
concept
extreme limit
considers
lagrange multipliers
time
case
subproblem
numerical quadratic programming
heuristics
pair
lagrange multipliers
step
practice
smo
scaling
number
data points
quadratic depending
particular application
kernel functions correspond
products
feature spaces
dimensionality
terms
kernel function
feature space
fore
support vector machines
curse
sparse kernel machines
section
mensionality
case
constraints
feature
effective dimensionality
feature space
simple second-order polynomial kernel
terms
components k
z
+
=
+ x1z1 + x2z2
+ x2 =
√2x1
√2x2
x2 = φ
x
tφ
z
+ x2
√2x1x2
x2
√2z1
√2z2
√2z1z2
z2
t
kernel function
inner product
feature space
dimensions
mapping
input space
space
vector function φ
x
coefﬁcients
different features
forms
set
points
original two-dimensional space x
two-dimensional nonlinear manifold
six-dimensional feature space
fact
support vector machine
probabilistic outputs
classiﬁcation decisions
new in- put vectors
veropoulos
al
discuss modiﬁcations
svm
trade-off
false negative errors
svm
module
probabilistic system
probabilistic predictions
class label t
new inputs x
issue
platt
logistic sigmoid
outputs
support vector machine
required conditional probability
form p
t =
= σ
ay
x
+ b
y
x
values
parameters a
b
cross-entropy error function
training
consisting
pairs
values y
xn
tn
data
sigmoid
original svm
order
two- stage approach
output y
x
support vector machine
log-odds
class t
svm training procedure
svm
poor approximation
posterior probabilities
relation
logistic regression
separable case
svm
nonseparable distri- butions
terms
minimization
regularized error function
similarities
differences
logistic regression model
data points
correct side
margin boundary
section
 
n n figure
plot
‘ hinge ’ error function
support vector machines
error function
logistic regression
factor
ln
point
misclassiﬁcation error
squared error
maximum margin classiﬁers
e
z
z
points
− yntn
objective function
overall multiplicative constant
form esv
yntn
+ λw2
λ =
−1
esv
·
hinge error function
esv
yntn
− yntn ] +
[ · ] +
positive part
hinge error function
shape
figure
approximation
misclassiﬁcation error
i.e.
error function
figure
logistic regression model
section
convenient
target variable t ∈
comparison
support vector machine
reformulate maximum likelihood logistic regression
target variable t ∈
−1
p
t =
= σ
y
y
x
σ
y
logistic sigmoid function
p
t = −1|y
− σ
y
= σ
−y
properties
logistic sigmoid function
p
t|y
= σ
yt
exercise
error function
negative logarithm
likelihood function
quadratic regularizer
form
elr
yntn
+ λw2
n=1 elr
yt
= ln
+ exp
−yt
sparse kernel machines
comparison
other error functions
ln
error function
point
error function
figure
similar form
support vector error function
key difference
ﬂat region
esv
yt
solutions
logistic error
hinge loss
continuous approx- imations
misclassiﬁcation error
continuous error function
classiﬁcation problems
squared error
figure
property
emphasis
data points
long way
decision boundary
correct side
such points
expense
misclassiﬁed points
objective
mis- classiﬁcation rate
error function
choice
multiclass svms
support vector machine
two-class classiﬁer
practice
problems
k >
classes
various meth- ods
multiple two-class svms
order
multiclass classiﬁer
approach
vapnik
k separate svms
kth model yk
x
data
class ck
positive examples
data
k −
classes
negative examples
one-versus-the-rest approach
figure
decisions
individual classiﬁers
inconsistent results
input
classes
problem
predictions
new inputs
y
x
max k yk
x
heuristic approach suffers
problem
different classiﬁers
different tasks
guarantee
real- valued quantities
x
different classiﬁers
appropriate scales
problem
one-versus-the-rest approach
training sets
instance
classes
equal numbers
data points
individual classiﬁers
data sets
% negative examples
% positive examples
symmetry
original problem
variant
one-versus-the-rest scheme
lee
al
target values
positive class
+1
negative class
−1/
k −
weston
watkins
single objective function
k svms
margin
classes
training
k separate optimization problems
n data points
overall cost
o
kn
single optimization problem
size
k −
n
overall cost
o
k
maximum margin classiﬁers
approach
k
k −1
different 2-class svms
possible pairs
classes
test points
class
high- est number
‘ votes
approach
one-versus-one
again
figure
ambiguities
resulting classiﬁcation
large k
approach
training time
one-versus-the-rest approach
test points
computation
latter problem
pairwise classiﬁers
directed acyclic graph
probabilistic graphical model
dagsvm
platt
k classes
dagsvm
total
k
k −
/2 classiﬁers
new test point
k −
pairwise classiﬁers
particular classiﬁers
path
graph
different approach
classiﬁcation
error-correcting out- put codes
dietterich
bakiri
vector machines
allwein
al
generalization
scheme
one-versus-one approach
general partitions
classes
individual classiﬁers
k
particular sets
responses
two-class classiﬁers
suitable decoding scheme
robustness
errors
ambiguity
outputs
individual classiﬁers
application
svms
classiﬁcation problems
open issue
practice
one-versus-the-rest approach
spite
ad-hoc formula- tion
practical limitations
single-class support vector machines
unsuper- vised learning problem
probability density estimation
density
data
methods
region
high density
quantile
density
probability
data point drawn
distribution
region
ﬁxed number
advance
restricted problem
full density
speciﬁc applications
approaches
problem
support vector machines
algorithm
sch¨olkopf
al
hyperplane
ﬁxed fraction ν
training data
origin
same time
distance
margin
hyperplane
origin
tax
duin
look
sphere
feature space
fraction ν
data points
kernels k
x
x
functions
x − x
algorithms
svms
regression
support vector machines
problems
same time
property
sparseness
simple linear regression
  n=1 n n
  z
sparse kernel machines figure
plot
-insensitive error function
red
error
distance
insen- sitive region
compar- ison
quadratic error function
green
e
z
minimize
regularized error function
yn − tn
sparse solutions
quadratic error function
-insensitive error function
vapnik
error
absolute difference be-
prediction y
x
target t
simple example
-insensitive error function
linear cost
errors
insensitive region
e
y
x
− t
=
figure
|y
x
t| −
|y
x
t| <
regularized error function
c
e
y
xn
− tn
y
x
convention
inverse
regularization parameter
c
front
error term
optimization problem
slack variables
data point xn
slack variables
ξn �
ξn
corresponds
point
tn > y
xn
+
ξn >
corresponds
point
tn < y
xn
−
figure
condition
target point
-tube
yn − � tn � yn+
yn = y
xn
slack
points
tube
slack variables
corresponding conditions
tn � y
xn
+ ξn tn � y
xn
− ξn
    b b  n
  
n   
n   
n n      
n n ∂l ∂w ∂l ∂b ∂l ∂ξn ∂l ∂
y + y y − x
b b 
maximum margin classiﬁers figure
illustration
svm regression
regression curve
- insensitive ‘ tube ’
exam- ξ
points ples
slack
ξ
-tube
ξ =
points
-tube
ξ >
points
= ξ
y
x
ξ
error function
support vector regression
c
ξn + ξn
ξn
constraints
lagrange multipliers
�
�
µn �
µn �
lagrangian
ξn + ξn
−
µnξn + µn ξn
l = c −
+ ξn + yn − tn
+ ξn − yn + tn
y
x
derivatives
la- grangian
respect
b
ξn
⇒ w =
−
φ
xn
⇒
−
=
=
⇒
+ µn = c µn = c. =
⇒
+ exercise
results
corresponding variables
dual problem
          m=1 n=1 n=1 n=1 n n n n    −  
�
� c
� c
�    
n    
sparse kernel machines   l
=
−
−
k
+
+
−
tn
kernel k
x
respect
φ
x
tφ
x
constrained maximization
constraints
lagrange
multipliers
µn
� c
� c
box
condition
predictions
new inputs
y
x
=
−
k
xn
+ b
terms
kernel function
corresponding karush-kuhn-tucker
kkt
conditions
state
solution
product
dual variables
constraints
+ ξn + yn − tn
ξn − yn + tn
+
c −
ξn
c − ξn =
several useful results
first
coefﬁcient
+ ξn + yn −
data point
upper boundary
-tube
ξn =
upper ξn − yn +
boundary
ξn >
nonzero value
such points
boundary
-tube
− yn + tn =
ξn
ξn
nonnegative while
data point xn
furthermore
constraints
ξn + yn − tn
support vectors
data points
predictions
points
other words
boundary
-tube
tube
points
tube
          
− m=1 n=1 n=1 n=1 n n n n
  
�
� c/n
� c/n
�
−
=
n
+
� νc
maximum margin classiﬁers
=
sparse solution
only terms
=
predictive model
support vectors
parameter b
data point
< c
ξn =
satisfy + yn − tn
using
b
b = tn − − wtφ
xn
= tn − − n m=1
k
analogous result
point
< c.
practice
such estimates
b
classiﬁcation case
alternative formulation
svm
regression
parameter
complexity
intuitive interpretation
sch¨olkopf
width
insensitive region
parameter ν
fraction
points
tube
l
= −
−
−
k
xm
constraints
tn
νn data points
insensitive tube
νn data points
support vectors
tube
use
support vector machine
regression problem
sinusoidal data set
figure
parameters
c
hand
practice
values
cross- validation
sparse kernel machines figure
illustration
re- gression
sinusoidal synthetic data
gaussian kernels
predicted regression curve
red line
-insensitive tube corresponds
shaded region
data points
support vectors
blue circles
−1
computational learning theory historically
support vector machines
theoretical framework
computational learning theory
some- times
statistical learning theory
anthony
biggs
kearns
vazi- rani
vapnik
vapnik
origins
valiant
pac
framework
goal
pac framework
data set needs
order
good generalization
bounds
computational cost
ex
t [
f
d
= t
< suppose
data set d
size n
joint distribution p
t
x
input
class label
attention
free ’ situations
class labels
deterministic function t = g
x
pac
function f
d
space f
such functions
basis
training
d
good generalization
error rate
pre-speciﬁed threshold
·
indicator function
expectation
respect
dis- tribution p
quantity
left-hand side
random variable
training
d
pac framework
probability
− δ
data set d drawn
p
δ
pre-speciﬁed parameter
terminology ‘
correct ’
requirement
high probability
δ
error rate
choice
model space f
parameters
δ
pac
aims
bounds
minimum size n
data set
criterion
key quantity
pac learning
vapnik-chervonenkis dimension
vc dimension
measure
complexity
space
functions
pac framework
spaces
inﬁnite number
functions
bounds
pac framework
relevance vector machines
case
choice
distribution p
training
test examples
same distribu- tion
choice
function f
x
f.
real-world applications
machine learning
distributions
reg- ularity
example
large regions
input space
same class label
consequence
lack
assumptions
form
distribution
pac bounds
other words
size
data sets
generalization performance
reason
pac bounds
practical applications
attempt
tightness
pac
pac-bayesian framework
mcallester
distribution
space f
functions
prior
bayesian treatment
con- siders
possible choice
p
bounds
relevance vector machines support vector machines
variety
classiﬁcation
regres- sion applications
number
limitations
chapter
outputs
svm represent decisions
posterior probabilities
svm
classes
extension
k >
classes
complexity parameter c
parameter
case
regression
hold-out method
cross-validation
predictions
linear combinations
kernel functions
training data points
positive deﬁnite
relevance vector machine
rvm
tipping
bayesian sparse ker- nel technique
regression
classiﬁcation
shares
characteristics
svm whilst
principal limitations
models
faster performance
test data whilst
comparable generalization error
contrast
svm
regres- sion form
rvm ﬁrst
extension
classiﬁcation tasks
rvm
regression
relevance vector machine
regression
linear model
form
chapter
modiﬁed
results
sparse solutions
model
conditional distribution
real-valued target variable t
input vector x
form p
t|x
w
β
n
t|y
x
β−1
   
m n n m
ﬁxed nonlinear basis functions φi
x
constant term
corresponding weight parameter
‘ bias ’
relevance vector machine
speciﬁc instance
model
structure
support vector machine
basis functions
kernels
kernel
data points
training set
general expression
svm-like form y
x
wnk
xn
+ b
b
bias parameter
number
parameters
case
m = n +
y
x
same form
predictive model
svm
coefﬁcients
wn
subsequent analysis
arbitrary choices
basis function
generality
form
contrast
svm
restriction
positive- deﬁnite kernels
basis functions
number
location
training data points
suppose
set
n observations
input vector x
n =
n.
denote
data matrix x
nth row
target values
t =
t1
tn
t. thus
likelihood function
p
t|x
w
β
p
tn|xn
w
β−1
prior distribution
parameter vector w
chapter
zero-mean gaussian prior
key differ- ence
rvm
separate hyperparameter αi
weight parameters
hyperparameter
weight
form p
w|α
i=1 n
wi|0
α−1 i
precision
corresponding parameter wi
denotes
α1
αm
evidence
respect
hyperparameters
signiﬁcant proportion
inﬁnity
corresponding weight parameters
posterior distributions
zero
basis functions
parameters
play
role
sparse kernel machines
β = σ−2
noise precision
inverse noise variance
mean
linear model
form y
x
wiφi
x
wtφ
x
  
relevance vector machines
predictions
model
sparse model
result
linear regression models
posterior distribution
weights
form p
w|t
x
α
β
n
w|m
σ
mean
covariance
a + βφtφ m = βσφtt σ =
φ
n × m design matrix
elements
= φi
xn
a = diag
αi
note
speciﬁc case
model
= k
k
n +
×
n +
kernel matrix
elements
xm
values
α
β
type-2 maximum likelihood
evidence approximation
marginal likeli- hood function
weight parameters
p
t|x
α
β
p
t|x
w
β
p
w|α
dw
convolution
gaussians
log marginal likelihood
form ln p
t|x
α
β
lnn
t|0
c
n ln
+ ln|c| + ttc−1t
t =
t1
tn
t
n × n matrix c
c = β−1i + φa−1φt
goal
respect
hyperparameters
β
small modiﬁcation
results
section
evidence approximation
linear regression model
again
approaches
ﬁrst
required derivatives
marginal likelihood
following re-estimation equations
αnew
γi m2 i
βnew
= t − φm2 i
n − section
exercise
exercise
section
mi
ith component
posterior mean m
quantity γi
corresponding parameter wi
data
sparse kernel machines  γi
− αiσii
σii
ith diagonal component
posterior covariance σ
proceeds
initial values
α
β
mean
covariance
hyperparameters
posterior mean
covariance
suit- able convergence criterion
second approach
em algorithm
sec- tion
approaches
values
hyperparameters
evidence
direct optimization approach
convergence
tipping
result
optimization
proportion
hyperparameters
αi
principle inﬁnite
values
weight parameters
hyperparameters
posterior distributions
mean
variance
zero
thus
parameters
corresponding basis func- tions
x
model
role
predictions
new inputs
case
models
form
inputs
nonzero weights
relevance vectors
iden- tiﬁed
mechanism
automatic relevance determination
support vectors
svm
mechanism
sparsity
probabilistic models
automatic rele- vance determination
model
adaptive linear combination
basis functions
values
hyperparameters
marginal likelihood
predictive distribution
t
new input x
using
p
t|x
x
t
α
β
= n p
t|x
w
β
p
w|x
t
α
β
dw t|mtφ
x
σ2
x
predictive mean
w
posterior mean m
variance
predictive distribution
σ2
x
=
β
+ φ
x
tσφ
x
σ
α
β
optimized values
β
familiar result
context
linear regression
localized basis functions
predictive variance
linear regression models
regions
input space
basis functions
case
rvm
basis functions
data points
model
predictions
domain
data
rasmussen
qui˜nonero-candela
course
predictive distribution
gaussian process regression
section
exercise
section
relevance vector machines
figure
illustration
rvm regression
same data set
same gaussian kernel functions
figure
ν-svm regression model
mean
predictive distribu- tion
rvm
red line
standard- deviation predictive distribution
shaded region
data points
relevance vec- tors
blue circles
note
rele- vance vectors
sup- port vectors
fig- ure
t
−1
x
problem
computational cost
predictions
gaussian processes
rvm
figure
example
rvm
sinusoidal regression data set
noise precision parameter β
evidence maximization
number
relevance vectors
rvm
number
support vectors
svm
wide range
regression
classiﬁcation tasks
rvm
models
order
magnitude
corresponding support vector machine
signiﬁcant improvement
speed
test data
sparsity
reduction
generalization error
corresponding svm
principal disadvantage
rvm
svm
involves
nonconvex function
times
comparable svm
model
m basis functions
rvm
inversion
matrix
size m × m
o
m
computation
speciﬁc case
svm-like model
m = n +1
techniques
svms
cost
n.
course
case
rvm
option
number
basis functions
n +
relevance vector machine
parameters
complexity
noise variance
single training run
support vector machine
parameters c
cross-validation
multiple training runs
next section
alternative procedure
relevance vector machine
speed
analysis
sparsity
mechanism
automatic relevance determination
subset
parameters
detail
sparse kernel machines t2 c t t1 t2 ϕ c t t1 figure
illustration
mechanism
sparsity
bayesian linear regression model
training
vector
target values
t =
t1
t2
t
cross
model
basis vector ϕ =
φ
x1
φ
x2
t
target data vector t.
left
model
isotropic noise
c = β−1i
= ∞
β set
probable value
right
same model
ﬁnite value
α
case
red ellipse corresponds
unit mahalanobis distance
|c|
same value
plots
dashed green circle
contrition
noise term β−1
ﬁnite value
α
probability
observed data
probable solution
basis vector
mechanism
sparsity
context
relevance vector machine
process
procedure
hyper- parameters
direct techniques
mathematical analysis
informal insight
origin
sparsity
bayesian linear models
data set
n
observations t1
t2
model
single basis function φ
x
hyperparameter α
isotropic noise
pre- cision β
from
marginal likelihood
p
t|α
β
n
t|0
c
covariance matrix
form c
β
α ϕϕt
ϕ
n-dimensional vector
φ
x1
φ
x2
t
t =
t1
t2
t. notice
zero-mean gaussian process model
t
covariance c. given
particular observation
t
goal
α
marginal likelihood
figure
poor alignment
direction
ϕ
training data vector t
corresponding hyperparameter α
basis vector
model
ﬁnite value
α
probability
data
value
density
t
pro-
β
optimal value
ﬁnite value
α
distribution
direction
data
probability mass
regions
data
hence
value
density
target data vector
general case
m 
relevance vector machines
basis vectors
similar intuition
particular basis vector
data vector t
model
mechanism
sparsity
mathematical per- spective
general case
m basis functions
analysis
result
parameter αi
terms
right-hand side
functions
αi
results
rep- resent implicit solutions
iteration
single αi
other αj
j = i
different approach
optimization problem
rvm
dependence
marginal likelihood
particular αi
stationary points
faul
tipping
tipping
faul
contribution
αi
matrix c
c = β−1i + α−1 j ϕjϕt j + α−1 i
i j=i = c−i + α−1 i
i
ϕi
ith column
φ
other words
n-dimensional vector
elements
φi
x1
φi
xn
contrast
φn
nth row
φ
matrix c−i
matrix c
contribution
basis function i
matrix identities
c.7
c.15
determinant
inverse
c
exercise
|c| = |c−i||1 + α−1 i
c−1 −i ϕiϕt c−1 = c−1 αi + ϕt −i − i c−1 −i ϕi| i c−1 −i
ϕi i c−1 using
results
log marginal likelihood function
form
l
α−i
log marginal likelihood
basis function ϕi
quantity λ
αi
l
α
l
α−i
λ
αi
λ
αi
ln αi − ln
αi + si
q2 αi + si i
dependence
αi
quantities
= ϕt qi = ϕt i c−1 −i ϕi i c−1 −i t.
si
sparsity
qi
quality
ϕi
large value
si relative
value
qi means
basis function
sparse kernel machines
figure
plots log likelihood λ
αi
marginal ln αi
left
single maximum
ﬁnite αi
q2 i
q2 i > si
right
maximum
αi = ∞ i
q2 i < si
−2
−2 −4
−5
model
‘ sparsity ’
extent
basis function ϕi
other basis vectors
model
‘ quality ’
measure
alignment
basis vector ϕn
error
training set values
=
t1
tn
t
vector y −i
predictions
model
vector ϕi
tipping
faul
stationary points
marginal likelihood
respect
derivative dλ
αi
dαi = α−1 i
i −
αi + si
i − si
zero
possible forms
solution
αi
q2 i
si
αi
i < si
αi → ∞
solution
q2 exercise
αi = s2 i
i
si
solutions
figure
relative size
quality
sparsity terms
particular basis vector
model
complete analysis
faul
tipping
second derivatives
marginal likelihood
solutions
unique maxima
λ
αi
note
approach
closed-form solution
αi
values
other hyperparameters
insight
origin
sparsity
rvm
analysis
practical algorithm
hyperparameters
speed advantages
ﬁxed set
candidate basis vectors
cycles
turn
vector
model
sequential sparse bayesian learning algorithm
sequential sparse bayesian learning algorithm
regression problem
β
basis function ϕ1
hyperparameter α1
hyperparameters
j = i
inﬁnity
only ϕ1
model
relevance vector machines
σ
m
qi
si
basis functions
candidate basis function ϕi
model
update αi
si
αi < ∞
basis vector ϕi
i
si
αi = ∞
model
hyperpa- i � si
αi < ∞
basis function ϕi
model
rameter αi
αi = ∞
regression problem
update β
terminate
note
q2
model
action
si
αi = ∞
basis function ϕi
practice
quantities
quality
sparseness variables
form qi = ϕt si = ϕt i c−1t i c−1ϕi
qi = si = αiqi αi − si αisi αi − si
exercise
note
αi = ∞
= qi
si = si
using
c.7
qi = βϕt si = βϕt i
− β2ϕt i
− β2ϕt
φ
σ
basis vectors
hyperpa- rameters
stage
required computations
o
m
m
number
active basis vectors
model
number n
training patterns
rvm
classiﬁcation
relevance vector machine framework
prob- lems
ard
weights
probabilistic linear classiﬁcation model
kind
chapter
two-class prob- lems
binary target variable t ∈
model
form
linear combination
basis functions
logistic sigmoid function y
wtφ
x

sparse kernel machines
section
σ
·
logistic sigmoid function
gaussian prior
weight vector w
model
chapter
difference
rvm
model
ard prior
separate precision hyperparameter
weight parameter
contrast
regression model
parameter vector w.
laplace ap- proximation
related problem
bayesian logistic regression
section
hyperparameter vector α
value
α
gaussian approximation
posterior distribution
approximation
marginal likelihood
maximization
approxi- mate marginal likelihood
re-estimated value
α
process
convergence
laplace approximation
model
detail
ﬁxed value
α
mode
posterior distribution
w
ln p
w|t
α
p
t|w
p
w|α
ln p
t|α
tn ln yn +
− tn
ln
− yn
n wtaw + const
∇ ln p
w|t
α
φt
t − y
aw ∇∇ ln p
w|t
α
− φtbφ
a
b
n × n diagonal matrix
elements
= yn
− yn
vector y =
y1
yn
t
φ
design matrix
elements
= φi
xn
property
derivative
logistic sigmoid function
convergence
irls algorithm
inverse covariance matrix
gaussian approximation
posterior distribution
mode
approximation
posterior distribution
corre- sponding
mean
gaussian approximation
mean
covariance
laplace approximation
form w = a−1φt
t − y
σ = φtbφ
laplace approximation
marginal likelihood
general result
laplace approxi- exercise
= diag
αi
squares
irls
section
gradient vector
hessian matrix
log posterior distribution
    
relevance vector machines
mation
t|α
= p
t|w
p
w|α
dw
p
t|w
p
w|α
derivative
marginal likelihood
respect
 p
t|w
p
w|α
m/2|σ|1/2
exercise
−
i
−
σii
− αiσii
γi αnew i
w
re-estimation formula
regression rvm
approximate log marginal likelihood
form t = φw + b−1
t − y
ln p
t|α
β
n ln
+ ln|c| +
t
tc−1 t
c = b + φaφt
section
same form
regression case
same analysis
sparsity
same fast
algorithm
single hyperparameter αi
step
figure
relevance vector machine
synthetic classiﬁ- cation data set
relevance vectors
region
decision boundary
contrast
support vector machine
discussion
sparsity
rvm
basis function φi
x
data point
boundary
vector ϕi
training data vector
potential advantages
relevance vector machine
svm
probabilistic predictions
example
rvm
emission density
nonlinear extension
linear dynamical system
video sequences
williams
rvm
binary classiﬁcation problems
k >
classes
use
probabilistic approach
section
k linear models
form ak = wt k x
exp
ak
 j n 
k
log likelihood function
ln p
t|w1
wk
ytnk nk n=1 k=1
target values
1-of-k coding
data point n
t
matrix
elements
again
laplace approximation
hyperparameters
model
irls
principled approach
classiﬁcation
pairwise method
support vector machine
probabilis- tic predictions
new data points
principal disadvantage
hessian matrix
size m k×m k
m
number
active basis functions
additional factor
k
computational cost
training
two-class rvm
principal disadvantage
relevance vector machine
long training times
svm
avoidance
cross-validation runs
model complexity parameters
models
computation time
test points
important consideration
practice
sparse kernel machines
−2
figure
example
relevance vector machine
synthetic data set
left-hand plot
decision boundary
data points
relevance vectors
circles
comparison
results
figure
corresponding support vector machine
rvm
model
right-hand plot
posterior probability
rvm output
proportion
ink
probability
point
class
softmax function
outputs
x
aj
  n=1 n  exercises
 
suppose
data set
input vectors
target values
∈
−1
density
input vec- tors
class
parzen kernel density estimator
sec- tion
kernel k
write
minimum misclassiﬁcation-rate decision rule
classes
equal prior probability
show
kernel
x
= xtx
classiﬁcation rule reduces
new input vector
class
mean
show
kernel
form k
x
φ
x
tφ
x
classiﬁcation
mean
feature space φ
x

show
right-hand side
constraint
arbitrary constant γ >
solution
maximum margin hyperplane
 
show
irrespective
dimensionality
data space
data set consisting
data points
class
location
maximum-margin hyperplane
 
show
value ρ
margin
maximum-margin hyper- plane
subject
constraints
 
values
ρ
previous exercise
ρ2
ρ2
l
l
ρ2 = w2

logistic regression model
target variable t ∈
−1
t =
= σ
y
y
x
negative log likelihood
addition
quadratic regularization term
form

lagrangian
regression support vector machine
derivatives
respect
b
ξn
corresponding variables
dual lagrangian
sparse kernel machines

www
regression support vector machine
section
training data points
ξn
= c
points
ξn
= c.

results
mean
covariance
posterior distribution
weights
regression rvm
 
result
marginal likelihood function
regression rvm
gaussian integral
w
technique
square
 
above exercise
time
use
general result
 
show
direct maximization
log marginal likelihood
regression relevance vector machine
re-estimation equations
γi
 
evidence framework
rvm regression
re-estimation formulae
marginal likelihood
approach
inclusion
hyperpriors
gamma distributions
form
b.26
corresponding re-estimation formulae
α
β
corresponding posterior probability p
t
α
β|x
respect
β
 
result
predictive distribution
relevance vector machine
regression
show
predictive variance
 
www
results
marginal likelihood
form
λ
αn
sparsity
quality factors

second derivative
log marginal likelihood
regression rvm
respect
hyperparameter αi
stationary point
maximum
marginal likelihood
 
using
matrix identity
c.7
quantities sn
qn
form

www show
gradient vector
hessian matrix
log poste- rior distribution
classiﬁcation relevance vector machine
 
verify
maximization
approximate log marginal likelihood function
classiﬁcation relevance vector machine
result
re-estimation
hyperparameters
graphical models probabilities
central role
modern pattern recognition
chapter
probability theory
terms
simple equations
sum rule
product rule
probabilistic infer- ence
learning manipulations
book
matter
amount
application
equations
probabilistic models
algebraic ma- nipulation
analysis
diagrammatic representations
probability distributions
probabilistic graphical models
several useful properties
simple way
structure
probabilistic model
new models
insights
properties
model
conditional independence properties
inspection
graph
graphical models
complex computations
inference
learning
sophis- ticated models
terms
graphical manipulations
mathematical expressions
graph
nodes
vertices
links
edges
arcs
probabilistic graphical model
node
random
group
random variables
links
probabilistic relation- ships
variables
graph
way
joint distribution
random variables
product
factors
subset
variables
bayesian networks
directed graphical models
links
graphs
particular directionality
arrows
other major class
graphical models
markov random ﬁelds
undirected graphical models
links
arrows
directional signiﬁcance
graphs
causal relationships
random variables
whereas
graphs
soft con- straints
random variables
purposes
inference problems
undirected graphs
different representation
factor graph
chapter
key aspects
graphical models
applications
pattern recognition
machine learning
general treat- ments
graphical models
books
whittaker
lauritzen
jensen
castillo
al
jordan
cowell
al
jordan
bayesian networks
order
use
directed graphs
probability distributions
ﬁrst
arbitrary joint distribution p
b
c
variables
b
c. note
stage
anything
vari- ables
powerful aspects
graphical models
speciﬁc graph
probabilistic statements
broad class
distributions
application
product rule
probability
joint distribution
form p
b
c
= p
c|a
b
p
b
second application
product rule
time
second term
right- hand side
p
b
c
= p
c|a
b
p
b|a
p
note
decomposition
choice
joint distribution
right-hand side
terms
simple graphical model
first
node
random
b
c
node
corresponding conditional distribution
right-hand side
figure
a directed graphical model
joint probabil- ity distribution
variables
b
c
correspond- ing
decomposition
right-hand side
b
bayesian networks
c
conditional distribution
directed links
arrows
graph
nodes
variables
distribution
factor p
c|a
b
links
b
c
whereas
factor p
incoming links
result
graph
figure
link
node
node b
parent
node b
node b
child
node
note
formal distinction
node
variable
same symbol
interesting point
left-hand side
respect
variables
b
c
right-hand side
decomposition
particular ordering
b
c
different ordering
different decomposition
different graphical representation
point
moment
example
figure
joint distribution
k variables
p
xk
repeated application
product rule
probability
joint distribution
product
conditional distributions
variables
xk
= p
xk−1
p
x2|x1
p
x1
choice
k
directed graph
k nodes
conditional distribution
right-hand side
node
links
numbered nodes
graph
link
pair
nodes
general joint distributions
decompositions
representations
graphs
choice
distribution
absence
links
graph
interesting information
properties
class
distributions
graph
graph
figure
graph
instance
link
x1
graph
corresponding representation
joint probability distribution
terms
product
set
conditional dis- tributions
node
graph
such conditional distribution
parents
corresponding node
graph
in- stance
x5
x1
x3
joint distribution
variables  k
p
x1
p
x2
p
x3
p
x2
x3
p
x3
p
x6|x4
p
x5
reader
moment
correspondence
state
general terms
relationship
graph
corresponding distribution
variables
joint distribution
graph
product
nodes
graph
conditional distribution
node
variables
parents
node
graph
graph
k nodes
joint distribution
graphical models figure
example
directed acyclic graph
joint distribution
variables
x7
corresponding decomposition
joint distribution
x1 x2 x4 x3 x5 x6 x7 exercise
exercise
p
x
p
xk|pak
k=1
pak
set
parents
xk
x =
x1
key equation
factorization properties
joint distribution
directed graphical model
node
single variable
associate sets
variables
vector-valued variables
nodes
graph
representation
right- hand side
individual conditional distributions
directed graphs
important restric- tion
directed cycles
other words
closed paths
graph
node
links
direction
arrows
node
such graphs
acyclic graphs
dags
statement
ordering
nodes
links
node
numbered node
example
polynomial regression
illustration
use
directed graphs
probability distri- butions
bayesian polynomial regression model
sec-   n n figure
directed graphical model
joint distribution
corresponding
bayesian polynomial regression model
sec- tion
bayesian networks w t1
tn
tion
random variables
model
vector
polynomial coefﬁ- cients w
observed data t =
t1
tn
t.
addition
model
input data x =
x1
xn
t
noise variance σ2
hyperparameter α
precision
gaussian prior
w
parameters
model
variables
random variables
moment
joint distribution
product
prior p
w
n conditional distributions
tn|w
n =
n
p
t
w
= p
w
p
tn|w
joint distribution
graphical model
figure
complex models
book
multiple nodes
form t1
figure
graphical notation
such multiple nodes
single representative node tn
box
plate
n
n nodes
kind
graph
figure
way
graph
figure
parameters
model
stochastic variables
explicit
case
becomes
t
w|x
α
σ2
p
w|α
p
tn|w
xn
σ2
n=1 correspondingly
x
α explicit
graphical representation
convention
random variables
open circles
deterministic parameters
solid circles
graph
figure
deterministic parameters
graph
figure
graphical model
problem
machine learning
pattern recognition
random variables
observed figure
alternative
representation
graph
figure
plate
box
n
n nodes
single example tn
n w      n  n 
graphical models figure
same model
figure
deterministic parameters
solid nodes
α w n σ2 tn values
example
variables
tn
training
case
polynomial curve
graphical model
such observed variables
corresponding nodes
graph
figure
variables
tn
figure
note
value
w
example
latent variable
hidden variable
such variables
crucial role
many probabilistic models
focus
chapters
values
tn
posterior dis- tribution
polynomial coefﬁcients
section
moment
straightforward application
bayes ’ theorem p
w|t
∝ p
w
p
tn|w
n=1
deterministic parameters
order
nota- tion
model parameters
w
little direct interest
ultimate goal
predictions
new input values
suppose
corresponding probability dis-
new input value t
data
graphical model
tribution
problem
figure
corresponding joint distribution
random variables
model
deterministic parameters
p
t
t
w| x
x
α
σ2
p
tn|xn
w
σ2
p
w|α
p
n=1 x
w
σ2
t|
figure
figure
nodes
tn
corresponding random vari- ables
values
α w σ2 tn n b      b       tn  figure
polynomial regression model
figure
new input value
corresponding model prediction t.
bayesian networks α
ˆt n
required predictive distribution
probability
model parameters
t
sum rule
p
t| x
x
t
α
σ2
p
t
t
w| x
x
α
σ2
dw
random variables
t
speciﬁc
data set
details
calculation
chapter
generative models
many situations
samples
prob- ability distribution
whole
chapter
detailed discussion
methods
technique
ancestral sampling
graphical models
joint distribution p
xk
k variables
corresponding
acyclic graph
variables
links
node
numbered node
other words
node
number
parents
goal
sample xk
joint distribution
x1
lowest-numbered node
sample
distribution p
x1
nodes
or- der
node n
sample
conditional distribution p
xn|pan
parent variables
values
note
stage
parent values
lower- numbered nodes
techniques
speciﬁc distributions
detail
chapter
ﬁnal variable xk
objective
sample
joint distribution
sample
marginal distribu- tion
subset
variables
sampled values
required nodes
sampled values
nodes
example
sample
distribution p
x4
x4
full joint distribution
values values
xj=2,4
graphical models figure
a graphical model
process
images
objects
identity
object
discrete variable
position
orientation
object
continuous variables
independent prior probabilities
image
vector
pixel intensities
probability distribution
identity
object
position
orientation
object position orientation image
practical applications
probabilistic models
higher- numbered variables
terminal nodes
graph
observations
lower-numbered nodes
latent variables
primary role
latent variables
complicated distribution
observed variables
terms
model
simpler
exponential family
conditional distributions
such models
processes
data
instance
object recognition task
data point corresponds
image
vector
pixel intensities
objects
case
latent variables
interpretation
position
orientation
object
particular observed image
goal
posterior distribution
objects
possible positions
orientations
problem
graphical model
form show
figure
graphical model
causal process
pearl
ob-
data
reason
such models
generative models
contrast
polynomial regression model
figure
probability distribution
input variable x
synthetic data points
model
suitable prior distribution p
x
expense
complex model
hidden variables
probabilistic model
ex- plicit physical interpretation
complex joint distribution
simpler components
case
technique
generative model mimics
creation
data
give rise
fantasy ’ data
probability distribution
model
perfect representation
reality
observed data
practice
synthetic observations
generative model
form
probability distribution
model
discrete variables
importance
probability distributions
members
exponential family
family
many well- known distributions
particular cases
such distributions
useful building blocks
complex probability section
    k=1 k k k 
 figure
fully-connected graph
general distribu- tion
k-state discrete variables
total
k
−
parameters
b
link
nodes
number
parameters
k −
x1 x1
b
bayesian networks
x2 x2 distributions
framework
graphical models
way
building blocks
such models
nice properties
relationship
parent-child pair
directed graph
several examples
cases
note
parent
child node
correspond
variables
correspond
gaussian variables
cases
relationship
acyclic graphs
discrete case
probability distribution p
x|µ
single discrete variable x
k possible states
1-of-k representation
p
x|µ
µxk k
distribution
parameters
=
µ1
µk
t. due
constraint k µk =
k −
values
µk need
order
discrete variables
x1
x2
k states
joint distribution
probability
x1k
x2l =
parameter µkl
x1k
kth component
x1
x2l
joint distribution
x2|µ
k=1 l=1 µx1kx2l kl k l µkl =
distri-
parameters
constraint bution
k
−
parameters
total number
parameters
arbitrary joint distribution
m variables
km −
grows
number m
variables
product rule
joint distribution p
x2
form p
x2|x1
p
x1
two-node graph
link
x1 node
x2 node
figure
marginal distribution p
x1
k −
parameters
conditional distribution p
x2|x1
speciﬁcation
k −
parameters
k possible values
x1
total number
parameters
joint distribution
k −
+ k
k −
= k
variables x1
x2
graphical model
figure
b
variable
graphical models figure
chain
m discrete nodes
k states
speciﬁcation
k −
+
m −
k
k −
parameters
length m
chain
contrast
con- nected graph
m nodes
km −
param- eters
m. x1 x2
separate multinomial distribution
total number
parameters
k −
distribution
m independent discrete variables
k states
total number
parameters
m
k −
number
variables
graphical perspective
number
parameters
links
graph
expense
restricted class
distributions
m discrete variables
joint distribution
directed graph
node
conditional distribution
node
set
nonnegative pa- rameters
usual normalization constraint
graph
general distribution
km
parameters
links
graph
joint distribution
product
marginals
total number
parameters
m
k −
graphs
in- termediate levels
connectivity
general distributions
parameters
general joint distribution
illustration
chain
nodes
figure
marginal distribution p
x1
k −
parameters
m
condi- tional distributions
xi|xi−1
m
k
k −
parameters
total parameter count
k −
+
m −
k
k −
k
length m
chain
alternative way
number
independent parameters
model
parameters
tying
parameters
instance
chain example
figure
conditional distributions
xi|xi−1
m
same set
k
k−1
parameters
k−1 parameters
distribution
x1
total
k
−
parameters
order
joint distribution
graph
discrete variables
bayesian model
dirichlet priors
parameters
graphical point
view
node
additional parent
dirichlet distribution
pa- rameters
corresponding discrete node
chain model
figure
corresponding model
parame- ters
conditional distributions
xi|xi−1
m
figure
way
exponential growth
number
parameters
models
discrete variables
parameterized models
conditional distributions
complete tables
conditional probability values
illus- trate
idea
graph
figure
nodes
binary variables
parent variables
single parame- figure
extension
model
figure
dirich- let priors
param- eters
discrete distributions
figure
figure
sin- gle set
parameters
conditional distributions
xi|xi−1
µ1 x1 µ1 x1
bayesian networks µ
µm xm xm   m µ2 x2  ter µi
probability p
xi =
m parameters
parent nodes
conditional distribution p
xm
parameters
probability p
y =
possible settings
parent variables
number
parameters
conditional distribution
m.
parsimonious form
conditional distribution
logistic sigmoid function
linear combination
parent variables
section
p
y =
xm
σ w0 + wixi = σ
wtx
i=1
σ
=
−a
−1
logistic sigmoid
x =
x0
x1
xm
t
m +
-dimensional vector
parent states
additional variable x0
value
w =
w0
w1
wm
t
vector
m +
parameters
restricted form
conditional distribution
general case
number
parameters
m.
sense
choice
restrictive form
covariance matrix
example
diagonal matrix
multivariate gaussian distribution
motivation
logistic sigmoid representation
section
figure
a graph
m parents x1
sin- gle child y
idea
parameterized conditional distributions
discrete variables
xm y ⎞⎠ ⎞⎠2   i=1 d d   j∈pai ⎛⎝ ⎛⎝   xi
graphical models
linear-gaussian models
previous section
joint probability distributions
set
discrete variables
variables
nodes
directed acyclic graph
multivariate gaussian
directed graph
linear-gaussian model
component vari- ables
interesting structure
distribution
general gaussian
diagonal covariance
opposite ex- tremes
techniques
examples
linear-gaussian models
probabilistic principal component analysis
factor analysis
linear dy- namical systems
roweis
ghahramani
extensive use
results
section
later chapters
techniques
detail
arbitrary
acyclic graph
d variables
node i
single continuous random variable xi
gaussian distribution
mean
distribution
linear combination
states
parent nodes pai
node i p
xi|pai
n wijxj + bi
vi
wij
bi
parameters
mean
vi
variance
conditional distribution
xi
log
joint distribution
log
product
conditionals
nodes
graph
hence
form ln p
x
ln p
xi|pai
= −
xi − wijxj − bi j∈pai + const
x =
xd
t
‘
’ denotes terms
x
quadratic function
components
x
joint distribution p
x
mean
covariance
joint distribution
variable xi
states
parents
gaussian distribution
form
xi = wijxj + bi + √vi i
i
zero mean
unit variance gaussian random
e [ i
e [ i j ] = iij
iij
i
j element
identity matrix
expectation
[ xi ] =
[ xj ] + bi
j∈pai ⎧⎨⎩ ⎡⎣   ⎫⎬⎭⎤⎦  figure
a directed graph
gaussian variables
x1 x2
link
bayesian networks
x3 thus
components
e [ x ] =
e [ x1 ]
e [ xd ]
t
numbered node
graph
nodes
node
number
parents
i
j element
covariance matrix
p
x
form
recursion relation cov [ xi
xj ] = e [
xi − e [ xi ]
xj − e [ xj ]
= e
xi − e [ xi ]
wjk
xk − e [ xk ]
√vj j k∈paj = wjkcov [ xi
xk ] + iijvj k∈paj
covariance
numbered node
extreme cases
first
links
graph
comprises d
nodes
case
parameters
d parameters
d parameters
recursion relations
mean
p
x
b1
bd
t
covariance matrix
form diag
v1
vd
joint distribution
total
parameters
set
d inde- pendent univariate gaussian distributions
graph
node
num- bered nodes
parents
matrix wij
entries
ith row
hence
triangular matrix
entries
diagonal
total number
parameters
number d2
elements
d× d matrix
d
absence
elements
lead- ing diagonal
matrix
elements
total
d
d−1
/2
total number
independent parameters
wij
vi
covariance matrix
d
d +
/2
general symmetric covariance matrix
graphs
intermediate level
complexity correspond
gaus- sian distributions
covariance matrices
ex- ample
graph
figure
link
variables x1
x3
recursion relations
mean
covariance
joint distribution
µ =
b1
b2 + w21b1
b3 + w32b2 + w32w21b1
t σ = v1 w21v1 v2 +
w32w21v1 w32
v2 + w2 w32w21v1 w32
v2 +
v2 +
v3 + w2
section
exercise
⎛⎝ 
graphical models
linear-gaussian graphical model
case
nodes
graph represent multivariate gaussian variables
case
conditional distribution
node i
form p
xi|pai
n xi j∈pai wijxj + bi
σi
section
wij
matrix
xj
different dimen- sionalities
joint distribution
variables
note
speciﬁc example
linear-gaussian relationship
conjugate
mean µ
gaussian variable x
gaussian distribution
µ
joint distribution
x
µ
simple two-node graph
node
µ
parent
node
x
mean
distribution
µ
parameter
hyperparameter
value
hyperparameter
bayesian perspective
prior
hyperparameter
hyperprior
gaussian distribution
type
construction
principle
level
illustration
hierarchical bayesian model
further examples
later chapters
conditional independence
important concept
probability distributions
multiple variables
conditional independence
dawid
variables
b
c
conditional distribution
b
c
value
b
p
c
= p
a|c
b
different way
joint distribution
b
c
form p
b|c
= p
c
p
b|c
= p
a|c
p
b|c
product rule
probability
c
joint distribution
b factorizes
prod- uct
marginal distribution
marginal distribution
b
c
variables
b
c.
deﬁnition
conditional independence
figure
ﬁrst
examples
graphs
variables
b
c
conditional independence properties
directed graphical models
conditional independence
c
b 
possible value
c
values
shorthand notation
conditional independence
dawid
⊥⊥ b | c
b
c
conditional independence properties
important role
probabilis- tic models
pattern recognition
structure
model
computations
inference
model
examples
expression
joint distribution
set
variables
terms
product
conditional distributions
i.e.
mathematical representation
directed graph
principle test
poten- tial conditional independence property
repeated application
sum
product rules
probability
practice
approach
time con- suming
elegant feature
graphical models
conditional independence properties
joint distribution
graph
analytical manipulations
general framework
d-separation
‘ d ’
‘
’
pearl
concept
d-separation
general state- ment
d-separation criterion
formal proof
lauritzen
example graphs
discussion
conditional independence properties
graphs
simple examples
graphs
nodes
key concepts
d-separation
ﬁrst
examples
figure
joint distribution
graph
general result
none
variables
b
sides
respect
p
b
c
= p
a|c
p
b|c
p
c
p
b
= p
a|c
p
b|c
p
c
c
product p
p
b
⊥⊥ b | ∅

∅
empty set
symbol ⊥⊥
conditional inde- pendence property
course
particular distribution
virtue
speciﬁc numerical values
various conditional probabilities
structure
graph
variable c
graph
figure
from
conditional distribution
b
c
form p
b|c
= p
b
c
= p
a|c
p
b|c
conditional independence property p
c
⊥⊥ b | c.
simple graphical interpretation
result
path
node
node b
c.
node c
re- spect
path
node
tails
arrows
presence
path
nodes
b causes
nodes
de- pendent
node c
figure
conditioned node ‘ blocks
path
b
b
graph
figure
joint distribution
graph
general formula
p
b
c
= p
p
c|a
p
b|c
first
none
variables
again
b
c
graphical models figure
figure
value
variable c.
b p
b
= p
p
c|a
p
b|c
= p
p
b|a
c figure
second
examples
3-node graphs
conditional indepen- dence framework
directed graphical models
c b figure
figure
node c.
conditional independence
general
p
p
b
⊥⊥ b | ∅
b
node c
figure
bayes ’ theorem
p
b|c
= p
b
c
p
c
= p
p
c|a
p
b|c
p
c
= p
a|c
p
b|c
conditional independence property
⊥⊥ b | c.
results
node c
respect
path
node
node b
path
b
renders
c
figure
observation ‘ blocks
path
b
conditional independence property
⊥⊥ b | c.
third
3-node examples
graph
figure
subtle behaviour
previous graphs
joint distribution
general result
p
b
c
= p
p
b
p
c|a
b
case
none
variables
sides
c
p
b
= p
p
b
figure
examples
3-node graphs
conditional independence properties
graphi- cal models
graph
different properties
previous examples
b c
graphical models figure
figure
value
node c.
graph
act
induces
depen- dence
b c
b
variables
contrast
previous examples
result
c
figure
conditional distri- bution
b
⊥⊥ b | ∅
p
b|c
= p
b
c
p
c
= p
p
b
p
c|a
b
p
c
general
product p
p
b
⊥⊥ b | c. thus
third example
opposite behaviour
node c
respect
path
b
heads
arrows
node c
blocks
path
variables
b
c ‘ unblocks
path
renders
b dependent
subtlety
third example
first
terminology
node y
de- scendant
node x
path
x
step
path
directions
arrows
head-to-head path
node
descendants
tail-to-tail node
head-to-tail node
path
case
path
contrast
head-to-head node
path
node
and/or
descendants
path
moment
unusual behaviour
graph
figure
particular instance
graph
problem
binary random variables
fuel system
car
figure
variables
b
state
battery
b =
ﬂat
b =
f
state
fuel tank
fuel
f =
f =
g
state
electric fuel gauge
g =
empty exercise
b    given
state
fuel tank
battery
fuel gauge
proba- bilities
p
b =
p
f =
p
g
=
f =
p
g
=
f =
p
g
=
f =
p
g
=
f =
=
b f g
conditional independence
f b f g g figure
example
3-node graph
phenomenon
’
nodes
state
battery
b
state
fuel tank
f
reading
electric fuel gauge
g
text
details
g =
battery
fuel tank
prior probabilities
unreliable fuel gauge
probabilities
requirement
sum
complete speciﬁ- cation
probabilistic model
data
prior probability
fuel tank
f =
=
fuel gauge
discover
g =
middle graph
figure
bayes ’ theorem
posterior probability
fuel tank
first
denominator
bayes ’
p
g =
= b∈
f∈
p
g =
f
p
b
p
f
p
g
= b∈
results
g =
f =
p
b
p
f
= p
g
p
f =
p
g =


f
> p
f =
gauge
tank
next suppose
state
battery
ﬁnd
b =
states
fuel gauge
battery
right-hand graph
figure
posterior probability
fuel tank
observations
fuel gauge
battery state
p
f
=
b =
= p
g
=
f =
p
f =
p
g
=
f
p
f
graphical models f∈
prior probability p
b =
numerator
denom- inator
probability
tank
result
observation
state
battery
accords
intuition
battery
observation
fuel gauge
state
fuel tank
battery
result
reading
fuel gauge
fact
case
fuel gauge
state
descendant
g. note
probability p
f
=
b =

prior probability p
f =
observation
fuel gauge reads
evidence
favour
empty fuel tank
d-separation
general statement
d-separation property
pearl
graphs
graph
a
b
c
sets
nodes
union
complete set
nodes
graph
particular conditional independence statement a ⊥⊥ b | c
acyclic graph
possible paths
node
node
b
any such path
node
arrows
path meet
tail-to-tail
node
node
set c
b
arrows meet head-to-head
node
node
descendants
set c.
paths
a
b
c
joint distribution
variables
graph
a ⊥⊥ b | c.
concept
d-separation
figure
graph
path
b
node f
tail-to-tail node
path
node e
latter
head-to-head node
descendant c
conditioning set
conditional independence statement
⊥⊥ b | c
graph
graph
b
path
b
node f
tail-to-tail node
conditional independence property
⊥⊥ b | f
conditional independence
figure
illustration
con- cept
d-separation
text
details
f
e
f b e c
b
 b n section
distribution
graph
note
path
node e
e
head-to-head node
descendant
conditioning set
purposes
d-separation
parameters
α
σ2
figure
small ﬁlled circles
observed nodes
marginal distributions
such nodes
parameter nodes
parents
paths
nodes
hence
role
d-separation
example
conditional independence
d-separation
concept
i.i.d
data
sec- tion
problem
posterior distribution
mean
univariate gaussian distribution
directed graph
figure
joint distribution
prior p
to- gether
set
conditional distributions
xn|µ
n =
n.
practice
d =
x1
goal
µ
suppose
moment
µ
joint distribution
observations
d-separation
unique path
xi
other xj=i
path
respect
observed node µ
such path
observations d =
x1
µ
figure
graph corre- sponding
problem
mean µ
univariate gaussian dis- tribution
observations x1
xn
b
same graph drawn
plate notation
p
d|µ
p
xn|µ
n=1 µ
µ x1 xn xn
n
b
n      n  
graphical models figure
a graphical representation
‘ naive bayes ’ model
class label z
components
observed vector x =
x1
xd
t
classiﬁcation
z xd section
µ
observations
indepen- dent p
d
p
d|µ
p
µ
dµ = p
xn
n=1
µ
latent variable
value
example
model
i.i.d
data
graph
figure
bayesian polynomial regression
stochastic nodes corre- t.
node
w
respect
path
t
nodes tn
following conditional independence property t ⊥⊥ tn | w.
polynomial coefﬁcients w
predictive distribution
t
training data
t1
ﬁrst use
training data
posterior distribution
coefﬁcients w
training data
posterior distribution
w
predictions
t
new input observations
related graphical structure
approach
classiﬁcation
naive bayes model
conditional independence assumptions
model structure
observed variable consists
d-dimensional vector x =
x1
xd
t
observed values
x
k classes
1-of-k encoding scheme
classes
k- dimensional binary vector z
generative model
multinomial prior p
z|µ
class labels
kth component µk
µ
prior probability
class ck
conditional distribution p
x|z
observed vector x
key assumption
naive bayes model
class z
distributions
input variables
in- dependent
graphical representation
model
figure
observation
z blocks
path
xi
xj
j = i
such paths
node z
xj
z
z
z
tail-to-tail path
xi
xj
marginal density p
x
respect
components
x
simple application
naive bayes model
context
data
different sources
medical diagnosis
section
labelled training
inputs
x1
class labels
naive bayes model
training data
conditional independence
maximum likelihood
data
model
solution
model
class
correspondingly
data
example
probability density
class
case
naive bayes assumption
covariance matrix
gaussian
contours
constant density
class
axis-aligned ellipsoids
marginal density
superposition
diagonal gaussians
coefﬁcients
class priors
respect
components
naive bayes assumption
dimensionality d
input space
density estimation
full d-dimensional space
chal- lenging
input vector
continuous variables
appropriate models
e.g.
bernoulli distributions
binary observations
gaussians
real-valued vari- ables
conditional independence assumption
model
poor representations
class-conditional densities
nevertheless
assumption
model
good classiﬁcation performance
practice
decision boundaries
details
class-conditional densities
figure
graph
speciﬁc decomposition
joint probability distribution
product
conditional probabilities
graph
set
conditional independence statements
d-separation criterion
d-separation theorem
expression
equivalence
properties
order
directed graph
ﬁlter
suppose
particular joint probability distribution p
x
variables
nodes
graph
ﬁlter
distribution
terms
factorization
graph
ﬁlter
set
possible distributions
x
set
variables x
subset
distributions
ﬁlter
df
factorization
figure
graph
different kind
ﬁlter
ﬁrst
conditional independence properties
d-separation criterion
graph
distribution
properties
possible distributions
x
second kind
ﬁlter
d-separation theorem
set
distributions
set df
conditional independence properties
d-separation apply
probabilistic model
particular di-
graph
instance
variables
combination
again
particular graph
whole family
probability distributions
extreme
graph
conditional in- dependence properties
possible joint probability distribution
variables
set df
possible distribu-    k p
j=i
p
xd
p
xd
dxi p
xk|pak
p
xk|pak
dxi k
integral
summation
case
discrete variables
factor p
xk|pak
functional dependence
xi
xi
cancel
numerator
denominator
only factors
conditional distribution p
xi|pai
node
conditional distributions
nodes
node xi
conditioning set
p
xk|pak
other words
xi
parent
xk
conditional p
xi|pai
parents
node xi
conditionals
xk|pak
children
graphical models p
x
df figure
graphical model
case
graph
ﬁlter
prob- ability distribution p
x
ﬁlter
directed factorization property
set
possible probability distributions
x
ﬁlter
df
graph
distributions
conditional independencies
d-separation properties
graph
d-separation theorem
same set
distributions df
second kind
ﬁlter
tions
x
other extreme
graph
links
distributions
product
marginal distributions
variables
nodes
graph
note
graph
set
distributions df
dis- tributions
additional independence properties
graph
instance
distribution
ﬁlter
graph
corresponding set
variables
discussion
conditional independence properties
concept
markov blanket
markov
joint distribution p
xd
graph
d nodes
conditional distribution
particular node
variables
variables
factorization property
conditional distribution
form
markov random fields
figure
markov blanket
node xi
set
parents
children
co-parents
node
property
conditional distribution
xi
variables
graph
variables
markov blanket
xi
xi
co-parents
other words variables
parents
node
node xi
set
nodes
parents
children
co-parents
markov blanket
figure
markov blanket
node xi
minimal set
nodes
rest
graph
note
parents
children
node xi
phenomenon
observations
child nodes
paths
co-parents
co-parent nodes
markov random fields
graphical models
factorization
joint dis- tribution
set
variables
product
local conditional distributions
set
conditional independence properties
distribution
graph
second ma- jor class
graphical models
undirected graphs
factorization
set
conditional independence relations
markov random ﬁeld
markov network
undirected graphical model
kindermann
snell
set
nodes
group
variables
set
links
pair
nodes
links
arrows
case
undirected graphs
discussion
conditional independence properties
conditional independence properties
case
graphs
par- ticular conditional independence property
graphical test
d-separation
paths
sets
nodes
’
deﬁnition
blocked
presence
paths
head-to-head nodes
alternative graphical semantics
probability distributions
conditional independence
simple graph separation
case
corresponds
undirected graphical models
section
graphical models figure
example
undirected graph
path
node
set
node
set b
node
c. conse-
conditional independence property a ⊥⊥ b | c
probability distribution
graph
c b a directionality
links
graph
asymmetry
parent
child nodes
subtleties
head-to-head nodes
undirected graph
sets
nodes
a
b
c
conditional independence property a ⊥⊥ b | c.
property
probability distribution
graph
possible paths
nodes
set
nodes
set b
such paths
nodes
set c
such paths
’
conditional independence property
such path
property
distributions
graph
conditional independence relation
example
figure
note
d-separation crite- rion
’ phenomenon
conditional independence
undirected graphs
directed graphs
alternative way
conditional independence test
nodes
c
graph
links
nodes
path
node
node
b
such paths
conditional independence property
markov blanket
undirected graph
simple form
node
other nodes
neighbouring nodes
figure
factorization properties
factorization rule
undirected graphs
above conditional independence test
again
joint distribution p
product
functions
sets
variables
graph
appropriate notion
locality
case
markov random fields
figure
undirected graph
markov blanket
node xi
set
nodes
property
conditional distribution
xi
variables
graph
variables
markov blanket
nodes
link
variables
other nodes
graph
fact
direct path
nodes
other paths
nodes
paths
conditional independence property
p
i
j
p
i
j
p
i
j
i
j
set x
variables
xi
xj
factor- ization
joint distribution
xi
same factor
order
conditional independence property
possible distributions
graph
graphical concept
clique
subset
nodes
graph
link
pairs
nodes
subset
other words
set
nodes
clique
furthermore
maximal clique
clique
other nodes
graph
set
clique
concepts
undirected graph
variables
figure
graph
cliques
nodes
x1
x2
x2
x3
x3
x4
x4
x2
x1
x3
maximal cliques
x1
x2
x3
x2
x3
x4
set
x1
x2
x3
x4
clique
link
x1
factors
decomposition
joint distribution
functions
variables
cliques
fact
functions
maximal cliques
loss
generality
other cliques
subsets
maximal cliques
x1
x2
x3
maximal clique
arbitrary function
clique
factor
subset
variables
clique
c
set
variables
clique
xc
figure
a four-node undirected graph
clique
maximal clique
blue
x1 x3 x2 x4   c
z
joint distribution
product
potential functions
xc
maximal cliques
graph p
x
ψc
xc
graphical models
quantity z
partition function
normalization con- stant
z = ψc
xc
c
distribution p
x
potential functions
ψc
xc
p
x
x comprises discrete variables
framework
continuous variables
combination
summation
appropriate combination
summation
integration
note
choice
potential functions
speciﬁc probabilistic interpretation
conditional distributions
contrast
graphs
factor
conditional distribu- tion
corresponding variable
state
parents
special cases
instance
undirected graph
directed graph
potential functions
interpretation
consequence
generality
potential functions
xc
product
in- troduce
explicit normalization factor
directed graphs
joint distribution
consequence
normalization
conditional distributions
factorization
presence
normalization constant
major limitations
undirected graphs
model
m discrete nodes
k states
evaluation
normalization term
km states
case
size
model
partition function
parameter learning
function
parameters
potential functions
xc
evaluation
local conditional distributions
partition function
conditional
ratio
marginals
partition function cancels
numerator
denom- inator
ratio
local marginal probabil- ities
unnormalized joint distribution
marginals
end
marginals
small number
variables
evaluation
normalization coefﬁcient
notion
conditional independence
sim- ple graph separation
factorization
joint distribution
conditional independence structure
formal connection
conditional independence
factorization
undirected graphs
attention
poten- tial functions
xc
markov random fields
choice
xc
restriction
precise relationship
factorization
conditional independence
concept
graphical model
ﬁlter
corre- sponding
figure
set
possible distributions
ﬁxed set
variables
nodes
particular undirected graph
ui
set
such distributions
set
conditional independence statements
graph
graph separation
uf
set
such distributions
factorization
form
respect
maximal cliques
graph
hammersley-clifford theorem
clifford
sets ui
uf
convenient
exponentials
potential functions
xc
−e
xc
e
xc
energy function
exponential representation
boltzmann distribution
joint distribution
product
potentials
total energy
energies
maximal cliques
contrast
factors
joint distribution
directed graph
po- tentials
undirected graph
speciﬁc probabilistic interpretation
ﬂexibility
potential functions
normalization constraint
question
choice
potential function
particular application
potential function
conﬁgurations
local variables
others
global conﬁgurations
high probability
good balance
inﬂuences
clique potentials
speciﬁc example
use
undirected graphs
illustration
image de-noising
application
undirected graphs
example
noise removal
binary image
besag
geman
geman
besag
simple example
sophisticated applications
observed noisy image
array
binary pixel values
∈
−1
+1
index
d
pixels
image
unknown noise-free image
binary pixel values
∈
−1
+1
sign
pixels
small probability
example binary image
noise
image
sign
pixels
probability
%
figure
noisy image
goal
original noise-free image
noise level
strong correlation
xi
pixels xi
xj
image
prior knowledge
markov
graphical models figure
illustration
image de-noising
markov random ﬁeld
top row
original binary image
left
image
%
pixels
right
bottom row
restored images
iterated conditional models
icm
left
graph-cut algorithm
right
icm
image
%
pixels
original image
corresponding number
graph-cut
%
random ﬁeld model
undirected graph
figure
graph
types
cliques
variables
cliques
form
xi
yi
energy function
correlation
variables
simple energy function
cliques
form −ηxiyi
η
positive constant
desired effect
energy
probability
xi
yi
same sign
energy
opposite sign
cliques
pairs
variables
xi
xj
i
j
indices
pixels
again
energy
pixels
same sign
opposite sign
energy
−βxixj
β
positive constant
potential function
nonnegative function
maximal clique
nonnegative functions
subsets
clique
  xi
markov random fields
figure
undirected graphical model
markov random ﬁeld
image de-noising
xi
binary variable
state
pixel i
unknown noise-free image
corresponding value
pixel i
observed noisy image
corresponding energies
example
extra term hxi
pixel i
noise-free image
term
effect
model towards
values
particular sign
preference
complete energy function
model
form e
x
y
= h xi − β i
i
j
xixj − η i
joint distribution
x
p
z
−e
elements
y
observed values
pixels
noisy image
conditional distribution p
x|y
noise- free images
example
ising model
statistical physics
purposes
image restoration
image x
high probability
maximum probability
simple iterative technique
iterated conditional modes
icm
kittler
f¨oglein
application
coordinate-wise gradient ascent
idea
variables
xi
xi = yi
i
node xj
time
total energy
possible states
= +1
xj = −1
other node variables
xj
state
energy
probability
variable
simple local computation
update
site
suitable stopping criterion
nodes
systematic way
instance
raster
image
nodes
random
sequence
updates
site
changes
variables
deﬁnition
algorithm exercise
graphical models figure
example
directed graph
b
equivalent undirected graph
x1 x1
b
x2 x2 xn−1 xn xn xn−1
local maximum
probability
need
correspond
global maximum
purposes
simple illustration
parameters
η =
note
h
prior probabilities
states
xi
observed noisy image
initial conﬁguration
icm
convergence
de-noised image
panel
figure
note
β =
links
pixels
probable solution
xi = yi
i
observed noisy image
effective algorithm
high probability so- lutions
max-product algorithm
solutions
global maximum
posterior dis- tribution
certain classes
model
efﬁcient algorithms
graph cuts
global maximum
greig
boykov
al.
kolmogorov
zabih
right panel
figure
result
graph-cut algorithm
de-noising problem
exercise
section
relation
graphs
graphical frameworks
probability dis- tributions
undirected graphs
relation
problem
model
graph
undirected graph
cases
simple example
figure
joint distribution
directed graph
product
conditionals
form p
x
p
x1
p
x2|x1
p
x3|x2
p
xn|xn−1
undirected graph representation
fig- ure
undirected graph
maximal cliques
pairs
neigh- bouring nodes
joint distribution
form p
x
z ψ1,2
x1
x2
ψ2,3
x3
··· ψn−1
n
xn−1
xn
figure
example
simple directed graph
corre- sponding moral graph
b
x1 x3 x1 x3 x2
markov random fields
x4
x4
b
ψ1,2
x1
x2
p
x1
p
x2|x1
ψ2,3
x3
p
x3|x2
n
xn−1
xn
p
xn|xn−1
marginal p
x1
ﬁrst node
ﬁrst potential function
note
case
partition function z
construction
distribution speciﬁed
factorization
directed graph
speciﬁed
factorization
undirected graph
clique potentials
undirected graph
conditional distributions
directed graph
order
set
variables
conditional distributions
member
clique
undirected graph
nodes
graph
parent
directed link
undirected link
nodes
directed graph
parent
nodes
head-to-head ’ paths
discussion
conditional independence
simple
graph
nodes
figure
joint distribution
directed graph
form p
x
p
x1
p
x2
p
x3
p
x2
x3
factor p
x2
x3
variables
x2
x3
x4
single clique
conditional distribution
clique potential
extra links
pairs
parents
node x4
process
‘
parents
moralization
undirected graph
arrows
moral graph
moral graph
example
conditional independence properties
contrast
original directed graph
directed graph
undirected graph
add additional undirected links
pairs
parents
node
graph
graphical models section
section
arrows
original links
moral graph
clique potentials
moral graph
conditional distribution factor
original directed graph
clique potentials
maximal clique
variables
factor
result
moralization step
note
cases
partition function
z =
process
directed graph
undirected graph
important role
exact inference techniques
junction tree algorithm
directed representation
general presents problems
normalization constraints
undirected representation
conditional independence properties
graph
course
distribution
directed graph
undirected graph
undirected graph
conditional independence properties
process
moralization
extra links
maximum number
independence properties
procedure
conditional independence properties
undirected graphs
types
graph
different conditional independence properties
issue
detail
view
speciﬁc
graph
ﬁlter
set
possible distributions
variables
subset
conditional independencies
graph
graph
d map
‘ dependency map ’
distribution
conditional independence statement satisﬁed
distribution
graph
graph
links
trivial d map
distribution
speciﬁc distribution
ask
graphs
appropriate conditional independence properties
conditional indepen- dence statement
graph
speciﬁc distribution
graph
‘ independence map ’
distribution
graph
distribution
case
conditional independence property
distribution
graph
vice versa
graph
perfect map
figure
venn diagram
set
distributions p
set
variables
set
distributions d
perfect map
directed graph
set u
perfect map
undirected graph
d u p
inference
graphical models figure
a directed graph
conditional independence properties
undirected graph
variables
b c
distribution
perfect map
d map
set
distributions
distribution
directed graph
perfect map
set
set
distributions
distribution
undirected graph
perfect map
addition
distributions
undirected graphs
perfect map
venn diagram
figure
figure
example
directed graph
perfect map
distribution
conditional independence properties a ⊥⊥ b | ∅
a ⊥⊥ b | c.
undirected graph
vari- ables
perfect map
undirected graph
variables
fig- ure
graph
properties a ⊥⊥ b | ∅
c ⊥⊥ d
a ∪ b
a ⊥⊥ b | c ∪ d.
directed graph
variables
same set
conditional independence properties
graphical framework
consistent way
undirected links
chain graphs
lauritzen
wermuth
frydenberg
undirected graphs
special cases
such graphs
class
distributions
distributions
chain graph
perfect map
chain graphs
book
figure
undirected graph
conditional independence properties
terms
directed graph
same variables
b c d
inference
graphical models
problem
inference
graphical models
nodes
graph
values
posterior distributions
subsets
other nodes
graphical structure
efﬁcient algorithms
inference
graphical models figure
a graphical representation
bayes ’ see
text
details
theorem
y x y x y
b
c

structure
algorithms transparent
many algorithms
terms
propagation
local messages
graph
section
techniques
exact inference
chapter
number
approximate inference algorithms
graphical interpretation
bayes ’ theorem
suppose
joint distribution p
variables
product
factors
form p
p
x
p
y|x
directed graph
figure
value
y
shaded node
figure
b
marginal distribution p
prior
latent variable x
goal
corresponding posterior distribution
x
sum
product rules
probability
p
y
= p
y|x
p
x
bayes ’ theorem
p
x|y
p
y|x
p
x
p
y
joint distribution
terms
p
y
p
x|y
graphical perspective
joint distribution p
graph
figure
c
direction
arrow
simplest example
inference problem
graphical model
inference
chain
complex problem
chain
nodes
form
figure
example
foundation
discussion
exact inference
general graphs
section
undirected graph
figure
b
directed chain
equivalent undirected chain
directed graph
nodes
parent
addition
extra links
undirected versions
graph express
same set
conditional inde- pendence statements
    x1
inference
graphical models
joint distribution
graph
form p
x
z ψ1,2
x1
x2
ψ2,3
x3
··· ψn−1
n
xn−1
xn
speciﬁc case
n nodes
discrete vari- ables
k states
case
potential function ψn−1
n
xn
k × k table
joint distribution
n −
k
parameters
inference problem
marginal distribution p
xn
speciﬁc node xn
part way
chain
note
moment
observed nodes
deﬁnition
required marginal
joint distribution
variables
xn
p
xn
··· xn−1 xn+1 p
x
··· xn
naive implementation
joint distribution
summations
joint distribution
set
numbers
possible value
x
n variables
k states
k n values
x
evaluation
storage
joint distribution
marginalization
p
xn
storage
computation
scale
length n
chain
efﬁcient algorithm
con- ditional independence properties
graphical model
factor- ized expression
joint distribution
order
summations
multiplications
required marginal
instance
summation
xn
potential ψn−1
n
xn−1
xn
only one
xn
summation
ψn−1
n
xn−1
xn
function
xn−1
summation
xn−1
new function
potential ψn−2
n−1
xn−2
xn−1
other place
summation
x1 involves
potential ψ1,2
x2
function
x2
summation
distribution
removal
node
graph
group
potentials
summations
way
⎤⎦ + ···  x1
ψ2,3
x2
x3
 
x2 xn ⎤⎦ + ⎡⎣
⎡⎣
xn+1 xn−1
graphical models
desired marginal
form p
xn
z ψ1,2
x1
x2
ψn−1
xn
··· ψn
n+1
xn+1
µα
xn
ψn−1
n
xn−1
xn
···
µβ
xn
reader
idea
basis
later discussion
general sum-product algorithm
key concept
multiplication
addi- tion
ab +
b + c
left-hand side
arithmetic operations
right- hand side
operations
computational cost
re-ordered expression
n −
summations
k states
function
variables
instance
summation
x1 involves
function ψ1,2
x1
x2
table
k × k numbers
table
x1
value
x2
o
k
cost
vector
k numbers
matrix
numbers ψ2,3
x2
x3
o
k
n −
summations
multiplications
kind
total cost
marginal p
xn
o
n k
length
chain
contrast
exponential cost
naive approach
many conditional independence properties
simple graph
order
efﬁcient calcula- tion
graph
conditional independence properties
full joint distribution
powerful interpretation
calculation
terms
passing
local messages
graph
from
expression
marginal p
xn
product
factors
normalization constant p
xn
µα
xn
µβ
xn
z
µα
xn
message
forwards
chain
node xn−1
xn
µβ
xn
message
backwards ⎡⎣ ⎡⎣ ⎤⎦ ⎤⎦  x1     xn−1 xn−1 figure
marginal distribution p
xn
node xn
chain
messages
xn
µβ
xn
normaliz- ing
messages
mes- sages
ends
chain to- wards node xn
inference
graphical models µα
xn−1
µα
xn
µβ
xn
µβ
xn+1
xn
chain
xn
node xn+1
note
messages
prises
set
k values
choice
xn
product
mes- sages
point-wise multiplication
elements
messages
set
k values
message µα
xn
µα
xn
ﬁrst evaluate ψn−1
xn
xn−2 ψn−1
n
xn
µα
xn−1
µα
x2
ψ1,2
x1
x2
desired node
note
structure
message
equation
outgoing message µα
xn
incoming message µα
xn−1
local potential
node
node variable
message µβ
xn
node xn
µβ
xn
= ψn+1
n
xn
xn+2 xn+1 = ψn+1
n
xn
µβ
xn+1
xn+1
recursive message passing
figure
normalization con- stant z
right-hand side
states
xn
operation
o
k
computation
graphs
form
figure
markov chains
corresponding message
equations
example
chapman- kolmogorov equations
markov processes
papoulis
   
graphical models
marginals
xn
node n ∈
n
chain
simply
above procedure
node
computational cost
o
n
approach
computation
instance
p
agate
message µβ
node xn
x2
p
messages µβ
·
node xn
x3
computation
messages
cases
message µβ
xn−1
node xn
propagate corresponding messages
way
x1
message µα
x2
node x1
corre- sponding messages
way
xn
intermediate messages
way
node
marginal sim- ply
computational cost
marginal
single node
n
message
direction
link
graph
note
normalization constant z need
convenient node
nodes
graph
corresponding variables
observed values
summation
note
effect
variable xn
observed value xn
joint distribution
copies
additional function
value
such function
potentials
summations
term
xn = xn
value
xn = xn
exercise
chapter
joint distribution p
xn
nodes
chain
evaluation
marginal
single node
variables
few moments
required joint distribution
form p
xn
z µα
xn−1
ψn−1
xn
µβ
xn
joint distributions
sets
variables
potentials
message passing
marginals
useful result
practice
parametric forms
clique potentials
conditional distributions
directed graph
order
parameters
potentials
situa- tions
variables
em algorithm
local joint distributions
cliques
observed data
e step
examples
detail
chapter
trees
exact inference
graph
chain
nodes
time
number
nodes
inference
graphical models
figure
examples tree-
graphs
undirected tree
b
directed tree
c
directed polytree
b
c
terms
messages
chain
inference
local message
class
graphs
trees
message
formalism
chains
sum-product algorithm
efﬁcient framework
exact inference
tree-structured graphs
case
undirected graph
tree
graph
path
pair
nodes
such graphs
loops
case
graphs
tree
single node
root
parents
other nodes
parent
directed tree
undirected graph
moralization step
links
nodes
parent
consequence
corresponding moralized graph
undirected tree
examples
directed trees
figure
b
note
distribution
directed tree
undirected tree
vice versa
nodes
directed graph
parent
path
direction
arrows
nodes
graph
polytree
figure
c
graph
node
property
parents
undirected graph
loops
factor
sum-product algorithm
next section
trees
polytrees
simple
general form
new graphical construction
factor graph
frey
kschischnang
al.
undirected graphs
global function
several vari- ables
product
factors
subsets
variables
factor
decomposition explicit
additional nodes
fac- tors
addition
nodes
variables
details
factorization
joint distribution
set
variables
form
product
factors
xs
subset
variables
convenience
p
x
fs
xs
s
graphical models figure
example
factor graph
factorization
x1 x2 x3 fa fb fc
individual variables
xi
discussions
groups
variables
vectors
matrices
factor fs
function
corresponding set
variables xs
directed graphs
factorization
special cases
factors
xs
local conditional distributions
undirected graphs
special case
factors
po- tential functions
maximal cliques
normalizing coefﬁcient
factor
empty set
variables
factor graph
node
circle
variable
distribution
case
undirected graphs
additional nodes
small squares
factor fs
xs
joint dis- tribution
undirected links
factor node
variables nodes
factor
example
distribution
terms
factorization p
x
fa
x2
fb
x2
fc
x3
fd
x3
factor graph
figure
note
factors
x2
x1
x2
same set
variables
undirected graph
product
such factors
same clique potential
fc
x3
x3
single potential
x2
x3
factor graph
keeps such factors
detailed information
underlying factorization
x2 x1 x2 x1 f x3
b
x3
x2 fb fa x3
c
figure
undirected graph
single clique potential ψ
x2
x3
b
factor graph
factor f
x2
x3
ψ
x1
x2
x3
same distribution
undirected graph
c
different factor graph
same distribution
factors
fa
x2
x3
fb
x2
ψ
x1
x2
x3
x1 x2 x1 x2 x1
inference
graphical models
f x3
b
fc fa fb x3
c
x3
figure
a
graph
factorization p
x1
p
x2
p
x2
b
factor graph
same distribution
directed graph
factor satisﬁes f
x2
x3
p
x1
p
x2
p
x2
c
different factor graph
same distribution
factors
x1
p
x1
x2
= p
x2
x1
x2
x3
p
x2
factor graphs
distinct kinds
nodes
links
nodes
opposite type
factor graphs
rows
nodes
variable nodes
factor nodes
bottom
links
rows
example
figure
situations
other ways
graph
example
factor graph
undirected graph
distribution
terms
undirected graph
factor graph
variable nodes
nodes
original undirected graph
addi- tional factor nodes
maximal cliques
factors
xs
clique potentials
note
several different factor graphs
correspond
same undirected graph
concepts
figure
directed graph
factor graph
variable nodes
factor graph
nodes
directed graph
factor
conditional distributions
appropriate links
again
multiple factor graphs
correspond
same directed graph
conversion
directed graph
factor graph
figure
importance
tree-structured graphs
efﬁcient inference
undirected tree
factor graph
result
tree
other words
factor graph
loops
path
nodes
case
directed polytree
conversion
undirected graph results
loops
moralization step
whereas conversion
factor graph
results
tree
figure
fact
local cycles
directed graph
parents
node
conversion
factor graph
appropriate factor function
figure
multiple different factor graphs
undirected graph
factor graphs
graphical models
b
figure
a
polytree
b
result
polytree
undirected graph
creation
loops
c
result
polytree
factor graph
tree structure
precise form
factorization
figure
example
undirected graph
different factor graphs
b
joint distri- bution
general form p
x
f
x2
x3
whereas
c
speciﬁc factorization p
x
fa
x2
fb
x3
fc
x3
factorization
c
conditional independence properties
sum-product algorithm
use
factor graph framework
powerful class
efﬁcient
exact inference algorithms
tree-structured graphs
problem
local marginals
nodes
subsets
nodes
sum-product algorithm
technique
probable state
rise
max-sum algorithm
variables
model
marginalization corresponds
sums
framework
linear-gaussian models
case marginalization
integration
example
detail
linear dynamical systems
section
figure
fragment
graph
lo- cal cycle
b
conversion
fragment
factor graph
tree struc- ture
f
x2
x3
p
x1
p
x2|x1
p
x2
x1 x2 x1 x2 f
x2
x3
x3
x3
b
x1 x2 x1 x2 f
x2
x3
x3
x3
b
figure
a
undirected graph
b
c
factor
undirected graph
  x\x
inference
graphical models
x1 fa x2 fb fc x3
c
algorithm
exact inference
directed graphs
loops
belief propagation
pearl
lauritzen
spiegelhalter
equiv- alent
special case
sum-product algorithm
sum-product algorithm
original graph
undirected tree
directed tree
polytree
corresponding factor graph
tree structure
original graph
factor graph
undirected models
same framework
goal
structure
graph
things
i
efﬁcient
exact inference algorithm
marginals
ii
situations
several marginals
computations
problem
marginal p
x
partic- ular variable node x
moment
variables
algorithm
evidence
variables
deﬁnition
marginal
joint distribution
variables
x
p
x
p
x
x \ x
set
variables
x
variable x
idea
p
x
factor graph expression
summations
products
order
efﬁcient algorithm
fragment
graph
figure
tree structure
graph
factors
joint distribution
groups
group
factor
neighbour
variable node x
joint distribution
product
form p
x
fs
x
xs
s∈ne
x
x
set
factor nodes
neighbours
x
xs
set
variables
subtree
variable node x
factor node     x
s f
x xs
     x1   xm
graphical models figure
a fragment
factor graph
evaluation
marginal p
x
µfs→x
x
x fs
fs
x
xs
product
factors
group
factor fs
substituting
sums
products
tain p
x
fs
x
xs
s∈ne
x
s∈ne
x
µfs→x
x
µfs→x
x
xs fs
x
xs
set
functions µfs→x
x
messages
factor
fs
variable node x
required marginal p
x
product
incoming messages
node x
order
messages
figure
factor fs
x
xs
factor
graph
fs
x
xs
fs
xm
g1
x1
xs1
gm
xm
xsm
convenience
variables
factor fx
addition
x1
factorization
figure
note
set
variables
x
x1
set
variables
factor fs
xs
notation
µfs→x
x
=
fs
xm
fs
xm
m∈ne
fs
gm
xm
xsm
xxm µxm→fs
xm
x1 xm m∈ne
fs
  xsm
ne
fs
set
variable nodes
neighbours
factor node fs
fs
same set
node x
following messages
variable nodes
nodes µxm→fs
xm
gm
xm
xsm
distinct kinds
message
factor nodes
variable nodes
µf→x
x
variable nodes
nodes
µx→f
x
case
messages
link
function
variable node
connects
result
message
factor node
vari- able node
link
product
incoming messages
other links
factor node
multiply
factor
node
variables
incoming messages
figure
factor node
message
variable node
incoming messages
variable nodes
expression
messages
variable nodes
nodes
use
graph factorization
fig- ure
term gm
xm
xsm
node xm
product
terms fl
xm
xml
factor
fl
xm
fs
gm
xm
xsm
fl
xm
xml
l∈ne
xm
product
neighbours
node xm
node fs
note
factors fl
xm
xml
subtree
original graph
same kind
illustration
factorization
factor node fs
inference
graphical models
xm µxm→fs
xm
fs µfs→x
x
xm gm
xm
xsm
fl fl fl
xm
xml
fs  
graphical models figure
illustration
evaluation
message
variable node
adjacent factor node
µxm→fs
xm
fl
xm
xml
xml µfl→xm
xm
l∈ne
xm
= l∈ne
xm
deﬁnition
messages
factor nodes
variable nodes
message
variable node
adjacent factor node
connecting link
product
incoming messages
other links
variable node
neighbours
computation
passes messages
un- changed
variable node
message
factor node
incoming messages
other neighbouring factor nodes
goal
marginal
variable node x
product
incoming messages
links
node
messages
terms
other messages
order
recursion
node x
root
tree
begin
leaf nodes
deﬁnition
leaf node
variable node
message
link
figure
leaf node
factor node
message
form µx→f
x
µf→x
x
f
x
figure
sum-product algorithm
messages
leaf nodes
de- pend
leaf node
variable node
b
factor node
µx→f
x
µf→x
x
f
x
f f x
b
inference
graphical models
figure
b
point
particular version
sum- product algorithm
marginal p
x
variable node x
root
factor graph
initiating messages
leaves
graph
message
steps
messages
link
root node
messages
neighbours
node
message
root
messages
other neighbours
root node
messages
neighbours
required marginal
process
node
enough messages
message
simple inductive argument
clearly
graph
variable root node
several factor leaf nodes
algorithm
messages
form
leaves
root
general graph
time
particular graph
valid algorithm
factor
node
single link
overall graph
tree
new node
leaf node
message
node
turn
messages
order
own message
root
valid algorithm
proof
marginals
variable node
graph
above algorithm afresh
such node
required computations
efﬁcient procedure
‘
multiple message
algorithms
general sum-product algorithm
factor
node
root
propagate messages
leaves
root
point
root node
messages
neighbours
messages
neighbours
turn
messages
neighbours
messages
links
root
way
messages
outwards
root
way
leaves
message
directions
link
graph
node
message
neighbours
simple inductive argument
validity
message
protocol
variable node
messages
neighbours
marginal distribution
variable
graph
number
messages
number
links
graph
computation
single marginal
comparison
sum-product algorithm
node
amount
computation
size
graph
note
algorithm
fact
node
root
exercise
   
graphical models figure
sum-product algorithm
terms
messages
factor nodes
other factor nodes
example
outgoing message
blue arrow
product
in- coming messages
green arrows
mul- tiplying
factor fs
variables x1
x2
x2 fs x3
notion
node
special status
convenient way
message
protocol
next suppose
marginal distributions
xs
sets
variables
factors
similar argument
factor
product
messages
factor node
local factor
node exercise
p
xs
fs
xs
i∈ne
fs
µxi→fs
xi
factors
complete analogy
marginals
variable nodes
functions
values
parameters
em algorithm
marginals
quantities
e step
detail
hidden markov model
chapter
message
variable node
factor node
product
incoming messages
other links
sum-product algorithm
different form
messages
variable nodes
nodes
messages
factor nodes
example
figure
issue
normalization
factor graph
directed graph
joint distribution
marginals
sum-product algorithm
undirected graph
unknown normalization coefﬁcient
simple chain example
figure
unnormal- p
x
/z
ized version p
xi
sum-product algorithm
corresponding unnormalized marginals
marginals
normalization
single variable
entire set
variables
p
x
p
x
joint distribution
p
x
point
simple example
operation
sum-product algorithm
figure
simple 4-node factor        x2 x1 x2
x3
figure
a simple factor graph
x1 sum-product algorithm
inference
graphical models x2
fb fc x4 graph
unnormalized joint distribution
p
x
fa
x2
fb
x3
fc
x4
order
sum-product algorithm
graph
node x3
root
case
leaf nodes
x4
leaf nodes
following sequence
messages
direction
ﬂow
messages
figure
mes- sage propagation
messages
root
leaf nodes
fa
x2
µx1→fa
x1
µfa→x2
x2
µx4→fc
x4
µfc→x2
x2
µx2→fb
x2
µfa→x2
x2
µfc→x2
x2
µfb→x3
x3
fb
x3
µx2→fb
fc
x4
x3 fb
x3
µx3→fb
x3
µfb→x2
x2
µx2→fa
x2
µfb→x2
x2
µfc→x2
x2
µfa→x1
x1
µx2→fc
x2
µfa→x2
x2
µfb→x2
x2
µfc→x4
x4
fa
x2
µx2→fa
x2
fc
x4
µx2→fc
x2
  x4 x4  x3       x1 x3 x1 x1 x2 x1   x3       x4
b
x2 x3  
graphical models x1 x2 x4
figure
flow
messages
sum-product algorithm
example graph
figure
leaf
x1
x4
root node x3
b
root node
leaf nodes
message
direction
link
marginals
simple check
marginal p
x2
correct expression
using
messages
above results
x2
= µfa→x2
x2
µfb→x2
x2
µfc→x2
x2
fb
x3
fa
x2
fc
x4
fa
x2
fb
x3
fc
x4
= p
x
variables
graph
practical applications
subset
variables
cal- culate posterior distributions
observations
nodes
sum-product algorithm
suppose
hidden variables h
variables
observed value
v vi
v
v
product
h
sum-product algorithm
posterior marginals
hi|v = v
normalization coefﬁcient
value
local computation
summations
variables
v
collapse
single term
v.
joint distribution p
x
v
v = v
hence
unnormalized version
p
h|v = v
v
i
vi
section
discrete vari- ables
nothing speciﬁc
variables
graphical framework
probabilistic construction
sum-product algorithm
inference
graphical models
table
example
joint distribution
binary variables
maximum
joint distribution
dif- ferent variable values
maxima
marginals
x
y
y
section
continuous variables
summations
integrations
example
sum-product algorithm
graph
linear-gaussian variables
linear dynamical systems
max-sum algorithm
sum-product algorithm
joint distribution p
x
factor graph
ﬁnd marginals
component variables
other common tasks
setting
variables
prob- ability
value
probability
related algorithm
max-sum
application
dynamic programming
context
graphical models
cormen
simple approach
latent variable values
high probability
sum-product algorithm
marginals
xi
ev- ery variable
marginal
turn
value x i
marginal
set
values
practice
set
values
probability
other words
vector xmax
joint distribution
xmax =
p
x
value
joint probability
x p
xmax
max x p
x
set
x i values
simple example
joint distribution p
binary variables
joint distribution
x
=
value
marginal
p
x
values
y
p
x =
=
p
x =
=
marginal
y
p
y =
=
p
y =
=
marginals
x =
=
value
joint distribution
fact
examples
set
probable values
probability
joint distribution
efﬁcient algorithm
value
x
joint distribution p
x
value
joint distribution
maximum
problems
max operator
terms
components
x p
x
max x1
max xm p
x
exercise

graphical models
m
total number
variables
substitute
p
x
expansion
terms
product
factors
sum-product algorithm
use
distributive law
multiplication
use
analogous law
max operator max
ac
max
b
c
�
case
factors
graphical model
products
maximizations
simple example
chain
nodes
evaluation
probability maximum
max x p
x
z
z max x1 max x1 ··· max ψ1,2
x1
x2
[ ψ1,2
x1
x2
··· ψn−1
n
xn−1
xn
··· max ψn−1
n
xn−1
xn
xn
calculation
marginals
max
product operators results
efﬁcient computation
terms
messages
node xn backwards
chain
x1
result
arbitrary tree-structured factor graphs
expression
factor graph expansion
exchanging maximizations
products
structure
calculation
sum-product algorithm
results
present context
particular variable node
‘ root ’
graph
set
messages
inwards
leaves
tree
root
node
message
root
messages
other neighbours
ﬁnal maximization
product
messages
root node
maximum value
p
x
max-product algorithm
sum-product algorithm
summations
maximizations
note
stage
messages
leaves
root
other direction
practice
products
many small probabilities
numerical under- ﬂow problems
logarithm
joint distri- bution
logarithm
monotonic function
> b
> ln b
max operator
logarithm function
ln max x p
x
max x ln p
x
distributive property
max
+ b
+ c
+ max
b
c
logarithm
effect
products
max-product algorithm
sums
max-sum algorithm
⎡⎣  ⎤⎦
 ⎤⎦ ⎤⎦
⎡⎣ s∈ne
x
inference
graphical models
results
sum-product algorithm
max-sum algorithm
terms
message
‘ sum ’
‘ max ’
replacing products
sums
logarithms
µf→x
x
max x1
f
xm
+ µxm→f
xm
m∈ne
fs
x
l∈ne
x
µfl→x
x
initial messages
leaf nodes
analogy
root
maximum probability
analogy
µx→f
x
µf→x
x
ln f
x
= max x µfs→x
x
maximum
joint distribution
messages
leaves
root node
result
same irrespective
node
root
second problem
conﬁguration
variables
joint dis- tribution
maximum value
messages
leaves
root
process
value xmax
probable value
root node
xmax =
x
s∈ne
x
point
message
al- gorithm
send messages
root
leaves
variable nodes
mul- tiple conﬁgurations
x
rise
maximum value
p
x
such cases
strategy
individual variable values
product
messages
node
different maximizing conﬁgurations
overall conﬁguration
maximum
problem
different kind
message passing
root node
leaves
simple chain example
n variables x1
k states
 
graphical models figure
a lattice
trellis
diagram show- ing
k possible states
row
diagram
variables
chain model
illustration k =
ar- row
direction
message passing
max-product algorithm
state k
variable xn
n
dia- gram
function φ
xn
unique state
previous variable
black lines
paths
lattice correspond
conﬁgurations
global maximum
joint probability distribution
black lines
opposite direction
arrow
k =
k =
n
n n +
corresponding
graph
figure
node xn
root node
ﬁrst phase
messages
leaf node x1
root node
µxn→fn
n+1
xn
= µfn−1
n→xn
xn
µfn−1
xn
max xn−1 ln fn−1
xn
µxn−1→f n−1
xn
particular graph
initial message
leaf node
probable value
xn
µx1→f1,2
x1
n =
xmax xn µfn−1
n→xn
xn
states
previous variables
same maximizing conﬁguration
track
values
variables
rise
maximum state
variable
other words
quantities
φ
xn
arg max xn−1 ln fn−1
xn
µxn−1→f n−1
xn
chain
vari- ables
terms
lattice
trellis diagram
figure
note
probabilistic graphical model
nodes
individual states
variables
variable corresponds
column
such states
di- agram
state
variable
unique state
previous variable
probability
ties
random
function φ
xn
inference
graphical models
lines
nodes
probable value
ﬁ- nal node xn
link
probable state
node xn−1
back
initial node x1
message
chain
n−1 = φ
xmax
xmax
back-tracking
note
several values
xn−1
maximum value
values
back-tracking
conﬁguration
figure
paths
corresponds
global maximum
joint probability distribution
k =
=
represent possible values
xmax n
state
black lines
valid global maximum conﬁguration
note
forward pass
max-sum message passing
backward pass
node
states
path
other path
overall conﬁguration
global maximizer
track
states
forward pass
functions φ
xn
back-tracking
consistent solution
extension
general tree-structured factor graph
message
factor node f
variable node x
maximization
other variable nodes
neighbours
fac- tor node
maximization
record
values
variables x1
rise
maximum
back-tracking step
stored values
as- sign
states xmax m
max-sum algorithm
exact maximizing conﬁguration
variables
factor graph
tree
important application
technique
probable sequence
hidden states
hidden markov model
case
viterbi algorithm
sum-product algorithm
inclusion
evidence
form
observed variables
observed variables
observed values
maximization
hidden vari- ables
identity functions
observed variables
factor functions
sum-product algorithm
iterated conditional modes
icm
page
step
icm
simpler be- cause
‘
node
next comprise
single value
new state
node
conditional distribution
max-sum algorithm
messages
functions
node variables
set
k values
pos- sible state
x
max-sum
icm
global maximum
tree-structured graphs
section
graphical models
exact inference
general graphs
sum-product
max-sum algorithms
efﬁcient
exact solutions
problems
tree-structured graphs
many practical applications
graphs
loops
message
framework
arbitrary graph topolo- gies
exact inference procedure
junction tree algorithm
lau- ritzen
spiegelhalter
jordan
brief outline
key steps
detailed understanding
algorithm
ﬂavour
various stages
starting point
directed graph
undirected graph
moraliza- tion
undirected graph
step
graph
chord-less cycles
nodes
extra links
such chord-less cycles
instance
graph
figure
cycle a–c–b–d–a
link
a
b
c
d. note
joint dis- tribution
triangulated graph
product
same potential functions
functions
expanded sets
variables
triangulated graph
new tree-structured undirected graph
join tree
nodes
maximal cliques
triangulated graph
links
pairs
cliques
vari- ables
selection
pairs
cliques
way
maximal spanning tree
possible trees
cliques
one
weight
tree
weight
link
number
nodes
cliques
weight
tree
sum
weights
links
tree
clique
subset
clique
clique
junction tree
consequence
triangulation step
tree satisﬁes
intersection property
variable
cliques
clique
path
inference
variables
graph
two-stage message
algorithm
sum-product algorithm
junction tree
order
marginals
conditionals
junction tree algorithm
heart
simple idea
factorization properties
distribution
sums
products
partial summations
joint distribution
role
junction tree
precise
efﬁcient way
computations
graphical operations
junction tree
arbitrary graphs
sense
graph
approach
algorithm
joint distributions
node
clique
triangulated graph
compu- tational cost
algorithm
number
variables
inference
graphical models
clique
number
case
discrete variables
important concept
treewidth
graph
bodlaender
terms
number
variables
clique
fact
size
clique
tree
treewidth
multiple different junction trees
graph
treewidth
junction tree
clique
variables
treewidth
original graph
junction tree algorithm
loopy belief propagation
many problems
practical interest
exact in- ference
effective approximation methods
important class
such approximations
variational methods
detail
chapter
deterministic approaches
wide range
methods
monte carlo methods
stochastic numerical sampling
distributions
length
chapter
simple approach
inference
graphs
loops
previous discussion
exact inference
trees
idea
sum-product algorithm
guar- antee
good results
approach
loopy belief propa- gation
frey
mackay
message passing rules
sum-product algorithm
graph
cycles
information
many times
graph
models
algorithm
whereas
others
order
approach
message passing schedule
message
time
link
direction
message
node replaces
previous message
same direction
same link
function
recent messages
node
previous steps
algorithm
message
link
node
other messages
other links
loops
graph
problem
message
algorithm
initial message
unit function
link
direction
node
position
message
many possible ways
message passing schedule
example
schedule
message
link
directions
time step
whereas schedules
message
time
serial schedules
kschischnang
al
factor
node
message
link
node b
node
message
other links
last time
message
node
message
links
messages
other links
messages
graphical models exercise
other messages
previous message
same link
graphs
tree structure
schedule
pending messages
message
direction
link
point
messages
product
received messages
graphs
loops
algorithm
messages
practice
reasonable time
applications
algorithm
convergence
approximate
local marginals
product
incoming messages
variable node
factor node
link
applications
loopy belief propagation algorithm
poor re- sults
whereas
other applications
state-of-the-art algorithms
certain kinds
error-correcting codes
belief propagation
gallager
berrou
al.
mceliece
al.
mackay
neal
frey
graph structure
discussion
inference
graphical models
structure
graph
interest
go- ing
inference problem
graph structure
data
friedman
koller
space
possible struc- tures
measure
structure
bayesian viewpoint
posterior dis- tribution
graph structures
predictions
respect
distribution
prior p
m
graphs
m
posterior distribution
p
m|d
∝ p
m
p
d|m
d
observed data set
model evidence p
d|m
score
model
evaluation
evidence
marginalization
latent variables
challenging computational problem
many models
space
structures
number
different graph structures
number
nodes
heuristics
good candidates
exercises

www
variables
order
representation
joint distribution
directed graph
conditional distributions

www show
property
directed cycles
directed graph
statement
ordered numbering
nodes
node
links
lower-numbered node
joint distribution
binary variables
exercises
b
c
p
b
c
 m
 
consider
binary variables
b
∈
joint distribution
show
direct evaluation
distribution
property
b
p
b
= p
p
b
c
p
b|c
= p
a|c
p
b|c
c =
 
distributions
b|c
p
c|a
corresponding
joint distribution
hence show
direct evaluation
p
b
c
= p
p
c|a
p
b|c
corresponding
graph

draw
directed probabilistic graphical model
relevance vector machine

model
figure
number
parameters
conditional distribution p
xm
xi ∈
m +
use
logistic sigmoid represen- tation
alternative representation
pearl
p
y =
xm
−
− µ0
− µi
i=1
parameters
probabilities
xi =
µ0
additional parameters
� µ0 �
conditional distribution
‘ soft ’
form
logical or function
function
whenever
xi =
interpretation
µ0
 
recursion relations
mean
covari- ance
joint distribution
graph
figure

www show
⊥⊥ b
c | d
⊥⊥ b |

www
d-separation criterion
conditional distribution
node x
directed graph
nodes
markov blanket
variables
graph
graphical models figure
example
graphical model
con- ditional independence properties
head-to-head path a–c–b
descendant
c
node d
b c d

directed graph
figure
none
variables
⊥⊥ b | ∅
suppose
variable d. show
⊥⊥ b |
 
example
car fuel system
figure
state
fuel gauge g
gauge
driver d
reading
gauge
report
gauge
full d =
empty d =
driver
bit
following probabilities
d
p
d
=
driver
fuel gauge
other words
d =
probability
tank
observation
corresponding probability
observation
battery
second probability
intuition
result
result
figure

www show
m−1
distinct undirected graphs
set
m distinct random variables
possibilities
case
m =

use
iterated conditional modes
icm
energy function
write
expression
difference
values
energy
states
particular variable xj
other variables
ﬁxed
quantities
graph

particular case
energy function
coefﬁcients
= h
show
probable conﬁguration
latent variables
xi = yi
i
 
show
joint distribution p
xn
nodes
graph
figure
expression
form
 
inference problem
p
xn|xn
graph
figure
nodes
∈
n
message
algorithm
section
discuss
messages
way
 
graph
form
figure
n
nodes
nodes
x5
d-separation
x2 ⊥⊥ x5 | x3
show
message
algorithm
section
evalu- ation
p
x5
result
value
x5
 
show
distribution
directed tree
equivalent distribution
corresponding undirected tree
distribution
undirected tree
suitable normaliza- tion
clique potentials
directed tree
number
trees
undirected tree
 
sum-product algorithm
section
chain-of- nodes model
section
results
special case

message passing protocol
sum-product algorithm
tree-structured factor graph
messages
leaves
root node
root
leaves
use proof
induction
messages
order
step
node
message
incoming messages
messages
 
show
marginal distributions
xs
sets
variables xs
factors
xs
factor graph
sum-product message
algorithm
required marginals

tree-structured factor graph
subset
variable nodes
connected subgraph
variable node
subset
other variable nodes
single factor node
sum-product algorithm
marginal distribution
subset
 
www
section
marginal distribution p
xi
variable node xi
factor graph
product
messages
node
factor nodes
form
show
marginal p
product
incoming message
links
outgoing message
same link
 
marginal distribution
variables
factor fs
xs
tree-structured factor graph
sum-product message
algo- rithm
product
message
factor node
links
local factor f
xs
form
   
graphical models
 
sum-product algorithm
graph
figure
node x3
root node
correct marginal
x2
show
correct marginals
x1
x3
use
result
sum-product algorithm
graph
correct joint distribution
x1
x2

tree-structured factor graph
discrete variables
joint distribution p
xb
variables
common factor
procedure
sum- product algorithm
joint distribution
variables
values
 
consider
discrete variables x
possible states
example x
∈
joint distribution p
variables
marginal p
x
property
value y
marginal p
y
probability
value
joint distribution
p
y
x
 
concept
pending message
sum-product algorithm
factor graph
section
graph
cycles
message irrespective
algorithm
 
show
sum-product algorithm
factor graph
tree structure
loops
ﬁnite number
messages
messages
mixture models
em
joint distribution
latent variables
correspond- ing distribution
observed variables
marginalization
complex marginal distributions
observed variables
terms
tractable joint distributions
expanded space
latent variables
introduction
latent variables
complicated distributions
simpler components
chapter
mixture distributions
gaussian mixture
section
terms
discrete latent variables
continuous latent variables
subject
chapter
framework
complex probability dis- tributions
mixture models
data
discussion
mixture distributions
problem
clusters
set
data points
nonprobabilistic technique
k-means algorithm
lloyd
latent
section
mixture models and em section
section
section
view
mixture distributions
discrete latent variables
assignments
data points
components
mixture
gen- eral technique
maximum likelihood estimators
latent variable models
expectation-maximization
em
algorithm
gaussian mixture distribution
em algorithm
informal way
careful treatment
latent variable viewpoint
k-means algorithm
particular nonprobabilistic limit
em
mixtures
gaussians
em
generality
gaussian mixture models
data mining
pattern recognition
machine learning
statistical analysis
many applications
parameters
maximum likelihood
em algorithm
signiﬁcant limitations
maximum likelihood ap- proach
chapter
elegant bayesian treatment
framework
variational inference
little additional computation
em
principal difﬁculties
maxi- mum likelihood
number
components
mixture
data
 n  k
k-means
problem
groups
clusters
data points
multidimensional space
suppose
data set
x1
n observations
random d-dimensional euclidean variable x
goal
data set
number k
clusters
moment
value
k
cluster
group
data points
inter-point distances
distances
points
cluster
notion
ﬁrst
set
d-dimensional vectors
k
k
µk
prototype
kth cluster
µk
centres
clusters
goal
assignment
data points
clusters
set
vectors
µk
sum
squares
distances
data point
vector µk
minimum
point
notation
assignment
data points
clusters
data point xn
corresponding set
binary indicator variables
∈
k
k
k
data point xn
data point xn
k
j = k.
1-of-k coding scheme
objective function
distortion measure
j =
rnkxn − µk2
sum
squares
distances
data point
 n  
µk
n=1 rnk
xn − µk
µk =
n rnk
denominator
expression
number
points
k
result
simple interpretation
µk
mean
data points
k.
reason
procedure
k-means algorithm
phases
re-assigning data points
clusters
clus- ter means
turn
further change
assignments
maximum number
iterations
phase
value
objective function j
convergence
algorithm
global minimum
j
convergence properties
k-means algorithm
macqueen
k-means algorithm
old faithful data set
fig- ure
purposes
example
linear re-scaling
data
standardizing
variables
mean
unit standard deviation
example
k =
k-means
assigned vector µk
goal
values
rnk
µk
j
iterative procedure
iteration
successive steps
successive optimizations
respect
rnk
µk
first
initial values
µk
ﬁrst phase
respect
rnk
µk ﬁxed
second phase
respect
µk
rnk ﬁxed
two-stage optimization
convergence
stages
rnk
updating µk correspond
e
expectation
m
maximization
steps
em algorithm
terms e step
m step
context
k-means algorithm
determination
rnk
j
linear func- tion
rnk
optimization
closed form solution
terms
different n
n
rnk
whichever value
k
minimum value
xn − µk2
other words
nth data point
cluster centre
k =
xn − µj2
=
optimization
µk
rnk
ﬁxed
objective function j
quadratic function
µk
respect
section
exercise
appendix a
mixture models and em
−2
d
−2
g
−2
−2
−2
b
−2
e
−2
h
−2
−2
−2
c
−2
f
−2
−2
−2
i
−2
−2
figure
illustration
k-means algorithm
re-scaled old faithful data set
green points
data set
two-dimensional euclidean space
initial choices
centres µ1
µ2
blue crosses
b
initial e step
data point
red cluster
blue cluster
cluster centre
points
side
perpendicular bisector
cluster centres
magenta line
c
subsequent m step
cluster centre
mean
points
corresponding cluster
d
–
i
successive e
m steps
ﬁnal convergence
algorithm
k-means
figure
plot
cost function j
e step
blue points
m step
red points
k-
algorithm
example
figure
algo- rithm
third m step
ﬁnal em cycle
changes
as- signments
prototype vectors
j
case
assignment
data point
cluster centre
classiﬁcation
data points
side
perpendicular bisector
cluster centres
plot
cost function j
old faithful example
figure
note
poor initial values
cluster
algorithm
several steps
convergence
practice
initialization procedure
cluster
random subset
k data points
k-means algorithm
parameters
gaussian mixture model
em algorithm
direct implementation
k-means algorithm
e step
euclidean dis- tance
prototype vector
data point
various schemes
k-means algorithm
data structure
tree
nearby points
same subtree
paliwal
moore
other approaches
use
triangle inequality
distances
unnecessary dis- tance calculations
hodgson
elkan
batch version
k-means
whole data set
prototype vectors
on-line stochastic algorithm
macqueen
robbins-monro procedure
problem
roots
regression function
derivatives
j
respect
sequential update
data point xn
turn
prototype µk
section
section
exercise
µnew k = µold k + ηn
xn − µold k
ηn
rate parameter
data points
k-means algorithm
use
squared euclidean distance
measure
dissimilarity
data point
prototype vector
limit
type
data variables
cases
variables
categorical labels
instance
 n  k 
mixture models and em section
determination
cluster
outliers
k-means algorithm
general dissimilarity measure v
x
x
vectors
x
following distortion measure j =
rnkv
k-medoids algorithm
e step
cluster prototypes µk
data point
cluster
dissimilarity
corresponding prototype
computational cost
o
kn
case
standard k-means algorithm
general choice
dissimi- larity measure
m step
k-means
cluster prototype
data vectors
cluster
algorithm
choice
dissimilarity measure v
·
·
m step
cluster k
discrete search
nk points
cluster
o
n
one notable feature
k-means algorithm
iteration
data point
clusters
data points
particular centre µk
other centre
other data points
cluster centres
latter case
hard assignment
cluster
next section
probabilistic approach
‘ soft ’ assignments
data points
clusters
way
level
uncertainty
appropriate assignment
probabilistic formulation
numerous beneﬁts
k
evaluations
v
·
·
image segmentation
compression
illustration
application
k-means algorithm
related problems
image segmentation
image compression
goal
segmentation
image
regions
homogeneous visual appearance
objects
parts
objects
forsyth
ponce
pixel
image
point
3-dimensional space
intensities
green channels
segmentation algorithm
pixel
image
separate data point
note
space
euclidean
channel intensities
interval
]
k-means algorithm
difﬁ- culty
result
k-means
particular value
k
image
pixel vector
r
g
b
intensity triplet
centre µk
pixel
results
various values
k
figure
value
k
algorithm
image
palette
k colours
use
k-means
sophisticated approach
image segmentation
account
spatial proximity
different pixels
image segmentation problem
k =
k =
k =
original image
k-means
figure
examples
application
k-means clustering algorithm
image segmentation
initial images
k-means segmentations
various values
k.
use
vector quantization
data compression
values
k
compression
expense
image quality
subject
active research
behaviour
k-means algorithm
result
algorithm
data compres- sion
lossless data compression
goal
original data
compressed representation
lossy data compression
errors
reconstruction
return
levels
compression
lossless case
k-means algorithm
problem
lossy data compression
n data points
identity k
cluster
values
k clus- ter centres µk
data
k n.
data point
centre µk
new data points
ﬁrst
µk
label k
original data vector
framework
vector quantization
vectors
code-book vectors
mixture models and em  k 
image segmentation problem
illustration
use
data compression
original image
n pixels
r
g
b
bits
precision
whole image
bits
k-means
image data
original pixel intensity vectors
identity
vector µk
k such vectors
log2 k bits
pixel
k code book vectors µk
bits
total number
bits
image
+ n log2 k
integer
original image
figure
×
=
pixels
=
bits
comparison
compressed images
bits
k =
bits
k =
bits
k =
represent compression ratios
original image
%
%
%
trade-off
degree
compression
image quality
note
aim
example
k-means algorithm
good image compressor
small blocks
adjacent pixels
instance
correlations
natural images
nearby pixels
mixtures
gaussians
section
gaussian mixture model
simple linear super- position
gaussian components
class
density mod- els
single gaussian
formulation
gaussian mixtures
terms
discrete latent variables
insight
important distribution
expectation-maximization algorithm
recall
gaussian mixture distribution
linear superposition
gaussians
form p
x
πkn
x|µk
σk
k=1
k-dimensional binary random variable z
1-of-k repre- sentation
particular element zk
other elements
values
zk therefore satisfy zk ∈
k zk =
k possible states
vector z
element
joint distribution p
z
terms
marginal dis- tribution p
z
conditional distribution p
x|z
graphical model
figure
marginal distribution
z
terms
mixing coefﬁcients
p
zk =
= πk   k=1 k k k=1   k k k=1   figure
graphical representation
mixture model
joint distribution
form p
z
= p
z
p
x|z
mixtures
gaussians z
parameters
πk
�
πk
order
valid probabilities
z
1-of-k representation
distribution
form p
z
πzk k
conditional distribution
x
particular value
z
form p
x|zk =
= n
x|µk
σk
p
x|z
n
x|µk
σk
zk
exercise
joint distribution
p
z
p
x|z
marginal distribution
x
joint distribution
possible states
z
p
x
p
z
p
x|z
= z k=1 πkn
x|µk
σk
use
marginal distribution
x
gaussian mixture
form
several observations
marginal distribution
form p
x
= z p
z
observed data point xn
corresponding latent variable zn
equivalent formulation
gaussian mixture
explicit latent variable
joint distribution p
z
 j=1 k k j=1 
mixture models and em
marginal distribution p
x
simpliﬁca- tions
introduction
expectation-maximization
em
algorithm
quantity
important role
conditional probability
z
x
γ
zk
p
zk =
value
bayes ’ theorem γ
zk
≡ p
zk =
= p
zk =
p
x|zk =
p
zj =
p
x|zj =
πkn
σk
πjn
σj
=
section
πk
prior probability
zk =
quantity γ
zk
corresponding posterior probability
γ
zk
responsibility
component k
‘ explain- ing
observation x
technique
random samples
gaussian mixture model
z
marginal distribution p
z
value
z
value
x
conditional distribution p
x| z
techniques
standard distributions
chapter
samples
joint distribution p
z
points
values
x
value
z
other words
gaussian component
figure
similarly samples
marginal distribution p
x
samples
joint distribution
values
z
figure
b
x values
coloured labels
synthetic data set
‘ responsibilities
eval- uating
data point
posterior probability
component
mixture distribution
data set
value
responsibilities γ
znk
data point xn
corresponding point
proportions
green ink
γ
znk
k =
figure
c
instance
data point
γ
zn1
γ
zn2
= γ
zn3
equal proportions
green ink
cyan
figure
data points
true identity
component
maximum likelihood suppose
data set
observations
x1
data
mixture
gaussians
data set
n × d  n
 k 
mixtures
gaussians
b
c
figure
example
points
mixture
gaussians
figure
joint distribution p
z
p
x|z
states
z
components
mixture
blue
b
corresponding samples
marginal distribution p
x
values
z
x values
data set
whereas
b
c
same samples
colours
value
responsibilities γ
znk
data point xn
corresponding point
proportions
green ink
γ
znk
k =
matrix x
nth row
xt
similarly
corresponding latent variables
n × k matrix z
rows
data points
distribution
gaussian mixture model
i.i.d
data
graphical representation
figure
from
log
likelihood function
ln p
x|π
µ
σ
ln n=1 k=1 πkn
xn|µk
σk
function
signiﬁcant problem
maximum likelihood framework
gaussian mixture models
presence
singularities
sim- plicity
gaussian mixture
components
covariance matrices
σk = σ2 ki
unit matrix
conclusions
general covariance matrices
components
mixture model
jth component
mean µj
data figure
graphical representation
gaussian mixture model
set
n i.i.d
data points
xn
corresponding latent points
zn
n =
n. zn xn π µ σ n
mixture models and em figure
illustration
singularities
likelihood function arise
mixtures
gaussians
case
single gaus-
figure
singularities
p
x
points
µj = xn
value
n.
data point
term
likelihood function
form n
xn|xn
σ2 j
σj
limit σj →
term
inﬁnity
log likelihood function
inﬁnity
maximization
log likelihood function
well
problem
such singularities
gaussian components
collapses
speciﬁc data point
problem
case
single gaussian distribution
difference
note
single gaussian collapses
data point
multiplicative factors
likelihood function
other data points
factors
overall likelihood
inﬁnity
components
mixture
components
ﬁnite variance
assign ﬁnite probability
data points
other component
speciﬁc data point
additive value
log likelihood
figure
singularities
example
severe over-ﬁtting
maximum likelihood approach
difﬁculty
bayesian approach
moment
maximum likelihood
gaussian mixture models
steps
such pathological solutions
local maxima
likelihood function
singularities
suitable heuristics
instance
gaussian component
mean
value
covariance
large value
optimization
further issue
maximum likelihood solutions arises
fact
maximum likelihood solution
k-component mixture
total
k
equivalent solutions
k
ways
k sets
parameters
k components
other words
point
space
parameter values
further k
−1 additional points
rise
same distribution
problem
section
 n 
+
  n=1 n n
mixtures
gaussians
identiﬁability
casella
berger
important issue
parameter values
model
identiﬁability
models
continuous latent variables
chapter
purposes
good density model
equivalent solutions
log likelihood function
gaussian mixture model
complex problem
case
single gaussian
difﬁculty
presence
summation
k
logarithm
logarithm function
gaussian
derivatives
log likelihood
closed form solution
approach
gradient-based optimization techniques
fletcher
nocedal
wright
bishop
nabney
gradient-based techniques
important role
mixture density networks
chapter
alternative approach
em algorithm
broad applicability
foundations
discussion
variational inference techniques
chapter
em
gaussian
powerful method
maximum likelihood solutions
models
latent variables
expectation-maximization algorithm
em algorithm
dempster
mclachlan
krishnan
general treatment
em
em
variational inference framework
em algorithm
informal treatment
context
gaussian mixture model
em
broad applicability
context
variety
different models
book
conditions
maximum
likelihood function
derivatives
ln p
x|π
µ
σ
respect
means µk
gaussian components
= −
πkn
xn|µk
σk
j πjn
xn|µj
σj
γ
znk
σk
xn − µk
use
form
gaussian distribution
note
posterior probabilities
responsibilities
right-hand side
σ−1
µk =
nk γ
znk
nk = γ
znk
n=1
section
  
n n
 k 
mixture models and em
nk
effective number
points
k. note
form
solution
mean µk
kth gaussian component
weighted mean
points
data set
weighting factor
data point xn
posterior probability γ
znk
k
xn
derivative
ln p
x|π
µ
σ
respect
similar line
reasoning
use
result
maximum likelihood solution
covariance matrix
single gaussian
nk γ
znk
xn − µk
xn − µk
t
same form
corresponding result
single gaussian
data set
data point
corresponding poste- rior probability
denominator
effective number
points
corresponding component
ln p
x|π
µ
σ
respect
mixing coefﬁcients
account
constraint
coefﬁcients
lagrange multiplier
following quantity section
appendix e
p
x|π
µ
σ
λ πk −
k=1
= n
xn|µk
σk
j πjn
xn|µj
σj
λ
appearance
responsibilities
sides
πk
sum
k
use
constraint
λ = −n
λ
πk = nk n
mixing coefﬁcient
kth component
average respon- sibility
component
data points
results
stitute
closed-form solution
parameters
mixture model
responsibilities γ
znk
depend
parameters
complex way
results
simple iterative scheme
solution
maximum likelihood problem
instance
em algorithm
particular case
gaussian mixture model
initial values
means
covariances
coefﬁcients
updates
e step
mixtures
gaussians
−2 l
−2 −2
b
c
l
l
−2 l
−2
−2
−2 −2
d
e
f
figure
illustration
em algorithm
old faithful
illustration
k-means algorithm
figure
text
details
section
m step
reasons
expectation step
e step
current values
parameters
posterior probabilities
responsibilities
probabilities
maximization step
m step
means
covariances
mix- ing coefﬁcients
results
note
new means
new values
covariances
corresponding result
single gaussian distribution
update
parameters
e step
m step
log likelihood function
practice
algorithm
change
log likelihood function
parameters
threshold
em algorithm
mixture
gaussians
old faithful data set
figure
mixture
gaussians
centres
same values
k-means algorithm
figure
precision matrices
unit matrix
plot
data points
initial conﬁgura- tion
mixture model
standard-deviation contours
mixture models and em  j=1 gaussian components
blue
red circles
plot
b
result
initial e step
data point
proportion
blue ink
posterior probability
blue com- ponent
corresponding proportion
red ink
posterior probability
red component
points
signiﬁcant probability
cluster
purple
situation
ﬁrst m step
plot
c
mean
blue gaussian
mean
data set
probabilities
data point belonging
blue cluster
other words
centre
mass
blue ink
covariance
blue gaussian
covariance
blue ink
analogous results
red component
plots
d
e
f
results
complete cycles
em
plot
f
algorithm
convergence
note
em algorithm
iterations
approximate
convergence
k-means algorithm
cycle
computation
k-means algo- rithm
order
suitable initialization
gaussian mixture model
em
covariance matrices
sample covariances
clusters
k-means algorithm
mixing coefﬁcients
fractions
data points
respective clusters
gradient-based approaches
log like- lihood
techniques
singularities
likelihood function
gaussian component
particular data point
multiple local maxima
log likelihood function
em
maxima
em algorithm
gaussian mixtures plays
important role
em
gaussian mixtures given
gaussian mixture model
goal
likelihood function
respect
parameters
means
covariances
components
mixing coefﬁcients
means
covariances
coefﬁcients πk
initial value
log likelihood
e step
responsibilities
current parameter values γ
znk
= πkn
xn|µk
σk
πjn
σj
k
 
n n
    γ
znk
nk =
ln k=1 k n n  
t
alternative view
em
m step
parameters
current responsibilities
k =
nk σnew k πnew k =
nk = nk n
γ
znk
xn − µnew k
xn − µnew
log likelihood ln p
x|µ
σ
π
γ
znk
πkn
σk
convergence
parameters
log likelihood
convergence criterion
satisﬁed return
alternative view
em
section
complementary view
em algorithm
key role
latent variables
approach ﬁrst
abstract setting
illustration
case
gaussian mixtures
goal
em algorithm
maximum likelihood solutions
mod- els
latent variables
set
data
x
n
set
latent variables
z
nth row
corresponding row zt
set
model parameters
θ
log likelihood function
ln p
x|θ
ln p
x
z|θ
z
note
discussion
continuous latent variables
sum
z
key observation
summation
latent
logarithm
joint distribution p
x
z|θ
exponential  z
mixture models and em family
marginal distribution p
x|θ
result
sum- mation
presence
sum
logarithm
joint distribution
complicated expressions
maximum likelihood solution
observation
x
value
latent variable z
x
z
complete data set
data x
figure
likelihood function
complete data
form ln p
x
z|θ
maximization
complete-data log likelihood function
practice
complete data set
x
z
incomplete data x
state
knowledge
values
latent variables
z
posterior distribution p
z|x
θ
complete-data log likelihood
value
posterior distribution
latent variable
e step
em algorithm
subsequent m step
expectation
current estimate
parameters
pair
successive e
m steps
rise
estimate θnew
algorithm
value
parameters
use
expectation
motivation
choice
treatment
em
section
e step
current parameter values
posterior distribution
latent variables
p
z|x
θold
posterior distribution
expectation
complete-data log likelihood
general parameter value θ
expectation
q
θ
θold
q
θ
θold
p
z|x
θold
ln p
x
z|θ
m step
revised parameter estimate
function
= arg max q
θ
θold
θ
section
note
deﬁnition
q
θ
θold
logarithm
joint distribution p
x
z|θ
corresponding m-step maximization
sup- position
general em algorithm
property
cycle
em
incomplete-data log likelihood
local maximum
general em algorithm given
joint distribution p
x
z|θ
observed variables x
latent vari- ables z
parameters θ
goal
likelihood func- tion p
x|θ
respect
initial setting
parameters
alternative view
em
e step evaluate p
z|x
θold
m step evaluate θnew
θnew =
q
θ
θold
q
θ
θold
= z p
z|x
θold
ln p
x
z|θ
convergence
log likelihood
parameter values
convergence criterion
θold
θnew
em algorithm
map
maximum posterior
solutions
models
prior p
θ
parameters
case
e step
maximum likelihood case
whereas
m step
quantity
q
θ
θold
ln p
θ
suitable choices
prior
singularities
kind
figure
use
em algorithm
likelihood function
discrete latent variables
unobserved variables correspond
values
data set
distribution
observed values
joint distribution
variables
missing ones
em
corresponding likelihood function
example
application
technique
context
principal component analysis
figure
valid procedure
data values
random
mechanism
values
unobserved values
many situations
case
instance
sensor
value
quantity
exceeds
threshold
gaussian mixtures
application
latent variable view
em
spe- ciﬁc case
gaussian mixture model
goal
log likelihood function
observed data set x
difﬁcult
case
single gaussian distribution
presence
summation
k
logarithm
sup-
addition
data
x
values
corresponding discrete variables z
figure
‘ com- plete ’ data set
labels
component
data point
figure
b
corresponding ‘ incomplete ’ data set
graphical model
complete data
figure
 n k  n   k n
problem
likelihood
complete data set
x
z
from
likelihood function
form p
x
z|µ
σ
π
πznk k n
xn|µk
σk
znk n=1 k=1
znk
kth component
zn
logarithm
ln p
x
z|µ
σ
π
n=1 k=1 znk
ln πk + lnn
σk
comparison
log likelihood function
incomplete data shows
summation
k
logarithm
loga- rithm
gaussian distribution
member
exponential family
solution
maximum likelihood problem
maximization
respect
means
covariances
zn
k-dimensional vec- tor
elements
single element
value
complete-data log likelihood function
sum
k independent contribu- tions
mixture component
maximization
respect
mean
covariance
single gaussian
subset
data points
‘ assigned ’
component
maximization
respect
mixing coefﬁcients
different values
k
virtue
summation constraint
lagrange multiplier
result πk
n znk n=1
mixing coefﬁcients
fractions
data points
corresponding components
complete-data log likelihood function
closed form
practice
values
latent variables
expectation
respect
posterior distribution
latent variables
complete-data log likelihood
mixture models and em figure
same graph
figure
discrete variables zn
data variables xn
zn xn π µ σ n  k       
znk znj n n k
alternative view
em
using
bayes ’ theorem
posterior distribution
form p
z|x
µ
σ
π
[ πkn
xn|µk
σk
znk
k=1
factorizes
posterior distribution
zn
inspection
directed graph
figure
use
d-separation criterion
expected value
indicator variable znk
posterior distribution
e [ znk ] = znk [ πkn
xn|µk
σk
znk πjn
xn|µj
σj
znj = k πkn
xn|µk
σk
πjn
σj
γ
znk
responsibility
component k
data point xn
expected value
complete-data log likelihood function
ez [ ln p
x
z|µ
σ
π
= n=1 k=1 γ
znk
ln πk + lnn
σk
first
initial values
param- eters
responsibilities
e step
responsibilities ﬁxed
respect
σk
πk
m step
closed form solutions
µnew
σnew
πnew
em algorithm
gaussian mixtures
insight
role
complete-data log likelihood function
proof
convergence
em algorithm
section
relation
k-means comparison
k-means algorithm
em algorithm
gaussian mixtures
close similarity
k-means algorithm
hard assignment
data points
clusters
data point
cluster
em algorithm
soft assignment
posterior probabilities
fact
k-means algorithm
particular limit
em
gaussian mixtures
gaussian mixture model
covariance matrices
mixture components
variance parameter
exercise
section
exercise
    k  n
mixture models and em
components
identity matrix
p
σk
=
exp
x − µk2
em algorithm
mixture
k gaussians
form
parameter
posterior probabilities
responsibilities
particular data point xn
γ
znk
−xn − µk2/2
−xn − µj2/2 j πj exp
limit
denominator
term
xn − µj2
responsibilities γ
znk
data point
term j
responsi- bility γ
znj
unity
note
values
πk
none
πk
limit
hard assignment
data points
clusters
k-means algorithm
γ
znk
→ rnk
rnk
data point
cluster
mean
em re-estimation equation
µk
k-means result
note
re-estimation formula
mixing coefﬁcients
value
πk
fraction
data points
k
parameters
active role
algorithm
limit
complete-data log likelihood
ez [ ln p
x
z|µ
σ
π
→ −
rnkxn − µk2 + const
n=1 k=1 thus
limit
complete-data log likelihood
distortion measure j
k-means algorithm
note
k-means algorithm
covariances
clus- ters
cluster
hard-assignment version
gaussian mixture model
general covariance matrices
elliptical k-means algorithm
sung
poggio
mixtures
bernoulli distributions
chapter
distributions
continuous vari- ables
mixtures
gaussians
further example
mixture mod- elling
em algorithm
different context
mix- tures
discrete binary variables
bernoulli distributions
model
latent class analysis
lazarsfeld
henry
mclachlan
peel
practical importance
own right
discus- sion
bernoulli mixtures
foundation
consideration
hidden markov models
discrete variables
exercise
section
    i=1 i=1 d d n    k=1 k k   k=1 k k  
alternative view
em
consider
set
d binary variables
i
d
bernoulli distribution
parameter µi
x =
xd
t
µ =
µ1
µd
t.
individual variables xi
µ
mean
covariance
distribution
ﬁnite mixture
distributions
p
x|µ
µxi i
− µi
e [ x ] = µ cov [ x ] = diag
µi
− µi
p
π
πkp
x|µk
µ =
µ1
π =
π1
p
x|µk
µxi ki
− µki
mean
covariance
mixture distribution
e [ x ] = πkµk cov [ x ] = k=1 πk σk + µkµt k − e [ x ] e [ x ] t
σk = diag
µki
− µki
covariance matrix cov [ x ]
mixture distribution
correlations
vari- ables
single bernoulli distribution
data set x =
x1
log likelihood function
model
ln p
x|µ
π
ln n=1 k=1 πkp
xn|µk
appearance
summation
logarithm
maximum likelihood solution
form
em algorithm
likelihood function
mixture
bernoulli distributions
explicit latent  n           znk
k=1 znk znj n k k k k      i=1 d d
mixture models and em variable z
instance
x
case
gaussian mixture
z =
z1
zk
t
binary k-dimensional variable
single component
other components
conditional distribution
x
latent variable
prior distribution
latent variables
mixture
gaussians model
p
µ
p
x|µk
zk p
z|π
πzk k
exercise
product
p
µ
p
z|π
z
order
em algorithm
complete-data log likelihood function
ln p
x
z|µ
π
n=1 k=1 ln πk + [ xni
µki +
− xni
ln
− µki
]
x =
xn
z =
zn
expectation
complete-data log likelihood
respect
posterior distribution
latent variables
ez [ ln p
x
z|µ
π
= γ
znk
ln πk + i=1 [ xni
µki +
− xni
ln
− µki
]
γ
znk
e [ znk ]
posterior probability
responsibility
component k
data point xn
e step
responsibilities
bayes ’ theorem
form γ
znk
e [ znk ] = = znk [ πkp
xn|µk
znk πjp
xn|µj
znj k πkp
xn|µk
πjp
xn|µj
j=1
 n  n
alternative view
em
sum
n
responsibilities
terms
γ
znk
nk = xk =
nk γ
znk
xn
nk
effective number
data points
component k.
m step
expected complete-data log likelihood
respect
parameters
π
derivative
respect
terms
µk = xk
mean
component k
weighted mean
data
coefﬁcients
responsibilities
k
data points
maximization
respect
k πk
analogous lagrange multiplier
constraint steps
mixture
gaussians
πk = nk n
reasonable result
mixing coefﬁcient
com- ponent k
effective fraction
points
data set
component
note
contrast
mixture
gaussians
singularities
likelihood function
inﬁnity
likelihood function
� p
xn|µk
exist singularities
likelihood function
em
pathological starting point
em algorithm
value
likelihood function
local maximum
bernoulli mixture model
figure
handwritten digits
digit images
binary vectors
elements
values
elements
data set
n =
such digits
digits
’
’
’
mixture
k =
bernoulli distributions
iterations
em algorithm
mixing coefﬁcients
parameters
values
j µkj
range
constraint
mixture
bernoulli distributions
clusters
data set corresponding
different digits
conjugate
parameters
bernoulli distribution
beta distribution
beta
exercise
exercise
exercise
section
mixture models and em figure
illustration
bernoulli mixture model
top row
examples
digits data set
pixel values
grey scale
threshold
bottom row
ﬁrst
images
parameters
components
mixture model
comparison
same data set
single multivariate bernoulli distribution
maximum likelihood
counts
pixel
right-most image
bottom row
section
exercise
exercise
additional effective observations
x
priors
bernoulli mixture model
em
posterior probability distri- butions
analysis
bernoulli mixtures
case
multinomial binary variables
m
states
use
discrete dis- tribution
again
dirichlet priors
model parameters
em
bayesian linear regression
third example
application
em
evidence ap- proximation
bayesian linear regression
section
re- estimation equations
hyperparameters
evaluation
evidence
derivatives
expression
alternative approach
α
β
em algorithm
goal
evidence function p
t|α
β
respect
parameter vector w
latent variable
marginal likelihood function
em
e step
posterior distribution
w
current set- ting
parameters
β
expected complete-data log likelihood
m step
quantity
respect
β
posterior distribution
w
complete-data log likelihood function
ln p
t
w|α
β
ln p
t|w
β
ln p
w|α
     n   
m
alternative view
em
likelihood p
t|w
β
prior p
w|α
y
expectation
respect
posterior distribution
w
e [ ln p
t
w|α
β
= m
ln
e −
+ n
ln
− e
tn − wtφn
n=1
derivatives
respect
m step re-estimation equation α = m e [
] = m n mn + tr
sn
mt
analogous result
β
note
re-estimation equation
different form
corresponding result
direct evaluation
evidence function
computation
inversion
decomposition
m × m matrix
hence
comparable computational cost
iteration
approaches
α
course converge
same result
same local maximum
evidence function
ﬁrst
quantity γ
γ = m − α = m − αtr
sn
λi + α i=1 exercise
exercise
stationary point
evidence function
re-estimation equation
γ
αmt n mn = γ = m − αtr
sn
α
em re-estimation equation
ﬁnal example
related model
relevance vector machine
regression
section
direct max- imization
marginal likelihood
re-estimation equations
hyper- parameters
β
alternative approach
weight vector w
latent variable
em algorithm
e step
posterior distribution
weights
m step
expected complete-data log likelihood
exercise
expectation
respect
posterior distribution
‘ old ’ parameter values
new parameter values
respect
ew [ ln p
t|x
w
β
p
w|α
]
m2    z z n
expectation maximization algorithm
em algorithm
general technique
maximum likelihood solutions
probabilistic models
latent vari- ables
dempster
mclachlan
krishnan
general treatment
em algorithm
process
proof
em algorithm
sections
gaussian mixtures
likelihood function
csisz
ar
tusn
hath-
neal
hinton
discussion
basis
derivation
variational inference framework
probabilistic model
variables
x
hidden variables
z
joint distribution p
x
z|θ
set
parameters
goal
likelihood function
p
x|θ
p
x
z|θ
z
discussion
z com- prises continuous variables
combination
continuous variables
summation
integration
direct optimization
p
x|θ
opti- mization
complete-data likelihood function p
x
z|θ
distribution q
z
latent variables
serve
choice
q
z
following decomposition
ln p
x|θ
l
q
θ
kl
qp
l
q
θ
q
z
ln p
x
z|θ
q
z
p
z|x
θ
q
z
kl
qp
= − z q
z
ln
note
l
q
θ
appendix d
discussion
functionals
distribution q
z
function
parameters
mixture models and em αnew i
i
σii
βnew
= t − φmn2 + β−1 i γi
re-estimation equations
direct maxmization
em algorithm
general exercise
section
em algorithm
general
figure
illustration
decomposition
choice
distribution q
z
kullback-leibler divergence satisﬁes kl
qp
quan- tity l
q
θ
bound
log likelihood function ln p
x|θ
kl
q||p
l
q
θ
ln p
x|θ
exercise
section
forms
expressions
sign
l
q
θ
joint distribution
x
z
kl
qp
conditional distribution
z
x
decomposition
use
product rule
probability
ln p
x
z|θ
ln p
z|x
θ
ln p
x|θ
expression
l
q
θ
rise
terms
cancels kl
qp
required log likelihood ln p
x|θ
q
z
normalized distribution
from
kl
qp
kullback-leibler divergence
q
z
posterior distribution p
z|x
θ
kullback-leibler di- vergence satisﬁes kl
qp
equality
z
p
z|x
θ
l
q
θ
ln p
x|θ
other words
l
q
θ
bound
ln p
x|θ
decomposition
fig- ure
em algorithm
two-stage iterative optimization technique
maximum likelihood solutions
decomposition
em algorithm
log likelihood
current value
parameter vector
e step
bound l
q
θold
respect
z
θold ﬁxed
solution
maximization problem
value
ln p
q
z
value
l
q
kullback-leibler divergence vanishes
other words
q
z
posterior distribution p
z|x
θold
case
bound
log likelihood
figure
subsequent m step
distribution q
z
bound l
q
θ
respect
new value θnew
bound l
maximum
corresponding log likelihood function
distribution q
old parameter
new values
m step
new posterior distribution p
z|x
θnew
nonzero kl divergence
increase
log likelihood function
increase
bound
mixture models and em kl
q||p
figure
illustration
e step
em algorithm
q distribution
posterior distribution
current parameter val- ues
bound
same value
log like- lihood function
kl divergence vanishing
l
q
θold
ln p
x|θold
figure
z
p
z|x
θold
e step
bound
form l
q
θ
= z p
z|x
θold
ln p
x
z|θ
= q
θ
θold
+ const z p
z|x
θold
ln p
z|x
θold
constant
negative entropy
q distribution
θ
m step
quantity
expectation
complete-data log likelihood
case
mix- tures
gaussians
variable θ
logarithm
joint distribution p
z
x|θ
member
exponential family
product
such members
logarithm
lead
m step
maximization
corresponding incomplete-data log likelihood function p
x|θ
operation
em algorithm
space
parame- ters
figure
red curve
in- figure
illustration
m step
em algorithm
distribution q
z
bound l
q
θ
respect
parameter vector θ
value θnew
kl divergence
log likelihood ln p
x|θ
bound
kl
q||p
l
q
θnew
ln p
x|θnew
  z figure
em algorithm
bound
log likelihood
cur- rent parameter values
bound
new parameter values
text
full discussion
em algorithm
general
ln p
x|θ
l
q
θ
θnew exercise
complete data
likelihood function
value
initial parameter value θold
ﬁrst e step
poste- rior distribution
latent variables
rise
bound l
θ
θ
value
log likelihood
θ
blue curve
note
bound
tangential contact
log likelihood
θ
curves
same gradient
bound
convex function
unique maximum
mixture components
exponential family
m step
bound
value θ
value
log likeli- hood
subsequent e step
bound
θ
green curve
particular case
data set
x
n data points
z
n
latent variables
zn
n =
n.
independence assumption
p
zn
zn
x
p
x
z
n p
xn
sum
product rules
posterior probability
e step
form p
z|x
θ
p
x
z|θ
p
x
z|θ
= n=1 p
zn|xn
θ
p
zn|θ
p
zn|θ
z n=1
case
posterior distribution
respect
gaussian mixture model
responsibility
mixture components
particular data point xn
value
xn
parameters θ
mixture components
values
other data points
e
m steps
em algorithm
value
well-deﬁned bound
log likelihood function
  
n n  n
mixture models and em complete em cycle
model parameters
way
log likelihood
case
parameters
em algorithm
posterior distribution p
θ|x
models
prior p
θ
parameters
function
θ
θ|x
= p
x
/p
x
ln p
θ|x
ln p
x
ln p
x
use
decomposition
p
θ|x
l
q
θ
kl
qp
ln p
θ
ln p
x
l
q
θ
ln p
θ
ln p
x
ln p
x
right-hand side
respect
θ
optimization
respect
rise
same e- step equations
standard em algorithm
q
l
q
θ
m-step equations
introduction
prior term ln p
θ
small modiﬁcation
standard maximum likeli- hood m-step equations
em algorithm breaks
difﬁcult problem
likelihood function
stages
e step
m step
simpler
nevertheless
complex models
case
e step
m step
possible extensions
em algorithm
generalized em
gem
algorithm
problem
intractable m step
l
q
θ
respect
parameters
way
value
l
q
θ
bound
log likelihood function
complete em cycle
gem algorithm
value
log likelihood
parameters
local maximum
way
gem approach
nonlinear optimization strategies
conjugate gradients
m step
form
gem algorithm
expectation conditional maximization
ecm
algorithm
several constrained optimizations
m step
meng
rubin
instance
parameters
groups
m step
multiple steps
subset
remainder
ﬁxed
e step
em algorithm
optimization
l
q
θ
respect
z
neal
hinton
value
θ
unique maximum
l
q
θ
respect
z
posterior distribution qθ
z
p
z|x
θ
choice
q
z
bound l
q
θ
log likelihood function ln p
x|θ
algorithm
global maximum
l
q
θ
value
θ
global maximum
log likelihood ln p
x|θ
p
x
z|θ
continuous function
θ  
continuity
local maximum
l
q
θ
local maximum
ln p
x|θ
case
n independent data points x1
latent variables z1
zn
joint distribution p
x
z|θ
data points
structure
incremental form
em
em cycle
data point
time
e step
responsibilities
data points
responsibilities
data point
subsequent m step
computation
responsibilities
data points
mixture components
members
exponential family
responsibilities
simple sufﬁcient statistics
instance
case
gaussian mixture
update
data point m
corresponding
new values
responsibilities
γold
zmk
γnew
zmk
m step
required sufﬁcient statistics
instance
means
sufﬁcient statistics
exercise
exercises µnew k = µold k + γnew
zmk
− γold
zmk
n new k xm − µold
n new k = n old k + γnew
zmk
− γold
zmk
corresponding results
covariances
mixing coefﬁcients
e step
m step
ﬁxed time
total number
data points
parameters
data point
whole data set
incremental ver- sion
batch version
e
m step
incremental algorithm
value
l
q
θ
algorithm
maximum
l
q
θ
maximum
log likelihood function ln p
x|θ

k-means algorithm
section
consequence
ﬁnite number
possible assignments
set
discrete indicator variables
such assignment
unique optimum
µk
k-means algorithm
ﬁnite number
iterations

robbins-monro sequential estimation procedure
sec- tion
problem
roots
regression function
derivatives
j
respect
stochastic k-means algorithm
data point xn
prototype µk
  n k
mixture models and em

www
gaussian mixture model
marginal distribution p
z
latent variable
conditional distribution p
x|z
observed variable
show
marginal distribution p
x
p
z
p
x|z
possible values
z
gaussian mixture
form

suppose
em algorithm
posterior distri- bution
parameters
θ|x
model
latent variables
x
observed data set
e step
maximum likelihood case
whereas
m step
quantity
q
θ
θold
ln p
θ
q
θ
θold

directed graph
gaussian mixture model
figure
use
d-separation criterion
section
posterior distribution
latent
respect
different data points
p
z|x
µ
σ
π
p
zn|xn
µ
σ
π
n=1
 
special case
gaussian mixture model
covari- ance matrices
components
common value σ
derive
em equations
likelihood function
model

www verify
maximization
complete-data log likelihood
gaussian mixture model
result
means
covariances
component
group
data points
mixing coefﬁcients
fractions
points
group

www show
respect
responsibilities γ
znk
closed form solution

respect
πk
responsibilities γ
znk
closed form solutions
 
density model
mixture distribution p
x
πkp
x|k
k=1
vector x
parts
x =
xa
xb
show
conditional density p
xb|xa
mixture distribution
ﬁnd expressions
mixing coefﬁcients
component densities
 k=1 k 

section
relationship
k means
em
gaussian mixtures
mixture model
components
covariance
limit
complete- data log likelihood
model
distortion measure j
k-means algorithm

www
mixture distribution
form p
x
πkp
x|k
elements
x
combination
mean
covariance
p
x|k
µk
σk
mean
covariance
mixture distribution
 
re-estimation equations
em algorithm
mix- ture
bernoulli distributions
parameters
values
maximum
likelihood function
property
e [ x ] = n
n xn ≡ x. n=1
hence
parameters
model
compo- nents
same mean µk = µ
k =
k
em algorithm
iteration
choice
initial mixing coefﬁcients
solution
property µk = x
note
degenerate case
mixture model
components
practice
such solutions
appropriate initialization

joint distribution
latent
variables
bernoulli distribution
product
p
µ
p
z|π
joint distribution
respect
z

www show
expected complete-data log likelihood function
mixture
bernoulli distributions
respect
m step equation

expected complete-data log likelihood function
mixture
bernoulli distributions
respect
mixing coefﬁcients
lagrange multiplier
summation constraint
m step equation

www show
consequence
constraint
� p
xn|µk
discrete variable xn
incomplete-data log likelihood function
mixture
bernoulli distributions
singularities
likelihood
inﬁnity
    k=1 i=1 j=1 k p
x
πkp
x|µk
p
x|µk
= d m µxij kij
mixture models and em
 
bernoulli mixture model
section
prior distribution p
bk
parameter vectors
beta distribution
dirichlet prior p
π|α
em algorithm
posterior probability p
π|x
 
components i
multinomial variable
degree m
x
binary vector
components
i
d
j =
m
constraint
xij =
i
distribution
variables
mixture
discrete multinomial distributions
section
parameters
probabilities
xij =
� µkij
constraint j µkij
values
k
i
observed data
n =
n
e
m step equations
em algorithm
mixing coefﬁcients
component parameters
distribution
maximum likelihood

www show
maximization
complete-data log likelihood function
bayesian linear regression model
m step re- estimation result
α
 
evidence framework
section
m-step re-estimation equations
parameter β
bayesian linear regression model
result
α
 
maximization
complete-data log likelihood
m step equations
hyperpa- rameters
relevance vector machine
regression
 
www
section
direct maximization
marginal like- lihood
re-estimation equations
values
hyperparameters
regression rvm
section
em algorithm
same marginal likelihood
re-estimation equations
sets
re-estimation equations

relation
l
q
θ
kl
qp
exercises

www show
bound l
q
θ
q
z
p
z|x
θ
same gradient
respect
log likelihood function ln p
x|θ
point θ = θ

incremental form
em algorithm
mixture
gaussians
responsibilities
speciﬁc data point xm
m-step formulae
results
component
 
derive m-step formulae
covariance matrices
coefﬁcients
gaussian mixture model
responsibilities
result
means
approximate inference a central task
application
probabilistic models
evaluation
pos- terior distribution p
z|x
latent
z
data variables x
evaluation
expectations
respect
dis- tribution
model
deterministic parameters
implicit
moment
bayesian model
unknown parameters
prior distributions
set
latent variables
vector z
instance
em algorithm
expectation
complete-data log likelihood
respect
posterior distribution
latent variables
many models
practical interest
posterior distribution
expec- tations
respect
distribution
dimensionality
latent space
posterior distribution
complex form
expectations
case
continuous variables
required integrations
approximate inference analytical solutions
dimensionality
space
complexity
integrand
numerical integration
discrete variables
marginal- izations
possible conﬁgurations
hidden variables
principle
practice
many hidden states
exact calculation
such situations
approximation schemes
fall
classes
determin- istic approximations
stochastic techniques
markov chain monte carlo
de- scribed
chapter
widespread use
bayesian methods
many domains
property
inﬁnite computational resource
exact results
approximation
use
ﬁnite amount
processor time
practice
methods
use
small-scale problems
sampling scheme
independent samples
required distribution
chapter
range
deterministic approximation schemes
scale well
large applications
analytical ap- proximations
posterior distribution
example
particular way
speciﬁc parametric form
exact results
strengths
weaknesses
methods
section
laplace approximation
local gaussian approximation
mode
distribution
family
approximation techniques
variational inference
vari- ational bayes
global criteria
brief introduction
alternative variational framework
expectation propagation
variational inference variational methods
origins
18th century
work
euler
lagrange
others
calculus
variations
standard calculus
derivatives
functions
function
mapping
value
input
value
function
output
derivative
function
output value
inﬁnitesimal changes
input value
mapping
function
input
value
output
example
entropy h [ p ]
probability distribution p
input
quantity h [ p ] = p
x
p
x
dx

variational inference
output
concept
functional derivative
value
functional changes
response
inﬁnitesimal changes
input function
feynman
rules
calculus
variations
standard calculus
appendix d. many problems
terms
optimization problem
quantity
solution
possible input functions
one
minimizes
variational methods
broad applicability
such areas
ﬁnite element methods
kapur
maximum entropy
schwarz
nothing
variational methods
approximate solutions
range
functions
optimization
instance
quadratic functions
functions com-
linear combination
ﬁxed basis functions
coefﬁcients
linear combination
case
applications
probabilistic in- ference
restriction
example
form
factorization assumptions
jordan
jaakkola
detail
concept
variational optimization
inference problem
suppose
bayesian model
parameters
prior distributions
model
latent variables
parameters
set
latent variables
parameters
z
set
variables
x
example
set
n
data
x =
x1
z =
z1
zn
probabilistic model
joint distribution p
x
z
goal
approximation
posterior distribution p
z|x
model evidence p
x
discussion
em
log marginal probability
ln p
x
l
q
+ kl
qp
l
q
= q
z
ln kl
qp
− q
z
ln p
x
z
q
z
p
z|x
q
z
dz dz
differs
discussion
em
parameter vector
parameters
stochastic variables
z
chapter
continuous variables
integrations
summations
decomposition
analysis
variables
integrations
summations
bound l
q
optimization
respect
distribution q
z
kl divergence
possible choice
q
z
maximum
bound
kl diver- gence vanishes
q
z
posterior distribution p
z|x
approximate inference
−2
−2 figure
illustration
variational approximation
example
figure
left-hand plot
original distribution
laplace
approx- imations
right-hand plot
negative logarithms
corresponding curves
 m
model
true posterior distribution
restricted family
distributions
z
member
family
kl divergence
goal
family
tractable distributions
same time
family
good approximation
true posterior distribution
restriction
tractability
sub- ject
requirement
family
distributions
‘ over-ﬁtting ’
ﬂexible dis- tributions
ﬂexible approximations
true posterior distribution
way
family
distributions
paramet- ric distribution q
z|ω
set
parameters ω
bound l
q
function
ω
standard nonlinear optimization techniques
optimal values
parameters
example
approach
variational distribution
respect
mean
variance
figure
factorized distributions
alternative way
family
distri- butions q
z
suppose
elements
z
disjoint groups
zi
i
m.
q distribution
respect
groups
q
z
qi
zi
i=1
       qj qi i
      i
 i=j
variational inference
further assumptions
distri- bution
restriction
functional forms
individual factors
zi
factorized form
variational inference corresponds
ap- proximation framework
physics
mean ﬁeld theory
parisi
amongst
distributions
z
form
distri- bution
bound l
q
free form
optimization
l
q
respect
distributions
zi
respect
factors
turn
dependence
factors
zj
qj
zj
notation
l
q
= ln p
x
z
ln qi dz ln p
x
z
dzi qj ln qj dzj + const dzj − i=j p
x
zj
dzj −
new distribution qj ln = qj ln qj dzj + const
p
x
zj
relation ln p
x
zj
ei=j [ ln p
x
z
+ const
notation ei=j [ ··· ]
expectation
respect
q distributions
variables
i = j
ei=j [ ln p
x
z
= ln p
x
z
qi dzi
qi=j
ﬁxed
l
q
re- spect
possible forms
distribution qj
zj
rec- ognizing
negative kullback-leibler divergence
qj
zj
p
x
zj
kullback-leibler leonhard euler
euler
swiss mathematician
physicist
st. petersburg
berlin
mathematicians
time
collected works
volumes
amongst
many contributions
modern theory
function
lagrange
calculus
variations
formula eiπ = −1
important numbers
mathematics
years
life
half
results
period
    
approximate inference divergence
minimum
qj
zj
general expression
optimal solution q j
zj
j
zj
ei=j [ ln p
x
z
+ const
ln q p
x
zj
few moments
form
solution
basis
applications
variational methods
log
optimal so- lution
factor qj
log
joint distribution
visible variables
expectation
respect
other factors
qi
i = j
exponential
sides
normalize
additive constant
distribution q j
zj
j
zj
q exp
ei=j [ ln p
x
z
]
exp
ei=j [ ln p
x
z
]
dzj
practice
form
normalization constant
inspection
subsequent examples
set
equations
j =
m
set
con- sistency conditions
maximum
bound subject
factorization constraint
explicit solution
expres- j
zj
expectations sion
right-hand side
optimum q
respect
other factors
zi
i = j
consistent solution
ﬁrst
factors
zi
factors
turn
revised estimate
right-hand side
current estimates
other factors
convergence
bound
respect
factors
zi
boyd
vandenberghe
properties
approximations
approach
variational inference
factorized approximation
true posterior distribution
moment
problem
general distribution
factorized distribution
problem
gaussian distribution
factorized gaussian
useful insight
types
inaccuracy
factorized approximations
gaussian distribution p
z
= n
z|µ
λ−1
correlated variables
=
z1
z2
mean
precision
elements
= µ1 µ2
λ = λ11 λ12 λ21 λ22
= λ12
symmetry
precision matrix
distribution
factorized gaussian
form q
z
= q1
z1
q2
z2
general result
expression
z1
right-hand side
optimal factor
terms
functional dependence
z1
other terms
normalization constant
z1
ez2 [ ln p
z
+ const
variational inference
− µ1
−
z1 − µ1
λ12
z2 − µ2
ez2 −
+ z1µ1λ11 − z1λ12
e [ z2 ] − µ2
+ const
z2 = − + const
right-hand side
expression
quadratic function
z1
q
z1
gaussian distribution
q
zi
result
variational optimization
kl divergence
possible distributions
zi
note
additive constant
normalization constant
end
inspection
technique
square
mean
precision
gaussian
section
q
z1
= n
z1|m1
λ−1
symmetry
λ12
e [ z2 ] − µ2
z2
m1 = µ1 −
z2
= n
z2|m2
λ−1
q
m2 = µ2 −
λ21
e [ z1 ] − µ1
note
solutions
q
z1
expectations com-
respect
z2
vice versa
variational solutions
re-estimation equations
variables
turn
convergence criterion
example
problem
closed form solution
e [ z1 ] = m1
e [ z2 ] = m2
equations
e [ z1 ] = µ1
e [ z2 ] = µ2
only solution
dis- tribution
result
figure
mean
variance
q
z
direction
variance
p
z
variance
orthogonal direction
general result
factorized variational ap- proximation
approximations
posterior distribution
way
comparison
reverse kullback-leibler divergence kl
pq
form
kl divergence exercise
approximate inference
z2
figure
comparison
alternative forms
kullback-leibler divergence
green contours
standard deviations
correlated gaussian distribution p
z
variables
z2
red contours
corresponding levels
approximating q
z
same variables
product
independent univariate gaussian distributions
parameters
minimization
kullback- leibler divergence kl
qp
reverse kullback-leibler
b
divergence kl
pq
distribution
    i=1 z1 m i=j
z2
b
z1
section
exercise
alternative approximate inference framework
expectation prop- agation
general problem
kl
pq
q
z
factorized approximation
form
kl divergence
form kl
pq
− p
z
ln qi
zi
dz + const
constant term
entropy
p
z
q
z
respect
factors
zj
lagrange multiplier
j
zj
q p
z
dzi = p
zj
case
optimal solution
qj
zj
corre- sponding marginal distribution
p
z
note
closed-form solution
iteration
result
illustrative example
gaussian distribution p
z
vector
result
figure
b
mean
approximation
signiﬁcant probability mass
regions
variable space
low probabil- ity
difference
results
large positive contribution
kullback-leibler divergence kl
qp
− q
z
ln p
z
q
z
dz
variational inference
b
figure
comparison
alternative forms
kullback-leibler divergence
blue contours
bimodal distribution p
z
mixture
gaussians
red contours correspond
single gaussian distribution q
z
approximates
z
sense
kullback- leibler divergence kl
pq
b
red contours correspond
gaussian distribution q
z
numerical minimization
kullback-leibler divergence kl
qp
c
b
different local minimum
kullback-leibler divergence
regions
z space
p
z
near zero
q
z
form
kl divergence
distributions
z
avoid regions
p
z
kullback-leibler divergence kl
pq
distributions q
z
regions
p
z
further insight
different behaviour
kl diver- gences
multimodal distribution
figure
practical applications
true posterior distri- bution
posterior mass
number
small regions
parameter space
multiple modes
nonidentiﬁability
latent space
complex nonlin- ear dependence
parameters
types
multimodality
chapter
context
gaussian mixtures
multiple maxima
likelihood function
variational treatment
minimization
kl
qp
modes
contrast
kl
pq
resulting approximations
modes
context
mixture model
poor predictive distributions
average
good parameter values
good parameter value
use
kl
pq
useful inference procedure
different approach
one
detail
expectation propagation
forms
kullback-leibler divergence
members
alpha family section
        n

approximate inference
divergences
ali
silvey
amari
minka
dα
pq
−
− p
x
/2q
x
/2 dx
−∞ < α < ∞
continuous parameter
kullback-leibler divergence kl
pq
limit α →
whereas kl
qp
limit α → −1
values
α
dα
pq
equality
x
q
x
suppose p
x
ﬁxed distribution
dα
pq
respect
set
distributions
x
α �
divergence
forcing
values
x
p
x
q
x
x
support
p
x
mode
mass
α �
divergence
values
x
p
x
q
x
x
p
x
support
p
x
symmetric divergence
hellinger distance
dh
pq
= p
x
− q
x
dx
square root
hellinger distance
valid distance
example
univariate gaussian
factorized variational approximation
gaussian dis- tribution
single variable x
mackay
goal
posterior distribution
mean µ
precision τ
data set d =
x1
observed values
x
gaus- sian
likelihood function
p
d|µ
τ
n/2 τ
exp
−
xn − µ
conjugate prior distributions
µ
µ|µ0
λ0τ
−1 p
µ|τ
n p
τ
gam
τ|a0
b0
gam
τ|a0
b0
gamma distribution
distributions
gaussian-gamma conjugate prior distribution
simple problem
posterior distribution
form
gaussian-gamma distribution
tutorial purposes
factorized variational approximation
posterior distribution
q
τ
qµ
µ
qτ
τ
exercise
section
exercise
 n=1   
n n
variational inference
note
true posterior distribution
way
optimum factors
µ
τ
general result
qµ
q µ
µ
eτ [ ln p
d|µ
τ
ln p
µ|τ
const
xn − µ
λ0
µ − µ0
+ = − e [ τ
n + const
exercise
square
µ
qµ
µ
gaussian n mean
precision
µ|µn
λ−1 n
µn = λ0µ0 + n x λ0 + n λn =
λ0 + n
e [ τ ]
note
n → ∞
maximum likelihood result
µn = x
precision
optimal solution
factor qτ
τ
ln q τ
τ
eµ [ ln p
d|µ
τ
ln p
µ|τ
+ ln p
τ
const =
a0 −
ln τ − b0τ + n
ln τ
eµ −
xn − µ
+ λ0
µ − µ0
+ const
hence qτ
τ
gamma distribution gam
τ|an
bn
parameters
= a0 + n
bn = b0 +
xn − µ
+ λ0
µ − µ0
n=1
exercise
section
again
expected behaviour
n → ∞
speciﬁc functional forms
optimal distributions
µ
τ
structure
likelihood function
corresponding conjugate priors
expressions
optimal distributions
µ
τ
moments
respect
other distribution
ap- proach
solution
initial guess
moment e [ τ ]
distribution qµ
µ
distri- bution
required moments e [ µ ]
e [ µ2 ]
distribution qτ
τ
space
hidden variables
example
variational approxima- tion
posterior distribution
contours
true posterior
factorized approximation
figure
approximate inference
c
b
d
figure
illustration
variational inference
mean µ
precision τ
univariate gaussian distribu- tion
contours
true posterior distribution p
contours
initial factorized approximation qµ
µ
qτ
τ
blue
b
factor qµ
µ
c
factor qτ
τ
d
contours
optimal factorized approximation
iterative scheme converges
iterative approach
order
optimal factorized posterior distribution
simple example
explicit solution
simultaneous equations
optimal factors
µ
τ
expressions
noninformative priors
µ0 =
= b0 = λ0
parameter settings correspond
improper priors
posterior distribution
standard result e [ τ ] =
/bn
mean
gamma distribution
appendix b n
e [ τ ] = e
n
xn − µ
= x2 −
[ µ ] + e [ µ2 ]
n=1
ﬁrst
second order moments
 n
 
e [ τ ] m z =
exercise
section
exercise
exercise
variational inference
n
n
x2 − x2
xn − x
qµ
µ
form e [ µ ] = x
e [ µ2 ] = x2
ne [ τ ]
moments
e [ τ ]
right-hand side
familiar unbiased estimator
variance
univariate gaussian distribution
use
bayesian ap- proach
bias
maximum likelihood solution
model comparison
inference
hidden variables z
set
candidate models
index m
prior probabilities
m
goal
posterior probabilities
m|x
x
observed data
complex situation
different models
different structure
different dimensionality
hidden variables z
fore
factorized approximation q
z
q
m
nize
posterior
z
m
q
z
m
= q
z|m
q
m
decomposition
variational distribution ln p
x
lm − q
z|m
q
m
ln p
z
m|x
q
z|m
q
m
lm
bound
ln p
x
lm = m z q
z|m
q
m
ln p
z
x
m
q
z|m
q
m
discrete z
same analysis applies
continuous latent variables
summations
integrations
lm
respect
distribution q
m
lagrange multiplier
result
respect
q
z|m
solutions
different m
m.
q
z|m
optimization q
m
∝ p
m
lm
approximate inference   n k  k  n 
k
section
q
m
nor- malization
values
q
m
model selection
model
usual way
illustration
variational mixture
gaussians
discussion
gaussian mixture model
vari- ational inference machinery
previous section
good illustration
application
variational methods
bayesian treatment
difﬁculties
maximum likelihood approach
attias
reader
example
detail
many insights
practical appli- cation
variational methods
many bayesian models
sophisticated distributions
straightforward extensions
general- izations
analysis
point
likelihood function
gaussian mixture model
graphical model
figure
observation
corresponding latent variable zn
1-of-k binary vector
ele- ments znk
k =
k.
observed data
x =
x1
latent variables
z =
z1
zn
from
conditional distribution
z
mixing coefﬁcients
form p
z|π
πznk k n=1 k=1
conditional distribution
data vectors
latent variables
component parameters
x|z
µ
λ
n
k=1 xn|µk
λ−1 k znk
µ =
µk
λ =
λk
terms
precision matrices
covariance matrices
mathemat- ics
priors
parameters
λ
π
analysis
conjugate prior distributions
dirichlet distribution
mixing coefﬁcients
p
π
dir
π|α0
c
α0
k k=1
symmetry
same parameter α0
compo- nents
c
α0
normalization constant
dirichlet distribution
 k
illustration
variational mixture
gaussians figure
directed acyclic graph
bayesian mix- ture
gaussians model
box
plate
de- notes
set
n i.i.d
observations
µ denotes
µk
λ denotes
λk
λ µ zn xn n section
section
b.23
parameter α0
effective prior number
observations
component
mixture
value
α0
posterior distribution
data
independent gaussian-wishart
mean
precision
gaussian component
p
λ
p
µ|λ
p
λ
k=1 n µk|m0
β0λk
−1 w
λk|w0
ν0
conjugate prior distribution
mean
pre- cision
m0 =
symmetry
model
directed graph
fig- ure
note
link
λ
variance
distribution
µ
function
λ
example
nice illustration
distinction
latent vari- ables
parameters
variables
zn
plate
latent variables
number
such variables grows
size
data set
contrast
variables
µ
plate
number
size
data set
parameters
perspective
graphical models
fundamental difference
variational distribution
order
variational treatment
model
joint distribution
random variables
p
x
z
π
µ
λ
p
x|z
µ
λ
p
z|π
p
π
p
µ|λ
p
λ
various factors
reader
moment
decomposition
probabilistic graphical model
figure
note
variables x =
x1
 n    
k n n   k k
variational distribution
latent variables
parameters
q
z
π
µ
λ
q
z
q
µ
λ
only assumption
order
tractable practical solution
bayesian mixture model
functional form
factors
z
π
µ
λ
optimization
variational distribution
note
sub- scripts
q distributions
p distributions
arguments
different distributions
corresponding sequential update equations
factors
use
general result
derivation
update equation
factor q
z
log
optimized factor
ln q
z
= eπ
µ
λ [ ln p
x
z
π
µ
λ
+ const
use
decomposition
note
functional dependence
right-hand side
variable z
thus
terms
z
additive normalization constant
ln q
z
eπ [ ln p
z|π
+ eµ
λ [ ln p
x|z
µ
λ
+ const
conditional distributions
right-hand side
terms
z
additive constant
q
z
=
znk ln ρnk + const ln ρnk = e [
πk ] + d
[ ln|λk| ] −
xn − µk
tλk
xn − µk
ln
−
d
dimensionality
data variable x
exponential
sides
q
z
ρznk nk
exercise
distribution
value
quantities
sum
values
k
q
z
rznk nk n=1 k=1
   k=1 j=1 k k  
n
n n=1  
nk
nk n n  k
illustration
variational mixture
gaussians
rnk = ρnk ρnj k
optimal solution
factor q
z
same functional form
prior p
z|π
note
ρnk
exponential
real quantity
quantities
discrete distribution q
z
standard result e [ znk ] = rnk
quantities
role
responsibilities
note
optimal solution
q
z
moments
respect
distributions
other variables
variational update equations
point
statistics
observed data
respect
responsibilities
nk = xk = rnkxn sk = rnk
xn − xk
xn − xk
t. note
quantities
maximum likelihood em algorithm
gaussian mixture model
factor q
µ
λ
variational posterior distribu- tion
again
general result
q
µ
λ
ln p
π
ln p
λk
ez [ ln p
z|π
+ k=1 n=1 e [ znk ] lnn xn|µk
λ−1 k + const
right-hand side
expression
sum
terms
terms
µ
λ
variational posterior q
µ
λ
q
π
q
λ
further-
terms
µ
λ
sum
k
terms
µk
further factorization q
µ
λ
q
π
q
λk
k=1   k=1 k d i=1   
k n
α
  
approximate inference
terms
right-hand side
π
q
π
=
α0 −
ln πk + rnk ln πk + const
exponential
sides
q
π
dirichlet distribution q
π
dir
π|α
α
components
αk = α0 + nk
variational posterior distribution q
product
marginals
product rule
form q
λk
q
µk|λk
q
λk
factors
terms
µk
λk
result
gaussian-wishart distribution
µk|mk
βkλk
−1 q
λk
n w
λk|wk
νk
βk = β0 + nk
βk = w−1 w−1 k νk = ν0 + nk
β0m0 + nkxk
+ nksk + β0nk β0 + nk
xk − m0
xk − m0
t
update equations
m-step equations
em algorithm
maximum likelihood solution
mixture
gaussians
computations
order
variational posterior distribution
model parameters
evaluation
same sums
data set
arose
maximum likelihood treatment
order
variational m step
expectations e [ znk ] = rnk
responsibilities
ρnk
expression
expectations
respect
variational distributions
parameters
eµk
λk
xn − µk
tλk
xn − µk
dβ−1 k + νk
xn − mk
twk
xn − mk
− i
+ d
+ ln|wk|
ln λk ≡ e [
] =
πk ≡ e [
πk ] = ψ
αk
ψ
exercise
exercise
      appendix b section
section
exercise
illustration
variational mixture
gaussians
deﬁnitions
b.25
standard properties
wishart
dirichlet distributions
πk
ψ
·
digamma function k αk
results
λk
α =
use
following result
responsibilities
∝ πk λ1/2 k exp − d
−
xn − mk
twk
xn − mk
notice
similarity
corresponding result
responsibilities
maximum likelihood em
form rnk ∝ πk|λk|1/2
−
xn − µk
tλk
xn − µk
precision
place
covariance
similarity
optimization
variational posterior distribution
stages
e
m steps
maximum likelihood em algorithm
variational equivalent
e step
current distributions
model parameters
moments
evaluate e [ znk ] = rnk
subsequent variational equivalent
m step
responsibilities
variational distribution
parameters
case
variational posterior distribution
same functional form
corresponding factor
joint distribution
general result
consequence
choice
conjugate distributions
figure
results
approach
old faith- ful data set
gaussian mixture model
k =
components
convergence
components
expected values
mixing coefﬁcients
prior values
effect
terms
automatic trade-off
bayesian model
data
complexity
model
complexity penalty
components
parameters
prior values
components
responsibility
data points
nk
from
αk  α0
–
other parameters
prior values
principle such components
data points
broad priors
effect
varia- tional gaussian mixture
expected values
mixing coefﬁcients
posterior distribution
e [ πk ] = αk + nk kα0 + n
component
nk
αk  α0
prior
α0
e [ πk ] →
component
role
model
approximate inference figure
variational bayesian mixture
k =
gaussians
old faithful data set
ellipses
standard-deviation density contours
components
density
red ink
ellipse
mean value
mixing coefﬁcient
com- ponent
number
top left
diagram
num- ber
iterations
variational infer- ence
components
expected mixing coefﬁcient
mixing coefﬁcients
α0 → ∞
e [ πk ]
figure
prior
mixing coefﬁcients
dirichlet
form
figure
α0 <
prior favours solutions
mixing coefﬁcients
figure
components
nonzero mixing coefﬁcients
components
coefﬁcients
α =
components
mixing coefﬁcients
close similarity
variational solution
bayesian mixture
gaussians
em algorithm
maximum likelihood
fact
limit n → ∞
bayesian treatment converges
maximum likelihood em algorithm
anything
small data sets
dominant computational cost
variational algorithm
gaussian mixtures
evaluation
responsibilities
evaluation
inversion
weighted data covariance matrices
computations
maximum likelihood em algorithm
little computational overhead
bayesian approach
tradi- tional maximum
substantial advantages
first
singularities
maximum likelihood
gaussian com- ponent ‘
’
speciﬁc data point
bayesian treatment
      k=1 k n k  k  section
exercise
illustration
variational mixture
gaussians
singularities
map estimate
maximum likelihood
large number k
components
mixture
fig- ure
variational treatment
possibility
optimal number
components
mixture
techniques
cross validation
variational
bound
model
practice
bound
re-estimation
order
convergence
valuable check
math- ematical expressions
solutions
software implementation
step
iterative re-estimation procedure
value
bound
stage
test
correctness
mathematical derivation
update equations
software im- plementation
ﬁnite differences
update
maximum
bound
svens´en
bishop
variational mixture
gaussians
bound
l = q
z
π
µ
λ
ln p
x
z
π
µ
λ
dπ dµ dλ z q
z
π
µ
λ
e [ ln p
x
z
π
µ
λ
− e [ ln q
z
π
µ
λ
= e [ ln p
x|z
µ
λ
+ e [ ln p
z|π
+ e [ ln p
π
+ e [ ln p
λ
−e [ ln q
z
− e [ ln q
π
− e [ ln q
λ
]
notation
 superscript
q distributions
subscripts
expectation operators
expectation
respect
random variables
argument
various terms
bound
following results e [ ln p
x|z
µ
λ
= nk ln λk − dβ−1 k − νktr
skwk
−νk
xk − mk
twk
xk − mk
d ln
e [ ln p
z|π
= n=1 k=1 rnk ln πk e [ ln p
π
= ln c
α0
+
α0 −
ln k=1 πk
 λk − dβ0 βk   k=1 k       
ln k=1 k=1 k=1 k k    
k k n b z    
approximate inference e [ ln p
λ
= d ln
β0/2π
−β0νk
mk − m0
twk
mk − m0
ν0 − d −
ln k + λk
νktr
w−1
wk
k ln b
w0
ν0
e [ ln q
z
= rnk ln rnk e [ ln q
π
=
αk −
ln πk +
c
α
e [ ln q
λ
= λk + d
ln
d
− h [ q
λk
] −
d
dimensionality
x
h [ q
λk
]
entropy
wishart distribu- tion
b.82
coefﬁcients c
α
b
w
ν
b.23
b.79
note
terms
expectations
logs
q distributions
negative entropies
distributions
simpliﬁcations
combination
terms
expressions
bound
expressions sepa- rate
ease
bound
alternative approach
variational re-estimation equations
section
fact
model
conjugate priors
functional form
factors
variational posterior distribution
z
dirichlet
π
gaussian-wishart
µk
λk
general parametric forms
distributions
form
bound
function
parameters
distributions
bound
respect
parameters
required re-estimation equations
predictive density
applications
bayesian mixture
gaussians model
x
as- z
predictive density
new value
observation
corresponding latent variable dictive density
p
x|x
= z
µ
λ
p
p
x| z|π
p
µ
λ|x
dπ dµ dλ
exercise
         k=1 k=1 k k  
k α
illustration
variational mixture
gaussians
p
µ
λ|x
true posterior distribution
parameters
using
summation
z
p
x|x
πkn x|µk
λ−1 k p
µ
λ|x
dπ dµ dλ
integrations
predictive density
true posterior distribution p
µ
λ|x
variational approximation q
π
q
λ
p
x|x
πkn x|µk
λ−1 k q
π
q
λk
dπ dµk dλk
exercise
use
factorization
term
variables
µj
λj
j =
integrations
mixture
student ’
t-distributions p
x|x
αkst
x|mk
lk
νk
− d
kth component
mk
precision
lk =
νk
− d
βk
+ βk
wk
exercise
νk
size n
data set
predictive distribution
reduces
mixture
gaussians
section
exercise
number
components
bound
pos- terior distribution
number k
components
mixture model
subtlety
setting
parameters
gaussian mixture model
speciﬁc degenerate settings
other parameter settings
density
observed vari- ables
parameter values
re-labelling
components
instance
mixture
gaussians
single ob-
variable x
parameters
values
π2 = b
µ1 = c
µ2 = d
σ1 = e
σ2 = f.
parameter values
= b
π2
µ1 = d
µ2 = c
σ1 = f
σ2 = e
components
symmetry
rise
same value
p
x
mixture model com- prising k components
parameter setting
member
family
k
equivalent settings
context
maximum likelihood
redundancy
parameter optimization algorithm
example em
initial- ization
parameters
speciﬁc solution
other equivalent solu- tions
role
bayesian setting
approximate inference figure
plot
bound l
number k
com- ponents
gaussian mixture model
old faithful data
distinct peak
k =
components
value
k
model
different random starts
results
‘ + ’ symbols
small random hori- zontal perturbations
note
solutions
suboptimal local maxima
hap- pens
p
d|k
n
k parameter values
figure
true posterior distribution
variational inference
minimization
kl
qp
distribution
neighbourhood
modes
others
equivalent modes
equivalent predictive densities
concern
model
speciﬁc number k
components
different values
k
account
multimodality
simple approximate solution
term ln k
bound
model comparison
figure
plot
bound
multimodality fac- tor
number k
components
old faithful data set
maximum likelihood
values
likeli- hood function
increase
k
singular solutions
effects
local maxima
appropriate model complexity
contrast
bayesian inference
trade-off
model complexity
data
approach
determination
k
range
models
different k values
alternative approach
suitable value
k
mixing coefﬁcients
parameters
point estimates
values
bound
corduneanu
bishop
respect
probability distribution
bayesian approach
re-estimation equation exercise
section
exercise
πk
n rnk n=1
maximization
variational updates
q distribution
parameters
components
insufﬁcient contribution section
illustration
variational mixture
data
mixing coefﬁcients
optimization
model
automatic relevance determination
single training run
large initial value
k
surplus components
model
origins
sparsity
respect
hyperparameters
detail
context
relevance vector machine
induced factorizations
variational update equations
gaussian mixture model
particular factorization
variational posterior distribution
optimal solutions
various factors
additional factorizations
solution
q
λ
product
independent distribution q
λk
components k
mixture
variational posterior distribution q
z
latent variables
independent distribution q
zn
observation
further factorize
respect
value
n
znk
k
additional factorizations
consequence
interaction
assumed factorization
conditional independence properties
true distribution
directed graph
figure
additional factorizations
induced factorizations
cause
interaction
factorization
varia- tional posterior distribution
conditional independence properties
true joint distribution
numerical implementation
variational approach
account
such additional factorizations
instance
full precision matrix
gaussian distribution
set
variables
optimal form
distribution
diago- nal precision matrix
factorization
respect
individual variables
gaussian
such induced factorizations
simple graphical test
d-separation
latent variables
disjoint groups a
b
c
factorization
c
latent variables
q
a
b
c
q
a
b
q
c
general result
product rule
probabilities
optimal solution
q
a
b
ln q
a
b
ec [ ln p
x
a
b
c
const = ec [ ln p
a
b|x
c
+ const
solution
a
b
other words
q
a
b
= q
q
b
p
a
b|x
c
ln p
a|x
c
ln p
b|x
c
conditional inde- pendence relation
⊥⊥ b | x
c
approximate inference
relation
choice
a
b
use
d-separation criterion
bayesian mixture
gaussians
directed graph
figure
variational fac- torization
variational posterior distribution
parameters
π
param- eters
λ
paths
π
µ
λ
nodes
conditioning set
conditional inde- pendence test
respect
such paths
variational linear regression  n
second illustration
variational inference
bayesian linear regression model
section
evidence framework
integration
α
point estimates
log marginal likelihood
bayesian approach
hyperpa- rameters
parameters
exact integration
variational methods
tractable approximation
order
discussion
noise precision parameter β
true value
framework
distribution
β
linear regression model
variational treatment
evidence framework
good exercise
use
variational methods
foundation
variational treatment
bayesian logistic regression
section
likelihood function
w
w
exercise
n
tn|wtφn
β−1
p
t|w
= p
w|α
= n
w|0
α−1i
n=1
φn = φ
xn
prior distribution
α
dis- cussion
section
conjugate
precision
gaussian
gamma distribution
gam
·|·
·
b.26
joint distribution
variables
p
α
gam
α|a0
b0
p
t
w
α
p
t|w
p
w|α
p
α
directed graphical model
figure
variational distribution
ﬁrst goal
approximation
posterior distribution p
w
α|t
variational framework
section
variational linear regression figure
probabilistic graphical model
joint dis- regression
bayesian linear
tribution
model
φn β tn n 
n
α w
posterior distribution
factorized expression q
w
α
q
w
q
α
re-estimation equations
factors
distribution
use
general result
factor
log
joint distribution
variables
average
respect
variables
factor
distribution
α
terms
functional dependence
α
q
α
ln p
α
ew [ ln p
w|α
+ const =
a0 −
ln α − b0α + m
ln α −
e [
] + const
log
gamma distribution
coefﬁ- cients
α
ln α
q
α
gam
α|an
bn
an = a0 + m
[ wtw ]
bn = b0 + similarly
variational re-estimation equation
posterior distribution
w. again
general result
terms
functional dependence
w
q
w
ln p
t|w
eα [ ln p
w|α
+ const
wtφn − tn
[ α ]
+ const
wt e [ α ]
βφtφ w + βwtφtt + const
quadratic form
distribution q
w
square
usual way
mean
covariance
q
w
= n
w|mn
sn
= − = −
−1
 
approximate inference
mn = βsn φtt sn = e [ α ]
βφtφ note
close similarity
posterior distribution
α
ﬁxed parameter
difference
α
expecta- tion e [ α ]
variational distribution
same notation
covariance matrix sn
cases
standard results
b.27
b.38
b.39
required moments
e [ α ] =
/bn e [
] = mn mt n + sn
evaluation
variational posterior distribution begins
pa- rameters
distributions
w
q
α
factors
turn
suitable convergence criterion
ﬁed
terms
bound
variational solution
evidence framework
section
case
= b0 =
limit
inﬁnitely broad prior
α
mean
variational posterior distribution q
α
e [ α ] =
bn = m/2 e [
] /2 = m n mn + tr
sn
mt
comparison
case
simple model
variational approach
same expression
evidence function
em
point estimate
α
value
distribution q
w
q
α
expectation e [ α ]
approaches
identical results
case
inﬁnitely broad prior
predictive distribution
predictive distribution
t
new input x
model
gaussian variational posterior
parameters
t|x
t
 = = n
t|mt p
t|x
w
p
w|t
dw p
t|x
w
q
w
dw n
t|wtφ
x
β−1
n
w|mn
sn
dw n φ
x
σ2
x
  
variational linear regression
use
result
linear-gaussian model
input-dependent variance
σ2
x
+ φ
x
tsn φ
x
note
same form
result
ﬁxed α
value e [ α ]
deﬁnition
sn
lower bound
quantity
importance
bound l
l
q
e [ ln p
w
α
t
− e [ ln q
w
α
−eα [ ln q
w
w − e [ ln q
α
]
ew [ ln p
t|w
+ ew
α [ ln p
w|α
+ eα [ ln p
α
]
exercise
evaluation
various terms
use
results
previous chapters
e [ ln p
t|w
w = n
− e [ ln p
w|α
] w
α = − −
m
an
β
β
− ttt + βmt n φtt φtφ
mn mt n + sn
tr ln
+ m
ψ
an
− ln bn
mt n mn + tr
sn
e [ ln p
α
α = a0 ln b0 +
a0 −
[ ψ
an
− ln bn ]
an bn − ln γ
an
ln|sn| + m
+ ln
] −e [ ln q
w
w =
−e [ ln q
α
α = ln γ
an
−
an −
ψ
an
− ln bn
an
figure
plot
bound l
q
degree
polynomial model
synthetic data set
polynomial
prior parameters
= b0 =
noninformative prior p
α
ln α
section
section
quantity l
bound
log marginal likelihood p
t|m
model
equal prior probabilities
m
different values
m
l
approximation
poste- rior model probability p
m|t
variational framework
probability
model
m =
maximum likelihood result
residual error
models
complexity
residual error
maximum likelihood
over-ﬁtted models
 n    
approximate inference figure
plot
bound l ver- sus
order m
polyno- mial
polynomial model
set
data points
m =
inter- val
additive gaussian noise
variance
value
bound
log prob- ability
model
value
bound peaks
m =
true model
data set
exponential family distributions
chapter
important role
exponential family
distributions
conjugate priors
models
book
complete-data likelihood
exponential family
case
marginal likelihood function
data
example
mixture
gaussians
joint distribution
obser- vations xn
hidden variables zn
member
exponential family
marginal distribution
xn
mixture
gaussians
hence
variables
model
observed variables
hidden variables
further distinction
latent variables
z
parameters
θ
parameters
size
data set
whereas latent variables
scale
number
size
data set
example
gaussian mixture model
indicator variables zkn
component k
data point xn
latent variables
means
precisions
proportions
parameters
case
data
data values
x =
xn
n =
n
corresponding latent variables z =
zn
joint distribution
latent variables
member
exponential family
natural parameters
p
x
z|η
n=1 h
zn
g
η
ηtu
xn
zn
conjugate prior
η
p
v0
= f
χ0
g
η
exp νoηtχ0
recall
conjugate prior distribution
prior number ν0
observations
value χ0
u vector
variational        
n
  
exponential family distributions
distribution
latent variables
parameters
q
z
η
q
z
q
η
general result
factors
ln q
z
eη [ ln p
x
z|η
const = ln h
zn
e [ ηt ] u
zn
+ const
section
thus
sum
independent terms
value
n
solution
q
z
q
z
n q
zn
example
induced factorization
exponential
sides
zn
= h
zn
g
e [ η ]
e [ ηt ] u
zn
normalization coefﬁcient
comparison
standard form
exponential family
variational distribution
parameters
q
η
ln p
χ0
ez [ ln p
x
z|η
const ln g
η
ηtezn [ u
zn
= ν0 ln g
η
ηtχ0 + n
+ const
again
exponential
sides
normalization coef- ﬁcient
inspection
η
= f
χn
g
η
exp ηtχn
νn = ν0 + n n χn = χ0 + ezn [ u
zn
]
n=1
note
solutions
q
zn
η
two-stage procedure
variational e step
expected sufﬁcient statistics e [ u
zn
]
current posterior distribution q
zn
latent variables
revised posterior distribution q
η
parameters
subsequent variational m step
pa- rameter posterior distribution
expected natural parameters e [ ηt ]
rise
variational distribution
latent variables
variational message passing
application
variational methods
spe- ciﬁc model
bayesian mixture
gaussians
detail
model
   i
approximate inference
directed graph
figure
use
variational methods
models
directed graphs
number
applicable results
joint distribution
graph
decomposition p
x
p
xi|pai
xi
node i
pai
parent
i
note
xi
latent variable
set
observed variables
variational approximation
distribution q
x
respect
xi
q
x
qi
xi
note
observed nodes
factor q
xi
variational distribution
general result
ln q j
xj
ei=j ln p
xi|pai
+ const
i
terms
right-hand side
xj
fact
only terms
xj
additive constant
ditional distribution
xj
p
xj|paj
other conditional distributions
conditioning
deﬁnition
conditional distributions correspond
children
node j
co-parents
child nodes
i.e.
other parents
child nodes
node
set
nodes
q
xj
corresponds
markov blanket
node xj
figure
update
factors
variational posterior distribution
local calculation
graph
construction
general purpose software
variational inference
form
model
advance
bishop
case
model
conditional dis- tributions
conjugate-exponential structure
variational update proce- dure
terms
local message
algorithm
winn
bishop
distribution
particular node
node
messages
parents
children
turn
children
messages
co- parents
evaluation
bound
required quantities
part
message
scheme
distributed message
formulation
good scaling properties
large networks
local variational methods
local variational methods section
variational framework
sections
‘ global ’ method
sense
approximation
full poste- rior distribution
random variables
alternative ‘ local ’ approach
bounds
functions
individual variables
groups
variables
model
instance
bound
conditional distribution p
y|x
factor
probabilistic model speciﬁed
directed graph
purpose
bound
course
resulting distribution
local approximation
variables
turn
tractable approximation
section
practical example
approach
context
logistic regression
bounds
discussion
kullback-leibler divergence
convexity
logarithm function
key role
bound
global variational approach
func- tion
chord
function
convexity
central role
local variational framework
note
discussion
ply
functions
‘ min ’
‘ max ’
bounds
upper bounds
simple example
function f
x
exp
−x
convex function
x
left-hand plot
figure
goal
f
x
simpler function
linear function
x
figure
linear function
bound
f
tangent
tangent line y
x
speciﬁc value
x
x = ξ
ﬁrst order taylor expansion
y
x
f
x
equality
x = ξ
example function f
x
y
x
f
ξ
f
ξ
x − ξ
figure
left-hand ﬁg- ure
red curve
function exp
−x
blue line
tangent
x = ξ
ξ =
line
λ = f
ξ
− exp
−ξ
other tangent line
ex- ample
ones
value
y
x = ξ
right-hand ﬁgure
corresponding plot
function λξ − g
λ
g
λ
λ
ξ =
maximum
= − exp
−ξ
λξ − g
λ
−1 −0.5
approximate inference y f
x
y −g
λ
f
x
x λx − g
figure
left-hand plot
red curve
convex function f
x
blue line
linear function λx
bound
f
x
f
x
λx
x
value
slope
contact point
tangent line
same slope
respect
discrepancy
green dashed lines
f
x
dual function g
λ
intercept
tangent line
slope
exp
−x
tangent line
form y
x
exp
−ξ
exp
−ξ
x − ξ
linear function
ξ
consistency
subsequent discussion
λ = − exp
−ξ
y
λ
λx − λ + λ ln
−λ
different values
λ correspond
different tangent lines
such lines
bounds
function
x
y
function
form f
x
max λ
λx − λ + λ ln
−λ
convex function f
x
simpler
lin- ear function y
price
variational parameter λ
bound
respect
approach
framework
convex duality
rockafellar
jordan
al.
illustration
convex function f
x
left-hand plot
figure
example
function λx
bound
f
bound
linear function
slope λ
bound
tangent line
equation
tangent line
slope λ
λx − g
λ
intercept g
λ
slope λ
tangent
intercept
line
amount
vertical distance
line
function
figure
g
λ
− min = max x
f
x
x
λx − f
x
local variational methods
λ
x
particular x
λ
tangent plane
particular x
y value
tangent line
particular x
value
contact point
x
max λ
λx − g
λ
functions
x
g
λ
dual role
duality relations
simple example f
x
exp
−x
from
value
x
ξ = − ln
−λ
back-substituting
conjugate function g
λ
form g
λ
λ − λ ln
−λ
function λξ − g
λ
ξ =
right-hand plot
figure
check
value
λ = − exp
−x
original function f
x
exp
−x
concave functions
similar argument
upper bounds
max ’
‘ min ’
f
x
min g
λ
min λ
λx − g
λ
λx − f
x
function
interest
method
bound
seek invertible transformations
function
argument
con- vex form
conjugate function
original variables
important example
pattern recognition
logistic sigmoid function
σ
x
+ e−x
exercise
function
logarithm
function
from
corresponding conjugate function
form g
λ
min x
λx − f
x
−λ ln λ −
− λ
ln
− λ
appendix b
binary entropy function
probability
value
using
upper bound
log sigmoid ln σ
x
λx − g
λ
    
  
approximate inference
−6 λ
λ
ξ
−6 −ξ
figure
left-hand plot
logistic sigmoid function σ
x
examples
exponential upper bound
blue
right-hand plot
logistic sigmoid
gaussian
bound
blue
parameter ξ =
bound
x = ξ
x = −ξ
green lines
exercise
upper bound
logistic sigmoid
form σ
x
exp
λx − g
λ
values
λ
left-hand plot
figure
bound
sigmoid
functional form
jaakkola
jordan
transforma- tions
input
function
first
log
logistic function
ln σ
x
− ln
+ e−x
− ln e−x/2
ex/2 + e−x/2
x/2 − ln
ex/2 + e−x/2
function f
x
− ln
ex/2 + e−x/2
convex function
variable x2
second derivative
bound
f
x
linear function
x2
conjugate function
g
λ
max x2 λx2 − f
stationarity condition
= λ − dx dx2 d dx f
x
λ +
value
x
contact point
tangent line
particular value
λ
ξ
ξ
tanh
= − σ
ξ

   section
section
local variational methods
thinking
λ
variational parameter
role
expressions
conjugate function
g
λ
λ
ξ
− f
ξ
λ
ξ
+ ln
eξ/2 + e−ξ/2
bound
f
f
x
λx2 − g
λ
λx2 − λξ2 − ln
eξ/2 + e−ξ/2
bound
sigmoid
σ
x
σ
ξ
exp
x − ξ
− λ
ξ
x2 − ξ2
λ
ξ
bound
right-hand plot
figure
bound
form
exponential
quadratic function
x
gaussian representations
posterior distributions
logistic sigmoid functions
logistic sigmoid
probabilistic models
binary vari- ables
function
log
ratio
posterior prob- ability
corresponding transformation
multiclass distribution
softmax function
bound
logistic sigmoid
softmax
gibbs
method
gaussian distribution
bound
rigorous proof
local variational methods
problems
example
use
local variational bounds
sections
moment
general terms
bounds
suppose
integral
form
p
da
σ
logistic sigmoid
p
gaussian probability density
such integrals
bayesian models
instance
pre- dictive distribution
case p
posterior parameter distribution
variational bound
form σ
� f
ξ
ξ
variational parameter
inte- gral
product
exponential-quadratic functions
bound
ξ
p
da = f
ξ
freedom
variational parameter ξ
value
function f
ξ
value f
ξ
bound
family
bounds
approximation
optimized bound
    
n 
approximate inference
bound σ
� f
ξ
logistic sigmoid
required choice
ξ
value
bound
value
quantity f
ξ
values
value
compromise
distribution p
variational logistic regression
use
local variational methods
bayesian logistic regression model
section
use
laplace approximation
variational treatment
approach
jaakkola
jordan
laplace method
gaussian approximation
posterior distribution
ﬂexibility
variational approximation
accuracy
laplace method
furthermore
laplace method
variational approach
well
objective function
rigourous bound
model evidence
logistic regression
dybowski
roberts
bayesian perspective
monte carlo
techniques
variational posterior distribution
use
variational approximation
local bounds
section
likelihood function
logistic regres- sion
logistic sigmoid
expo- nential
quadratic form
convenient
conjugate gaussian prior
form
moment
hyperparam- eters m0
s0
ﬁxed constants
section
variational formalism
case
unknown hyper- parameters
values
data
variational framework
bound
marginal likelihood
bayesian logistic regression model
marginal likelihood
form p
t
= p
t|w
p
w
dw = p
tn|w
p
w
dw
conditional distribution
t
p
t|w
= σ
t
− σ
−
t
+
+ e−a
+ e−a = eatσ
−a
= wtφ
order
bound
p
t
use
bound
logistic sigmoid function
    n    n
    n
variational logistic regression
convenience σ
z
� σ
ξ
exp
z − ξ
− λ
ξ
z2 − ξ2
write λ
ξ
σ
ξ
p
t|w
= eatσ
−a
eatσ
ξ
−
+ ξ
− λ
ξ
a2 − ξ2
bound
terms
likelihood function
variational parameter ξn
training
observation
tn
= wtφ
multiplying
prior distribution
bound
joint distribution
t
w p
t
w
= p
t|w
p
w
� h
w
ξ
p
w
ξ
set
variational parameters
h
w
ξ
σ
ξn
n=1 wtφntn −
wtφn + ξn
− λ
ξn
[ wtφn
− ξ2 n
evaluation
exact posterior distribution
normalization
left- hand side
inequality
right-hand side
note
function
right-hand side
probability density
variational posterior distribution q
w
bound
logarithm function
inequality a � b
a � ln b
bound
log
joint distribution
t
w
form ln
p
t|w
p
w
ln p
w
ln σ
ξn
wtφntn −
wtφn + ξn
− λ
ξn
[ wtφn
− ξ2 n
prior p
w
right-hand side
inequality
function
w − m0
w − m0
+ n=1 wtφn
tn −
− λ
ξn
wt
φnφt
w + const
   n  n

approximate inference exercise
quadratic function
w
corresponding variational approximation
posterior distribution
quadratic terms
w
gaussian variational posterior
form
q
w
= n
w|mn
sn
mn = sn s−1
m0 +
tn −
φn n=1 s−1 n = s−1
λ
ξn
laplace framework
gaussian approximation
posterior distribution
additional ﬂexibility
vari- ational parameters
ξn
accuracy
approximation
jaakkola
jordan
batch
context
training data
bayesian methods
sequential learning
data points
time
formulation
variational approach
sequential case
note
bound
two-class problem
approach
problems
k >
classes
alternative bound
multiclass case
gibbs
variational parameters
normalized gaussian approximation
posterior distribution
predictive distribution
new data points
first
variational parameters
ξn
bound
marginal likelihood
inequality
marginal likeli- hood
ln p
t
ln p
t|w
p
w
dw � ln h
w
ξ
p
w
dw = l
ξ
optimization
hyperparameter α
linear regression model
section
approaches
ξn
ﬁrst approach
function l
ξ
integration
w
latent variable
em algorithm
second approach
w
direct maximization
ξ
em approach
em algorithm starts
initial values
parameters
e step
em algorithm
ξn
ξold
  n
variational logistic regression
parameter values
posterior distribution
w
m step
expected complete-data log likelihood
q
ξ
ξold
e [ ln h
w
ξ
p
w
]
expectation
respect
posterior distribution q
evalu-
ξold
p
w
ξ
h
w
ξ
q
ξ
ξold
ln σ
ξn
ξn/2 − λ
ξn
φt ne [ wwt ] φn − ξ2 n
+ const
‘ const ’
terms
ξ
respect
zero
few lines
algebra
use
deﬁnitions
σ
ξ
λ
ξ
= λ
ξn
φt n
ne [ wwt ] φn − ξ2
λ
ξ
monotonic function
ξ
ξ �
attention
nonnegative values
ξ
loss
generality
symmetry
bound
ξ =
λ
ξ
following re-estimation equations
ξnew
= φt ne [ wwt ] φn = φt
sn + mn mt n φn
em algorithm
variational posterior distri- bution
variational parameters
e step
posterior distribution
w
mean
covari- ance
m step
variational posterior
new value
ξ
e
m steps
suitable convergence criterion
practice
few iterations
alternative approach
re-estimation equations
ξ
w
deﬁnition
bound l
ξ
integrand
gaussian-like form
respect
rise
same re-estimation equations
em approach
application
variational methods
bound l
ξ
integration
w
p
w
h
w
ξ
exponential
quadratic function
w. thus
square
use
standard result
normalization coefﬁcient
gaussian distribution
closed form solution
form exercise
exercise
exercise
 n
−2 −4 −6
approximate inference
−2 −4 −6
−2
−2
figure
illustration
bayesian approach
logistic regression
simple linearly separable data set
plot
left
predictive distribution
variational inference
decision boundary
mid way
clusters
data points
contours
predictive distribution
data
uncertainty
classiﬁcation
such regions
plot
right
decision
samples
parameter vector
posterior distribution p
w|t
l
ξ
+ ln |sn| |s0|
mt n s−1 n
+
mt
s−1
m0 ln σ
ξn
ξn − λ
ξn
variational framework
situations
data
case
jaakkola
jordan
gaussian posterior distribution
w
prior p
w
data point arrives
posterior
use
bound
updated posterior distribution
predictive distribution
posterior dis- tribution
same form
laplace approximation
section
figure
variational predictive distributions
syn- thetic data set
example
interesting insights
concept
‘ large margin ’
section
similar be- haviour
bayesian solution
inference
hyperparameters so
hyperparameter α
prior distribution
constant
bayesian logistic regression model
value
parameter
data set
local variational approximations
single framework
bound
marginal likelihood
stage
combined approach
bishop
svens´en
context
bayesian treatment
hierarchical mixture
experts model
    
variational logistic regression
speciﬁcally
simple isotropic
distribu- tion
form p
w|α
= n
w|0
α−1i
analysis
general gaussian priors
instance
different hyperparameter
different subsets
parame- ters
conjugate hyperprior
α
gamma distribution
constants a0
b0
marginal likelihood
model
form p
α
gam
α|a0
b0
p
t
= p
w
α
t
dw dα
joint distribution
p
w
α
t
= p
t|w
p
w|α
p
α
intractable integration
w
α
global variational approaches
same model
variational distribution q
w
α
decomposition
instance
form
bound l
q
kullback-leibler divergence kl
qp
ln p
t
= l
q
+ kl
qp
l
q
= q
w
α
dw dα kl
qp
− q
w
α
dw dα
p
w
α
t
q
w
α
p
w
α|t
q
w
α
point
bound l
q
form
likelihood factor p
t|w
local variational bound
logistic sigmoid factors
inequality
place
bound
l
q
bound
log marginal likelihood ln p
t
� l
q
� l
q
ξ
q
w
α
= h
w
ξ
p
w|α
p
α
q
w
α
dw dα
variational distribution factorizes
parameters
hyperparameters
q
w
α
q
w
q
α
   n
 n=1 n  
n
approximate inference
factorization
general result
expressions
optimal factors
distribution q
w
terms
w
q
w
eα [
h
w
ξ
p
w|α
p
α
+ const = ln h
w
ξ
eα [ ln p
w|α
+ const
ln h
w
ξ
ln p
w|α
ln q
w
− e [ α
wtw +
tn −
wtφn − λ
ξn
nw + const
quadratic function
w
solution
q
w
square
usual way
w
= n
w|µn
σn
σ−1 n µn =
tn −
φn σ−1 n = e [ α ]
λ
ξn
n. similarly
optimal solution
factor q
α
ln q
α
ew [ ln p
w|α
+ ln p
α
+ const
ln p
w|α
ln p
α
ln q
α
m
ln α −
e wtw +
a0 −
ln α − b0α + const
log
gamma distribution
α
gam
α|an
bn
γ
a0
αa0−1e−b0α an = a0 + m
bn = b0 + wtw
    
expectation propagation
variational parameters
l
q
ξ
terms
ξ
bound integrating
α
l
q
ξ
q
w
ln h
w
ξ
dw + const
note
same form
result
direct optimization
marginal likelihood function
re-estimation equations
form
ξnew n
= φt n σn + µn µt n φn
appendix b
re-estimation equations
quantities
w
α
ξ
suitable initializations
cycle
quan- tities
turn
required moments
e [ α ] =
bn wtw e = σn + µt n µn
expectation propagation
chapter
alternative form
deterministic approx- imate inference
expectation propagation
ep
minka
minka
variational bayes methods
minimization
kullback-leibler divergence
reverse form
approximation
different properties
moment
problem
kl
pq
respect
z
p
z
ﬁxed distribution
q
z
member
exponential family
form q
z
= h
z
g
η
ηtu
z
function
η
kullback-leibler divergence
kl
pq
− ln g
η
ηtep
z
[ u
z
+ const
constant terms
natural parameters
mize kl
pq
family
distributions
gradient
respect
negative gradient
ln g
η
expectation
u
z
distribution q
z
results
−∇ ln g
η
ep
z
[ u
z
]
eq
z
[ u
z
= ep
z
[ u
z
]
   
p
d
approximate inference
optimum solution
suf- ﬁcient statistics
instance
q
z
gaussian n
z|µ
σ
kullback-leibler divergence
mean µ
q
z
mean
distribution p
z
covariance σ
covariance
p
z
moment matching
example
figure
result
practical algorithm
approximate inference
many probabilistic models
joint distribution
data d
hidden variables
parameters
product
factors
form p
d
θ
fi
θ
example
model
data
factor fn
θ
p
xn|θ
data point xn
factor f0
θ
p
θ
model
directed probabilistic graph
factor
conditional distribution
nodes
undirected graph
factor
clique potential
posterior distribution p
θ|d
purpose
predictions
model evidence p
d
purpose
model comparison
from
posterior
fi
θ
p
θ|d
=
model evidence
p
d
fi
θ
dθ
continuous variables
following discussion applies
variables
integrals
summations
pose
marginalization
θ
marginalizations
respect
posterior distribution
predictions
form
approximation
expectation propagation
approximation
posterior distribu- tion
product
factors
θ
z fi
θ
i
fi
θ
approximation
factors
factor fi
θ
true posterior
factor
constant
left-hand side
integrates
unity
order
θ
way
practical algorithm
factors
exponential family
product
factors
exponential family
   
          q\j
θ
q
θ
fj
θ
fj
θ
fi
θ
fi
θ
z i
p
d
fj
θ
fi
θ
 
expectation propagation
ﬁnite set
sufﬁcient statistics
example
overall approximation q
θ
ideally
fi
θ
kullback-leibler divergence
true posterior
approximation
kl
pq
kl fi
θ
note
reverse form
kl divergence
varia- tional inference
minimization
kl di- vergence
respect
true distribution
rough approx- imation
kl divergences
corresponding pairs fi
θ
θ
factors
simpler problem
advantage
algorithm
fac- tor
product
factors
poor approximation
expectation propagation
approximation
factor
turn
context
factors
fi
θ
cycles
factors
time
factors
spirit
update
factors
variational bayes framework fj
θ
suppose
factor fi
θ
factor
product
revised form
factor fj
θ
product i=j qnew
θ
fi
θ
fi
θ
i = j
factors approximation
regions
high posterior probability
factors
example
effect
ep fj
‘ clutter problem ’
factor current approximation
posterior
unnormalized distribution section
note
q\j
product
factors
j
practice division
factor fj
θ
distribution
zj fj
θ
q\j
θ

−2  

approximate inference
−2
figure
illustration
expectation propagation approximation
gaussian distribution
example
figures
left-hand plot
original distribution
laplace
global variational
ep
blue
approximations
right-hand plot
corresponding negative logarithms
distributions
note
ep distribution
variational inference
consequence
different form
kl divergence
zj
normalization
zj = fj
θ
q\j
θ
dθ
revised factor gence fj
θ
kullback-leibler diver- kl fj
θ
q\j
θ
zj qnew
θ
distribution qnew
θ
ex- ponential family
result
parameters
qnew
θ
sufﬁcient statistics
corresponding moments
tractable oper- ation
example
gaussian distribution n
θ|µ
σ
µ
mean
distribution fj
θ
q\j
θ
σ
covariance
required ex- pectations
member
exponential family
expected statistics
derivatives
normalization coefﬁcient
ep approximation
figure
from
revised factor qnew
θ
factors
fj
fj
θ
k qnew
θ
q\j
θ
coefﬁcient k
         q\j
θ
q
θ
fj
θ
q\j
θ
dθ q
θ
fi
θ
fi
θ
fi
θ
z i
  sides
q\i
θ
k =
expectation propagation
fact
qnew
θ
value
k
zeroth-order moments fj
θ
q\j
θ
dθ = fj
θ
q\j
θ
dθ
k = zj
practice
several passes
set
factors
factor
turn
posterior distribution p
θ|d
model evidence p
d
factors
θ
approximations
θ
expectation propagation
joint distribution
data d
stochastic variables
form
product
factors
d
θ
=
posterior distribution p
θ|d
distribution
form
q
θ
=
model evidence p
d
fi
θ
factors
posterior approximation
convergence
factor
b
remove fj
θ
fj
posterior
division
fj
θ
   zj = q\j
θ
fj
θ
dθ
d
evaluate
new factor qnew
θ
q\j
θ
approximation
model evidence fj
θ
zj p
d
fi
θ
dθ
i
approximate inference
c
new posterior
sufﬁcient statistics
moments
qnew
θ
q\j
θ
fj
θ
evaluation
normalization
a special case
ep
assumed density
adf
moment matching
maybeck
lauritzen
boyen
koller
opper
winther
factors
ﬁrst
unity
pass
factors
assumed density
on-line learning
data points
sequence
data point
next point
batch
opportunity
data points many times
order
ac- curacy
idea
expectation propagation
adf
data
results
undesirable dependence
order
data points
ep
disadvantage
expectation propagation
guarantee
iterations
approximations q
θ
exponential family
iterations
converge
solution
stationary point
particular energy function
minka
iteration
ep
value
energy function
contrast
variational bayes
bound
log marginal likelihood
iteration
bound
ep cost function
case
algorithms
difference
variational bayes
ep arises
form
kl divergence
algorithms
former mini- mizes kl
qp
latter minimizes kl
pq
figure
distributions
θ
kl
pq
poor approximations
ep
mixtures
results
approximation
modes
posterior distribution
logistic-type models
ep
local variational methods
laplace approximation
kuss
rasmussen
expectation propagation
figure
illustration
clutter problem
data space dimensionality
d =
data points
crosses
mixture
gaussians
components
goal
mean
green gaussian
data
 n
example
clutter problem
minka
ep algorithm
simple exam- ple
goal
mean θ
multivariate gaussian distribution
variable x
set
observations
distribution
problem
observations
background clutter
figure
distribu- tion
observed values
mixture
gaussians
form p
x|θ
=
− w
n
x|θ
wn
ai
w
proportion
background clutter
θ
gaussian p
θ
n
θ|0
bi
minka
parameter
b =
joint distribution
n observations d =
x1
θ
p
d
θ
p
θ
p
xn|θ
n=1
posterior distribution
mixture
gaussians
computational cost
problem
size
data set
exact solution
large n.
ep
clutter problem
factors
θ
p
θ
θ
p
xn|θ
distribution
expo- nential family
example
spherical gaussian q
θ
n
θ|m
vi
   
approximate inference
factor approximations
form
exponential-quadratic functions
form fn
θ
snn
vni
n =
n
prior p
θ
note
use
n
θ|·
·
right-hand side
well-deﬁned gaussian density
fact
variance parameter vn
fn
θ
n =
n
shorthand notation
approximations
unity
=
d/2
vn → ∞
d
dimensionality
x
hence
θ
initial q
θ
factors
factor fn
θ
time
applying
note
term f0
θ
ep update
term
state
results
reader
details
first
current estimate fn
q
θ
division
q\n
θ
mean
inverse variance
m\n = m + v\nv−1 n
m − mn
v\n
= v−1 − v−1 n
normalization constant zn
zn =
− w
n
xn|m\n
v\n +
wn
mean
variance
qnew
θ
mean
variance
q\n
θ
fn
θ
exercise
exercise
exercise
m = m\n + ρn v = v\n − ρn
quantity v\n v\n
v\n
v\n +
xn − m\n
+ ρn
− ρn
v\n
− m\n2 d
v\n +
− w znn
xn|0
ai
simple interpretation
probability
point
clutter
fn
θ
parameters
reﬁned factor n =
vnew
−1 −
v\n
v−1 mn = m\n +
vn + v\n
v\n
−1
mnew − m\n
sn =
zn
d/2n
mn|m\n
vn + v\n
reﬁnement process
suitable termination criterion
instance
maximum change
parameter values
 n 
n
expectation propagation
θ
e e 
−5 figure
examples
approximation
speciﬁc factors
one-dimensional version
clutter fn
θ
q\n
θ
notice
current form
q\n
θ
problem
fn
θ
blue
range
θ
fn
θ
good approximation
θ
pass
factors
threshold
approximation
model evidence
p
d

d/2 exp
b/2
sn
−d/2
b =
mnew
tmnew v nmn mt vn
n=1 examples factor approximations
clutter problem
one-dimensional pa- rameter space θ
figure
note
factor approximations
inﬁnite
negative values
‘ variance ’ parameter vn
approximations
upwards
downwards
overall approximate posterior q
θ
posi- tive variance
figure
performance
ep
variational bayes
mean ﬁeld theory
laplace approximation
clutter problem
expectation propagation
graphs
general discussion
ep
factors
θ
distribution p
θ
functions
components
θ
f
θ
distribution q
θ
factors situations
factors
subsets
variables
such restric- tions
framework
probabilistic graphical models
chapter
factor graph representation
undirected graphs
      
approximate inference
r o r r e
posterior mean laplace vb
ep
flops
r o r r e
laplace
evidence vb
flops figure
comparison
expectation propagation
variational inference
laplace approximation
clutter problem
left-hand plot
error
predicted posterior mean
number
point operations
right-hand plot
corresponding results
model evidence
section
case
approximating distribution
case expectation propagation reduces
belief propagation
minka
context
simple example
general case
first
recall
kullback-leibler diver- gence kl
pq
respect
distribution q
optimal solution
factor
corresponding marginal
p.
factor
left
figure
context
sum-product algorithm
joint distribution
p
x
fa
x2
fb
x3
fc
x4
approximation q
x
same factorization
q
x
fa
x2
fb
x3
fc
x4
note
normalization constants
end
local normalization
belief propagation
sup- pose
attention
approximations
factors
respect
individual variables
q
x
fa1
x1
fa2
x2
fb2
x2
fb3
x3
fc2
x2
fc4
x4
factor
right
figure
individual factors
overall distribution q
x
fac- torized
ep algorithm
approximation
sup- pose
factors
factor                           x3             
expectation propagation
x2 x3 x1 x2 x3 x1 fa fb fc x4 ˜fa1 ˜fa2 ˜fb2 ˜fb3 ˜fc2 ˜fc4 x4 figure
left
simple factor graph
figure
convenience
right
corresponding factorized approximation
fb
x3
distribution
fb2
x2
fb3
x3
factor
q\b
x
fa1
x1
fa2
x2
fc2
x2
fc4
x4
exact factor fb
x3
fa1
x1
fa2
x2
p
x
q\b
x
fb
x3
=
pqnew
qnew
x
kullback-leibler divergence kl
result
qnew
z
product
factors
variable xi
factor
corresponding marginal
p
x
marginals
fc4
x4
fb
x3
fc2
x2
p
x1
p
x2
p
x3
p
x4
fa1
x1
fa2
x2
fc4
x4
fc2
x2
fb
x3
fb
x3
fc2
x2
fa2
x2
x
marginals
fb
factors
q
x
fb
x2
x3
variables
fb
reﬁned factor fb2
x2
fb3
qnew
x
q\b
x
fb2
x2
fb
x3
fb3
x3
x2 fb
x3
fa2
x2
fc2
x2
              i=j k i k i
      
approximate inference section
messages
belief propagation
mes- sages
variable nodes
nodes
messages
fb2
x2
message factor nodes
variable nodes
µfb→x2
x2
factor node fb
variable node x2
simi- fa2
corre-
x2
x2
x2
message fb3
x3
x3
result
standard belief propagation
messages
directions
same time
ep procedure
standard form
sum-product algorithm
fb2
x2
unchanged factors
time
instance
x3
deﬁnition
reﬁned version
term
time
order
reﬁnements
tree-structured graph
two-pass update scheme
standard belief propagation schedule
exact inference
factor marginals
initialization
approximation factors
case
fb3
x3
general factor graph
distribution p
θ
fi
θi
subset
variables
factor fi
distribution
form q
θ
fik
θk
θk corresponds
individual variable node
fjl
θl
other terms
term
particular term fj
q
θ
q\j
θ
fik
θk
fjl
θl
exact factor fj
θj
reﬁned term
functional dependence
θl
corresponding marginal
q\j
θ
fj
θj
up
multiplicative constant
marginal
fj
θj
terms
q\j
θ
functions
variables
θj
terms
fi
θi
i = j
numerator
correspond
other factors
q\j
θ
fjl
θl
fj
θj
fkm
θm
θm=l∈θj k m=l 
sum-product rule
form
messages
vari- able nodes
nodes
example
fjm
θm
message µfj→θm
θm
figure
quantity
factor node j
variable node m
product
k
factors
variables
variables
variable θl
factor fj
θj
other words
outgoing message
factor node
product
incoming messages
other factor nodes
multiply
local factor
sum-product algorithm
special case
expectation propa- gation
approximating distribution
distributions
nected graphs
accuracy
generalization
group factors
θi
sets
factors
set
iteration
approaches
improvements
accuracy
minka
problem
combination
group- ing
disconnection
open research issue
variational message passing
expectation propagation op- timize
different forms
kullback-leibler divergence
minka
broad range
message
algorithms
com- mon framework
minimization
members
alpha family
diver- gences
variational message passing
loopy belief propagation
expectation propagation
range
other algorithms
space
tree-reweighted message pass- ing
wainwright
fractional belief propagation
wiegerinck
heskes
power ep
minka

www verify
log marginal distribution
observed data ln p
x
terms
form
l
q
kl
qp

properties e [ z1 ] = m1
e [ z2 ] = m2
simultaneous equa- tions
show
original distribution p
z
unique solution
means
factors
approxi- mation distribution
e [ z1 ] = µ1
e [ z2 ] = µ2
 
www
factorized variational distribution q
z
form
technique
lagrange multipliers
minimization
kullback-leibler divergence kl
pq
respect
factors
zi
other factors
solution
 
p
x
ﬁxed distribution
gaussian distribution q
x
n
x|µ
σ
form
kl divergence kl
pq
gaussian q
x
approximate inference minimization
kl
pq
respect
leads
result
µ
expectation
x
p
x
σ
covariance
 
www
model
set
hidden stochastic variables
z
latent variables
model parameters
suppose
variational distribution
la- tent variables
parameters
q
z
θ
qz
z
qθ
θ
distribution qθ
θ
point estimate
form qθ
θ
δ
θ − θ0
θ0
vector
free parameters
variational optimization
factorized distribution
em algorithm
e step
qz
z
m step
expected complete-data log posterior distribution
θ
respect
 
alpha family
divergences
show
kullback- leibler divergence kl
pq
p = exp
ln p
+ ln p + o
kl
qp
→ −1
 
problem
mean
precision
univariate gaus- sian
factorized variational approximation
section
show
factor qµ
µ
form n
µ|µn
λ−1 n
mean
precision
similarly show
factor qτ
τ
gamma distribution
form gam
τ|an
bn
parameters

variational posterior distribution
precision
univariate gaussian
parameters
standard results
mean
variance
gamma distribution
b.27
b.28
n → ∞
variational posterior distribution
inverse
maximum likelihood estimator
variance
data
variance
 
use
standard result e [ τ ] =
/bn
mean
gamma distribution
result
reciprocal
expected precision
factorized variational treat- ment
univariate gaussian

decomposition
approxi- mate posterior distributions
models
variational inference
 
www
lagrange multiplier
normalization constraint
distribution q
m
maximum
bound
 
joint distribution
general result
optimal variational distribution q
z
latent variables
bayesian mixture
gaussians
steps
text
 
result
optimum vari- ational posterior distribution
µk
λk
bayesian mixture
gaussians
expressions
parameters
distribution
–
 
distribution
result

result
b.17
expected value
mixing coefﬁcients
variational mixture
gaussians
 
results
ﬁrst
terms
bound
variational gaussian mixture model
  
results
–
terms
bound
variational gaussian mixture model
  
exercise
variational re-estimation equations
gaussian mixture model
direct differentiation
bound
variational distribution
factorization
factors
bound
function
parameters
varia- tional distribution
bound
respect
parameters
re-estimation equations
factors
variational distribution
section
 
result
predictive distribution
variational treat- ment
bayesian mixture
gaussians model
 
exercise
variational bayes solution
mixture
gaussians model
size n
data set
maximum likelihood solution
em
chap- ter
note
results
appendix b
exercise
first show
posterior distribution q
λk
precisions
maximum likelihood solution
posterior dis- tribution
means
µk|λk
next consider
posterior distribution q
π
mixing coefﬁcients
maximum likelihood solution
responsibilities
corresponding maximum likelihood values
large n
use
following asymptotic result
digamma function
large x ψ
x
ln x + o
use
large n
predictive distribution
mixture
gaussians

show
number
equivalent parameter settings
sym- metries
mixture model
k components
k
approximate inference
 
mode
posterior distribution
gaussian mix- ture model
member
family
k
equivalent modes
result
variational inference algorithm
approximate posterior distribu- tion q
neighbourhood
modes
full posterior distribution
mixture
k
such q distributions
mode
equal mixing coefﬁcients
negligible overlap
components
q mixture
bound differs
single component q distribution
ad- dition
extra term ln k
 
www
variational gaussian mixture model
prior distribution
coefﬁcients
πk
mixing coefﬁcients
parameters
values
bound
log marginal likelihood
bound
respect
mixing coefﬁcients
lagrange multiplier
constraint
mixing coefﬁcients
re-estimation result
note
need
terms
bound
dependence
bound
πk
 
www
section
singularities
max- imum likelihood treatment
gaussian mixture models
bayesian treatment
discuss
such singularities
bayesian model
map
estimation
 
variational treatment
bayesian mixture
gaussians
section
use
factorized approximation
posterior distribu- tion
figure
factorized assumption
variance
posterior distribution
certain directions
parameter space
discuss
effect
variational approximation
model evidence
effect
number
components
mixture
hence
variational gaussian mixture
optimal number
components
  
variational treatment
bayesian linear regression
gamma hyperprior gam
β|c0
d0
β
factorized variational distribution
form q
w
q
α
q
β
variational update equations
factors
variational distribution
expression
bound
predictive distribution
 
use
formulae
appendix b show
bound
linear basis function regression model
form
various terms
–
  
rewrite
model
bayesian mixture
gaussians
section
conjugate model
exponential family
section
hence
general results
speciﬁc results

www show
function f
x
ln
x
< x < ∞
form
dual function g
λ
minimization
λx − g
λ
respect
function ln
x

second derivative
log logistic function f
x
− ln
+ e−x
variational upper bound
second order taylor expansion
log logistic function
point x = ξ
 
respect
function f
x
− ln
ex/2 + e−x/2
concave function
x
second derivatives
respect
variable x2
hence show
convex function
x2
plot graphs
f
x
x2
bound
logistic sigmoid function
ﬁrst order taylor series expan- sion
function f
x
variable x2
value ξ2
 
variational treatment
logistic regression
sequen- tial learning
data points
time
next data point arrives
gaussian ap- proximation
posterior distribution
use
bound
distribution
data point
corresponding variational parameter ξn

quantity q
ξ
ξold
respect
variational parameter ξn show
update equation
ξn
bayesian logistic regression model
 
exercise
re-estimation equations
variational parame- ters
bayesian logistic regression model
section
direct maximization
bound
derivative
l
ξ
re- spect
use
result
derivative
log
determinant
expressions
mean
covariance
variational posterior distribution q
w
 
result
bound l
ξ
variational logistic regression model
expressions
gaussian prior q
w
= n
w|m0
s0
bound h
w
ξ
likelihood function
l
ξ
next gather
terms
w
square
gaussian integral
standard result
normalization coefﬁcient
multivariate gaussian
logarithm
 
adf approximation scheme
section
inclusion
factor fj
θ
update
model evidence
form pj
d
pj−1
d
zj
  
approximate inference
zj
normalization constant
result
p0
d
result p
d
zj
j

expectation propagation algorithm
section
factors
θ
deﬁnition
same expo- nential family functional form
distribution q
θ
f0
θ
factor f0
θ
situation
factors
prior p
θ
prior factor
f0
θ
θ
ep update
  
exercise
results
–
expectation propagation
clutter problem
division formula
expressions
mean
variance
normalization constant zn
clutter problem
use
general result
  
show
mean
variance
qnew
θ
ep
clutter problem
following results
expectations
θ
θθt
qnew
θ
e [ θ ] = m\n + v\n∇m\n ln zn e [ θtθ ]
v\n
ln zn +
[ θ ] tm\n − m\n2
use
result
zn
next
results
–
square
exponential
use
result
methods
probabilistic models
practical interest
exact inference
form
approximation
chapter
inference algorithms
deterministic approximations
methods
variational bayes
expectation propagation
approxi- mate inference methods
numerical sampling
monte carlo techniques
applications
posterior distribution
unobserved vari- ables
direct interest
situations
posterior distribution
purpose
expectations
example
order
predictions
fundamental problem
chapter
expectation
function f
z
respect
probability distribution p
z
components
z
continuous variables
combination
case
  
l     f
z
z
sampling methods figure
schematic illustration
function f
z
expectation
respect
distribution p
z
p
z
variables
expectation e [
] = f
z
p
z
dz
summation
case
discrete variables
single continuous variable
figure
such expectations
analyt- ical techniques
general idea
methods
set
samples z
l
l
l
distribution p
z
expectation
ﬁnite sum f
l f
z
l
exercise
samples z
l
distribution p
z
e [
estimator
f ] = e [
] f
correct mean
variance
estimator
var [ f ] = e
f − e [ f ]
l
variance
function f
z
distribution p
z
worth emphasiz- ing
accuracy
estimator therefore
dimension- ality
z
principle
high accuracy
small number
samples z
l
practice
independent samples
expectation
accuracy
problem
samples
z
l
effective sample size
apparent sample size
figure
f
z
regions
p
z
vice versa
expectation
regions
small probability
large sample sizes
sufﬁcient accuracy
many models
joint distribution p
z
terms
graphical model
case
directed graph
observed variables
 m
sampling methods
straightforward
joint distribution
conditional distributions
node
following ances- tral sampling approach
brieﬂy
section
joint distribution
p
z
= p
zi|pai
i=1
zi
set
variables
node i
pai
set
variables
parents
node i
sample
joint distribution
pass
set
variables
order z1
conditional distributions
zi|pai
possible be- cause
step
parent values
pass
graph
sample
joint distribution
case
directed graph
nodes
observed values
principle
above procedure
case
nodes
discrete variables
following logic
approach
henrion
special case
impor- tance
section
step
value
variable zi
value
sampled value
observed value
sample value
al- gorithm proceeds
next variable
turn
value
observed value disagree
whole sample
algorithm
ﬁrst node
graph
algorithm
posterior distribution
samples
joint distribution
hidden variables
data variables
samples
data
slight saving
joint distribution
contradictory value
overall probability
sample
posterior decreases
number
observed variables increases
number
states
variables
increases
approach
practice
case
probability distributions
undirected graph
one-pass sampling strategy
prior distribution
observed variables
expensive techniques
gibbs sampling
section
conditional distributions
samples
marginal distribution
strategy
joint distribution p
v
samples
marginal distribution p
values
v
sample
numerous texts
monte carlo methods
partic- ular interest
statistical inference perspective
chen
al
gamerman
gilks
al
liu
neal
robert
casella
review articles
besag
al
brooks
diaconis
saloff-coste
jerrum
sinclair
neal
tierney
andrieu
al
additional information
 dz   dy  
algorithms
section
simple strategies
random samples
distribution
samples
computer algorithm
fact
pseudo-random numbers
appropriate tests
randomness
such numbers
several subtleties
press
lie
scope
book
algorithm
pseudo-random numbers
software environments
facility
standard distributions
random numbers
simple nonuniform dis- tributions
source
random numbers
z
interval
values
z
function f
·
y
f
z
distribution
y
case
p
z
goal
function f
z
values
y
speciﬁc
distribution p
y
integrating
p
z
z = h
y
≡ p
y
d y −∞
sampling methods methods
statistical inference
diagnostic tests
convergence
markov chain monte carlo algorithms
robert
casella
practical guidance
use
methods
context
machine learning
bishop
nabney
exercise
indeﬁnite integral
p
y
h−1
z
random numbers
function
inverse
indeﬁnite integral
desired distribution
figure
example
exponential distribution p
y
λ exp
−λy
� y < ∞
case
limit
− exp
−λy
variable z
y = −λ−1 ln
− z
y
exponential distribution
∂
z1
zm
y ∂
y1
ym
 p
y
algorithms
h
figure
geometrical interpretation
trans- formation method
random numbers
h
y
indeﬁnite integral
desired dis- tribution p
y
random variable z
y = h−1
z
ac- cording
y
example
distribution
transformation method
cauchy distribution p
y
π
+ y2
exercise
case
inverse
terms
‘ tan ’ function
generalization
variables
ja- cobian
change
variables
p
ym
= p
z1
zm
ﬁnal example
transformation method
box-muller method
samples
gaussian distribution
first
suppose
gen- erate pairs
random numbers
variable distributed uniformly
z
uniform next
pair
z2 distribution
points
unit circle
p
z1
z2
figure
pair z1
z2
quantities
+ z2 figure
box-muller method
gaussian dis-
random numbers starts
samples
uniform distribution
unit circle
z2 −1−1
      ∂
z1
z2
 ∂
zp
sampling methods exercise
exercise
y1 = z1 y2 =
−2 ln z1 −2 ln z2
r2 =
+ z2
joint distribution
y1
y2
p
p
z1
z2
√2π exp
√2π exp
y2
gaussian distribution
zero mean
unit variance
y
gaussian distribution
zero mean
unit variance
σy + µ
gaussian distribution
mean µ
variance σ2
variables
multivariate gaussian distribution
mean µ
co- variance σ
use
cholesky decomposition
form σ = llt
press
z
vector
components
mean
unit vari- ance
= µ + lz
mean µ
covariance σ
transformation technique
success
ability
indeﬁnite integral
required distribution
such operations
limited number
simple distributions
alternative approaches
search
general strategy
techniques
rejection sampling
importance sampling
al-
distributions
problems
many dimensions
important components
general strategies
rejection
rejection
framework
complex distributions
certain constraints
univariate dis- tributions
extension
dimensions
suppose
distribution p
z
simple
standard distributions
p
z
dif- ﬁcult
furthermore
case
p
z
value
z
normalizing constant z
p
z
= p
z
p
z
zp
order
rejection sampling
simpler distribution q
z
proposal distribution
samples
e e   z      
algorithms
figure
rejection
method
samples
sim- ple distribution q
z
grey area be-
unnormalized distribu- tion p
z
scaled distribu- tion kq
z
resulting samples
p
z
normalized version
p
z
kq
z
˜p
z
kq
z0
u0 z0 exercise
constant k
value
kq
z
� p
z
values
z
function kq
z
comparison function
univariate distribution
figure
step
rejection sampler
random numbers
first
number z0
distribution q
z
number u0
uniform distribution
kq
z0
]
pair
random numbers
uniform distribution
curve p
z0
sample
function kq
z
u0 > u0
pair
grey
region
fig- p
z
pairs
uniform distribution
curve
corresponding z values
p
z
original values
z
distribution q
z
sam- p
z
/kq
z
probability
ples
probability sample
p
k p
z
/kq
z
q
z
dz
p
z
dz
fraction
points
method
ratio
p
z
area
curve kq
z
area
unnormalized distribution
constant k
possible subject
limitation
kq
z
p
z
illustration
use
rejection sampling
task
gamma distribution gam
z|a
b
= baza−1 exp
−bz
γ
bell-shaped form
figure
suitable proposal distribution
cauchy
transformation method
cauchy
value
gamma distribution
uniform random variable y
z = b tan y + c
random numbers
z
q
z
+
− c
minimum reject rate
−
b2
constant k
requirement kq
z
� p
z
comparison function
figure
adaptive rejection
many instances
rejection sampling
difﬁcult
suitable analytic form
envelope distribution q
z
alternative approach
envelope function
ﬂy
mea- sured values
distribution p
z
gilks
wild
construction
envelope function
cases
p
z
log con- cave
other words
ln p
z
derivatives
functions
z
construction
suitable envelope function
figure
function ln p
z
gradient
initial set
grid points
intersections
tangent lines
envelope function
sample value
envelope distribution
log
envelope distribution
succession exercise
figure
case
distributions
log concave
envelope function
use
rejection sampling
tangent lines
set
grid points
sample point
set
grid points
envelope distribution
ln p
z
z1 z2 z3
sampling methods figure
plot
gamma distribu- tion
green curve
scaled cauchy pro- posal distribution
red curve
samples
gamma distribution
cauchy
rejection sam- pling criterion
p
z
algorithms
figure
illustrative example
rejection
gaussian distribution p
z
green curve
rejection
proposal distri- bution q
z
scaled version kq
z
red curve
p
z
−5
z
linear functions
envelope distribution
piecewise exponential distribution
form q
z
−λi
z − zi−1
zi−1 < z � zi
sample
usual rejection criterion
sample
draw
desired distribution
sample
set
grid points
new tangent line
envelope function
number
grid points increases
envelope function
approximation
desired distribution p
z
probability
rejection decreases
variant
algorithm
evaluation
derivatives
gilks
adaptive rejection
framework
distri- butions
rejection
step
metropolis-hastings step
section
rise
adaptive rejection metropolis sampling
gilks
clearly
rejection sampling
practical value
com- parison function
required distribution
rate
rejection
rejection
spaces
high dimensionality
sake
illustration
artiﬁcial problem
zero-mean mul- pi
unit matrix
tivariate gaussian distribution
covariance
rejection
proposal distribution
zero-mean gaussian p
order
distribution
covariance σ2
k
kq
z
� p
z
d-dimensions
optimum value
k
k =
σq/σp
d
d =
figure
acceptance rate
ratio
volumes
p
z
z
distribu- tions
acceptance rate diminishes
dimensionality
σq exceeds
percent
d =
acceptance ratio
illustrative example
comparison function
required distribution
practical exam- ples
desired distribution
good proposal distribution
comparison function
qi
σ2 q �
sampling methods figure
importance sampling
prob- lem
expectation
func- tion f
z
respect
distribution p
z
samples
samples
z
l
simpler distribution q
z
corresponding terms
summation
ratios p
z
l
/q
z
l
p
z
q
z
f
z
z  l furthermore
exponential decrease
acceptance rate
dimensionality
generic feature
rejection
rejection
useful technique
dimensions
problems
high dimensionality
role
subroutine
sophisticated algorithms
high dimensional spaces
importance
principal reasons
complicated probability distributions
expectations
form
technique
importance
framework
expectations
mechanism
samples
distribution p
z
ﬁnite sum approximation
expectation
samples
distribution p
z
suppose
p
z
p
z
value
z
simplistic strategy
expectations
z-space
uniform grid
integrand
sum
form e [
]  l=1 p
z
l
f
z
l
obvious problem
approach
number
terms
summation
dimensionality
z
kinds
probability distributions
interest
mass
small regions
z space
uniform sampling
high-dimensional problems
small proportion
samples
signiﬁcant contribution
sum
sample points
regions
p
z
product p
z
f
z
case
rejection sampling
importance sampling
use
proposal distribution q
z
samples
figure
expectation
form
ﬁnite sum
           zq zp
l =
l =  rl          f
z
l l=1 l=1 l=1 l l l  
zq
l   p
z
 p
z
e [
] = f
z
p
z
dz = zq zp q
z
q
z
dz rlf
z
l
samples
z
l
q
z
e [
] =
algorithms f
z
p
z
dz f
z
p
z
q
z
q
z
dz p
z
l
q
z
l
f
z
l
quantities
= p
z
l
/q
z
l
importance weights
bias
wrong distribution
note
rejection sampling
samples
case
distribution p
z
normalization constant
p
z
= p
z
whereas zp
importance
distribution q
z
= q
z
/zq
same property
z
/zp
rl = p
z
l
/
zp/zq
result q
z
l
same sample
ratio p
z
dz = q
z
q
z
dz zp zq = 
wl = e [
]  wlf
z
l
rm m p
z
l
/q
z
l
p
z
m
/q
z
m
rejection sampling
success
importance
approach
distribution q
z
    
sampling methods distribution p
z
case
p
z
f
z
sig- niﬁcant proportion
mass
small regions
z space
set
importance weights
rl
few weights
large values
weights
effective sample size
apparent sample size l.
prob- lem
none
samples
regions
p
z
f
z
case
apparent variances
rl
rlf
z
l
estimate
expectation
major draw- back
importance
method
results
error
diagnostic indication
key re- quirement
distribution q
z
regions
p
z
distributions
terms
graphical model
impor- tance
technique
various ways
discrete variables
simple approach
uniform sampling
joint distribution
directed graph
sample
joint distribution
variables
evidence
observed values
variables
uniform distribution
space
possible instantiations
corresponding weight associ- q
z
sample z
l
distribution p
z
x
subset
possible choices
z
variables
equality
fact
sample z
evidence
weights rl
z
note
variables
order
approach
poor results
posterior distribution
case
practice
p
z|x
improvement
approach
likelihood
sampling
fung
chang
shachter
peot
ancestral sampling
variables
variable
turn
variable
evidence set
instantiated value
evidence set
conditional distribution p
zi|pai
conditioning variables
values
weighting
sample z
r
z
= p
zi|pai
p
zi|pai
zi∈e zi∈e p
zi|pai
= zi∈e p
zi|pai
method
self-importance sampling
shachter
peot
importance
distribution
posterior distribution
rejection
method
section
depends
part
success
determination
suitable value
constant k.
many pairs
distributions
z
z
suitable       
    
algorithms
value
k
value
bound
distribution
small acceptance rates
case
rejection sampling
sampling-importance-resampling
sir
approach
use
sampling distribution q
z
de- termine
constant k.
stages
scheme
ﬁrst stage
l
z
z
l
q
z
second stage
weights w1
second set
l samples
discrete distribution
z
z
l
probabilities
weights
w1
wl
l samples
p
z
distribution
limit l → ∞
univariate case
cumulative distribution
resampled values
p
= = wl l
z
l
l
z
l
p
z
l
/q
z
l
p
z
l
/q
z
l
l
indicator function
argument
limit l → ∞
suitable regularity
dis- tributions
sums
integrals
original sampling distribution q
z
p
z
/q
z
q
z
dz p
z
/q
z
q
z
dz
p
z
dz p
z
dz p
= = =
p
z
dz
cumulative distribution function
p
z
again
normal- ization
p
z
ﬁnite value
l
initial sample set
resampled values
desired distribution
rejection sam- pling
approximation
distribution q
z
distribution p
z
q
z
= p
z
initial samples
z
z
l
distribution
weights
resampled values
distribution
moments
respect
distribution p
z
    l
   [ l  =
sampling methods
original samples
weights
e [ f
z
= f
z
p
z
dz f
z
[ p
z
/q
z
] q
z
dz p
z
/q
z
] q
z
dz wlf
zl
em algorithm
addition
mechanism
direct implementation
bayesian framework
monte carlo methods
role
frequentist paradigm
example
maximum likelihood solutions
methods
e step
em algorithm
models
e step
model
hidden variables z
variables x
parameters
function
respect
m step
expected complete-data log likelihood
q
θ
θold
p
z|x
θold
ln p
z
x|θ
dz
methods
ﬁnite sum
sam- ples
z
l
current estimate
posterior distribution p
z|x
θold
q
θ
θold
l ln p
z
l
x|θ
l=1
q function
usual way
m step
procedure
monte carlo em algorithm
problem
mode
posterior distribution
θ
map estimate
prior distribution p
θ
ln p
θ
function q
θ
θold
m step
particular instance
monte carlo em algorithm
stochastic em
ﬁnite mixture model
sample
e step
latent variable z
k components
mixture
data point
e step
sample
z
posterior distribution p
z|x
θold
x
data set
hard assignment
data point
components
mixture
m step
approximation
posterior distribution
model parameters
usual way
   l
markov chain monte carlo
maximum likelihood approach
full bayesian treatment
posterior distribution
param- eter vector θ
principle
samples
joint posterior p
z|x
suppose fur- ther
complete-data parameter posterior p
x
data augmentation algorithm
nates
steps
i-step
imputation step
e step
p-step
posterior step
m step
ip algorithm i-step
p
z|x
relation p
z|x
p
z|θ
x
p
θ|x
dθ
hence
l =
l
sample θ
l
current esti- mate
p
θ|x
sample z
l
p
z|θ
l
x
p-step
relation p
θ|x
p
x
p
z|x
dz
samples
z
l
i-step
revised estimate
posterior distribution
θ
p
θ|x
l p
θ|z
l
x
l=1
assumption
approximation
i-step
note
distinction
parameters θ
hidden variables z
from
distinction
problem
samples
posterior distribution
markov chain monte carlo
previous section
rejection sampling
importance sam- pling strategies
expectations
functions
severe limitations
spaces
high dimensionality
section
powerful framework
markov chain monte carlo
mcmc
large class
distributions
    
sampling methods section
a
z
z
τ
p
z
p
z
τ
dimensionality
sample space
markov chain monte carlo methods
origins
physics
metropolis
ulam
end
1980s
signiﬁcant impact
ﬁeld
statistics
rejection
importance sampling
proposal distribution
time
record
current state z
τ
proposal distribution q
z|z
τ
current state
sequence
samples z
z
markov chain
again
p
z
= p
z
/zp
p
z
value
z
value
zp
proposal distribution
samples
cycle
algorithm
candidate sample z
proposal distribution
sample
appropriate criterion
basic metropolis algorithm
metropolis
proposal distribution
za|zb
= q
zb|za
values
za
zb
candidate sample
probability
random number u
uniform distribution
unit interval
sample
a
z
z
τ
note
step
z
τ
causes
increase
value
p
z
candidate point
candidate sample
z
τ +1
= z
candidate point z
z
τ +1
τ
candidate sample
distribution q
z|z
τ +1
contrast
rejection sampling
re- jected samples
metropolis algorithm
candidate point
previous sample
ﬁnal list
samples
multiple copies
samples
course
practical implementation
single copy
retained sample
integer
factor
many times
state
q
za|zb
values
za
zb
sufﬁcient
necessary condition
distribution
z
τ
z
τ → ∞
sequence z
z
set
independent samples
p
z
successive samples
independent samples
sequence
re- tain
m th sample
m
samples
practical purposes
figure
simple illustrative exam- ple
two-dimensional gaussian distribution
metropolis algorithm
proposal distribution
isotropic gaussian
further
nature
markov chain monte carlo algorithms
properties
speciﬁc example
simple random
markov chain monte carlo
figure
a simple illustration
metropo- lis algorithm
gaussian distribution
standard-deviation contour
ellipse
proposal distribu- tion
isotropic gaussian distri- bution
standard deviation
steps
green lines
steps
total
candidate samples
exercise
walk
state space z consisting
integers
probabilities
z
τ +1
= z
τ
p
z
τ +1
= z
τ
p
z
τ +1
= z
τ
=
z
τ
state
step τ
initial state
z
=
sym- metry
state
time τ
e [ z
τ
e [
z
τ
] = τ /2
τ steps
random walk
distance
average
square root
τ
square root dependence
random
behaviour
random walks
state space
central goal
markov chain monte carlo methods
random
behaviour
markov chains
markov chain monte carlo methods
detail
general properties
markov chains
detail
circumstances
markov chain converge
dis- tribution
ﬁrst-order markov chain
series
random variables
z
m
following conditional independence property
m ∈
m
p
z
m+1
|z
z
m
p
z
m+1
|z
m
course
directed graph
form
chain
ex- ample
figure
markov chain
probability distribution
initial variable p
z
   z z
m
k  
sampling methods conditional probabilities
subsequent variables
form
transition probabil- ities tm
z
m
m+1
p
z
m+1
|z
m
markov chain
transition probabilities
marginal probability
particular variable
terms
marginal probability
previous variable
chain
form p
z
m+1
p
z
m+1
|z
m
p
z
m
distribution
respect
markov chain
step
chain
distribution invariant
homogeneous markov chain
transition probabilities t
z
z
distribution p
z
p
z
= t
z
z
p
z
note
markov chain
invariant distribution
instance
transition probabilities
identity transformation
distribution
sufﬁcient
condition
required distribu- tion p
z
transition probabilities
property
detailed balance
p
z
t
z
z
= p
z
t
z
z
particular distribution p
z
transition probability
detailed balance
respect
particular distribution
distribution invariant
p
z
t
z
z
z z p
z
t
z
z
= p
z
p
z|z
= p
z
z a markov chain
detailed balance
goal
markov chains
distribution
markov chain
desired distribution
m → ∞
distribution p
z
m
invariant distribution p
z
choice
initial dis- tribution p
z
property
ergodicity
invariant distribution
equilibrium distribution
clearly
ergodic markov chain
equilibrium distribution
homogeneous markov chain
weak restrictions
invariant distribution
transition probabilities
neal
practice
transition probabilities
set
‘ base ’ transitions b1
bk
mixture distribution
form t
z
z
= αkbk
z
z
k=1
      
markov chain monte carlo
set
coefﬁcients
αk =
base transitions
successive application
t
z
z
=
z1 zn−1 b1
z
z1
bk−1
zk−2
zk−1
bk
zk−1
z
distribution
respect
base transitions
respect
t
z
z
case
mixture
base transitions sat- isﬁes detailed balance
mixture transition t
detailed bal- ance
transition probability
order
application
base transitions
form b1
b2
bk
bk
b2
b1
detailed balance
common ex- ample
use
composite transition probabilities
base transition
subset
variables
metropolis-hastings
basic metropolis algorithm
demon- strating
required distribution
proof
generalization
metropolis-hastings algorithm
hast- ings
case
proposal distribution
symmetric function
arguments
step
algorithm
cur- rent state
z
τ
sample z
distribution qk
z|z
τ
probability ak
z
zτ
ak
z
z
τ
p
z
qk
z
τ
|z
p
z
τ
qk
z|z
τ
labels
members
set
possible transitions
again
evaluation
acceptance criterion
knowledge
normal- izing constant zp
probability distribution p
z
= p
z
/zp
symmetric proposal distribution
metropolis-hastings criterion
reduces
stan- dard metropolis criterion
p
z
invariant distribution
markov chain
metropolis-hastings algorithm
detailed balance
using
z
qk
z|z
ak
z
z
= min
p
z
qk
z|z
z
qk
z|z
min
p
z
qk
z|z
p
z
qk
z|z
p
z
qk
z|z
ak
z
z
speciﬁc choice
proposal distribution
marked effect
performance
algorithm
continuous state spaces
common choice
current state
important trade-off
variance parameter
distribution
variance
sampling methods figure
schematic illustration
use
isotropic gaussian proposal distribution
blue circle
correlated multivariate gaussian distribution
red ellipse
different stan- dard deviations
different directions
metropolis-hastings algorithm
order
rejection rate
scale ρ
proposal distribution
order
standard deviation σmin
behaviour
number
steps
states
order
σmax/σmin
σmax
standard deviation
σmax ρ proportion
accepted transitions
state space
form
slow random walk
long correlation times
variance parameter
rejection rate
kind
complex problems
steps
states
probability p
z
multivariate distribution p
z
strong correlations
components
z
fig- ure
scale ρ
proposal distribution
high rejection rates
ρ
same order
length scale σmin
system
distribution
extended direction
means
random walk
number
steps
state
original state
order
σmax/σmin
fact
dimensions
increase
rejection rate
ρ increases
steps sizes
transitions
multivariate
number
steps
independent samples scales
σmax/σ2
σ2
second-smallest stan- dard deviation
neal
details
case
length scales
distributions
different directions
metropolis hastings algorithm
slow convergence
gibbs sampling gibbs sampling
geman
geman
applicable markov chain monte carlo algorithm
special case
metropolis- hastings algorithm
distribution p
z
= p
z1
zm
initial state
markov chain
step
gibbs sampling procedure
value
variables
value drawn
distribution
values
variables
value drawn
distribution p
zi|z\i
zi
ith component
z
z\i denotes
zm
zi
procedure
variables
gibbs
particular order
step
random
distribution
example
distribution p
z1
z2
z3
variables
conditional distri-
step
algorithm
values z
τ
z
τ
bution
new value z
τ +1
z
τ
z
τ
p
z1|z
τ
z
τ
conditional
z
τ
distribution
value z
τ +1
new value
z1
steps
z3
sample z
τ +1
drawn
p
z2|z
τ +1
τ
p
z3|z
τ +1
z
τ +1
variables
turn
gibbs
zi
i
m
τ =
t
– sample z
τ +1
sample z
τ +1
p
z1|z
τ
p
z2|z
τ +1
z
τ
z
τ
m
z
τ
τ
m
– sample z
τ +1
j ∼ p
zj|z
τ +1
z
τ +1
j−1
z
τ
j+1
z
τ
m
– sample z
τ +1
m ∼ p
zm|z
τ +1
z
τ +1
z
τ +1
m−1
josiah willard gibbs
gibbs
entire life liv- ing
house
father
new haven
connecticut
gibbs
ﬁrst phd
engineering
united states
ﬁrst chair
mathematical physics
united states
yale
post
time
publications
ﬁeld
vector analysis
contri- butions
planetary orbits
famous work
ontheequilibriumofhet- erogeneous substances
foundations
science
physical chemistry
sampling methods
procedure
required distribution
note
distribution p
z
invariant
gibbs
steps
whole markov chain
fact
p
z\i
marginal distribution p
z\i
value
z\i
step
deﬁnition samples
correct conditional distribution p
zi|z\i
marginal distributions
joint distribution
joint distribution
second requirement
order
gibbs
proce- dure samples
correct distribution
sufﬁcient condition
ergodicity
none
conditional distributions
zero
case
point
z space
other point
ﬁnite number
steps
update
component variables
requirement
conditional distributions
ergodicity
distribution
initial states
order
algorithm
samples
many iterations
distribution
course
successive samples
markov chain
samples
sequence
gibbs
procedure
particular instance
metropolis-hastings algorithm
metropolis-hastings sampling step
variable zk
variables
transition probability
z
z
qk
z|z
= p
z k|z\k
\k = z\k
components
sampling
z step
p
z
= p
zk|z\k
p
z\k
factor
acceptance probability
metropolis-hastings
k|z \k
p
z p
z p
zk|z\k
p
z\k
p
z \k
p
zk|z \k
k|z\k
a
z
z
= p
z
qk
z|z
p
z
qk
z|z
z
\k = z\k
metropolis-hastings steps
metropolis algorithm
insight
behaviour
gibbs
application
gaussian distribution
variables
figure
con- ditional distributions
width l
marginal distributions
width l.
typical step size
conditional distributions
order l.
state
random walk
number
steps
independent samples
distribution
order
l/l
course
gaussian distribution
gibbs sampling procedure
simple problem
coordinate sys- tem
order
variables
practical applications
such transformations
approach
behaviour
gibbs sampling
over-relaxation
adler
original form
problems
z2 figure
illustration
gibbs
alter- nate updates
variables
distribution
correlated gaussian
step size
stan- dard deviation
conditional distri- bution
green curve
o
l
lead- ing
progress
direction
elongation
joint distribution
red ellipse
number
steps
independent sample
distribution
o
l/l
gibbs
l l
conditional distributions
gaussian
general class
distributions
multivariate gaussian
example
non-gaussian distribution p
z
y
∝ exp
−z2y2
gaussian conditional distributions
step
gibbs sampling algorithm
conditional distribution
particular component zi
mean µi
variance σ2 i
over-relaxation frame- work
value
zi
zi = µi + α
zi − µi
σi
− α2 i
ν
gaussian random
zero mean
unit variance
α
parameter
< α <
α =
method
gibbs sampling
α <
step
opposite side
mean
step
desired distribution invariant
zi
µi i
zi
effect
over-relaxation
σ2
motion
state space
variables
framework
ordered over-relaxation
neal
approach
non- gaussian distributions
practical applicability
gibbs
depends
ease
samples
conditional distributions
zk|z\k
case
probability distributions
graphical models
conditional distribu- tions
individual nodes
variables
corresponding markov blankets
figure
directed graphs
wide choice
condi- tional distributions
individual nodes
parents
conditional distributions
gibbs sampling
log concave
adaptive re- jection
methods
section
framework
monte carlo
graphs
broad applicability
graph
distributions
exponential family
parent-child relationships
conjugacy
full conditional distri- butions
gibbs sampling
same functional form
sampling methods figure
gibbs
method
samples
conditional distribution
variable condi-
variables
graphical models
conditional distribution
function
states
nodes
markov blanket
undirected graph
set
neighbours
left
directed graph
markov blanket
parents
children
co-parents
right
inal conditional distributions
parents
node
techniques
full conditional distributions
complex form
use
standard sam- pling algorithms
conditionals
log concave
adaptive rejection sampling
corresponding variable
stage
gibbs sampling algorithm
sample
corresponding conditional distribution
point estimate
maximum
conditional distribution
iterated conditional modes
icm
algorithm
section
icm
greedy approximation
gibbs sampling
basic gibbs sampling technique
variable
time
strong dependencies
successive samples
opposite extreme
samples
joint distribution
operation
successive samples
simple gibbs sampler
intermediate strategy
groups
variables
individual vari- ables
blocking gibbs
algorithm
blocks
variables
variables
block
turn
variables
jensen
slice
difﬁculties
metropolis algorithm
sensi- tivity
size
result
slow decorrelation
behaviour
result
inefﬁciency
high rejec- tion rate
technique
slice sampling
neal
adaptive step size
characteristics
distribution
unnormalized distribution p
z
univariate case
slice
involves
z
additional variable u
samples
z
u
space
example
approach
hybrid monte carlo
section
goal
area
distribution e ˜p
z
u z
τ
e    z      
z
slice
˜p
z
zmin zmax u z
τ
b
value z
τ
value
p
z
τ
‘ slice ’
distribution
solid horizontal
b
slice
new sample
z
region figure
illustration
slice sampling
region
� u � lines
zmin � z � zmax
previous value z
τ
p
z
u
� u �
p
z
zp = p
z
dz
marginal distribution
z
p
z
u
du = p
z
du = p
z
zp = p
z
p
z
range
� u � p
z
>
figure
p
z
p
z
u
u values
z
u
value
z p
z
sample z
‘ slice ’
distribution
z
practice
slice
distribu- tion
sampling scheme
uniform distribution p
z
u
invariant
detailed balance
satisﬁed
current value
z
z
τ
corresponding sample u
next value
z
region zmin � z � zmax
z
τ
choice
region
adap- tation
characteristic length scales
distribution
place
region
slice
large moves
z space
region
slice
approach
choice
region
region
z
τ
width w
end points
slice
end point
region
direction
increments
value w
end point
region
candidate value z
region
slice
z
τ +1
slice
region
z forms
end point
region
z
τ
sampling methods candidate point
region
value
z
slice
slice
distributions
variable
turn
manner
gibbs
component zi
function
zi|z\i
hybrid monte carlo algorithm
major limitations
metropolis algorithm
behaviour
distance
state space
square root
number
steps
problem
steps
high rejection rate
section
sophisticated class
transitions
analogy
physical systems
property
large changes
system state
rejection probability
distributions
continuous variables
gradient
log probability
respect
state variables
dynamical systems framework
section
section
metropolis algorithm
pow- erful hybrid monte carlo algorithm
background
physics
section
key results
ﬁrst principles
dynamical systems
dynamical approach
stochastic sampling
origins
algorithms
behaviour
physical systems
hamiltonian dynam- ics
markov chain monte carlo simulation
goal
probability distribution p
z
framework
hamiltonian dynamics
probabilistic simulation
form
hamiltonian system
order
literature
area
use
relevant dynamical systems terminology
appropriate
dynamics
corresponds
evolution
state variable z =
zi
continuous time
τ
classical dynamics
newton ’
second law
motion
acceleration
object
force
second-order differential equa- tion
time
second-order equation
ﬁrst- order equations
intermediate momentum variables
rate
change
state variables z
components ri = dzi dτ
zi
position variables
dynamics
thus 
e
z
potential energy
system
state z
system acceleration
rate
change
momentum
applied force
negative gradient
potential energy dri dτ = − ∂e
z
∂zi
hybrid monte carlo algorithm
position variable
corresponding momentum variable
joint space
position
momentum variables
phase space
loss
generality
probability distribution p
z
form p
z
zp exp
−e
z
dynamical system
hamiltonian framework
kinetic energy
k
r
=
r2 i
i
total energy
system
sum
kinetic energies h
z
r
= e
z
+ k
r
exercise
h
hamiltonian function
using
dynamics
system
terms
hamiltonian equa- tions
dzi dτ dri dτ = ∂h ∂ri = − ∂h ∂zi
william hamilton
william rowan hamilton
irish mathematician
physicist
child prodigy
pointed professor
astronomy
trinity college
dublin
be- fore
hamilton ’
important contributions
new formulation
dynamics
signiﬁcant role
later development
quantum mechanics
other great achievement
development
quaternions
concept
complex numbers
distinct square roots
minus
i2 = j2 = k2 = ijk = −1
equations
royal canal
dublin
wife
october
equations
side
broome bridge
evidence
carving
stone plaque
bridge
discovery
quaternion equations
   i
sampling methods
evolution
dynamical system
value
hamiltonian h
differentiation ∂h ∂zi dzi dτ + ∂h ∂ri dri dτ dh dτ = = ∂h ∂zi ∂h ∂ri − ∂h ∂ri ∂h ∂zi
second important property
hamiltonian dynamical systems
li- ouville ’ s theorem
volume
phase space
other words
region
space
variables
z
r
region
equations
hamiltonian dynamics
shape
volume
ﬂow ﬁeld
rate
change
location
phase space
dz dτ
divergence
ﬁeld vanishes v =
dr dτ
div v = ∂ ∂zi dzi dτ dri dτ + ∂ ∂ri + ∂ ∂ri = i
∂zi − ∂h ∂ri ∂h ∂zi
joint distribution
phase space
total energy
distribution
p
z
r
zh exp
−h
z
r
results
conservation
volume
conservation
h
hamiltonian dynamics
p
z
r
invariant
small region
phase space
h
evolution
hamiltonian equations
ﬁnite time
volume
region
value
h
region
probability density
function
h
h
values
z
r
hamiltonian dynamics
ﬁnite time duration
large changes
systematic way
avoids
behaviour
evolution
hamiltonian dynamics
p
z
r
value
h
order
ergodic sampling scheme
additional moves
phase space
value
h
distribution p
z
r
invariant
way
value
r
drawn
distribution
z
gibbs
step
           exercise
hybrid monte carlo algorithm
section
desired distribution invariant
z
r
distribution p
z
r
conditional distribution p
r|z
practical application
approach
problem
numerical integration
hamiltonian equations
numerical errors
scheme
impact
such errors
fact
integration schemes
liouville ’ s theorem
property
hybrid monte carlo algorithm
section
scheme
leapfrog discretization
updat- r
position
momentum variables
discrete-time approximations
z
ri
τ + /2
zi
τ +
ri
τ +
=
z
τ
ri
τ
∂zi ri
τ + /2
zi
τ
∂e ri
τ + /2
z
τ +
form
half-step update
momentum variables
step size /2
full-step update
position variables
step size
second half-step update
momentum variables
several leapfrog steps
succession
half-step updates
momentum variables
full-step updates
step size
successive updates
position
momentum variables
order
dynamics
time interval τ
τ / steps
error
discretized approximation
continuous time dynamics
smooth function e
z
limit
nonzero
practice
residual error
section
effects
such errors
hybrid monte carlo algorithm
hamiltonian dynamical approach
be- tween
series
leapfrog updates
resampling
momentum variables
marginal distribution
note
hamiltonian dynamics method
basic metropolis algo- rithm
use
information
gradient
log probability distribution
distribution
analogous situation
domain
function optimization
cases
gradient informa- tion
use
fact
space
dimension d
additional computational cost
gradient
function
ﬁxed factor
d
d-dimensional gradient vector conveys d pieces
information
piece
information
function
sampling methods
hybrid monte carlo
previous section
nonzero step size
discretiza- tion
leapfrog algorithm
errors
integration
hamil- tonian dynamical equations
hybrid monte carlo
duane
neal
combines hamiltonian dynamics
metropolis algorithm
thereby
bias
discretization
algorithm
markov chain
alternate stochastic updates
momentum variable r
hamiltonian dynamical updates
leapfrog algorithm
application
leapfrog algorithm
candidate state
metropolis criterion
value
hamiltonian h. thus
z
r
initial state
z
r
state
leapfrog integration
candidate state
probability min
h
z
r
− h
z
r
leapfrog integration
hamiltonian dynamics
such candidate step
value
h
numerical errors
value
h
metropolis criterion
bias
effect
resulting samples
required dis- tribution
order
case
update equations
leapfrog integration satisfy
balance
leapfrog scheme
start
leapfrog integration sequence
random
equal probability
forwards
time
step size
backwards
time
step size −
leapfrog integration scheme
integration
l steps
step size −
effect
integration
l steps
step size
leapfrog integration
phase-space volume
fact
step
leapfrog scheme
zi variable
ri variable
amount
function
figure
effect
region
phase space
volume
results
detailed balance
small region r
phase space
sequence
l leapfrog iterations
step size
maps
region r
conservation
volume
leapfrog iteration
r
volume δv
r
initial point
distribution
l leapfrog interactions
probability
transition
r
r
zh exp
−h
r
exp
−h
r
+ h
r
factor
arises
probability
positive step size
negative one
probability
hybrid monte carlo algorithm
ri zi
zh exercise
ri   figure
step
leapfrog algorithm
–
modiﬁes
position variable zi
momentum variable ri
change
variable
function
region
phase space
change
volume
zi region r
backwards
time
region r
exp
−h
r
exp
−h
r
+ h
r
probabilities
detailed balance
note
proof
overlap
regions r
r
such overlap
examples
leapfrog algorithm returns
position
ﬁnite number
iterations
such cases
random replacement
momentum values
leapfrog integration
ergodicity
position variables
such phenomena
magnitude
step size
random
small interval
leapfrog integration
insight
behaviour
hybrid monte carlo algo- rithm
application
multivariate gaussian
convenience
gaussian distribution p
z
independent components
hamiltonian
h
z
r
i
σ2 i
r2 i
i
conclusions
gaussian distribution
components
hybrid monte carlo
exhibits rotational isotropy
leapfrog integration
pair
phase-space variables
acceptance
rejection
candidate point
value
h
values
variables
signiﬁcant integration error
variables
high prob- ability
rejection
order
discrete leapfrog integration
sampling methods good approximation
true continuous-time dynamics
leapfrog integration scale
length-scale
potential
value
σi
σmin
goal
leapfrog integration
hybrid monte carlo
substantial distance
phase space
new state
initial state
high probability
acceptance
order
leapfrog integration
number
iterations
order σmax/σmin
contrast
behaviour
simple metropolis algorithm
isotropic gaussian proposal distribution
variance s2
order
high rejection rates
value
s
order σmin
exploration
state space
proceeds
random walk
order
σmax/σmin
steps
independent state
   
partition function
algorithms
chapter re- quire
functional form
probability distribution
multiplicative constant
pe
z
ze exp
−e
z
value
normalization constant ze
partition func- tion
order
samples
p
z
knowledge
value
ze
bayesian model comparison
model evidence
probability
observed data
model
interest
value
direct evaluation
integrating
function exp
−e
z
state space
z
model comparison
ratio
partition functions
models
multiplication
ratio
ratio
prior probabilities
ratio
posterior probabilities
model selection
model
way
ratio
partition functions
importance
distribution
energy function g
z
z exp
−e
z
z exp
−g
z
z exp
−e
z
+ g
z
exp
−g
z
ze zg = z exp
−g
z
= eg
z
[ exp
−e + g
 exp
−e
z
l
g
z
l
 l
z
l
samples
distribution
pg
z
dis- tribution pg
partition function
example
gaussian
absolute value
ze
approach
accurate results
importance
distri- bution pg
distribution pe
ratio pe/pg
wide variations
practice
speciﬁed importance
distributions
kinds
complex models
book
alternative approach
samples
markov chain
importance-sampling distribution
transition probability
markov chain
t
z
z
sample set
z
z
l
distribution
zg exp
−g
z
t
z
l
z
l=1
partition function
methods
ratio
partition functions
suc- cess
corresponding distributions
absolute value
partition function
complex distribution
simple distributions
partition function
ratio
partition functions
problem
technique
chaining
neal
barber
bishop
succession
intermediate distributions p2
interpolate
simple distribution p1
z
normalization coefﬁcient z1
complex distribution pm
z
zm z1 = z2 z1 z3 z2 ··· zm zm−1
intermediate ratios
monte carlo methods
way
sequence
intermediate systems
energy function
continuous parameter
� α
distributions eα
z
=
− α
e1
z
+ αem
z
intermediate ratios
monte carlo
efﬁcient
single markov chain run
markov chain
ratio
case
markov chain
system p1
suitable number
steps moves
next distribution
sequence
note
system
equilibrium distribution
stage
   
sampling methods

www show
ﬁnite sample estimator f
e [ f ]
variance

suppose
z
random
uniform distribution
y = h−1
z
h
y
y
distribution p
y

random variable z
trans- formation y = f
z
y
cauchy distribution
 
z1
z2
unit circle
figure
change
variables
show
y2

www let z
d-dimensional random
gaussian distribu- tion
zero mean
unit covariance matrix
positive deﬁnite symmetric matrix σ
cholesky decomposition σ = llt
l
lower- triangular matrix
zeros
diagonal
show
variable y = µ + lz
gaussian distribution
mean µ
covariance σ
technique
samples
general multivariate gaus-
samples
zero mean
unit variance
p
z
/kq
z
 
www
exercise
rejection sampling
samples
distribution p
z
proposal dis- tribution
z
probability
sample value z
p
unnormalized distribution
z
constant k
value
z
� p
z
values
z
note
probability
value z
probability
value
q
z
probability
value
make use
sum
product rules
probability
normalized form
distribution
z
p
z

suppose
z
uniform distribution
interval [
]
show
variable y = b tan z + c
cauchy distribution
 
determine expressions
coefﬁcients
envelope distribution
adaptive rejection
requirements
continuity
nor- malization
 
use
technique
section
single exponential distribution
algorithm
piecewise exponential distribution

show
simple random walk
integers
property
e [
z
τ
] = e [
z
τ−1
] +
induction
e [
z
τ
] = τ /2
figure
a probability distribution
variables
z2
shaded regions
z2
z1
 
show
gibbs sampling algorithm
section
satisﬁes detailed balance

distribution
figure
discuss
standard gibbs sampling procedure
distribution
distribution
 
simple 3-node graph
figure
observed node x
gaussian distribution n
x|µ
τ−1
mean µ
precision τ
marginal distributions
mean
precision
n
µ|µ0
s0
gam
τ|a
b
gam
·|·
·
gamma distribution
write
expressions
conditional distributions
τ
p
µ
order
gibbs
posterior distribution p
τ|x

over-relaxation update
zi
µi
variance σi
ν
unit variance
value zi
mean µi
variance σ2 i

www using
hamiltonian equation
show

use
conditional dis- tribution p
r|z
figure
a graph
observed gaussian variable x
prior distributions
mean µ
precision τ. µ τ
sampling methods

www verify
probabilities
detailed balance
hybrid monte carlo algorithm
sequential data so
book
sets
data points
i.i.d.
assumption
likelihood function
product
data points
prob- ability distribution
data point
many applications
i.i.d
assumption
important class
such data sets
sequential data
measurement
time series
example
rainfall measurements
suc- cessive days
particular location
daily values
currency exchange rate
acoustic features
successive time frames
speech recognition
example
speech data
figure
sequential data
contexts
time series
example
sequence
nucleotide base pairs
strand
dna
sequence
characters
english sentence
convenience
past ’
‘ future ’ observations
sequence
models
chapter
sequential data figure
example
spectro- gram
spoken words “ bayes ’ theo- rem ”
plot
intensity
spectral coefﬁcients
time index
forms
sequential data
temporal sequences
nonstationary sequential dis- tributions
stationary case
data evolves
time
distribution
complex nonstationary situa- tion
generative distribution
time
stationary case
many applications
ﬁnancial forecasting
pre- dict
next value
time series
observations
previous values
in-
recent observations
historical observations
future values
example
figure
successive observations
speech spectrum
general dependence
future observations
previous observations
complexity
model
limit
number
observations increases
markov models
future predictions
inde-  n pendent
recent observations
such models
general framework
tractability
introduction
latent variables
state space models
chapters
complex models
simpler components
distributions
exponential family
framework
probabilistic graphical models
important examples
state space models
hid- den markov model
latent variables
linear dynamical systems
latent variables
models
graphs
tree structure
loops
inference
sum-product algorithm
markov models figure
simplest approach
sequence
ob- servations
correspond- ing
graph
links
markov models
x3 x4
way
sequential data
sequential aspects
observations
i.i.d.
graph
figure
approach
sequential patterns
data
correlations
observations
sequence
suppose
instance
particular day
time series
recent observations
next day
data
i.i.d.
only information
data
relative frequency
rainy days
practice
weather
trends
several days
today
signiﬁcant help
tomorrow
such effects
probabilistic model
i.i.d
as- sumption
ways
markov model
first
loss
generality
product rule
joint distribution
sequence
observations
form p
xn
= p
xn−1
n=1
conditional distributions
right-hand side
previous observations
ﬁrst-order markov chain
graphical model
figure
  n n
sequential data figure
a ﬁrst-order markov chain
ob- servations
xn
dis- tribution p
xn|xn−1
particu- lar observation xn
value
previous ob- servation xn−1
x2 x3 x4 section
exercise
joint distribution
sequence
n observations
model
p
xn
= p
x1
p
xn|xn−1
d-separation property
conditional distribution
observa- tion xn
observations
time n
p
xn−1
= p
xn|xn−1
direct evaluation
prod- uct rule
probability
model
next observation
sequence
distribution
predictions
value
observation
observations
applications
such models
conditional distributions
xn|xn−1
model
assump- tion
stationary time series
model
homogeneous markov chain
instance
conditional distributions
adjustable parameters
values
set
data
condi- tional distributions
chain
share
same values
parameters
independence model
re- strictive
many sequential observations
trends
data
several successive observations
important information
predict-
next value
way
observations
inﬂuence
higher-order markov chains
predictions
previous-but-one value
second-order markov chain
graph
figure
joint distribution
p
xn
= p
x1
p
x2|x1
p
xn−2
n=3
again
d-separation
direct evaluation
conditional distri- bution
xn
xn−1
xn−2
observations x1
xn−3
figure
second-order markov chain
conditional distribution
particular observation xn
values
previous observations
xn−2
x2 x3 x4 figure
sequen- tial data
markov chain
latent variables
observation condi-
state
corresponding latent variable
important graphical structure
foundation
hidden markov model
linear dy- namical systems
z1 x1 z2
markov models
zn−1 xn−1 zn xn zn+1 xn+1
observation
previous observations
extensions
m th order markov chain
conditional distri- bution
particular variable depends
previous m variables
price
ﬂexibility
number
parameters
model
observations
discrete variables
k states
conditional distribution p
xn|xn−1
ﬁrst-order markov chain
set
k −
parameters
k states
xn−1
total
k
k −
parameters
model
m th order markov chain
joint distribution
conditionals p
xn|xn−m
xn−1
variables
conditional distri- butions
general conditional probability tables
number
parameters
model
km−1
k −
parameters
m
approach
values
m.
continuous variables
linear-gaussian conditional distributions
node
gaussian distribution
mean
linear function
parents
ar model
box
al.
thiesson
al.
alternative approach
parametric model
p
neural network
technique
tapped delay line
previous m values
observed variable
order
next value
number
parameters
general model
ex- ample
m
expense
restricted family
conditional distributions
suppose
model
sequences
markov assumption
order
limited number
free parameters
additional latent variables
rich class
models
simple components
mixture distributions
chapter
continuous latent variable models
chapter
observation
corresponding latent variable zn
different type
dimensionality
variable
latent variables
markov chain
rise
graphical structure
state space model
figure
key conditional independence property
zn−1
zn+1
indepen- dent
zn+1 ⊥⊥ zn−1 | zn
n p
xn|zn
n=1
 n
sequential data
joint distribution
model
p
z1
zn
= p
z1
n=2 p
zn|zn−1
d-separation criterion
path
variables xn
xm
latent variables
path
predictive distribution p
xn
observation xn+1
previous observations
conditional independence prop- erties
predictions
xn+1
previous observations
observed variables
markov property
order
predictive distribution
later sections
chap- ter
important models
sequential data
graph
latent variables
hidden markov model
hmm
elliott
note
observed variables
hmm
variety
different conditional distributions
latent
observed variables
linear-gaussian dependence
conditional distributions
parents
linear dynamical system
section
section
hidden markov models
hidden markov model
speciﬁc instance
state space model
figure
latent variables
single time slice
model
mixture distribution
component densities
p
x|z
extension
mixture model
choice
mixture com- ponent
observation
choice
component
previous observation
hmm
speech recognition
jelinek
rabiner
juang
natural language modelling
manning
sch¨utze
on-line handwriting recognition
nag
analysis
biological sequences
proteins
dna
krogh
durbin
al.
baldi
brunak
case
standard mixture model
latent variables
discrete multinomial variables
component
mixture
corresponding observation xn
again
1-of-k coding scheme
mixture models
chapter
probability distribution
zn
state
previous latent variable zn−1
conditional distribution p
zn|zn−1
latent variables
k-dimensional binary variables
conditional distribution corresponds
table
numbers
a
elements
transition probabilities
ajk ≡ p
znk =
j =
probabilities
� ajk
k ajk =
matrix a    j=1 k k k k=1  figure
transition diagram
model
la- tent variables
possible states
boxes
black lines
elements
transition matrix ajk
hidden markov models
a32 a23
=
a11 a22 a21 a12
=
k =
a31 a13 a33
k
k−1
independent parameters
conditional distribution
form p
zn|zn−1
a
= azn−1
j znk jk
initial latent node z1
parent node
marginal distribution p
z1
vector
probabilities
elements
≡ p
z1k =
p
z1|π
πz1k k k=1
k πk
transition matrix
states
nodes
state transition diagram
figure
case
k =
note
probabilistic graphical model
nodes
separate variables
states
single variable
states
boxes
circles
section
state transition diagram
kind
figure
time
alternative representation
transitions
latent states
lattice
trellis diagram
case
hidden markov model
figure
speciﬁcation
probabilistic model
con- ditional distributions
observed variables
φ
φ
set
pa- rameters
distribution
emission probabilities
example
gaussians
form
elements
x
continuous variables
conditional probability tables
x
xn
distribution p
φ
value
φ
vector
k numbers
k possible states
binary vector zn
k  n  n
=
k =
sequential data figure
state transition dia- gram
figure
time
lattice
trellis
representation
latent states
column
diagram
latent
zn
a11 a11 a11 a33
n
emission probabilities
form k =
a33 a33
p
φ
p
xn|φk
znk
k=1
attention
homogeneous models
condi- tional distributions
latent variables share
same parameters a
emission distributions share
same parameters
extension
general cases
note
mixture model
i.i.d
data
corresponds
special case
parameters ajk
values
j
conditional distribution p
zn|zn−1
zn−1
horizontal links
graphical model
figure
joint probability distribution
observed variables
p
x
z|θ
p
z1|π
p
zn|zn−1
a
n=2 m=1 p
x =
x1
z =
z1
zn
θ =
π
a
φ
set
parameters
model
discussion
hidden markov model
particular choice
emission probabilities
model
wide range
emission distributions
discrete tables
gaussians
mixtures
gaussians
discriminative models
neural networks
emission density p
x|z
representation
p
z|x
required emission density p
x|z
bayes ’ theorem
bishop
understanding
hidden markov model
generative point
view
samples
mixture
exercise
k =
k =
k =
hidden markov models
figure
illustration
hidden markov model
3-state latent variable z
gaussian emission model p
x|z
x
contours
constant probability density
emission distributions
states
latent variable
b
a sample
points
hidden markov model
colour
component
lines
successive observations
transition matrix
state
% probability
transition
other states
% probability
same state
gaussians
components
random
probability
mixing coefﬁcients
sample vector x
correspond- ing gaussian component
process
n times
data set
n independent samples
case
hidden markov model
procedure
initial latent variable z1
probabilities
parameters
corresponding observation x1
state
variable z2
transition probabilities
z2|z1
instantiated value
z1
sample
z1 corresponds
state j
state k
z2
probabilities ajk
k =
k. once
sample
x2
next latent variable z3
example
directed graphical model
instance
model
diago- nal transition elements akk
off-diagonal elements
typical data sequence
long runs
points
single compo- nent
infrequent transitions
component
generation
samples
hidden markov model
figure
many variants
standard hmm model
instance
constraints
form
transition
a
rabiner
particular practical importance
left-to-right hmm
elements ajk
a
< j
section
sequential data figure
example
state transition diagram
3-state left-to-right hidden markov model
note
state
a11 a22 a33 a12 a23
=
a13 k =
k
state transition diagram
3-state hmm
figure
such models
initial state probabilities
p
z1
p
z11
p
z1j
j =
other words
sequence
state j
transition matrix
large changes
state index
ajk
> j + ∆
type
model
lattice diagram
figure
many applications
hidden markov models
example speech recognition
on-line character recognition
use
left-to-right architectures
illus- tration
left-to-right hidden markov model
example
handwritten digits
on-line data
meaning
digit
trajectory
pen
function
time
form
sequence
pen coordinates
contrast
off-line digits data
appendix a
images
ink
examples
on- line digits
figure
hidden markov model
subset
data
examples
digit
’
k =
states
line segment
ﬁxed length
possible angles
emission distribution
×
table
probabilities
angle values
state index value
transition prob- abilities
set
state index
model parameters
iterations
em
insight
model
figure
figure
lattice diagram
3-state left- to-right hmm
state index k
transition
k
k =
k =
a11 a11 a11 a33
a33 a33
hidden markov models
figure
top row
examples
on-line handwritten digits
bottom row
synthetic digits
left-to-right hid- den markov model
data set
handwritten digits
powerful properties
hidden markov models
ability
degree
invariance
local warping
compression
time axis
way
digit
’
on-line handwritten
example
typical digit
distinct sections
cusp
ﬁrst part
digit
top left
sweeping arc
cusp
loop
bottom
second more- or-less straight sweep
bottom right
natural variations
style
relative sizes
sections
location
cusp
loop
temporal sequence
generative perspective such variations
hidden markov model
changes
number
transitions
same state
number
transitions
successive state
note
digit ‘
’
reverse order
bottom right
top left
pen tip coordinates
example
training set
probability
observations
model
speech recognition context
warping
time axis
natural variations
speed
speech
hidden markov model
distortion
maximum likelihood
hmm
data set x =
x1
param- eters
hmm
maximum likelihood
likelihood function
joint distribution
latent variables
x|θ
p
x
z|θ
z
joint distribution p
x
z|θ
contrast
mixture distribution
chapter
summations
summations
n variables
k states
re- sulting
total
k n terms
number
terms
summation
 
sequential data section
length
chain
fact
summation
cor- responds
many paths
lattice diagram
figure
similar difﬁculty
infer- ence problem
simple chain
variables
figure
use
conditional independence properties
graph
summations
order
algorithm
cost scales
length
chain
similar technique
hidden markov model
further difﬁculty
expression
likelihood function
generalization
mixture distribution
summation
emission models
different settings
latent variables
direct maximization
likelihood function
complex ex- pressions
closed-form solutions
case
simple mixture models
mixture model
i.i.d
data
special case
hmm
expectation maximization algorithm
efﬁcient framework
likelihood function
hidden markov models
em algorithm
initial selection
model parameters
θold
e step
parameter values
posterior distribution
latent variables
z|x
θold
posterior distri- bution
expectation
logarithm
complete-data likelihood function
function
parameters
function q
θ
θold
q
θ
θold
= z p
z|x
θold
ln p
x
z|θ
point
notation
γ
zn
marginal posterior distribution
latent variable zn
ξ
zn−1
zn
joint posterior distribution
successive latent variables
γ
zn
= p
zn|x
θold
ξ
zn−1
zn
= p
zn−1
zn|x
θold
value
γ
zn
set
k nonnegative numbers
unity
ξ
zn−1
zn
k × k matrix
nonnegative numbers
unity
γ
znk
conditional probability
znk =
similar use
notation
ξ
zn−1
j
znk
other probabilistic variables
expectation
binary random variable
probability
value
znk
e [ znk ] = z γ
z
znk ξ
zn−1
j
znk
e [ zn−1
jznk ] = γ
z
zn−1
jznk
z
joint distribution p
x
z|θ
  k=1 k n  k  k  k      γ
z1k
k γ
z1j
n=2 n=2 j=1 n n k n
hidden markov models
use
deﬁnitions
γ
ξ
q
θ
θold
γ
z1k
ln πk + ξ
zn−1
j
znk
ln ajk j=1 k=1 +
γ
znk
ln p
xn|φk
goal
e step
quantities γ
zn
ξ
zn−1
zn
efﬁ-
detail
m step
q
θ
θold
respect
parameters
π
a
φ
zn
ξ
zn−1
zn
maximization
respect
a
appropriate lagrange multipliers
results
= ajk =
ξ
zn−1
j
znk
ξ
zn−1
j
znl
l=1
em algorithm
values
π
a
course
summation constraints
probabilis- tic interpretation
note
elements
π
a
subsequent em updates
typical initialization procedure
values
parameters
summa- tion
non-negativity constraints
particular modiﬁcation
em results
case
left-to-right models
initial values
elements ajk
appropriate elements
q
θ
θold
respect
ﬁnal term
φk
term
same form
data-dependent term
corresponding function
standard mixture dis- tribution
i.i.d
data
comparison
case
gaussian mixture
quantities γ
znk
role
responsibil- ities
parameters
different components
term
sum
terms
value
k
weighted log likeli- hood function
emission density p
x|φk
weights γ
znk
maximization
instance
case
exercise
exercise
  
n
n n     
k d n k=1 µk = γ
znk
xn γ
znk
γ
znk
xn − µk
xn − µk
t σk =
γ
znk
case
variables
conditional distribution
observations
form p
x|z
µxizk ik
corresponding m-step equations
n γ
znk
µik = n=1 n
γ
znk
analogous result
bernoulli
variables
em algorithm
initial values
parameters
emission dis- tribution
way
data
i.i.d
emission density
maximum likelihood
values
parameters
em
efﬁcient procedure
quantities γ
znk
ξ
zn−1
j
znk
e step
em algorithm
graph
hidden markov model
figure
tree
posterior distribution
latent variables
two- stage message
algorithm
particular context
hidden markov model
forward-backward algorithm
rabiner
baum-welch algorithm
baum
fact several variants
basic algorithm
lead
exact marginals
precise form
sequential data gaussian emission densities
x|φk
= n
x|µk
σk
maximization
function q
θ
θold
exercise
section
hidden markov models
messages
chain
jordan
alpha-beta algorithm
great practical importance
own right
forward- backward algorithm
nice illustration
concepts
chapters
section
‘ con- ventional ’ derivation
forward-backward equations
use
sum
product rules
probability
conditional independence properties
corresponding graphical model
d-separation
section
forward-backward algorithm
speciﬁc example
sum-product algorithm
section
evaluation
posterior distributions
latent variables
form
emission density p
observed variables
values
quantities
xn|zn
value
zn
n.
section
explicit dependence
model parameters
following conditional independence properties
jordan
p
x|zn
p
xn|zn
p
xn|zn
p
zn
= p
xn−1|zn
p
zn
= p
xn−1|zn−1
p
zn+1
= p
xn|zn+1
p
xn+1
p
xn|zn+1
p
x|zn−1
zn
= p
xn−1|zn−1
p
zn +1|zn
x
p
zn +1|zn
p
xn +1|x
zn +1
p
xn +1|zn +1
p
xn|zn
p
xn|zn
x =
x1
relations
d-separation
instance
ﬁrst
results
path
nodes x1
node xn
node zn
such paths
conditional independence property
reader
few moments
properties
turn
exercise
application
d-separation
relations
effort
joint distribution
hidden markov model
sum
product rules
proba- bility
γ
znk
discrete multinomial ran- dom
expected value
components
probability
component
value
posterior distribution p
zn|x1
xn
zn
observed data set x1
exercise
sequential data
vector
length k
entries
values
znk
bayes ’ theorem
zn
= p
zn|x
= p
x|zn
p
zn
p
x
    zn−1 zn−1 zn−1 zn−1  zn−1 note
denominator p
x
parameters
hmm
hence
likelihood function
conditional independence property
product rule
probability
γ
zn
= p
zn
p
xn|zn
p
x
α
zn
β
zn
p
x
α
zn
≡ p
zn
β
zn
≡ p
xn|zn
quantity α
zn
joint probability
data
time n
value
zn
whereas β
zn
conditional probability
future data
time
n
value
zn
again
α
zn
β
zn
represent set
k numbers
possible settings
1-of-k coded binary vector zn
notation α
znk
value
α
zn
znk
analogous interpretation
β
znk
recursion relations
α
zn
β
zn
again
use
conditional independence properties
sum
product rules
α
zn
terms
α
zn−1
α
zn
= p
zn
= p
xn|zn
p
zn
= p
xn|zn
p
xn−1|zn
p
zn
= p
xn|zn
p
zn
= p
xn|zn
p
zn−1
zn
= p
xn|zn
p
xn|zn
p
xn|zn
p
zn|zn−1
p
zn−1
p
xn−1|zn−1
p
zn|zn−1
p
zn−1
p
zn−1
p
zn|zn−1
use
deﬁnition
α
zn
α
zn−1
p
zn|zn−1
α
zn
= p
xn|zn
 k     zn+1 zn+1 zn+1 figure
illustration
forward recursion
evaluation
α variables
fragment
lattice
quantity α
zn1
elements α
zn−1
j
α
zn−1
step
weights
aj1
val- ues
p
zn|zn−1
multiplying
data contribution p
xn|zn1
p
xn|zn,1
hidden markov models α
zn−1,1
α
zn,1
a11 k =
a21 α
zn−1,2
k
a31 α
zn−1,3
k
moment
recursion relation
detail
note
k terms
summation
right-hand side
k values
zn
step
α recursion
computational cost
o
k
forward recursion equation
α
zn
lattice diagram
figure
order
recursion
initial condition
α
z1
= p
z1
= p
z1
p
x1|z1
πkp
x1|φk
z1k k=1
α
z1k
k =
k
value πkp
x1|φk
ﬁrst node
chain
chain
α
zn
latent node
step
recursion
k × k matrix
overall cost
quantities
whole chain
o
k
recursion relation
quantities β
zn
use
conditional independence properties
β
zn
= p
xn|zn
p
zn+1|zn
= = = p
zn+1
p
zn+1|zn
p
xn|zn+1
p
zn+1|zn
p
xn|zn+1
p
xn+1|zn+1
p
zn+1|zn
sequential data figure
illustration
β
zn,1
β
zn+1,1
  n=1 n n zn+1 n=1 k =
 
n k =
k =
backward recursion
evaluation
β variables
fragment
lattice
quantity β
zn1
components β
zn+1
k
β
zn+1
step
weights
products
a1k
correspond- ing
values
p
zn+1|zn
values
emission density p
k
a11 a12 p
xn|zn+1,1
β
zn+1,2
a13 p
xn|zn+1,2
β
zn+1,3
p
use
deﬁnition
β
zn
β
zn
= β
zn+1
p
xn+1|zn+1
p
zn+1|zn
note
case
backward message
algorithm
β
zn
terms
β
zn+1
step
effect
observation xn+1
emission probability p
xn+1|zn+1
transition matrix p
zn+1|zn
zn+1
figure
again
starting condition
recursion
value
β
zn
n = n
α
zn
deﬁnition
p
zn|x
= p
x
zn
β
zn
p
x
zn
settings
zn
m step equations
quantity p
x
instance
m-step equation
µk
form γ
znk
xn α
znk
β
znk
xn
µk = = γ
znk
α
znk
β
znk
n=1
quantity p
x
likelihood function
value
em optimization
sides
zn
fact
left-hand side
normalized distribution
x
α
zn
β
zn
zn

hidden markov models
thus
likelihood function
sum
conve- nient choice
n.
instance
likelihood function
α recursion
start
end
chain
result
n = n
use
fact
β
zn
vector
case
β recursion
x
α
zn
zn
moment
result
p
x
likelihood
joint distribution p
x
z
possible values
z
such value
particular choice
hidden state
time step
other words
term
summation
path
lattice diagram
many such paths
likelihood function
form
computational cost
length
chain
order
summation
multiplications
time step
contributions
paths
states
intermediate quantities
zn
evaluation
quantities ξ
zn−1
zn
correspond
values
conditional probabilities
zn−1
zn|x
k × k settings
zn−1
zn
deﬁnition
ξ
zn−1
zn
bayes ’ theorem
zn−1
zn
= p
zn−1
zn|x
= p
x|zn−1
zn
p
zn−1
zn
= p
xn−1|zn−1
p
xn|zn
p
xn|zn
p
zn|zn−1
p
zn−1
= α
zn−1
p
xn|zn
p
zn|zn−1
β
zn
p
x
p
x
p
x
use
conditional independence property
deﬁnitions
α
zn
β
zn
ξ
zn−1
zn
results
α
β recursions
steps
hidden markov model
em algorithm
initial selection
parameters
θ ≡
π
a
φ
a
π parameters
uniform distribution
non-negativity
summa- tion constraints
initialization
parameters
form
distribution
instance
case
gaussians
parameters
k-means algorithm
data
covariance matrix
corresponding k
cluster
forward α recursion
backward β recursion
results
γ
zn
ξ
zn−1
zn
stage
likelihood function
    zn+1 zn+1 zn+1 zn+1
    zn zn zn  p
xn +1|x
= = = = = p
xn +1
zn +1|x
p
xn +1|zn +1
p
zn +1|x
p
xn +1|zn +1
p
zn +1
zn|x
p
xn +1|zn +1
p
xn +1|zn +1
p
zn +1|zn
p
zn|x
p
zn +1|zn
p
zn
x
p
x
p
x
zn+1 p
xn +1|zn +1
zn p
zn +1|zn
α
zn
ﬁrst
forward
recursion
ﬁnal summations
zn
zn +1
result
ﬁrst summation
zn
value
xn +1
order
α recursion
next step
order
subsequent value xn +2
sequential data
e step
results
revised set
parameters
m-step equations
section
e
m steps
convergence criterion
instance
change
likelihood function
threshold
note
recursion relations
observations
conditional distributions
form p
xn|zn
recursions
type
dimensionality
observed variables
form
conditional distribution
value
k possible states
zn
observed variables
xn
quantities
xn|zn
functions
zn
start
em algorithm
chapters
maximum likelihood approach
number
data points
relation
number
parame- ters
hidden markov model
max- imum likelihood
training sequence
use
multiple shorter sequences
straightforward modiﬁcation
hidden markov model em algorithm
case
left-to-right models
observation sequence
state transition
nondiagonal element
a
quantity
interest
predictive distribution
data
x =
x1
xn +1
real-time applications
ﬁnancial forecasting
use
sum
product rules
conditional independence properties
figure
a fragment
fac- tor graph representation
hidden markov model
hidden markov models
zn−1 ψn zn g1 gn−1 gn xn−1 xn z1 x1 section
section
note
inﬂuence
data
x1
k values
α
zn
predictive distribution
ﬁxed amount
storage
real-time applications
estimation
parameters
hmm
max- imum likelihood
framework
maximum likeli- hood
priors
model parameters
φ
values
posterior probability
em algorithm
e step
m step
log
prior distribution p
θ
function q
θ
θold
maximization
straightforward application
techniques
various points
book
variational meth- ods
bayesian treatment
hmm
parameter distributions
mackay
maximum likelihood
two-pass forward-backward recursion
posterior probabilities
sum-product algorithm
hmm
directed graph
hidden markov model
fig- ure
tree
problem
local marginals
hidden variables
sum-product algorithm
forward-backward algorithm
previous section
sum-product algorithm therefore
simple way
alpha-beta recursion formulae
directed graph
figure
factor graph
representative fragment
figure
form
fac- tor graph
variables
latent
purpose
inference problem
variables x1
xn
factor graph
emission
transition probability factors
sim- pliﬁed factor graph representation
figure
factors
h
z1
= p
z1
p
x1|z1
fn
zn−1
zn
= p
zn|zn−1
p
xn|zn
   z1
sequential data figure
a simpliﬁed form
fac- tor graph
hidden markov model
h fn zn−1 zn
alpha-beta algorithm
ﬁnal hidden variable zn
root node
ﬁrst pass messages
leaf node h
root
general results
message propagation
messages
hidden markov model
form µzn−1→fn
zn−1
= µfn−1→zn−1
zn−1
µfn→zn
zn
= fn
zn−1
zn
µzn−1→fn
zn−1
zn−1
equations
propagation
messages
chain
alpha recursions
previous section
variable nodes zn
neighbours
computation
µzn−1→fn
zn−1
recur- sion
f → z messages
form µfn→zn
zn
= fn
zn−1
zn
µfn−1→zn−1
zn−1
deﬁnition
zn
= µfn→zn
zn
alpha recursion
quantities α
zn
initial condition
α
z1
h
z1
= p
z1
p
x1|z1
initial α
same equation
subsequent α quantities
messages
root node
leaf node
form µfn+1→fn
zn
zn+1 fn+1
zn
zn+1
µfn+2→fn+1
zn+1
messages
type z → f
variable nodes
computation
deﬁnition
fn+1
zn
zn+1
β
zn
= µfn+1→zn
zn
hidden markov models
beta recursion
again
beta
implies
initial mes- sage send
root variable node
µzn→fn
zn
initialization
β
zn
section
sum-product algorithm
marginals
messages
result
local marginal
node zn
product
incoming messages
variables x =
x1
joint distribution p
zn
x
µfn→zn
zn
µfn+1→zn
zn
= α
zn
β
zn
sides
p
x
γ
zn
= p
zn
x
p
x
α
zn
β
zn
p
x
 exercise
agreement
result
factors
important issue
use
forward backward algorithm
practice
recursion relation
step
new value α
zn
previous value α
zn−1
quantities p
zn|zn−1
p
xn|zn
probabilities
unity
way
chain
values
α
zn
moderate lengths
chain
calculation
α
zn
dynamic range
computer
double precision
point
case
i.i.d
data
problem
eval- uation
likelihood functions
logarithms
sums
products
small numbers
fact
possible paths
lattice diagram
figure
work
re-scaled versions
α
zn
β
zn
values
order unity
corresponding scaling factors
re-scaled quantities
em algorithm
α
zn
= p
zn
joint distri- bution
observations
latent variable zn
normalized version
α
α
zn
= p
zn|x1
xn
= α
zn
p
xn
probability distribu- tion
k variables
value
n.
order
original al- pha variables
factors
conditional distributions
observed variables
= p
xn−1
      
 n      zn−1 n n
     zn+1    
sequential data
product rule
xn
cm m=1
zn
= p
zn|x1
xn
p
xn
cm α
zn
m=1
recursion equation
α
α
cn α
zn
= p
xn|zn
α
zn−1
p
zn|zn−1
α
zn
stage
forward message
phase
cn
coefﬁcient
right-hand side
re-scaled variables
zn
β
zn
β
zn
cm β
zn
machine precision
quan- tities β
zn
ratio
conditional probabilities
zn
= p
xn|zn
p
xn
recursion result
β
following recursion
re-scaled variables
β
zn
= β
zn+1
p
xn+1|zn+1
p
zn+1|zn
recursion relation
use
scaling factors
α phase
from
likelihood function
p
x
cn
exercise
required marginals
γ
zn
= α
zn
β
zn
ξ
zn−1
zn
cn α
zn−1
p
xn|zn
p
zn|z−1
β
zn
   section
hidden markov models
α
zn
β
zn
finally
alternative formulation
forward-backward algorithm
jordan
backward pass
recursion
quantities γ
zn
= β
zn
α–γ recursion α
zn
forward pass
ﬁrst
quantities
backward pass
forward
backward passes
α–β algorithm
algorithms
comparable computational cost
α–β version
case
hidden markov models
whereas
linear dynamical systems
recursion
α–γ form
viterbi algorithm
many applications
hidden markov models
latent variables
meaningful interpretation
interest
probable sequence
hidden states
observation sequence
instance
speech recognition
probable phoneme sequence
series
acoustic observations
graph
hidden markov model
directed tree
problem
max-sum algorithm
discussion
section
problem
probable sequence
latent states
set
states
latter problem
ﬁrst
sum-product
algorithm
latent variable marginals
zn
duda
set
such states
correspond
probable sequence
states
fact
set
states
sequence
probability
successive states
isolation
transition
element
zero
practice
probable sequence
states
max-sum algorithm
context
hidden markov models
viterbi algorithm
viterbi
note
max-sum algorithm
log probabilities
need
re-scaled variables
forward-backward algorithm
figure
fragment
hidden markov model
lattice diagram
number
possible paths
lattice
length
chain
viterbi algorithm
space
paths
probable path
computational cost
length
chain
sum-product algorithm
hidden markov model
factor graph
figure
again
variable node zn
root
pass messages
root
leaf nodes
results
messages
max-sum algorithm
µzn→fn+1
zn
= µfn→zn
zn
µfn+1→zn+1
zn+1
max zn ln fn+1
zn
zn+1
+ µzn→fn+1
zn
 
sequential data figure
a fragment
hmm lattice
possible paths
viterbi algorithm
probable path
many possibilities
path
corresponding probability
product
elements
tran- sition matrix ajk
probabil- ities
zn+1|zn
segment
path
emission densities
asso-
node
path
k =
k =
n
µzn→fn+1
zn
equations
use
recursion
f → z messages
form ω
zn+1
ln p
xn+1|zn+1
max zn
ln p
x+1|zn
ω
zn
notation ω
zn
≡ µfn→zn
zn
from
messages
ω
z1
ln p
z1
ln p
x1|z1
note
notation
dependence
model parameters
probable sequence
exercise
viterbi algorithm
deﬁnition
joint distribution
logarithm
maximizations
summations
quantities ω
zn
probabilistic interpretation ω
zn
max p
z1
zn
z1
ﬁnal maximization
zn
value
joint distribution p
x
z
probable path
sequence
latent variable values
path
use
back-tracking procedure
sec- tion
maximization
zn
k possible values
zn+1
suppose
record
values
zn
correspond
maxima
value
k values
zn+1
function
ψ
kn
k ∈
k
mes- sages
end
chain
probable state
zn
function
chain
n = ψ
kmax n+1
   r=1 r  r
hidden markov models
viterbi algorithm
many paths
lattice
probability
path
proba- bility
dramatic saving
computational cost
path
probability
products
transition
emission probabilities
way
path
lattice
particular time step
particular state k
time step
many possible paths
correspond- ing node
lattice diagram
particular path
probability
k states
time step
track
k such paths
time step
k
possible paths
k possible paths
k current states
k
path
state
time n+1
ﬁnal time step n
state corresponds
probable path
unique path
state
path
n
state
time
lattice
state n =
extensions
hidden markov model
basic hidden markov model
standard training algorithm
maximum likelihood
numerous ways
requirements
particular applications
important examples
digits example
figure
hidden markov models
poor generative models
data
synthetic digits
training data
goal
sequence classiﬁca- tion
signiﬁcant beneﬁt
parameters
hidden markov models
maximum likelihood techniques
suppose
training set
r observation sequences xr
r
r
class m
m
m.
class
separate hidden markov model
own parameters
problem
parameter values
standard classiﬁcation problem
cross-entropy
p
m
prior probability
class m. optimization
cost function
maximum likelihood
kapadia
particular using bayes ’
terms
sequence probabilities
hidden markov models
p
mr|xr
ln r=1 p
xr|θr
p
mr
l=1 p
xr|θl
p
lr
m
sequential data figure
section
autoregressive hidden markov model
distribution
observation xn
subset
previous observations
hidden state zn
example
distribution
xn
previous observations
xn−2
zn−1 xn−1 zn xn zn+1 xn+1 
training sequence
models
or- der
denominator
hidden markov models
discriminative training methods
speech recognition
kapadia
signiﬁcant weakness
hidden markov model
way
distribution
times
system
state
problem
note
probability
sequence
hidden markov model
t steps
state k
transition
different state
p
t
=
akk
t
− akk
exp
−t ln akk
t
function
t
many applications
unrealistic model
state duration
problem
state duration
diagonal coefﬁcients akk
set
state k
probability distribution p
t|k
pos- sible duration times
generative point
view
state k
value t
number
time steps
system
state k
p
t|k
model
t values
observed variable xt
corresponding emis- t=1 p
xt|k
approach
straightforward sion density
modiﬁcations
em optimization procedure
rabiner
limitation
standard hmm
long- range correlations
observed variables
variables
many time steps
ﬁrst-order markov chain
hidden states
longer-range effects
principle
extra links
graphical model
figure
way
hmm
autoregressive hidden markov model
ephraim
example
figure
discrete observa- tions
tables
conditional probabilities
emis- sion distributions
case
gaussian emission density
linear- gaussian framework
conditional distribution
xn
values
previous observations
value
zn
mean
linear combination
values
conditioning variables
number
additional links
graph
number
free parameters
example
figure
observation
figure
example
input-output hidden markov model
case
emission probabilities
transition probabilities
values
sequence
observations
hidden markov models
un−1 zn−1 xn−1 un zn xn un+1 zn+1
observed variables
hidden state
d-separation
fact
simple probabilistic structure
zn
standard hmm
values
zn−1
zn+1
conditional independence property
veri- ﬁed
path
node zn−1
zn+1 passes
observed node
respect
path
consequence
forward-backward recursion
e step
em algorithm
posterior distributions
latent variables
computational time
length
chain
m step
minor modiﬁcation
standard m-step equations
case
gaussian emission densities
parameters
standard linear regression equations
chapter
autoregressive hmm
natural extension
standard hmm
graphical model
fact
probabilistic graphical modelling viewpoint
plethora
different graphical structures
hmm
example
input-output hidden markov model
bengio
frasconi
sequence
observed variables
addition
output variables
values
dis- tribution
latent variables
output variables
example
figure
hmm framework
domain
supervised learn- ing
sequential data
use
d-separation criterion
markov property
chain
latent variables
path
node zn−1
zn+1
respect
observed node zn
conditional inde- pendence property
formulation
computationally efﬁcient learn- ing algorithm
parameters θ
model
likelihood function l
θ
p
x|u
θ
u
matrix
rows
ut n.
consequence
conditional independence property
likelihood function
em algorithm
e step
backward recursions
variant
hmm worthy
mention
factorial hidden markov model
ghahramani
jordan
multiple independent exercise
sequential data figure
a factorial hidden markov model
markov chains
latent vari- ables
continuous observed variables
possible choice
emission model
linear-gaussian density
mean
gaussian
linear combi- nation
states
corresponding latent variables
z
n−1 z
n−1 xn−1 z
n z
n z
n+1 z
n+1 xn xn+1 markov chains
latent variables
distribution
observed variable
time step
states
corresponding latent vari- ables
same time step
figure
corresponding graphical model
motivation
factorial hmm
order
bits
information
time step
standard hmm
k =
=
latent states
factorial hmm
use
binary latent chains
primary disadvantage
factorial hmms
additional complexity
m step
factorial hmm model
observation
x variables
dependencies
latent chains
difﬁculties
e step
figure
variables
n
path
node xn
hence
exact e step
model
backward recursions
m markov chains
key conditional independence property
individual markov chains
factorial hmm model
d-separation
figure
m chains
hidden nodes
simplicity
latent variables
same number k
states
approach
km combinations
latent variables
time step
z
figure
example
path
green
head-to-head
observed nodes
xn+1
head-to-tail
unobserved nodes
n
z
n+1
path
conditional independence property
individual la- tent chains
factorial hmm model
consequence
efﬁcient exact e step
model
n−1
z
z
n−1 z
n−1 xn−1 z
n z
n z
n+1 z
n+1 xn xn+1 section
linear dynamical systems
model
equivalent standard hmm
single chain
latent variables
km latent states
standard forward-backward recursions
e step
computational com- plexity o
n k
number m
latent chains
anything
small values
m. one solution
methods
chapter
elegant deterministic al- ternative
ghahramani
jordan
variational inference techniques
tractable algorithm
approximate inference
simple variational posterior distribution
respect
latent variables
powerful approach
variational distribution
independent markov chains
chains
latent variables
original model
latter case
variational inference
independent forward
backward recursions
chain
correlations
variables
same chain
many possible probabilistic structures
needs
particular applications
graphical models
general technique
motivating
describing
such structures
variational methods
powerful framework
inference
models
exact solution
linear dynamical systems
order
concept
linear dynamical systems
following simple problem
practical settings
suppose
value
unknown quantity z
noisy sensor
observation x
value
zero-mean gaussian noise
single measurement
guess
z
z = x
estimate
z
lots
measurements
random noise terms
’
situation
quantity z
time
regular measurements
x
point
time
x1
corresponding values
measurements
error
noise
single averaged estimate
value
z
new source
error
bit
value
zn
recent few measurements
z
random noise level
sensor
sense
long window
observations
signal
noise levels
estimate
zn
weighted average
recent measurements
   
sequential data
contribution
recent ones
sort
intuitive argument
weighted average
sort
hand-crafted weighing
problems
probabilistic model
time evolution
measurement processes
inference
methods
chapters
model
linear dynamical system
hmm
state space model
figure
latent variables
arbitrary emission probability distributions
graph
course
class
probability distributions
extensions
other distributions
latent variables
continuous latent variables
summations
sum-product algorithm become integrals
general form
inference algorithms
hidden markov model
hidden markov models
linear dynamical systems
graphical models
deep relationship
key requirement
efﬁcient algorithm
inference
length
chain
instance
zn−1
posterior probability
zn
observations
quantity x1
xn
transition probability p
zn|zn−1
emission probability p
xn|zn
zn−1
distribution
α
zn−1
zn
same functional form
distribution
stage
parameter values
only distributions
property
multiplication
exponential family
important example
practical perspective
linear-gaussian state space model
latent variables
zn
observed variables
xn
multi- variate gaussian distributions
means
linear functions
states
parents
graph
directed graph
linear-gaussian units
joint gaussian distribution
variables
α
zn
functional form
mes- marginals
sages
efﬁcient inference algorithm
contrast
emission densities
xn|zn
mixture
k gaussians α
z1
mean
zn
α
z3
mixture
k
quantity gaussians
exact inference
practical value
z2
mixture
k gaussians
hidden markov model
extension
mixture models
chapter
sequential correlations
data
similar way
linear dynamical system
generalization
continuous latent variable models
chapter
probabilistic pca
factor analysis
pair
nodes
zn
xn
linear-gaussian latent
linear dynamical systems
model
particular observation
latent variables
zn
markov chain
model
tree-structured directed graph
inference problems
sum-product algorithm
forward re- cursions
α messages
hidden markov model
kalman ﬁlter equations
kalman
zarchan
musoff
back- ward recursions
β messages
kalman smoother equations
rauch-tung-striebel
rts
equations
rauch et al.
kalman ﬁlter
many real-time tracking applications
linear dynamical system
linear-gaussian model
joint distri- bution
variables
marginals
conditionals
sequence
probable latent variable values
probable latent sequence
need
analogue
viterbi algorithm
linear dynamical system
model
linear-gaussian conditional distributions
transition
emission distributions
general form p
zn|zn−1
= n
zn|azn−1
γ
p
xn|zn
n
xn|czn
σ
initial latent variable
gaussian distribution
p
z1
= n
z1|µ0
v0
exercise
exercise
note
order
notation
additive constant terms
means
gaussians
fact
distributions
equiv- alent form
terms
noisy linear equations
zn = azn−1 +
xn = czn + vn z1 = µ0 + u
noise terms
distributions
∼ n
w|0
γ
v ∼ n
v|0
σ
∼ n
v0
parameters
model
θ =
a
γ
c
σ
µ0
v0
maximum likelihood
em algorithm
e step
inference problem
local posterior marginals
latent variables
sum-product algorithm
next section
   
sequential data
inference
lds
problem
marginal distributions
latent
observation sequence
parameter settings
predictions
next latent state zn
next observation xn
data x1
xn−1
use
real-time applications
inference problems
sum-product algorithm
context
linear dynamical system
rise
kalman ﬁlter
kalman
equations
linear dynamical system
linear- gaussian model
joint distribution
latent
observed variables
principle
inference problems
standard results
previous chapters
marginals
conditionals
multivariate gaussian
role
sum-product algorithm
efﬁcient way
such computations
linear dynamical systems
identical factorization
markov models
factor graphs
figures
inference
same form
summations
latent variables
integrations
forward equations
root node
propagate messages
leaf node h
z1
root
from
initial message
factors
subsequent messages
convention
messages
nor- malized marginal distributions
zn|x1
xn
α
zn
propagation
scaled variables
discrete case
hidden markov model
recursion equa- tion
form α
zn
= n
zn|µn
vn
cn α
zn
= p
xn|zn
α
zn−1
p
zn|zn−1
dzn−1
conditionals
zn|zn−1
p
xn|zn
use
becomes
zn|µn
vn
n
xn|czn
σ
n
zn|azn−1
γ
n
zn−1|µn−1
vn−1
dzn−1
µn−1
vn−1
inte- gral
values
µn
vn
use
result
n
zn|azn−1
γ
n
zn−1|µn−1
vn−1
dzn−1 = n
zn|aµn−1
pn−1

linear dynamical systems
pn−1 = avn−1at + γ
result
ﬁrst factor
right-hand side
use
µn = aµn−1 + kn
xn − caµn−1
vn =
knc
pn−1 cn = n
xn|caµn−1
cpn−1ct + σ
use
matrix inverse identities
c.5
c.7
kalman gain matrix kn = pn−1ct
values
µn−1
vn−1
new observation xn
gaussian marginal
zn
mean µn
covariance vn
normalization coefﬁcient cn
cpn−1ct +
initial conditions
recursion equations
−1
p
z1
p
x1|z1
use
c1
µ1
v1
α
z1
= p
z1
p
x1|z1
c1 µ1 = µ0 + k1
x1 − cµ0
v1 =
k1c
v0 c1 = n
x1|cµ0
cv0ct + σ
k1 = v0ct cv0ct + σ −1
likelihood function
linear dynamical system
factors
kalman
equations
steps
posterior marginal
zn−1
posterior marginal
quantity aµn−1
prediction
mean
zn
mean
zn−1
step
transition probability
predicted mean
predicted observation
xn
cazn−1
emission probability
c
hidden state mean
update equation
mean
hidden variable distribution
mean aµn−1
correction
error xn − cazn−1
observation
actual observation
coefﬁcient
correction
kalman gain matrix
kalman ﬁlter
process
successive predictions
predictions
light
new observations
figure
     
sequential data zn−1 zn zn figure
linear dynamical system
sequence
steps
un- certainty
state
diffusion
arrival
new data
left-hand plot
blue curve
distribution p
zn−1|x1
xn−1
data
diffusion
nonzero variance
transition probability p
zn|zn−1
distribution p
zn|x1
xn−1
centre plot
note
blue curve
centre plot
comparison
next data observation xn
emission density p
xn|zn
function
zn
right-hand plot
note
density
respect
inclusion
new data point
distribution p
zn|x1
xn
state density
observation
data
distribution
zn|x1
xn−1
right-hand plot
comparison
exercise
situation
measurement noise
rate
latent variable
posterior distribution
zn
current measurement xn
accordance
intuition
simple example
start
section
latent variable
observation noise level
posterior mean
zn
measurements
time
important applications
kalman ﬁlter
simple example
object moving
dimensions
figure
inference problem
posterior marginal
node zn
observations
x1
xn
problem
marginal
node zn
observations
temporal data
inclusion
past observations
real-time prediction
key role
parameters
model
analogy
hidden markov model
problem
messages
node xn
com- bining
information
forward message passing stage
α
zn
lds literature
backward recursion
terms β
zn
γ
zn
β
zn
terms
γ
zn
=
form α
zn
γ
zn
= α
zn
β
zn
= n
zn| µn
vn
required recursion
backward recursion
     
linear dynamical systems
figure
illustration
linear dy- namical system
object
blue points
true positions
object
two-dimensional space
successive time steps
green points
noisy measurements
positions
red crosses
means
inferred posterior distributions
positions
kalman ﬁl- tering equations
covari- ances
inferred positions
red ellipses
standard deviation
β
zn
continuous latent variables
form cn+1 β
zn
= β
zn+1
p
xn+1|zn+1
p
zn+1|zn
dzn+1
α
zn
substitute
p
sides
p
zn+1|zn
use
manipulation
µn = µn + jn vn = vn + jn µn+1 − aµn vn+1 − pn jt n
use
avn = pnjt n. note
recursions
forward pass
ﬁrst
quantities µn
vn
backward pass
jn = vnat
pn
−1
em algorithm
pairwise posterior marginals
form ξ
zn−1
zn
=
cn
= n
zn−1|µn−1
vn−1
n
zn|azn−1
γ
n
xn|czn
σ
n
α
zn−1
p
xn|zn
p
zn|z−1
β
zn
µn
vn
cn α
zn
α
zn
ξ
zn−1
zn
components γ
zn−1
γ
zn
covariance
zn
zn−1
cov [ zn
zn−1 ] = jn−1 vn
exercise
exercise
               n
  n 
sequential data
learning
lds so
inference problem
linear dynamical systems
model parameters
=
a
γ
c
σ
µ0
v0
determination
parameters
maximum likelihood
ghahra- mani
hinton
model
latent variables
em algorithm
general terms
chapter
em algorithm
linear dynamical system
estimated parameter values
particular cycle
algorithm
θold
parameter values
inference algorithm
posterior distribution
latent variables
z|x
θold
local posterior marginals
m step
following expectations e [ zn ] = µn e znzt e
n = jn−1 = vn + vn + µt
µt µn n−1
complete-data log likelihood function
logarithm
ln p
x
z|θ
ln p
z1|µ0
v0
ln p
zn|zn−1
a
γ
n=1 ln p
c
σ
dependence
parameters
expectation
complete-data log likelihood
respect
posterior distri- bution p
z|x
θold
function q
θ
θold
ez|θold [ ln p
x
z|θ
]
m step
function
respect
components
θ
parameters
v0
p
z1|µ0
v0
expectation
respect
z
q
θ
θold
ln|v0| − ez|θold
− µ0
tv−1
− µ0
+ const
terms
µ0
v0
additive constant
maximization
respect
v0
use
maximum likelihood solution
gaussian distribution
section
exercise
              µnew vnew n
ln|σ|
xne n
n=2 n=1 n=1 e
n n n n n n
n −xne
n n −
znzt n−1      
n n      −1  
linear dynamical systems
a
γ
p
zn|zn−1
a
γ
] − e [ z1 ] e [
]
e [ z1 ] = e [
q
θ
θold
− ln|γ|
− azn−1
tγ−1
zn − azn−1
+ const
constant comprises terms
a
γ
respect
parameters
anew = γnew = −e znzt
e zn−1zt
e znzt
anewe zn−1zt
anew + anewe zn−1zt n−1
anew
t
note
anew
ﬁrst
result
γnew
order
new values
c
σ
p
c
σ
q
θ
θold
− −ez|θold
xn − czn
tς−1
xn − czn
+ const
respect
c
σ
cnew = σnew = zt
e
znzt n xnxt n − cnewe [ zn ] xt cnew + cnewe znzt
−1
cnew
exercise
exercise

chapter

sequential data
parameter
linear dynamical system
maximum likelihood
inclusion
priors
map estimate
bayesian treatment
analytical approxima- tion techniques
chapter
detailed treatment
space
extensions
lds
hidden markov model
considerable interest
basic linear dynamical system
order
capabilities
assumption
linear-gaussian model
algorithms
inference
learning
marginal distribution
observed variables
gaussian
signiﬁcant limitation
simple extension
linear dynamical system
gaussian mixture
initial distribution
z1
mixture
k components
forward recursion equations
mixture
k gaussians
hidden variable zn
model
many applications
gaussian emission density
poor approximation
mixture
k gaussians
emission density
α
z1
mixture
k gaussians
posterior α
zn
α
z2
mixture
k
gaussians
mixture
k n gaussians
number
components grows
length
chain
model
transition
emission models
depart
other exponential family
model
intractable infer- ence problem
deterministic approximations
assumed den- sity
expectation propagation
use
methods
section
approach
gaussian approximation
mean
predicted distribution
rise
extended kalman ﬁlter
zarchan
musoff
hidden markov models
interesting extensions
ba- sic linear dynamical system
graphical representation
example
state space model
ghahramani
hinton
combination
hidden markov model
set
linear dynamical systems
model
multiple markov chains
continuous linear-gaussian latent vari- ables
latent chain
linear dynamical system
markov chain
discrete variables
form
hidden markov model
output
time step
continuous latent chains
state
discrete latent variable
switch
observation
corresponding conditional output distribution
exact inference
model
vari- ational methods
efﬁcient inference scheme
forward-backward recursions
discrete markov chains
note
multiple chains
discrete latent variables
switch
remainder
analogous model
discrete latent variables
hidden markov model
     
l  chapter
linear dynamical systems
particle ﬁlters
dynamical systems
example
non-gaussian emission density
methods
order
tractable inference algorithm
sampling- importance-resampling formalism
section
sequential monte carlo
particle ﬁlter
class
distributions
graphical model
fig- ure
suppose
observed values
=
x1
xn
l samples
posterior distribution p
zn|xn
bayes ’ theorem
zn
p
zn|xn
dzn f
zn
p
zn|xn
xn−1
dzn f
zn
p
xn|zn
p
zn|xn−1
dzn p
xn|zn
p
zn|xn−1
dzn e [ f
zn
= = =  w
l
n f
z
l
n
z
l
n
set
samples
p
zn|xn−1
use
conditional independence property p
xn−1
p
xn|zn
graph
figure
weights
w
l
n
w
l
p
xn|z
l
n
m=1 p
xn|z
m
n
l
l w
l
same samples
numerator
denominator
posterior distribution p
zn|xn
set
samples
z
l
n
weights
� w
l
corresponding weights
w
l
sequential sampling scheme
set
samples
weights
time step
value
xn+1
weights
sam- ples
time step
distribution p
zn+1|xn
     p
zn+1|xn
= = = = p
zn+1|zn
xn
p
zn|xn
dzn p
zn+1|zn
p
zn|xn
dzn p
zn+1|zn
p
zn|xn
xn−1
dzn p
zn+1|zn
p
xn|zn
p
zn|xn−1
dzn p
xn|zn
p
zn|xn−1
dzn n p
zn+1|z
l
n
w
l
l
sequential data straightforward
bayes ’ theorem
use
conditional independence properties
zn+1|zn
xn
p
zn+1|zn
p
xn−1
p
xn|zn
application
d-separation criterion
graph
fig- ure
distribution
mixture distribution
samples
component l
probability
coef- ﬁcients w
l
sample
corresponding component
step
particle ﬁlter algorithm
stages
time step
sample representation
posterior dis- n
weights
w
l
tribution p
zn|xn
samples
z
l
n
mixture representation
form
corresponding representation
next time step
draw l samples
mixture distribution
sample
new obser- vation xn+1
corresponding weights w
l
n+1
case
single variable z
figure
n+1 ∝ p
xn+1|z
l
particle ﬁltering
sequential monte carlo
approach
literature
various names
bootstrap ﬁlter
gordon
survival
ﬁttest
kanazawa
condensation algorithm
isard
blake

www use
technique
d-separation
section
markov model
figure
n nodes
total satisﬁes
conditional independence properties
n =
n. similarly
model
graph
figure
n nodes
total p
zn|xn
p
zn+1|xn
p
xn+1|zn+1
p
zn+1|xn+1
z figure
schematic illustration
operation
particle ﬁlter
one-dimensional latent space
time step
posterior p
zn|xn
mixture distribution
circles
sizes
weights w
l
n
set
l samples
distribution
new weights w
l
n+1
p
xn+1|z
l
n+1
conditional independence properties
xn−1
= p
n =
n.
 
joint probability distribution
corresponding
graph
figure
sum
product rules
probability
joint distribution
conditional independence property
n =
n. similarly
second-order markov model
joint distribution
conditional independence property p
xn−1
= p
n =
n.

d-separation
distribution p
xn
observed data
state space model
directed graph
figure
conditional independence properties
hence
markov property
ﬁnite order
 
www
hidden markov model
emission densities
parametric model p
w
linear regression model
neural network
w
vector
adaptive parameters
describe
parameters
data
maximum likelihood
sequential data
 
m-step equations
initial state probabili- ties
transition probability parameters
hidden markov model
maximiza- tion
complete-data log likelihood function
appropriate lagrange multipliers
summation constraints
components
π
a

elements
parameters
a
hidden markov model
elements
subsequent updates
em algorithm

hidden markov model
gaussian emission densities
maximization
function q
θ
θold
respect
mean
covariance parameters
gaussians
rise
m-step equations
 
www
hidden markov model
discrete observations
multinomial distribution
conditional distribution
observations
hidden variables
corresponding m step equa- tions
write
analogous equations
conditional distribution
m step equations
case
hidden markov
multiple binary output variables
bernoulli conditional dis- tribution
hint
refer
sections
discussion
corresponding maximum likelihood solutions
i.i.d
data
 
www use
d-separation criterion
conditional indepen- dence properties
–
joint distribution
hidden markov model
  
sum
product rules
probability
condi- tional independence properties
–
joint distribution
hidden markov model
 
expression
marginal distribution
vari- ables
factor
factor graph
results
messages
sum-product algorithm
section
result
joint posterior distribution
successive latent variables
hidden markov model
 
suppose
hidden markov model
maximum likelihood
data
r independent sequences
observations
x
r
r
r. show
e step
em algorithm
posterior probabilities
latent variables
α
β recursions
sequences
m step
initial probability
transition probability parameters
re-estimated            
n=2 r=1 r=1 r=1 j=1 r=1 l=1 k n r k r n r=1 r n r
modiﬁed forms
exercises
r γ
z
r
γ
z
r
πk = ajk = ξ
z
r
n−1
j
z
r
n
k
ξ
z
r
n−1
j
z
r
n
l
γ
z
r
nk
x
r
n
notational convenience
sequences
same length
generalization
sequences
different lengths
m-step equation
re-estimation
means
gaussian emission models
µk = r n
γ
z
r
nk
r=1 n=1 note
m-step equations
other emission model parameters
distributions
analogous form
 
www use
deﬁnition
messages
factor node
variable node
factor graph
expression
joint distribution
hidden markov model
deﬁnition
alpha message
deﬁnition
 
deﬁnition
messages
factor node
variable node
factor graph
expression
joint distribution
hidden markov model
deﬁnition
beta message
deﬁnition
 
expressions
marginals
hidden markov model
corresponding results
terms
re-scaled variables
  
exercise
forward message
equation
viterbi
expression
joint distribution
hidden variables z1
zn
log- arithm
maximizations
summations
recursion
sequential data
quantities ω
zn
show
initial condition
recursion

www show
directed graph
input-output hidden markov model
figure
tree-structured factor graph
form
figure
expressions
initial factor h
z1
general factor fn
zn−1
zn
� n � n.
  
result
exercise
recursion equations
initial conditions
forward-backward algorithm
input-output hidden markov model
figure

kalman ﬁlter
smoother equations
posterior distribu- tions
individual latent variables
observed variables
linear dynamical systems
sequence
latent variable values
posterior distributions
probable sequence
latent values
joint distribution
latent
variables
linear dynamical system
conditionals
marginals
use
result
 
www use
result
 
results
matrix identities
c.5
c.7
results
kalman gain matrix kn
 
www using
deﬁnitions
result
 
using
deﬁnitions
result
 
www
generalization
constant terms
c
gaussian means
p
zn|zn−1
= n
zn|azn−1
γ
p
xn|zn
n
xn|czn + c
σ
show
extension
framework
chapter
state vector z
additional component
unity
matrices a
c
extra columns
parameters
c.
 
exercise
kalman ﬁlter equations
independent observations
results
section
maximum likelihood solution
single gaussian distribution
prob- lem
mean µ
single gaussian random variable x
set
independent observations
x1
exercises
linear dynamical system
latent variables
z1
zn
c
identity matrix
transition prob- ability a
observations
parameters
v0
initial state
µ0
σ
σ2
write
corresponding kalman ﬁlter equations
general results
results
consider- ing independent data
  
special case
linear dynamical system
section
probabilistic pca
transition
a =
covariance γ =
noise covariance σ = σ2i
use
matrix inversion identity
c.7
show
emission density matrix c
w
posterior distribution
hidden states
reduces
result
probabilistic pca

www
linear dynamical system
form
sec- tion
amplitude
observation noise
σ
show
posterior distribution
zn
xn
variance
accords
intuition
noise
current observation xn
state variable zn
previous observations
  
special case
linear dynamical system
section
state variable zn
previous state variable
a =
simplicity
v0 → ∞
initial conditions
z
predictions
data
use proof
induction
posterior mean
state zn
average
x1
intuitive result
state variable
estimate
observations
  
backwards recursion equation
rts smoothing equations
gaussian linear dynamical sys- tem
 
result
pairwise posterior marginal
state space model
speciﬁc form
case
gaussian linear dynamical system
 
result
α
zn
result
covariance
zn
zn−1
 
results
m-step equations
µ0
v0
linear dynamical system
 
results
m-step equations
γ
linear dynamical system
sequential data
 
results
m-step equations
c
σ
linear dynamical system
models
chapters
range
different models
classiﬁ- cation
regression problems
improved performance
multiple models
way
single model
isolation
instance
l different models
predictions
average
predictions
model
such combinations
models
committees
section
cuss ways
committee concept
practice
insight
effective procedure
important variant
committee method
boosting
multiple models
sequence
error function
par- ticular model
performance
previous models
substantial improvements
performance
use
single model
section
predictions
set
models
alternative form
  k=1 k model combination
models
prediction
choice
model
function
input variables
different models
predictions
different regions
input space
framework
kind
decision tree
selec- tion process
sequence
binary selections
traversal
tree structure
section
case
individual models
overall ﬂexibility
model
input-dependent selection process
decision trees
classiﬁcation
regression problems
limitation
decision trees
division
input space
hard splits
model
predictions
value
input variables
decision process
probabilistic framework
models
section
example
set
k models
conditional distribution p
t|x
k
x
input
target
=
k
model
probabilistic mixture
form p
t|x
= πk
x
p
t|x
k
πk
x
p
k|x
input-dependent mixing coefﬁcients
such models
mixture distributions
component densities
mixing coefﬁcients
input variables
mixtures
experts
mixture density network model
section
combining models
bayesian model averaging section
model combination methods
bayesian model
difference
con- sider
example
density estimation
mixture
gaussians
several gaussian components
model
binary latent variable z
component
mixture
corresponding data point
model
terms
joint distribution
corresponding density
observed variable x
marginal- izing
latent variable p
z
p
x
p
z
z
 n   n   k=1 k h
case
gaussian mixture example
distribution
form
committees
p
x
πkn
x|µk
σk
usual interpretation
symbols
example
model combi- nation
data
marginal probability
data set x =
x1
form p
x
n=1 p
xn
p
zn
n=1 zn thus
data point xn
corresponding latent variable zn
several different models
h =
h
prior probabilities
h
instance
model
mixture
gaussians
model
mixture
cauchy distributions
marginal distribution
data set
p
x
p
x|h
p
h
h=1
example
bayesian model
interpretation
summa- tion
h
model
whole data set
probability distribution
h
uncertainty
model
size
data set increases
uncertainty
posterior probabilities
h|x
models
key difference
bayesian model
model combination
bayesian model
whole data set
single model
contrast
multiple models
different data points
data set
different values
latent variable z
hence
different components
marginal probability p
x
same consid- erations
predictive density p
conditional distributions
p
t|x
x
t
committees
way
committee
predictions
set
individual models
procedure
frequentist perspective
trade-off
bias
variance
er- ror
model
bias component
differences
model
true function
variance component
repre- sents
sensitivity
model
individual data points
figure
    
m ex
m m m m
m
m m=1 ⎡⎣
m  ⎤⎦   ⎤⎦
combining models
multiple polynomials
sinusoidal data
resulting functions
contribution
variance term
predictions
set
low-bias mod- els
order polynomials
accurate predictions
sinusoidal function
data
practice
course
single data set
way
variability
different models
committee
approach
bootstrap data sets
section
regression problem
value
single continuous variable
m bootstrap data sets
separate copy ym
x
predictive model
m
m.
committee prediction
ycom
x
ym
x
procedure
bootstrap aggregation
bagging
breiman
true regression function
h
x
output
models
true value
error
form ym
x
h
x
m
x
average sum-of-squares error
form ex
ex [ · ]
frequentist expectation
respect
distribution
input vector x
average error
models
ym
x
h
x
m
x
= ex eav = m
x
error
committee
ecom = ex = ex
ym
x
h
x
m
x
errors
mean
ex [ m
x
ex [ m
x
l
x
l
exercise
ecom
m eav
dramatic result
average error
model
factor
m
m versions
model
key assumption
errors
individual models
practice
errors
reduc- tion
overall error
expected committee error
expected error
constituent models
ecom � eav
order
signiﬁcant improvements
sophisticated technique
building committees
boosting
exercise
boosting
powerful technique
multiple ‘ base ’ classiﬁers
form
committee
performance
base classiﬁers
used form
algorithm
adaboost
‘ adaptive boosting ’
freund
schapire
good results
base classiﬁers
performance
random
base classiﬁers
weak learners
classiﬁcation problems
regression
friedman
principal difference
committee methods
base classiﬁers
sequence
base classiﬁer
weighted form
data set
weighting coefﬁcient
data point
performance
previous classiﬁers
points
base classiﬁers
weight
next classiﬁer
sequence
classiﬁers
predictions
weighted majority
scheme
figure
two-class classiﬁcation problem
training data comprises
vectors x1
binary target variables
tn ∈
−1
data point
associated weighting parameter wn
data points
procedure
base classiﬁer
data
function y
x
−1
stage
algorithm
adaboost
new classiﬁer
data set
weighting coefﬁcients
performance
classiﬁer
weight
misclassiﬁed data points
desired number
base classiﬁers
committee
coefﬁcients
different weight
different base classiﬁers
precise form
adaboost algorithm
w
m
n
ym
x
αmym
x
 m m  
n n 
n
combining models figure
schematic illustration
boosting framework
base classiﬁer ym
x
weighted form
train- ing set
blue arrows
weights w
m
depend
performance
pre- vious base classiﬁer ym−1
x
green arrows
base classiﬁers
ﬁnal classiﬁer ym
x
red arrows
w
n
y1
x
adaboost
w
n
y2
x
ym
x
data weighting coefﬁcients
wn
w
m =
m
n =
n.
fit
classiﬁer ym
x
training data
weighted error function jm = n
xn
= tn
w
m
xn
= tn
indicator function
equals
ym
xn
tn
b
quantities
xn
= tn
w
m
m = n=1 w
m
n
αm =
− m m
c
data weighting coefﬁcients
w
m+1
w
m
αmi
ym
xn
= tn
  m   m  n
predictions
ﬁnal model
ym
x
sign αmym
x
m=1
ﬁrst base classiﬁer y1
x
coefﬁ- cients w
n
usual procedure
single classiﬁer
from
subsequent iterations
weighting coefﬁcients
m
data points
data points
successive classiﬁers
emphasis
points
previous classiﬁers
data points
successive classiﬁers
weight
quantities
represent
mea- sures
error rates
base classiﬁers
data set
weighting coefﬁcients
weight
accurate classiﬁers
overall output
adaboost algorithm
figure
subset
data points
toy classiﬁcation data
figure a.7
base
consists
threshold
input variables
simple classiﬁer corresponds
form
decision tree
‘ decision stumps ’
i.e.
deci- sion tree
single node
base learner
input
input
threshold
parti- tions
space
regions
linear decision surface
axes
exponential error boosting
statistical learning theory
upper bounds
generalization error
bounds
practical value
actual performance
bounds
friedman
al
simple interpretation
terms
sequential minimization
exponential error function
exponential error function
e =
−tnfm
xn
fm
x
classiﬁer
terms
linear combination
base classiﬁers
x
form fm
x
αlyl
x
l=1
∈
−1
training
target values
goal
e
respect
weighting coefﬁcients
parameters
base classiﬁers
x
section
 
n n
combining models m
m
−2
−2
−2
−2 −1
−1 m
m
−2
−2 −1
m
m
figure
illustration
base learners
simple thresholds
axes
ﬁgure
number m
base learners
decision boundary
recent base learner
dashed black line
combined decision boundary
solid green line
data point
circle
radius
weight
data point
base learner
instance
points
m
base learner
weight
m
base learner
global error function minimization
pose
base classiﬁers
x
ym−1
x
coefﬁcients
αm−1
respect
x
contribution
base classiﬁer ym
x
error function
form e = exp −tnfm−1
xn
tnαmym
xn
w
m
tnαmym
xn
−
coefﬁcients
m
−tnfm−1
xn
constants
αm
ym
x
tm
set
data points
ym
x
misclassiﬁed points
mm
error function
 form e =
n∈tm  n  w
m
n
w
m
− e−αm/2
n
xn
= tn
e−αm/2 w
m
w
m
n
n=1
respect
x
second term
con- stant
overall multiplica- tive factor
front
summation
location
minimum
respect
m
from
αm
ym
x
weights
data points
w
m+1
n
use
fact
w
m
tnαmym
xn
−
tnym
xn
ym
xn
= tn
weights w
m
n
next iteration
exp
−αm/2
αmi
ym
xn
= tn
n n = w
m
w
m+1
term exp
−αm/2
data points
same factor
base classiﬁers
new data points
sign
combined function
factor
sign
error functions
exponential error function
adaboost algorithm differs
previous chapters
insight
nature
exponential error function
expected error
ex
t [
−ty
x
= t exp
−ty
x
p
t|x
p
x
dx
variational minimization
respect
possible functions
x
y
x
p
t =
p
t = −1|x
exercise
exercise
combining models figure
plot
cross-entropy
error functions
hinge er- ror
support vector machines
misclassiﬁcation
large error
negative values
z = ty
x
linearly in- creasing penalty
expo- nential loss
penalty
e
z
z
adaboost algorithm
approx- imation
log odds ratio
space
functions
linear combination
base classiﬁers
minimization
sequential optimization strategy
result
use
sign function
ﬁnal classiﬁcation decision
minimizer y
x
cross-entropy error
two-class classiﬁcation
posterior class probability
case
target variable t ∈
−1
error function
ln
+ exp
−yt
exponential error function
fig- ure
cross-entropy error
constant factor ln
point
ease
comparison
continuous approximations
ideal misclassiﬁcation error func- tion
advantage
exponential error
sequential minimization
simple adaboost scheme
drawback
large negative values
ty
x
cross-entropy
large negative values
ty
cross-entropy grows
|ty|
exponential error function
|ty|
ex- ponential error function
outliers
misclassiﬁed data points
important difference
cross-entropy
exponential er- ror function
latter
log likelihood function
well-deﬁned probabilistic model
exponential error
problems
k >
classes
contrast
cross-entropy
probabilistic model
interpretation
sequential optimization
additive model
exponential error
friedman
door
wide range
boosting-like algorithms
multiclass extensions
choice
error function
extension
problems
friedman
sum-of-squares error function
regression
sequential minimization
additive model
form
new base classiﬁer
residual errors
xn
previous model
sum-of-squares error
outliers
section
exercise
section
exercise
figure
comparison
squared error
absolute error
latter places
emphasis
large errors
hence
outliers
data points
tree-based models
e
z
algorithm
absolute deviation |y − t|
error functions
figure
z
tree-based models
models
work
input space
cuboid regions
edges
axes
simple model
example
constant
region
model combination method
model
predictions
point
input space
process
speciﬁc model
new input x
sequential decision
process
traversal
binary tree
branches
node
particular tree-based framework
classiﬁcation
regression trees
cart
breiman
many other variants
such names
id3
c4.5
quinlan
quinlan
figure
illustration
recursive binary partitioning
input space
corresponding tree structure
example
ﬁrst step figure
illustration
two-dimensional in- put space
ﬁve regions
axis-aligned boundaries
θ3 θ2 b a e c d θ1 θ4
combining models figure
binary tree
par- input space
fig- titioning
x1 > θ1 x2 � θ2 x2 > θ3 x1 �
a b c d e
whole
input space
regions
x1 � θ1
x1 > θ1
θ1
parameter
model
subregions
instance
region x1 � θ1
x2 � θ2
x2 > θ2
rise
regions
b
recursive subdivision
traversal
binary tree
figure
new input x
region
top
tree
root node
path
speciﬁc leaf node
decision criteria
node
note
such decision trees
probabilistic graphical models
region
separate model
target
instance
regression
constant
region
classiﬁcation
region
speciﬁc class
key property
tree-
models
ﬁelds
medical diagnosis
example
humans
sequence
binary decisions
individual input variables
in- stance
patient ’ s disease
“
temperature
threshold
answer
ask “
blood pressure
threshold
leaf
tree
speciﬁc diagnosis
order
model
training set
structure
tree
input variable
node
split criterion
value
threshold parameter θi
split
values
predictive variable
region
regression problem
goal
single target variable t
d-dimensional vector x =
x1
xd
t
input variables
training data consists
input vectors
x1
continuous labels
t1
partitioning
input space
sum-of-squares error function
optimal value
predictive variable
region
average
values
tn
data points
region
structure
decision tree
ﬁxed number
nodes
tree
problem
optimal structure
choice
input
split
corresponding thresh- exercise
 
nτ |t| τ
pruning criterion
qτ
t
tn − yτ
c
t
qτ
t
regularization parameter λ
trade-off
overall residual sum-of-squares error
complexity
model
number |t|
leaf nodes
value
cross-validation
classiﬁcation problems
process
tree
sum-of-squares error
appropriate measure
tree-based models
olds
sum-of-squares error
large number
possible solutions
greedy opti- mization
single root node
whole input space
tree
time
step
number
candidate regions
input space
addition
pair
leaf nodes
tree
choice
d input variables
value
threshold
joint optimization
choice
region
choice
input
exhaustive search
choice
split
optimal choice
predictive variable
local average
data
possible choices
one
residual sum-of-squares error
greedy strategy
tree
issue
nodes
simple approach
reduction
residual error
threshold
none
available splits
signiﬁcant reduction
error
splits
substantial error reduction
reason
com- mon practice
large tree
criterion
number
data points
leaf nodes
resulting tree
pruning
criterion
residual error
measure
model complexity
starting tree
t0
t ⊂ t0
subtree
t0
nodes
t0
other words
internal nodes
corresponding regions
leaf nodes
τ =
leaf node τ
region rτ
input space
nτ data points
|t|
total number
leaf nodes
optimal prediction
region rτ
yτ = tn xn∈rτ
corresponding contribution
residual sum-of-squares
  k=1 k k qτ
t
pτ k
− pτ k
k=1
exercise
vanish
pτ k =
k =
maximum
pτ k =
formation
regions
high proportion
data points
class
cross entropy
gini index
measures
misclassiﬁcation rate
tree
node probabilities
misclassiﬁcation rate
optimization methods
subsequent pruning
tree
misclassiﬁcation rate
human interpretability
tree model
cart
major strength
practice
particular tree structure
details
data set
small change
training data
different set
splits
hastie
other problems
tree-based methods
kind
section
splits
axes
feature space
instance
classes
optimal decision boundary runs
degrees
axes
large number
axis-parallel splits
input space
single non-axis-aligned split
splits
decision tree
region
input space
node model
last issue
regression
smooth functions
tree model
piecewise-constant predictions
discontinuities
split boundaries
conditional mixture models
standard decision trees
axis-aligned splits
input space
constraints
expense
interpretability
probabilistic splits
functions
input variables
time
leaf models
probabilistic inter- pretation
probabilistic tree-based model
hierarchical mixture
experts
section
alternative way
hierarchical mixture
experts model
standard probabilistic mixtures
unconditional density models
gaussians
component densities
conditional distributions
mixtures
linear regression models
section
mixtures
chapter
combining models
performance
pτ k
proportion
data points
region rτ
class k
k
k
choices
cross-entropy qτ
t
pτ k ln pτ k
gini index     k=1 k=1 k k  
n k  n 
conditional mixture models
logistic regression models
section
case
coefﬁ- cients
input variables
further generalization
mixing coefﬁcients
inputs
mixture
experts model
component
mixture model
mixture
experts model
hierarchical mixture
experts
mixtures
linear regression models
many advantages
probabilistic interpretation
lin- ear regression model
component
complex probabilistic models
instance
conditional distribution
linear regression model
node
directed prob- abilistic graph
simple example
mixture
linear regression models
straightforward extension
gaus- sian mixture model
section
case
conditional gaussian distributions
k linear regression models
own weight parameter wk
many applications
common noise variance
precision parameter β
k components
case
restrict attention
single target variable t
extension
outputs
mixing coefﬁcients
πk
mixture distribution
t|θ
= πkn
t|wt k φ
β−1
θ
set
adaptive parameters
model
w =
wk
π =
πk
β
log likelihood function
model
data set
observations
φn
tn
form ln p
t|θ
ln πkn
tn|wt k φn
β−1
t =
t1
tn
t
vector
target variables
order
likelihood function
appeal
em algorithm
simple extension
em algorithm
unconditional gaussian mixtures
section
expe- rience
unconditional mixture
set z =
zn
binary latent variables
znk ∈
data point n
elements
=
k
single value
component
mixture
data point
joint distribution
latent
variables
graphical model
figure
complete-data log likelihood function
form ln p
t
z|θ
n=1 k=1 znk ln πkn
tn|wt k φn
β−1
exercise
exercise
 zn tn φn n    
k n  
n πk = π β w  n
combining models figure
probabilistic
graph
mixture
linear regression models
em algorithm begins
ﬁrst
initial value θold
model param- eters
e step
parameter values
posterior probabilities
responsibilities
component k
data point n
γnk = e [ znk ] = p
tn|wt j πjn
tn|wt k φn
β−1
j φn
β−1
responsibilities
expectation
respect
posterior distribution p
z|t
θold
complete-data log likelihood
form q
θ
θold
ez [ ln p
t
z|θ
= γnk ln πk + lnn
tn|wt k φn
β−1
m step
function q
θ
θold
respect
γnk ﬁxed
optimization
respect
mixing coefﬁcients
k πk =
aid
account
constraint lagrange multiplier
m-step re-estimation equation
πk
form exercise
n γnk
note
same form
corresponding result
simple mixture
unconditional gaussians
next consider
maximization
respect
parameter vector wk
kth linear regression model
gaussian distribution
function q
θ
θold
function
parameter vector wk
form q
θ
θold
γnk n=1 β
− tn − wt k φn
+ const
constant term
contributions
other weight vectors
j = k. note
quantity
standard sum-of-squares error
single linear regression model
inclusion
responsibilities
weighted least squares
  γnk
n=1 n n k n=1 k=1  n  k
conditional mixture models
problem
term
nth data point
weighting coefﬁcient
βγnk
effective precision
data point
component linear regression model
mixture
own parameter vector wk
whole data set
m step
data point
responsibility
model k
data point
derivative
respect
= tn − wt k φn φn
matrix notation
= φtrk
t − φwk
rk = diag
γnk
diagonal matrix
size n × n. solving
wk
set
modiﬁed normal equations
squares problem
same form
context
logistic regression
note
e step
matrix rk
normal equations
subsequent m step
−1 φtrkt
wk = finally
q
θ
θold
respect
terms
β
function q
θ
θold
q
θ
θold
n=1 k=1 ln β −
tn − wt k φn
respect
m-step equation
β
form
β
n γnk tn − wt k φn
figure
em algorithm
simple example
mixture
straight lines
data set
input variable x
target variable t.
predictive density
figure
converged parameter values
em algorithm
right-hand plot
figure
ﬁgure
result
single linear regression model
unimodal predictive density
mixture model
representation
data distribution
likelihood value
mixture model
signiﬁcant probability mass
regions
data
predictive distribution
values
x
problem
model
mixture
functions
x
models
mixture density networks
section
hierarchical mixture
experts
section
combining models
−0.5 −1
−0.5 −1 −1.5 −1
−1
−1
−1
−0.5 −1
−1
−1
k=1 k
figure
example
synthetic data set
green points
input variable x
target variable t
mixture
linear regression models
mean functions
k ∈
blue
red lines
plots
initial conﬁguration
result
iterations
em
centre
result
iterations
em
β
reciprocal
true variance
set
target values
plots
corresponding responsibilities
vertical line
data point
length
blue segment
posterior probability
blue line
data point
red segment
mixtures
logistic models
logistic regression model
conditional distribution
target
input vector
component distribution
mixture model
rise
family
conditional distributions
single logistic regression model
example
straightforward combination
ideas
sections
book
reader
conditional distribution
target
probabilistic mixture
k logistic regression models
p
t|φ
θ
πkyt k [
− yk ]
φ
feature vector
= σ
adjustable parameters
wk
wt k φ
data set
φn
tn
corresponding likelihood
output
component k
θ       k=1 k k   n n
  
conditional mixture models
figure
left plot
predictive conditional density
solution
figure
log likelihood value
−3.0
a vertical slice
plots
particular value
corresponding conditional distribution p
t|x
plot
right
predictive density
single linear regression model
same data set
maximum likelihood
model
log likelihood
−27.6
function
πkytn p
t|θ
=
ynk = σ
wt k φn
t =
t1
tn
likelihood function
use
em algorithm
latent variables
correspond
1-of-k coded binary indicator variable
data point
complete-data likelihood function
− ynk ]
n=1 p
t
z|θ
πkytn nk [
− ynk
znk
z
matrix
latent variables
elements znk
em algorithm
initial value θold
model parameters
e step
parameter values
posterior probabilities
com- ponents
data point n
γnk = e [ znk ] = p
πkytn j πjytn nk
− ynk
− ynj
responsibilities
expected complete-data log likelihood
function
θ
q
θ
θold
ez [ ln p
t
z|θ
= k n γnk
ln πk + tn ln ynk +
− tn
ln
− ynk
  n  
n n=1 n=1 n n k
combining models section
section
exercise
m step
maximization
function
respect
γnk
ﬁxed
maximization
respect
usual way
πk =
lagrange multiplier
summation constraint
familiar result πk = γnk
wk
q
θ
θold
function
sum
terms
vectors
different vectors
m step
em algorithm
other words
different components
responsibilities
m step
note
m step
closed-form solution
instance
squares
irls
algorithm
gradient
hessian
vector wk
∇kq =
γnk
tn − ynk
hk = −∇k∇kq = γnkynk
− ynk
φnφt n
∇k
gradient
respect
ﬁxed γnk
indepen- dent
wj
j = k
wk
irls algorithm
m-step equations
component k correspond
single logistic regression model
weighted data set
data point n
weight γnk
figure
example
mixture
logistic regression models
simple classiﬁcation problem
extension
model
mixture
softmax models
classes
mixtures
experts
section
mixture
linear regression models
section
analogous mixture
linear classiﬁers
simple mixtures
ﬂexibility
linear models
com- plex
multimodal
predictive distributions
capability
such models
mixing
functions
input
p
t|x
k=1 πk
x
pk
t|x
mixture
experts model
jacobs
mix- ing coefﬁcients
x
functions
individual component densities
t|x
experts
notion
terminology
differ- ent components
distribution
different regions
input space
‘ experts
predictions
own regions
gating functions
components
region
functions πk
x
usual constraints
co- efﬁcients
� πk
x
πk
x
example
linear softmax models
form
experts
regression
classiﬁcation
models
whole model
em algorithm
squares
m step
jordan
jacobs
section
model
signiﬁcant limitations
use
linear models
gating
expert functions
ﬂexible model
multilevel
function
hierarchical mixture
experts
hme model
jordan
jacobs
structure
model
mixture distribution
component
mixture
mixture distribution
simple unconditional mixtures
hierarchical mixture
single ﬂat mixture distribution
mixing coefﬁcients
hierarchical model
hme model
probabilistic version
decision trees
section
maximum likelihood
em algorithm
irls
m step
bayesian treatment
hme
bishop
svens´en
variational inference
hme
detail
close connection
mixture density network
section
principal advantage
mixtures
experts model
em
m step
mixture component
model
convex optimization
overall optimization
con- trast
advantage
mixture density network approach
component
conditional mixture models
figure
illustration
mixture
logistic regression models
left plot shows
points
classes
background colour
pure
blue
true probability
class label
centre plot
result
single logistic regression model
maximum likelihood
background colour
corresponding probability
class label
colour
near-uniform purple
model
probability
classes
input space
right plot
result
mixture
logistic regression models
probability
correct labels
points
blue class
combining models exercises  m  m densities
mixing coefﬁcients share
hidden units
neural network
furthermore
mixture density network
splits
input space
further relaxed
hierarchical mixture
experts
 
www
set models
form p
t|x
zh
θh
h
x
input vector
t
target vector
h indexes
different models
zh
latent vari-
model h
θh
set
parameters
model h. suppose
models
prior probabilities
h
training
x =
x1
t =
t1
write
formulae
predic- tive distribution p
t|x
x
t
latent variables
model index
formulae
difference
bayesian averaging
different models
use
latent variables
single model

sum-of-squares error eav
simple committee model
expected error
committee
individual errors
result

www
use
jensen ’ s inequality
special case
convex function f
x
= x2
average expected sum-of-squares
eav
members
simple committee model
expected error ecom
committee
satisfy ecom � eav
 
use
jensen ’ s
equality
result
previous exercise hods
error function e
y
sum-of- squares
convex function
y
 
www
committee
unequal weighting
constituent models
ycom
x
m=1 αmym
x
order
predictions
x
sensible limits
sup- pose
value
x
minimum
maximum values
members
committee
ymin
x
ycom
x
ymax
x
show
sufﬁcient condition
constraint
coefﬁ- cients αm satisfy αm �
αm
m=1

www
error function
respect
parameters
adaboost algorithm
m

variational minimization
exponential error function
respect
possible functions
x
minimizing function

show
exponential error function
adaboost algorithm
log likelihood
well-behaved probabilistic model
corresponding conditional distribution p
t|x

www show
sequential minimization
sum-of-squares
func- tion
additive model
form
style
new base classiﬁer
residual errors
xn
previous model

sum-of-squares error
set
training values
tn
single predictive value t
optimal solution
t
mean
tn
 
data set
data points
class c1
data points
class c2
suppose
tree model
ﬁrst leaf node
second leaf node
m
n points
c1
m points
c2
second tree model b
misclassiﬁcation rates
trees
hence
cross-entropy
gini index
trees
tree b
tree a
 
results
section
mixture
linear regression models
case
multiple target values
vector t.
use
results
section 3.1.5

www verify
complete-data log likelihood function
mixture
linear regression models

technique
lagrange multipliers
appendix e
m-step re-estimation equation
mixing coefﬁcients
mixture
linear regression models
maximum likelihood em

www
squared loss function
regres- sion problem
optimal prediction
target
new input vector
conditional mean
predictive distribution
show
conditional mean
mixture
linear regression models
section
linear combination
means
component dis- tribution
note
conditional distribution
target data
conditional mean
poor predictions
k
combining models
  
logistic regression mixture model
section
mixture
softmax classiﬁers
c
classes
em algorithm
parameters
model
maximum likelihood
 
www
mixture model
conditional distribution p
t|x
form p
t|x
k=1 πkψk
t|x
mixture component ψk
t|x
mixture model
two-level hierarchical mixture
conventional single-level mixture model
mixing coefﬁcients
levels
hierar- chical model
arbitrary functions
x
again
hierarchical model
single-level model
x-dependent mixing coefﬁcients
case
mixing coefﬁcients
levels
hi- erarchical mixture
linear classiﬁcation
softmax
models
hierarchical mixture
single-level mixture
linear classiﬁcation models
mixing coefﬁcients
hint
single counter-example
mixture
components
components
mixture
components
coefﬁcients
linear-logistic models
single-level mixture
components
coefﬁcients
linear-softmax model
