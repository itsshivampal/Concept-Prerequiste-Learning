{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.chunk import RegexpParser\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index_file = \"index_text/bprml.txt\"\n",
    "# chapter_file = \"chapter_text/bprml.txt\"\n",
    "\n",
    "index_file = \"index_text/iandl.txt\"\n",
    "chapter_file = \"chapter_text/iandl.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following functions are use for extracting text file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toggle_state(class_indexing):\n",
    "    if class_indexing: class_indexing = False\n",
    "    else: class_indexing = True\n",
    "    return class_indexing\n",
    "   \n",
    "\n",
    "def line_split(line):\n",
    "    tokens = line.split(\",\")\n",
    "    tokens_length = len(tokens)\n",
    "    keyword = []\n",
    "    keyword.append(tokens[0].strip())\n",
    "    for i in range(tokens_length - 1):\n",
    "        token = tokens[i+1].strip()\n",
    "        if \"see\" in token:\n",
    "            keyword.append(token[4:])\n",
    "    return keyword\n",
    "\n",
    "\n",
    "def data_append(parent_class, keyword):\n",
    "    size = len(keyword)\n",
    "    sub_class = keyword[0]\n",
    "    if size > 1: reference = keyword[1]\n",
    "    else: reference = \"\"\n",
    "    data = {\n",
    "        'Parent Class': parent_class,\n",
    "        'Sub Class': sub_class,\n",
    "        'Reference': reference\n",
    "    }\n",
    "    return (data)\n",
    "\n",
    "\n",
    "def line_operation(line, parent_class, class_indexing):\n",
    "    keyword = line_split(line)\n",
    "    if class_indexing:\n",
    "        data = data_append(parent_class, keyword)\n",
    "    else:\n",
    "        parent_class = keyword[0]\n",
    "        data = data_append(parent_class, keyword)\n",
    "    return (parent_class, data)\n",
    "\n",
    "    \n",
    "def data_extraction(line, parent_class, class_indexing):\n",
    "    line = line.lower()\n",
    "    \n",
    "    if line[0] == ',' or (line[0] >= '0' and line[0] <= '9'):\n",
    "        return (0, parent_class, class_indexing, \"\")\n",
    "    \n",
    "    elif line[0] == '\\n':\n",
    "        class_indexing = toggle_state(class_indexing)\n",
    "        return (0, parent_class, class_indexing, \"\")\n",
    "    \n",
    "    else:\n",
    "        parent_class, data = line_operation(line, parent_class, class_indexing)\n",
    "        return (1, parent_class, class_indexing, data)\n",
    "\n",
    "def print_processed_data(df):\n",
    "    parent_class = df[[\"Parent Class\"]]\n",
    "    sub_class = df[[\"Sub Class\"]]\n",
    "    reference = df[[\"Reference\"]]\n",
    "\n",
    "    for i in range(df.shape[0]):\n",
    "        if parent_class.iloc[i].values[0] == sub_class.iloc[i].values[0]:\n",
    "            print(sub_class.iloc[i].values[0], \",\", reference.iloc[i].values[0])\n",
    "        else:\n",
    "            print(\"\\t\", sub_class.iloc[i].values[0], \",\", reference.iloc[i].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction from index file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_index_data(file_location, df):\n",
    "#     df = pd.DataFrame(columns=['Parent Class', 'Sub Class', 'Reference'])\n",
    "    parent_class = \"\"\n",
    "    class_indexing = False\n",
    "    \n",
    "    file = open(file_location)\n",
    "    for line in file:\n",
    "        state, parent_class, class_indexing, data = data_extraction(line, parent_class, class_indexing)\n",
    "        if state:\n",
    "            df = df.append(data, ignore_index=True)\n",
    "    \n",
    "    save_file = file_location.split(\"/\")[-1].split(\".\")[0] + \".csv\"\n",
    "    df.to_csv(save_file)\n",
    "    \n",
    "    file.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Parent Class', 'Sub Class', 'Reference'])\n",
    "df = extract_index_data(index_file, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# maximum length of keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_class = df[[\"Parent Class\"]]\n",
    "sub_class = df[[\"Sub Class\"]]\n",
    "index_length = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1\n",
    "\n",
    "def cal_max_length(n1, n2):\n",
    "    if n1 > n2: return n1\n",
    "    else: return n2\n",
    "\n",
    "for i in range(index_length):\n",
    "    x1 = parent_class.iloc[i].values[0]\n",
    "    x2 = sub_class.iloc[i].values[0]\n",
    "    if x1 == x2:\n",
    "        max_length = cal_max_length(max_length, len(x2.split(\" \")))\n",
    "    else:\n",
    "        max_length = cal_max_length(max_length, len(x2.split(\" \")) + len(x1.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keywords extraction from CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "581\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "singular_terms = []\n",
    "different_terms = []\n",
    "\n",
    "for i in range(index_length):\n",
    "    x1 = parent_class.iloc[i].values[0]\n",
    "    x2 = sub_class.iloc[i].values[0]\n",
    "    if x1 == x2:\n",
    "        singular_terms.append(x2)\n",
    "    else:\n",
    "        terms = [x1, x2]\n",
    "        different_terms.append(terms)\n",
    "\n",
    "print(len(singular_terms))\n",
    "print(len(different_terms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyphrase Extraction from Book Chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_candidate_keywords(chunkGram, text):\n",
    "    chunkParser = nltk.RegexpParser(chunkGram)\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "\n",
    "    chunked = chunkParser.parse(tagged)\n",
    "    candidate_keywords = []\n",
    "\n",
    "    for tree in chunked.subtrees():\n",
    "        if tree.label() == 'PHRASE':\n",
    "            candidate_keyword = ' '.join([x for x,y in tree.leaves()])\n",
    "            candidate_keywords.append(candidate_keyword)\n",
    "    \n",
    "    return candidate_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_phrase(phrase):\n",
    "    new_string = \"\"\n",
    "    for word in phrase.split(\" \"):\n",
    "        word = porter.stem(word)\n",
    "        new_string += word + \" \"\n",
    "    return new_string[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "raw = open(chapter_file).read()\n",
    "\n",
    "chunkGram1 = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}\n",
    "        \n",
    "    PHRASE:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}\n",
    "\"\"\"\n",
    "\n",
    "chunkGram2 = r\"\"\" PHRASE: \n",
    "                {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_keywords = extract_candidate_keywords(chunkGram1, raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_keywords = [word.lower() for word in candidate_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"candidate_keyword.txt\", 'w+')\n",
    "\n",
    "for word in candidate_keywords:\n",
    "    f.write(word)\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "candidate_keywords = set(candidate_keywords)\n",
    "candidate_keywords = [w for w in candidate_keywords if len(w.split(' ')) < max_length]\n",
    "\n",
    "print(len(candidate_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290\n"
     ]
    }
   ],
   "source": [
    "filtered_keywords = [clean_phrase(w) for w in candidate_keywords]\n",
    "\n",
    "filtered_keywords = set(filtered_keywords)\n",
    "\n",
    "print(len(filtered_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Singular Keywords from Extracted List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matched_words = []\n",
    "unmatched_words = []\n",
    "for w in singular_terms:\n",
    "    count = 0\n",
    "    for w1 in filtered_keywords:\n",
    "        if clean_phrase(w) in w1:\n",
    "            count += 1\n",
    "    if count == 0:\n",
    "        unmatched_words.append(w)\n",
    "    else:\n",
    "        matched_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched_words:  26\n",
      "not matched:  555\n"
     ]
    }
   ],
   "source": [
    "print(\"matched_words: \", len(set(matched_words)))\n",
    "print(\"not matched: \", len(unmatched_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adam', 'ais', 'broadcasting', 'contrast', 'deep learning', 'dot product', 'element-wise product', 'example', 'generalization', 'graph', 'hadamard product', 'identity matrix', 'main diagonal', 'matrix', 'matrix inverse', 'matrix product', 'nat', 'operation', 'precision', 'regularization', 'regularizer', 'scalar', 'set', 'tensor', 'transpose', 'vector']\n"
     ]
    }
   ],
   "source": [
    "print(matched_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching Parent and Sub Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between-class covariance\n",
      "within-class covariance\n",
      "partitioned covariance matrix\n",
      "conditional entropy\n",
      "differential entropy\n",
      "relative entropy\n",
      "functional derivative\n",
      "conditional gaussian\n",
      "gaussian marginal\n",
      "gaussian mixture\n",
      "directed graphical model\n",
      "undirected graphical model\n",
      "autoregressive hidden markov model\n",
      "factorial hidden markov model\n",
      "input-output hidden markov model\n",
      "left-to-right hidden markov model\n",
      "extended kalman ﬁlter\n",
      "gaussian kernel function\n",
      "fisher linear discriminant\n",
      "linear regression problem\n",
      "variational linear regression\n",
      "bayesian logistic regression\n",
      "logistic regression mixture model\n",
      "multiclass logistic regression\n",
      "margin error\n",
      "soft margin\n",
      "homogeneous markov chain\n",
      "message passing schedule\n",
      "variational message passing\n",
      "conditional mixture model\n",
      "logistic regression mixture model\n",
      "neural network input imag convolutional\n",
      "perceptron convergence theorem\n",
      "perceptron hardware\n",
      "conjugate prior\n",
      "consistent gaussian prior\n",
      "improper prior\n",
      "noninformative prior\n",
      "bayesian probability\n",
      "probability density\n",
      "probability mass function\n",
      "prior probability\n",
      "probability sum rule\n",
      "probability theory\n",
      "tikhonov regularization\n"
     ]
    }
   ],
   "source": [
    "def find_best_string(all_strings):\n",
    "    return max(set(all_strings), key = all_strings.count)\n",
    "\n",
    "\n",
    "def matching_pattern(keyword1, keyword2, phrase):\n",
    "#     phrase = clean_phrase(phrase)\n",
    "    stem1 = clean_phrase(keyword1)\n",
    "    stem2 = clean_phrase(keyword2)\n",
    "    if stem1 in phrase and stem2 in phrase:\n",
    "        len1 = phrase.index(stem1)\n",
    "        len2 = phrase.index(stem2)\n",
    "        \n",
    "        if len1 > len2:\n",
    "            start = len2 + len(stem2)\n",
    "            end = len1\n",
    "            string = keyword2 + phrase[start:end] + keyword1\n",
    "        else:\n",
    "            start = len1 + len(stem1)\n",
    "            end = len2\n",
    "            string = keyword1 + phrase[start:end] + keyword2\n",
    "        return 1, string\n",
    "    else: return 0, \"\"\n",
    "\n",
    "count = 0\n",
    "successful_phrases = []\n",
    "unsuccessful_phrases = []\n",
    "\n",
    "\n",
    "for pair in different_terms:\n",
    "    match_count = 0\n",
    "    possible_strings = []\n",
    "    for phrase in filtered_keywords:\n",
    "        result, string = matching_pattern(pair[0], pair[1], phrase)\n",
    "        if result != 0:\n",
    "            possible_strings.append(string)\n",
    "            match_count += 1\n",
    "    if match_count > 0:\n",
    "        main_keyword = find_best_string(possible_strings)\n",
    "        successful_phrases.append(main_keyword)\n",
    "        count += 1\n",
    "        print(main_keyword)\n",
    "    else:\n",
    "        unsuccessful_phrases.append(pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "print(len(successful_phrases))\n",
    "print(len(unsuccessful_phrases))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
