Contents

Preface

Mathematical notation

Contents

1

Introduction
1.1
1.2

. . . . . . . .
. . . . . . .
. . . . . . . .

. . .

. . . . . . . .

Probability densities

Example: Polynomial Curve Fitting . .
Probability Theory .
1.2.1
. . . . .
1.2.2 Expectations and covariances
1.2.3 Bayesian probabilities
1.2.4 The Gaussian distribution . . . . . . .
1.2.5 Curve ﬁtting re-visited . . . . .
1.2.6 Bayesian curve ﬁtting . .

. . . . .

.
. .

. . .

. . .

. . .

. . . . . . . . . .
. . . . . .

. . .

1.3 Model Selection . . . . . . . . . .
1.4
1.5 Decision Theory . . . . . . . . . .

The Curse of Dimensionality . . . . .

1.5.1 Minimizing the misclassiﬁcation rate
1.5.2 Minimizing the expected loss
1.5.3 The reject option . .
1.5.4
1.5.5 Loss functions for regression . . . .
Information Theory . . . . . . . . .
1.6.1 Relative entropy and mutual information . .

Inference and decision . . . . . . . . .
. .

. . . . . . . . . .

. . . . . . . .
.

. .
. .

. . .

. . .

.

. . . . . . . .

1.6

Exercises

.

.

. . .

. . . .

. . . . . . . .

. . . . .

. . . . .

. . . . . . . .

. . . . . . . .

. .

. . . . . .

. . .
. . . .

. . .
. . . . .

. . . . . . .
. .
. . . . .
. .
. .
. . .
. . . . . . . . .
.
.
.
. . .
. . .
.
.
. . .
. .
. . . . .
. . .
. . . . . . . .
. . .
.
. . . .
.
. . .
.
.
. . . . .
.
. . . . .
. . . . . . .

. . .
. . .
. . . . .
. . . . . . .

. . .

. . .

. . . . . . . . . . .

. . .

. . . . . .
. . . .

vii

xi

xiii

1
4
12
17
19
21
24
28
30
32
33
38
39
41
42
42
46
48
55
58

xiii

2.1.1 The beta distribution .

. . . . . . . . .

2.2 Multinomial Variables . . . . .

. . .

. . . . . . . . .

. . .

. . . .
. . .

. . . . .
. . .
. . . . . . . . . . .

. . . . . .

. . . . . . . . .
. . . . . . . . .

. . .

. . .

. . .

. . . . . . . . .

Sequential estimation . . . .

2.2.1 The Dirichlet distribution . . . . . .
The Gaussian Distribution .
2.3.1 Conditional Gaussian distributions . . .
2.3.2 Marginal Gaussian distributions . . . .
2.3.3 Bayes’ theorem for Gaussian variables . . . . .
2.3.4 Maximum likelihood for the Gaussian . . . . . .
2.3.5
. . . . . . . .
2.3.6 Bayesian inference for the Gaussian . . . . . . .
2.3.7
2.3.8
2.3.9 Mixtures of Gaussians . . .
The Exponential Family . . . . . . . .
2.4.1 Maximum likelihood and sufﬁcient statistics
2.4.2 Conjugate priors
. .
2.4.3 Noninformative priors

Student’s t-distribution .
Periodic variables . . . .

. . . . . . .
. . . . . . . .

. . . . . .
. . . . . .

. . . . . . . .

. . . .
. . .

. . .
. . .

. . .
. . .

. . .
. . .

. . .

. . .

. . . . . . .

2.3

2.4

3.1

xiv

CONTENTS

2

Probability Distributions
2.1 Binary Variables . . .

. . .

. . . . .

. . . .
. .
. . . .

. . .
. . .
. .
. . . . .
. .
. . .
. .
. .
. . . . . . .
. .
.
. .

67
68
71
74
76
78
85
88
90
93
94
97
. . . 102
. . . 105
. 110
. . . . . 113
. . . . 116
. . . . 117
. . .
. 117
. . . . 120
. . 122
. 124
. 127

. . .
. . . .

. . . . .

. . .

. . . .
. . .
.
. . .
. .

. . . .
. . .
. .
. . . . .

2.5 Nonparametric Methods . . . . . . . . . . .

. . .

2.5.1 Kernel density estimators . . . . . .
2.5.2 Nearest-neighbour methods

. . . . . .

. . .

. . .

. . . . . . .

Exercises

.

.

. .

. . . . . . .

. . .

. . . . . . . .

. . . . . . .

3 Linear Models for Regression

. . .

Linear Basis Function Models . . . . . . . .
3.1.1 Maximum likelihood and least squares . .
3.1.2 Geometry of least squares
3.1.3
3.1.4 Regularized least squares . . . . .
3.1.5 Multiple outputs
The Bias-Variance Decomposition . . .

Sequential learning . . . . . . . . . . .

. . . . . . . . . . . .

. . .

. .

.

.

. . . . . . . .

. .

3.2
3.3 Bayesian Linear Regression . . . .
Parameter distribution . . .
Predictive distribution .

3.3.1
3.3.2
3.3.3 Equivalent kernel . . . . . . . . . .

. . .

. .

. .
. . . . . . . .
. . . . . . .

. . .
. . . . . . . .

. . . . . .

. .

. . . . .

. . . . . . . .

3.4 Bayesian Model Comparison . . .
3.5

. . .

The Evidence Approximation . . .
3.5.1 Evaluation of the evidence function . . . .
3.5.2 Maximizing the evidence function . .
. . .
3.5.3 Effective number of parameters
. . . . . . . .
Limitations of Fixed Basis Functions
. . . . . . . .
. . . . .

. . . . . . .

. . . . .

. . . .

. .
. .

. . .

.

.

.

3.6
Exercises

.

. . .

. . .

. . . .

. . . . . . .

. . . . . . .
. . . . . . . .

137
. . 138
. . 140
. . . 143
. . 143
. . . 144
. . .
. . . . 146
. . . . . . . 147
. . . 152
. . . .
. 152
. 156
. . . 159
. . 161
. . 165
. . . . . . 166
. . . . . . . . . . 168
. . . . . . . . . . 170
. . . . . . . 172
. . . . . . . 173

. . . . . . . .
. . . . .

. . .
. . .

. .
. . .

. . .

. . .

. . .

. . .

4 Linear Models for Classiﬁcation

4.1 Discriminant Functions . . . . . . .

. . .

. . . . . . . . . .

. . . . . . . . . .

.

. . .

. . .

. . .

. . . . .

. . . . .

. . . . . . . . .

. . .
Fisher’s linear discriminant . . . . . .

. .
Fisher’s discriminant for multiple classes
. .

4.1.1 Two classes . . .
4.1.2 Multiple classes . . .
4.1.3 Least squares for classiﬁcation .
4.1.4
4.1.5 Relation to least squares .
4.1.6
4.1.7 The perceptron algorithm . . . . . .
Probabilistic Generative Models . . . . . . . . .
4.2.1 Continuous inputs
4.2.2 Maximum likelihood solution . . . .
4.2.3 Discrete features . . . . . . . . . . . .
4.2.4 Exponential family . . . . . . .
Probabilistic Discriminative Models . .
4.3.1
4.3.2 Logistic regression . . . . .
4.3.3
Iterative reweighted least squares
4.3.4 Multiclass logistic regression . .
4.3.5
. . . . . . . . .
4.3.6 Canonical link functions . . . . . . . .
The Laplace Approximation . . . . . . . . .
4.4.1 Model comparison and BIC . . .
. . .

Fixed basis functions .

Probit regression . . .

. . . . . . . . .

. .
. . .

. .

. .
. . . . . . . .

. . . . . . . .

. . . . . .

. . .
. . . . . . . .

. . .
. . . . . . .

CONTENTS

xv

. .

. .

. .

. .

. .

. .

. . .
. . . . . . . .
. . . . . . . .
. . . . .

. . .

. . .

. . .

. . .

. . . . .

. . . . .

. . . . . . .

. . . . . . . . .

179
. . . . 181
. 181
. . . . 182
. . . . 184
. . .
. 186
. 189
. . .
. . . . . 191
. . 192
. . . . . 196
. . . 198
. 200
. . . . 202
. . . 202
. . 203
. . 204
. . . 205
. . . . . . . . . . 207
. . . . 209
. . . . 210
. . . 212
. . 213
. . 216
. . 217
. . . 217
. 218
. 220

. .
. . . . . .
. . . . . . .
. . . . . . . . .
. . . . .

. . . . . . .
. . .

. . . . .
. . .

. . .
. . . .

. . . .
. . .

. . .

. . .

. . .

. .
. . . . . . . .
. . .

. .
. .
. . .

. . . . . . .

4.2

4.3

4.4

5.3

5.4

4.5 Bayesian Logistic Regression . . .

4.5.1 Laplace approximation .
Predictive distribution .
4.5.2
Exercises
.

. . . . . . .

. . .

. .

.

. . . . . . .
. . .

. .
. . . . . .

. . . . . . . .

5 Neural Networks

5.1

Feed-forward Network Functions
5.1.1 Weight-space symmetries

5.2 Network Training . . . .

. . .

. . . . . . . .

. . .
. . . . . . . .

. . . . . . . .
. . .
. . . . .
. . . . . . . .

.

.

. . .

. . . . . . . . . .

Parameter optimization . . . .

5.2.1
5.2.2 Local quadratic approximation . . . . . . . . . .
5.2.3 Use of gradient information . . . . . . .
5.2.4 Gradient descent optimization . . . . . . . . . .
Error Backpropagation . .
5.3.1 Evaluation of error-function derivatives . . . . . . .
. . . . . . . . . .
5.3.2 A simple example
5.3.3 Efﬁciency of backpropagation .
. . .
.
5.3.4 The Jacobian matrix . . . . .
.
. . . . .
The Hessian Matrix . . . .
. . .
5.4.1 Diagonal approximation . . . .
5.4.2 Outer product approximation . .
. . . . . . . .
. . .
5.4.3

. . . . . . . .
. . . . . . . .
. . . . .
. . . . . . . .
. . .

Inverse Hessian . . .

. . . . . . . . . .

. . . . . . . . . .

. . . . .

. . . .

. . .

. . .

. . .

.

. . . .
. . . . .
. . . .

225
. . . . . . . 227
. . 231
. . . . .
. . . . .
. . 232
. . . 236
. . .
. . 237
. . 239
. . 240
. . . . . 241
. . . . 242
. . . . 245
. . . . 246
. . . 247
. . 249
. . . 250
.
. 251
. . .
. . . . 252

. . .

xvi

CONTENTS

Finite differences . . . . . . .

5.4.4
5.4.5 Exact evaluation of the Hessian . . . . . . . . .
5.4.6

Fast multiplication by the Hessian . . . .

. . . . . . . .

. . .

.

. . .

. . . . . . . .

. .
. .

. . .
. .

5.5 Regularization in Neural Networks

. . . . .

5.5.1 Consistent Gaussian priors . . . .
5.5.2 Early stopping .
5.5.3
Invariances . . .
5.5.4 Tangent propagation . .
5.5.5 Training with transformed data . . . .
5.5.6 Convolutional networks
. . .
5.5.7

. . . . . . . . . .
. . . . . . . . . .

.
Soft weight sharing . . . .
5.6 Mixture Density Networks . . . .
5.7 Bayesian Neural Networks . . . . .

. . .

. . .

. .

. . . . . .
. .
. . . . . . . .

. . .
. . . . . . . . . .
. . . . . . . .

. . . . . . .

5.7.1
Posterior parameter distribution . . .
5.7.2 Hyperparameter optimization . . . .
5.7.3 Bayesian neural networks for classiﬁcation .

. .
. . . . . . . .

. . .

Exercises

.

.

. . .

. . . .

. . . . . . . .

. . . . .

. . . . .

.

. . .

. . .

. . .

. . . .

. . . .

. . . .

. . . 252
. 253
. . . . . . . 254
. . . . . . . . . 256
. . 257
. . . . . 259
. . . . . . 261
. . .
. . 263
. . . . . . . . . . 265
. . . 267
. . . 269
. . . 272
. . . 277
. . . . . . . . . . 278
. . . . 280
. . . . . 281
. . . . . . . 284

.
. . .
. . . . . . .
. . . .

. .

6 Kernel Methods

. . .
6.1 Dual Representations . . . . . . . .
6.2 Constructing Kernels . . . . . . . .
. . .
6.3 Radial Basis Function Networks . . . . .

. . . . . . . . .
. . . . . . . . .
. . . . . . .

6.3.1 Nadaraya-Watson model

. . . .

. . .

. . . . . . . .

6.4 Gaussian Processes . . . . . . . . .

. . .

. . . . . . . .

6.4.1 Linear regression revisited . . .
. . .
6.4.2 Gaussian processes for regression . .
6.4.3 Learning the hyperparameters .
. . .
6.4.4 Automatic relevance determination . . . . .
6.4.5 Gaussian processes for classiﬁcation . .
6.4.6 Laplace approximation . . . . .
6.4.7 Connection to neural networks . . . . .

. . .

. .

. .

. . . . . . . .
. . . . . . . .
. . . . . . . .

. . . . . . . .

. . . . . . .

. . .

. . . . . . .

Exercises

.

.

. . .

. . . .

. . . . . . . .

. . . . .

. . . . .

. .

. . . . .

. . . . . .

291
. . . . . 293
. . . . . 294
. 299
. . 301
. 303
. .
. . 304
. . . . 306
. . . . 311
. . . 312
. . 313
. . . 315
. . 319
. . . . . . . 320

. .

.

7

Sparse Kernel Machines
7.1 Maximum Margin Classiﬁers

. . .

. . .

. . . . . . .

7.1.1 Overlapping class distributions . . . . . . . .
7.1.2 Relation to logistic regression . . . . . . . .
7.1.3 Multiclass SVMs . . . . .
. . . . . . . . . .
7.1.4
7.1.5 Computational learning theory . . . .

SVMs for regression . . . .

. . . . . . . .

. .

7.2 Relevance Vector Machines

. . . .

. . .

. . . . . . .

7.2.1 RVM for regression . . . .
7.2.2 Analysis of sparsity . . . . .
7.2.3 RVM for classiﬁcation .

. . . . .

. . . . . . . .

. . . . . . . .

. . . . . . .
. . . . . . . .

. .
. . . . . . .

Exercises

.

.

. .

. . . . . . .

. . .

. . . . .

. . .
.

. . .
. . .
. .
. . .

325
. . 326
. . . . . 331
. 336
. . . .
. . . 338
. . 339
. . . . . . . . . . 344
. . 345
. . 345
. . . 349
. 353
. 357

. . . . . . . . .
. . . .

. . . . .
. . .

. . .

. . .

. . .

CONTENTS

xvii

8 Graphical Models

8.1 Bayesian Networks . . .

. . .

. . . . . . . .

. . . . . .

8.1.1 Example: Polynomial regression . . . .
8.1.2 Generative models .
8.1.3 Discrete variables . . . . . .
. . . . . . . .
8.1.4 Linear-Gaussian models . . . . . . . . . .

. . . . . .

. . .

. .

. . . . . . .

. . . . . .
. . .
. . .

. . .
. . .
. . .
. . . . . . . . .

8.2 Conditional Independence .
8.2.1 Three example graphs
8.2.2 D-separation . . .
.

8.3 Markov Random Fields

. . . . . .

. . .

. . . . . .
. . . . . . . .
. . . . . .

. . .

. . .
. . .

. . . . . .
. .

. . .

. . . . . . . .
. . .
. . .

. . .

. . . . .

. . . . .

359
. 360
. . 362
. . . 365
. . . 366
. . . 370
. . . 372
. . 373
. 378
. . . . . 383
. . . . . 383
. . . 384
. .
. 387
. . 390
. . 393
. . . 394
. . 398
. . . . . 399
. 402
. . .
. . .
. 411
. . . . 416
. . . 417
.
. .
. . 418
. . . . . . . 418

. . .
. .
. . . . .

. . .

. . . .
. . . . .
. . .

423
. . . . 424
. . 428
. 430
. 432
. . . 435
. . . . 439
. . 441
. . 443
. . 444
. . . . . . . 448
. . 450
. 455

. . . . .
. .
. . . .

. . . .

461
. . . . 462
. . . . .
. . 464
. . . . . . . 466
. . 470
. . 473
. . 474

. . . .
. . .
.
. . . . .

8.4

. . . . . . . .
. . . . . . . .

. . . . . .

Inference on a chain . .

Factorization properties
Illustration: Image de-noising . . . .
.

8.3.1 Conditional independence properties . . . . .
8.3.2
. . . . .
8.3.3
8.3.4 Relation to directed graphs . . . .
Inference in Graphical Models . . .
. . .
8.4.1
8.4.2 Trees
8.4.3
8.4.4 The sum-product algorithm . . .
8.4.5 The max-sum algorithm . . . . .
8.4.6 Exact inference in general graphs
8.4.7 Loopy belief propagation .
. . .
8.4.8 Learning the graph structure . .

. . . . .
. . . . . . . .

Factor graphs . . . .

. . . . . . . .

. . . . . . .

. . . . .

. . . .

. .

. .

.
. . . . . . .

. . .
.
.
. .
. . .
. . .
. . . . .

. . . . . . . .
. . . . . . . .
. . . . . . . .
. . . . . . . .
. . . . . . . .
. . . . .

. . . .

. . . . . . .
.

. . .

Exercises

.

.

. . .

. . . .

. . . . . . . .

9 Mixture Models and EM

9.1 K-means Clustering . . . . . . . . . .

. . .

. . . . . . . .

9.1.1

9.2 Mixtures of Gaussians . .

Image segmentation and compression . . . . . .
. . . . . .
. . . . . . . .

. . . . . . . . . .
. . .

9.2.1 Maximum likelihood . . . . . .
9.2.2 EM for Gaussian mixtures . . . . .

. .

9.3 An Alternative View of EM . . . . . . . . .

. .
. . .

. . . . . .
. . . . .

9.3.1 Gaussian mixtures revisited . . . . . . .
9.3.2 Relation to K-means . . . . . . . .
9.3.3 Mixtures of Bernoulli distributions . . . . . . . .
9.3.4 EM for Bayesian linear regression . . . .
The EM Algorithm in General .

. . . . . . . .

. . .

. . .

. . .

. .

. . . . . . .

. . . . . .

.

.

. .

. . . . . . .

. . .

. . . . . . . .

. . . . . . .

9.4
Exercises

10 Approximate Inference

10.1 Variational Inference . . .

. . . . . . . . . .

. . .

. . . . .

10.1.1 Factorized distributions .
. . .
10.1.2 Properties of factorized approximations . . . .
10.1.3 Example: The univariate Gaussian . . . . . . . .
. . . . . . . .
10.1.4 Model comparison .

. . . . . .

. . . .

. . .

. . .

10.2 Illustration: Variational Mixture of Gaussians . .

. . .

xviii

CONTENTS

. . . . . . .

10.2.1 Variational distribution .
. . . . . . .
10.2.2 Variational lower bound . . . . . .
10.2.3 Predictive density . .
. . . . . . . .
10.2.4 Determining the number of components . . . . .
10.2.5 Induced factorizations

. . . . . . .
10.3 Variational Linear Regression . . . . . . . .

. .
. . .

. . . .

. . .

. .

.

10.3.1 Variational distribution .
10.3.2 Predictive distribution . . .
10.3.3 Lower bound . .

. . .

. . . . . .

. . .

. . . . . . . .

. . . . . . . . . .

. .

. . .

10.4 Exponential Family Distributions

. . . . . .

. . .

10.4.1 Variational message passing . . .

. . . . . . . .

10.5 Local Variational Methods .
10.6 Variational Logistic Regression .

. . . . . . . . .
. . . . . .

. . .
. . .
. .

10.6.1 Variational posterior distribution . . . .
10.6.2 Optimizing the variational parameters . . . .
10.6.3 Inference of hyperparameters

. . .

.

. . . . . . . .

10.7 Expectation Propagation . . . . .

10.7.1 Example: The clutter problem . . . .
10.7.2 Expectation propagation on graphs . .
. . . . .

. . . . . . . .

. . . .

. . .

.

.

. . . . . . . .
. .
. . . . . . . .
. . . . .

Exercises

. . .

. . .

. . .

. . . . .

. . . . . . .

. . 475
. . . . . . . .
. . 481
. . . .
.
. . 482
. . .
. . . . . . 483
. . . 485
. . . . . . . . . 486
. . 486
. . . 488
. . . . . 489
. . . . . . . . . 490
. . 491
. . . .
. . . . . . .
. . 493
. . . . . . . . . 498
. . 498
. . . . . . .
. . . . . 500
. . . . 502
. . . . . . .
. . . 505
. . . . . . . . . . 511
. . . . 513
. . . . . . . 517

. . .

11 Sampling Methods

11.1 Basic Sampling Algorithms

. . . .

. . .

. . . . . . .

. .

. . . . .

11.1.1 Standard distributions
11.1.2 Rejection sampling . . . . .
11.1.3 Adaptive rejection sampling . . . . . . .
11.1.4 Importance sampling . . . .
11.1.5 Sampling-importance-resampling . . . .
11.1.6 Sampling and the EM algorithm . . . . . . . . .

. . . . . . . .

. . . . . . . .

. . .

. . .

. . .

. . . . . . . .

. . .
. . . . . . .

. . . .
. . . . . . . . . .

11.2 Markov Chain Monte Carlo . . . . . .

. . .

11.2.1 Markov chains
. . . . . . . . .
11.2.2 The Metropolis-Hastings algorithm . .

. . . .

. . .
. .

.

. . . . . . .

11.3 Gibbs Sampling . . . . . . . . . .
11.4 Slice Sampling . . . . . .
. . . . .
11.5 The Hybrid Monte Carlo Algorithm . . . . . . . .

. . .
. . . . . .
. . . . . . . . .

. .

. . . . .
. . . . .

11.5.1 Dynamical systems . . . .
11.5.2 Hybrid Monte Carlo . . . . . . . . . .

. . . . . . . . . .

. .

11.6 Estimating the Partition Function . . . . . . . .
Exercises

. . . . . . . .

. . . . .

. . . .

. . .

.

.

. . . . .

. . .
. . . .
. . . . .

.
. .

. . . . .

.
. . .

523
. . 526
. . . 526
. 528
. . . . . . . 530
. . . 532
. . . 534
. . 536
. . 537
. . . . 539
. . 541
. . . 542
. . . 546
. . . . . . . 548
. 548
.
. . .
. . 552
. . . . . 554
. . . . . . . 556

. . .

. . .

559
. . . . . . . 561
. . . . 561
. . 563
. . .
. 565
. . . . 569

12 Continuous Latent Variables

12.1 Principal Component Analysis . . .

. . .

. . . . . . .

12.1.1 Maximum variance formulation . . .
12.1.2 Minimum-error formulation . . . . . .
12.1.3 Applications of PCA . .
. . .
12.1.4 PCA for high-dimensional data

. . .

. . . . . . . .

. . .

.
. . . . . . . .

. . . . . . . .

. .

. . . . . . .

CONTENTS

xix

12.2 Probabilistic PCA . . . . . . . .

.
12.2.1 Maximum likelihood PCA .
12.2.2 EM algorithm for PCA . .
. . .
12.2.3 Bayesian PCA . . .
12.2.4 Factor analysis . . .
. . .

. . . . . . . . . . .
. . . . . .

. .

. . . . . . .

. . . . . . . . . .
. . . . . . . . . .
. . . . . . . . . .

. . .
.
.

. . .
. . .
. . . . .
. . . . . . . . .

. . .

12.3 Kernel PCA . . .
12.4 Nonlinear Latent Variable Models . . . . .

. . . . . . . . .

. . .

. . .

12.4.1 Independent component analysis . . . . . . .
12.4.2 Autoassociative neural networks . . .
12.4.3 Modelling nonlinear manifolds . . . .
. . . . .

. . . . . . . .

. . . . . . . .
. . . . . . . .
. . . . .

. . . .

. . .

. . .

.

.

Exercises

. .

. . . . .
.

. 570
. 574
. . . 577
. . . . 580
. . . . 583
. . . . 586
. . . . 591
. . . . . 591
. . . . 592
. . . . 595
. . . . . . . 599

. . .

. . . . .

605
. . 607
. . . . 610
. . . . 615
. . . . 618
. . 625
. . . . 627
. . . 629
. . . . . 631
. . 635
. . . 638
. . . 642
. . 644
. . 645
. . . . . . . 646

. . .
. . .

. . .
. . .

. . . . .

653
. . 654
. . 655
. . . . . . 657
. . . . . . 659
. 661
. 663
. 666
. . 667
.
. 670
. . 672
. . . . . . . 674

.
. . . . .
. . . . . .

. . . .
. . . .
.
. . .

13 Sequential Data

13.1 Markov Models . . . . . . . . . . .
13.2 Hidden Markov Models
. . .

. . .

. . . . . . .

. . .
. . . . . . . .

. . . . .
. . . . . . . .
. . . . . . . .

13.2.1 Maximum likelihood for the HMM .
13.2.2 The forward-backward algorithm . .
13.2.3 The sum-product algorithm for the HMM . . . . . . . .
13.2.4 Scaling factors
13.2.5 The Viterbi algorithm . . . . . . . . . .
13.2.6 Extensions of the hidden Markov model . . .

. . . . . . . . . .

. . . . . .

. . .

. . .

. . .

. .

.

13.3 Linear Dynamical Systems . . . . .
. . . .

13.3.1 Inference in LDS . .
13.3.2 Learning in LDS . . . . .
13.3.3 Extensions of LDS . . . .
13.3.4 Particle ﬁlters . . . .

. . . . .
. . . . .

. . . . . . .

. . .
. . . . . . . .

. . .
. . . . . . .
. . . . . . . .
.

. . .

. . . .
. . . . . . . .

. . . . . . . .
. . . . .

. . . . .

Exercises

.

.

. . .

. . . .

14 Combining Models

14.1 Bayesian Model Averaging . . . . . . . . .
14.2 Committees . . . . . .
14.3 Boosting .

. . . . . . .
. . . . . . . .
14.3.1 Minimizing exponential error
14.3.2 Error functions for boosting . . . . . .

. . .
. . . . . .

. . . . .

. . . . . . .
. .

. . . . .

. . .

. . . . . . . .
. . . . . . .

. . . . .
. . .
. . . . . . .

14.4 Tree-based Models . . . . . . . . .
14.5 Conditional Mixture Models . . . .

. . . . . . . .

. . .
. . . . . . . .

. .

14.5.1 Mixtures of linear regression models . . . . . . .
. . . . . . . . . . .
14.5.2 Mixtures of logistic models
. . .
14.5.3 Mixtures of experts . . . . .
. . . . . . . .
. . . . . . . .
. . . . .

. . . . .

. . . .

. . .

.

.

Exercises

Appendix A Data Sets

Appendix B Probability Distributions

Appendix C Properties of Matrices

677

685

695

