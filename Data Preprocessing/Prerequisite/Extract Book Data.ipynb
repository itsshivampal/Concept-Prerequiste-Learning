{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_file = \"toc.txt\"\n",
    "chapter_data = \"booktext1.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_array = []\n",
    "file = open(toc_file)\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    if line != \"\\n\" and re.search('[a-zA-Z0-9]', line):\n",
    "        toc_array.append(line)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_array = []\n",
    "file = open(chapter_data)\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    if line != \"\\n\" and re.search('[a-zA-Z0-9]', line):\n",
    "        book_array.append(line)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separation(line):\n",
    "    line = line.strip()\n",
    "    pos = line.find(\" \")\n",
    "    index_num = line[:pos]\n",
    "    title = line[pos+1:]\n",
    "    return (index_num, title)\n",
    "\n",
    "def create_file(line):\n",
    "    line = line.replace(\" \", \"_\")\n",
    "    file_name = \"book_data/\" + line + \".txt\"\n",
    "    file = open(file_name, \"w+\")\n",
    "    file.close()\n",
    "    return file_name\n",
    "\n",
    "def match_line(line1, line2):\n",
    "    line1 = line1.lower()\n",
    "    line2 = line2.lower()\n",
    "    index1, title = data_separation(line1)\n",
    "    if index1 in line2 or index1+\".\" in line2:\n",
    "        if title in line2: return 1\n",
    "        else: return 0\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_len = len(toc_array)\n",
    "book_len = len(book_array)\n",
    "\n",
    "i = 0\n",
    "\n",
    "current_line = toc_array[i]\n",
    "next_line = toc_array[i + 1]\n",
    "\n",
    "file_name= create_file(current_line)\n",
    "current_file = open(file_name, \"a\")\n",
    "\n",
    "for j in range(book_len):\n",
    "    line = book_array[j]\n",
    "    if match_line(next_line, line):\n",
    "        if i < toc_len - 2:\n",
    "            i += 1\n",
    "            current_line = toc_array[i]\n",
    "            next_line = toc_array[i + 1]\n",
    "        else:\n",
    "            current_line = toc_array[toc_len - 1]\n",
    "        current_file.close()\n",
    "        file_name = create_file(current_line)\n",
    "        current_file = open(file_name, \"a\")\n",
    "    else:\n",
    "        line += \"\\n\"\n",
    "        current_file.write(line)\n",
    "        \n",
    "current_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_data = {}\n",
    "i = 0\n",
    "\n",
    "for topic in toc_array:\n",
    "    index_num, title = data_separation(topic)\n",
    "    topic = topic.replace(\" \", \"_\")\n",
    "    file_name = \"book_data/\" + topic + \".txt\"\n",
    "    file = open(file_name, \"r\")\n",
    "    content = file.read()\n",
    "    file.close()\n",
    "    data = {\n",
    "        'Section': index_num,\n",
    "        'Title': title,\n",
    "        'Content': content\n",
    "    }\n",
    "    book_data[i] = data\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_line(content, line, state):\n",
    "    line = line.strip()\n",
    "    if state == True:\n",
    "        content += \" \" + line\n",
    "    else: content += line\n",
    "    return content\n",
    "\n",
    "def incomplete_words(content):\n",
    "    content_array = content.split(\"\\n\")\n",
    "    state = True\n",
    "    new_content = \"\"\n",
    "    for i in range(len(content_array)-1):\n",
    "        if content_array[i][-1] == \"-\":\n",
    "            new_content = add_line(new_content, content_array[i][:-1], state)\n",
    "            state = False\n",
    "        else:\n",
    "            new_content = add_line(new_content, content_array[i], state)\n",
    "            state = True\n",
    "    new_content = new_content.strip()\n",
    "    return new_content\n",
    "\n",
    "def handle_special_char(content):\n",
    "    new_content = \"\"\n",
    "    punctuations = \"!\\\"#$%&()*+-.,:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for ch in content:\n",
    "        if ch == \"'\":\n",
    "            ch = \"\"\n",
    "        if ch in punctuations:\n",
    "            ch = \" \"\n",
    "        if ord(ch) < 97 or ord(ch) > 122:\n",
    "            ch = \" \"\n",
    "        new_content += ch\n",
    "    return new_content\n",
    "\n",
    "def handle_words(content):\n",
    "    words = content.split(\" \")\n",
    "    new_list = [word for word in words if len(word) > 2]\n",
    "    content = \" \".join(new_list)\n",
    "    return content\n",
    "\n",
    "\n",
    "def clean_content(content):\n",
    "    content = content.lower()\n",
    "    content = incomplete_words(content)\n",
    "    content = handle_special_char(content)\n",
    "    content = re.sub(\"\\s\\s+\" , \" \", content)\n",
    "    content = handle_words(content)\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(book_data)):\n",
    "    book_data[i][\"Content\"] = clean_content(book_data[i][\"Content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "introduction the problem searching for patterns data fundamental one and has long and successful history for instance the extensive astronomical observations tycho brahe the century allowed johannes kepler discover the empirical laws planetary motion which turn provided springboard for the development classical mechanics similarly the discovery regularities atomic spectra played key role the development and verification quantum physics the early twentieth century the field pattern recognition concerned with the automatic discovery regularities data through the use computer algorithms and with the use these regularities take actions such classifying the data into different categories consider the example recognizing handwritten digits illustrated figure each digit corresponds pixel image and can represented vector comprising real numbers the goal build machine that will take such vector input and that will produce the identity the digit the output this nontrivial problem due the wide variability handwriting could introduction figure examples hand written digits taken from zip codes tackled using handcrafted rules heuristics for distinguishing the digits based the shapes the strokes but practice such approach leads proliferation rules and exceptions the rules and and invariably gives poor results far better results can obtained adopting machine learning approach which large set digits called training set used tune the parameters adaptive model the categories the digits the training set are known advance typically inspecting them individually and hand labelling them can express the category digit using target vector which represents the identity the corresponding digit suitable techniques for representing categories terms vectors will discussed later note that there one such target vector for each digit image the result running the machine learning algorithm can expressed function which takes new digit image input and that generates output vector encoded the same way the target vectors the precise form the function determined during the training phase also known the learning phase the basis the training data once the model trained can then determine the identity new digit images which are said comprise test set the ability categorize correctly new examples that differ from those used for training known generalization practical applications the variability the input vectors will such that the training data can comprise only tiny fraction all possible input vectors and generalization central goal pattern recognition for most practical applications the original input variables are typically preprocessed transform them into some new space variables where hoped the pattern recognition problem will easier solve for instance the digit recognition problem the images the digits are typically translated and scaled that each digit contained within box fixed size this greatly reduces the variability within each digit class because the location and scale all the digits are now the same which makes much easier for subsequent pattern recognition algorithm distinguish between the different classes this pre processing stage sometimes also called feature extraction note that new test data must pre processed using the same steps the training data pre processing might also performed order speed computation for example the goal real time face detection high resolution video stream the computer must handle huge numbers pixels per second and presenting these directly complex pattern recognition algorithm may computationally infeasible instead the aim find useful features that are fast compute and yet that introduction also preserve useful discriminatory information enabling faces distinguished from non faces these features are then used the inputs the pattern recognition algorithm for instance the average value the image intensity over rectangular subregion can evaluated extremely efficiently viola and jones and set such features can prove very effective fast face detection because the number such features smaller than the number pixels this kind pre processing represents form dimensionality reduction care must taken during pre processing because often information discarded and this information important the solution the problem then the overall accuracy the system can suffer applications which the training data comprises examples the input vectors along with their corresponding target vectors are known supervised learning problems cases such the digit recognition example which the aim assign each input vector one finite number discrete categories are called classification problems the desired output consists one more continuous variables then the task called regression example regression problem would the prediction the yield chemical manufacturing process which the inputs consist the concentrations reactants the temperature and the pressure other pattern recognition problems the training data consists set input vectors without any corresponding target values the goal such unsupervised learning problems may discover groups similar examples within the data where called clustering determine the distribution data within the input space known density estimation project the data from high dimensional space down two three dimensions for the purpose visualization finally the technique reinforcement learning sutton and barto concerned with the problem finding suitable actions take given situation order maximize reward here the learning algorithm not given examples optimal outputs contrast supervised learning but must instead discover them process trial and error typically there sequence states and actions which the learning algorithm interacting with its environment many cases the current action not only affects the immediate reward but also has impact the reward all subsequent time steps for example using appropriate reinforcement learning techniques neural network can learn play the game backgammon high standard tesauro here the network must learn take board position input along with the result dice throw and produce strong move the output this done having the network play against copy itself for perhaps million games major challenge that game backgammon can involve dozens moves and yet only the end the game that the reward the form victory achieved the reward must then attributed appropriately all the moves that led even though some moves will have been good ones and others less this example credit assignment problem general feature reinforcement learning the trade off between exploration which the system tries out new kinds actions see how effective they are and exploitation which the system makes use actions that are known yield high reward too strong focus either exploration exploitation will yield poor results reinforcement learning continues active area machine learning research however introduction figure plot training data set points shown blue circles each comprising observation the input variable along with the corresponding target variable the green curve shows the function sin used generate the data our goal predict the value for some new value without knowledge the green curve detailed treatment lies beyond the scope this book although each these tasks needs its own tools and techniques many the key ideas that underpin them are common all such problems one the main goals this chapter introduce relatively informal way several the most important these concepts and illustrate them using simple examples later the book shall see these same ideas emerge the context more sophisticated models that are applicable real world pattern recognition applications this chapter also provides self contained introduction three important tools that will used throughout the book namely probability theory decision theory and information theory although these might sound like daunting topics they are fact straightforward and clear understanding them essential machine learning techniques are used best effect practical applications\n",
      "\n",
      "\n",
      "\n",
      "begin introducing simple regression problem which shall use running example throughout this chapter motivate number key concepts suppose observe real valued input variable and wish use this observation predict the value real valued target variable for the present purposes instructive consider artificial example using synthetically generated data because then know the precise process that generated the data for comparison against any learned model the data for this example generated from the function sin with random noise included the target values described detail appendix now suppose that are given training set comprising observations written together with corresponding observations the values denoted figure shows plot training set comprising data points the input data set figure was generated choosing values for spaced uniformly range and the target data set was obtained first computing the corresponding values the function example polynomial curve fitting sin and then adding small level random noise having gaussian distribution the gaussian distribution discussed section each such point order obtain the corresponding value generating data this way are capturing property many real data sets namely that they possess underlying regularity which wish learn but that individual observations are corrupted random noise this noise might arise from intrinsically stochastic random processes such radioactive decay but more typically due there being sources variability that are themselves unobserved our goal exploit this training set order make predictions the value the target variable for some new value the input variable shall see later this involves implicitly trying discover the underlying function sin this intrinsically difficult problem have generalize from finite data set furthermore the observed data are corrupted with noise and for given probability theory discussed there uncertainty the appropriate value for section provides framework for expressing such uncertainty precise and quantitative manner and decision theory discussed section allows exploit this probabilistic representation order make predictions that are optimal according appropriate criteria for the moment however shall proceed rather informally and consider simple approach based curve fitting particular shall fit the data using polynomial function the form wjxj where the order the polynomial and denotes raised the power the polynomial coefficients are collectively denoted the vector note that although the polynomial function nonlinear function linear function the coefficients functions such the polynomial which are linear the unknown parameters have important properties and are called linear models and will discussed extensively chapters and the values the coefficients will determined fitting the polynomial the training data this can done minimizing error function that measures the misfit between the function for any given value and the training set data points one simple choice error function which widely used given the sum the squares the errors between the predictions for each data point and the corresponding target values that minimize where the factor included for later convenience shall discuss the motivation for this choice error function later this chapter for the moment simply note that nonnegative quantity that would zero and only the introduction figure the error function corresponds one half the sum the squares the displacements shown the vertical green bars each data point from the function function were pass exactly through each training data point the geometrical interpretation the sum squares error function illustrated figure can solve the curve fitting problem choosing the value for which small possible because the error function quadratic function the coefficients its derivatives with respect the coefficients will linear the elements and the minimization the error function has unique solution denoted which can found closed form the resulting polynomial given the function exercise there remains the problem choosing the order the polynomial and shall see this will turn out example important concept called model comparison model selection figure show four examples the results fitting polynomials having orders and the data set shown figure notice that the constant and first order polynomials give rather poor fits the data and consequently rather poor representations the function sin the third order polynomial seems give the best fit the function sin the examples shown figure when much higher order polynomial obtain excellent fit the training data fact the polynomial passes exactly through each data point and however the fitted curve oscillates wildly and gives very poor representation the function sin this latter behaviour known over fitting have noted earlier the goal achieve good generalization making accurate predictions for new data can obtain some quantitative insight into the dependence the generalization performance considering separate test set comprising data points generated using exactly the same procedure used generate the training set points but with new choices for the random noise values included the target values for each choice can then evaluate the residual value given for the training data and can also evaluate for the test data set sometimes more convenient use the root mean square figure plots polynomials having various orders shown red curves fitted the data set shown figure rms error defined erms which the division allows compare different sizes data sets equal footing and the square root ensures that erms measured the same scale and the same units the target variable graphs the training and test set rms errors are shown for various values figure the test set error measure how well are doing predicting the values for new data observations note from figure that small values give relatively large values the test set error and this can attributed the fact that the corresponding polynomials are rather inflexible and are incapable capturing the oscillations the function sin values the range give small values for the test set error and these also give reasonable representations the generating function sin can seen for the case from figure example polynomial curve fitting introduction figure graphs the root mean square error defined evaluated the training set and independent test set for various values training test for the training set error goes zero might expect because this polynomial contains degrees freedom corresponding the coefficients and can tuned exactly the data points the training set however the test set error has become very large and saw figure the corresponding function exhibits wild oscillations this may seem paradoxical because polynomial given order contains all lower order polynomials special cases the polynomial therefore capable generating results least good the polynomial furthermore might suppose that the best predictor new data would the function sin from which the data was generated and shall see later that this indeed the case know that power series expansion the function sin contains terms all orders might expect that results should improve monotonically increase can gain some insight into the problem examining the values the coefficients obtained from polynomials various order shown table see that increases the magnitude the coefficients typically gets larger particular for the polynomial the coefficients have become finely tuned the data developing large positive and negative values that the correspondtable table the coefficients for polynomials various order observe how the typical magnitude the coefficients increases dramatically the order the polynomial increases example polynomial curve fitting figure plots the solutions obtained minimizing the sum squares error function using the polynomial for data points left plot and data points right plot see that increasing the size the data set reduces the over fitting problem ing polynomial function matches each the data points exactly but between data points particularly near the ends the range the function exhibits the large oscillations observed figure intuitively what happening that the more flexible polynomials with larger values are becoming increasingly tuned the random noise the target values also interesting examine the behaviour given model the size the data set varied shown figure see that for given model complexity the over fitting problem become less severe the size the data set increases another way say this that the larger the data set the more complex other words more flexible the model that can afford fit the data one rough heuristic that sometimes advocated that the number data points should less than some multiple say the number adaptive parameters the model however shall see chapter the number parameters not necessarily the most appropriate measure model complexity also there something rather unsatisfying about having limit the number parameters model according the size the available training set would seem more reasonable choose the complexity the model according the complexity the problem being solved shall see that the least squares approach finding the model parameters represents specific case maximum likelihood discussed section and that the over fitting problem can understood general property maximum likelihood adopting bayesian approach the over fitting problem can avoided shall see that there difficulty from bayesian perspective employing models for which the number parameters greatly exceeds the number data points indeed bayesian model the effective number parameters adapts automatically the size the data set for the moment however instructive continue with the current approach and consider how practice can apply data sets limited size where section may wish use relatively complex and flexible models one technique that often used control the over fitting phenomenon such cases that regularization which involves adding penalty term the error function order discourage the coefficients from reaching large values the simplest such penalty term takes the form sum squares all the coefficients leading modified error function the form where wtw and the coefficient governs the relative importance the regularization term compared with the sum squares error term note that often the coefficient omitted from the regularizer because its inclusion causes the results depend the choice origin for the target variable hastie may included but with its own regularization coefficient shall discuss this topic more detail section again the error function can minimized exactly closed form techniques such this are known the statistics literature shrinkage methods because they reduce the value the coefficients the particular case quadratic regularizer called ridge regression hoerl and kennard the context neural networks this approach known weight decay figure shows the results fitting the polynomial order the same data set before but now using the regularized error function given see that for value the over fitting has been suppressed and now obtain much closer representation the underlying function sin however use too large value for then again obtain poor fit shown figure for the corresponding coefficients from the fitted polynomials are given table showing that regularization has the desired effect reducing exercise introduction figure plots polynomials fitted the data set shown figure using the regularized error function for two values the regularization parameter corresponding and the case regularizer corresponding shown the bottom right figure example polynomial curve fitting table table the coefficients for polynomials with various values for the regularization parameter note that corresponds model with regularization the graph the bottom right figure see that the value increases the typical magnitude the coefficients gets smaller the magnitude the coefficients the impact the regularization term the generalization error can seen plotting the value the rms error for both training and test sets against shown figure see that effect now controls the effective complexity the model and hence determines the degree over fitting the issue model complexity important one and will discussed length section here simply note that were trying solve practical application using this approach minimizing error function would have find way determine suitable value for the model complexity the results above suggest simple way achieving this namely taking the available data and partitioning into training set used determine the coefficients and separate validation set also called hold out set used optimize the model complexity either many cases however this will prove too wasteful valuable training data and have seek more sophisticated approaches far our discussion polynomial curve fitting has appealed largely intuition now seek more principled approach solving problems pattern recognition turning discussion probability theory well providing the foundation for nearly all the subsequent developments this book will also section figure graph the root mean square error versus for the polynomial training test introduction give some important insights into the concepts have introduced the context polynomial curve fitting and will allow extend these more complex situations\n",
      "\n",
      "\n",
      "\n",
      "key concept the field pattern recognition that uncertainty arises both through noise measurements well through the finite size data sets probability theory provides consistent framework for the quantification and manipulation uncertainty and forms one the central foundations for pattern recognition when combined with decision theory discussed section allows make optimal predictions given all the information available even though that information may incomplete ambiguous will introduce the basic concepts probability theory considering simple example imagine have two boxes one red and one blue and the red box have apples and oranges and the blue box have apples and orange this illustrated figure now suppose randomly pick one the boxes and from that box randomly select item fruit and having observed which sort fruit replace the box from which came could imagine repeating this process many times let suppose that doing pick the red box the time and pick the blue box the time and that when remove item fruit from box are equally likely select any the pieces fruit the box this example the identity the box that will chosen random variable which shall denote this random variable can take one two possible values namely corresponding the red box corresponding the blue box similarly the identity the fruit also random variable and will denoted can take either the values for apple for orange begin with shall define the probability event the fraction times that event occurs out the total number trials the limit that the total number trials goes infinity thus the probability selecting the red box figure use simple example two coloured boxes each containing fruit apples shown green and oranges shown orange introduce the basic ideas probability figure can derive the sum and product rules probability considering two random variables which takes the values where and which takes the values where this illustration have and consider total number instances these variables then denote the number instances where and nij which the number points the corresponding cell the array the number points column corresponding denoted and the number points row corresponding denoted probability theory nij and the probability selecting the blue box write these probabilities and note that definition probabilities must lie the interval also the events are mutually exclusive and they include all possible outcomes for instance this example the box must either red blue then see that the probabilities for those events must sum one can now ask questions such what the overall probability that the selection procedure will pick apple given that have chosen orange what the probability that the box chose was the blue one can answer questions such these and indeed much more complex questions associated with problems pattern recognition once have equipped ourselves with the two elementary rules probability known the sum rule and the product rule having obtained these rules shall then return our boxes fruit example order derive the rules probability consider the slightly more general example shown figure involving two random variables and which could for instance the box and fruit variables considered above shall suppose that can take any the values where and can take the values where consider total trials which sample both the variables and and let the number such trials which and nij also let the number trials which takes the value irrespective the value that takes denoted and similarly let the number trials which takes the value denoted the probability that will take the value and will take the value written and called the joint probability and given the number points falling the cell fraction the total number points and hence here are implicitly considering the limit similarly the probability that takes the value irrespective the value written and given the fraction the total number points that fall column that nij because the number instances column figure just the sum the number instances each cell that column have nij and therefore introduction from and have which the sum rule probability note that sometimes called the marginal probability because obtained marginalizing summing out the other variables this case consider only those instances for which then the fraction such instances for which written and called the conditional probability given obtained finding the fraction those points column that fall cell and hence given nij from and can then derive the following relationship nij nij which the product rule probability far have been quite careful make distinction between random variable such the box the fruit example and the values that the random variable can take for example the box were the red one thus the probability that takes the value denoted although this helps avoid ambiguity leads rather cumbersome notation and many cases there will need for such pedantry instead may simply write denote distribution over the random variable denote the distribution evaluated for the particular value provided that the interpretation clear from the context with this more compact notation can write the two fundamental rules probability theory the following form the rules probability sum rule product rule here joint probability and verbalized the probability and similarly the quantity conditional probability and verbalized the probability given whereas the quantity marginal probability and simply the probability these two simple rules form the basis for all the probabilistic machinery that use throughout this book from the product rule together with the symmetry property immediately obtain the following relationship between conditional probabilities probability theory which called bayes theorem and which plays central role pattern recognition and machine learning using the sum rule the denominator bayes theorem can expressed terms the quantities appearing the numerator can view the denominator bayes theorem being the normalization constant required ensure that the sum the conditional probability the left hand side over all values equals one figure show simple example involving joint distribution over two variables illustrate the concept marginal and conditional distributions here finite sample data points has been drawn from the joint distribution and shown the top left the top right histogram the fractions data points having each the two values from the definition probability these fractions would equal the corresponding probabilities the limit can view the histogram simple way model probability distribution given only finite number points drawn from that distribution modelling distributions from data lies the heart statistical pattern recognition and will explored great detail this book the remaining two plots figure show the corresponding histogram estimates and let now return our example involving boxes fruit for the moment shall once again explicit about distinguishing between the random variables and their instantiations have seen that the probabilities selecting either the red the blue boxes are given respectively note that these satisfy now suppose that pick box random and turns out the blue box then the probability selecting apple just the fraction apples the blue box which and fact can write out all four conditional probabilities for the type fruit given the selected box introduction figure illustration distribution over two variables which takes possible values and which takes two possible values the top left figure shows sample points drawn from joint probability distribution over these variables the remaining figures show histogram estimates the marginal distributions and well the conditional distribution corresponding the bottom row the top left figure again note that these probabilities are normalized that and similarly can now use the sum and product rules probability evaluate the overall probability choosing apple from which follows using the sum rule that probability theory suppose instead are told that piece fruit has been selected and orange and would like know which box came from this requires that evaluate the probability distribution over boxes conditioned the identity the fruit whereas the probabilities give the probability distribution over the fruit conditioned the identity the box can solve the problem reversing the conditional probability using bayes theorem give from the sum rule then follows that can provide important interpretation bayes theorem follows had been asked which box had been chosen before being told the identity the selected item fruit then the most complete information have available provided the probability call this the prior probability because the probability available before observe the identity the fruit once are told that the fruit orange can then use bayes theorem compute the probability which shall call the posterior probability because the probability obtained after have observed note that this example the prior probability selecting the red box was that were more likely select the blue box than the red one however once have observed that the piece selected fruit orange find that the posterior probability the red box now that now more likely that the box selected was fact the red one this result accords with our intuition the proportion oranges much higher the red box than the blue box and the observation that the fruit was orange provides significant evidence favouring the red box fact the evidence sufficiently strong that outweighs the prior and makes more likely that the red box was chosen rather than the blue one finally note that the joint distribution two variables factorizes into the product the marginals that then and are said independent from the product rule see that and the conditional distribution given indeed independent the value for instance our boxes fruit example each box contained the same fraction apples and oranges then that the probability selecting say apple independent which box chosen\n",
      "\n",
      "\n",
      "\n",
      "well considering probabilities defined over discrete sets events also wish consider probabilities with respect continuous variables shall limit ourselves relatively informal discussion the probability real valued variable falling the interval given for then called the probability density over this illustrated figure the probability that will lie interval then given introduction figure the concept probability for discrete variables can extended that probability density over continuous variable and such that the probability lying the interval given for the probability density can expressed the derivative cumulative distribution function because probabilities are nonnegative and because the value must lie somewhere the real axis the probability density must satisfy the two conditions under nonlinear change variable probability density transforms differently from simple function due the jacobian factor for instance consider change variables then function becomes now consider probability density that corresponds density with respect the new variable where the suffices denote the fact that and are different densities observations falling the range will for small values transformed into the range where and hence one consequence this property that the concept the maximum probability density dependent the choice variable the probability that lies the interval given the cumulative distribution function defined which satisfies shown figure have several continuous variables denoted collectively the vector then can define joint probability density such exercise probability theory that the probability falling infinitesimal volume containing the point given this multivariate probability density must satisfy which the integral taken over the whole space can also consider joint probability distributions over combination discrete and continuous variables note that discrete variable then sometimes called probability mass function because can regarded set probability masses concentrated the allowed values the sum and product rules probability well bayes theorem apply equally the case probability densities combinations discrete and continuous variables for instance and are two real variables then the sum and product rules take the form formal justification the sum and product rules for continuous variables feller requires branch mathematics called measure theory and lies outside the scope this book its validity can seen informally however dividing each real variable into intervals width and considering the discrete probability distribution over these intervals taking the limit then turns sums into integrals and gives the desired result\n",
      "\n",
      "\n",
      "\n",
      "one the most important operations involving probabilities that finding weighted averages functions the average value some function under probability distribution called the expectation and will denoted for discrete distribution given that the average weighted the relative probabilities the different values the case continuous variables expectations are expressed terms integration with respect the corresponding probability density either case are given finite number points drawn from the probability distribution probability density then the expectation can approximated introduction finite sum over these points shall make extensive use this result when discuss sampling methods chapter the approximation becomes exact the limit sometimes will considering expectations functions several variables which case can use subscript indicate which variable being averaged over that for instance denotes the average the function with respect the distribution note that will function can also consider conditional expectation with respect conditional distribution that with analogous definition for continuous variables the variance defined var and provides measure how much variability there around its mean value expanding out the square see that the variance can also written terms the expectations and particular can consider the variance the variable itself which given var var for two random variables and the covariance defined cov which expresses the extent which and vary together and are independent then their covariance vanishes the case two vectors random variables and the covariance matrix cov xyt consider the covariance the components vector with each other then use slightly simpler notation cov cov exercise exercise probability theory\n",
      "\n",
      "\n",
      "\n",
      "far this chapter have viewed probabilities terms the frequencies random repeatable events shall refer this the classical frequentist interpretation probability now turn the more general bayesian view which probabilities provide quantification uncertainty consider uncertain event for example whether the moon was once its own orbit around the sun whether the arctic ice cap will have disappeared the end the century these are not events that can repeated numerous times order define notion probability did earlier the context boxes fruit nevertheless will generally have some idea for example how quickly think the polar ice melting now obtain fresh evidence for instance from new earth observation satellite gathering novel forms diagnostic information may revise our opinion the rate ice loss our assessment such matters will affect the actions take for instance the extent which endeavour reduce the emission greenhouse gasses such circumstances would like able quantify our expression uncertainty and make precise revisions uncertainty the light new evidence well subsequently able take optimal actions decisions consequence this can all achieved through the elegant and very general bayesian interpretation probability the use probability represent uncertainty however not hoc choice but inevitable are respect common sense while making rational coherent inferences for instance cox showed that numerical values are used represent degrees belief then simple set axioms encoding common sense properties such beliefs leads uniquely set rules for manipulating degrees belief that are equivalent the sum and product rules probability this provided the first rigorous proof that probability theory could regarded extension boolean logic situations involving uncertainty jaynes numerous other authors have proposed different sets properties axioms that such measures uncertainty should satisfy ramsey good savage definetti lindley each case the resulting numerical quantities behave precisely according the rules probability therefore natural refer these quantities bayesian probabilities the field pattern recognition too helpful have more general nothomas bayes thomas bayes was born tunbridge wells and was clergyman well amateur scientist and mathematician studied logic and theology edinburgh university and was elected fellow the royal society during the century issues regarding probability arose connection with gambling and with the new concept insurance one particularly important problem concerned called inverse probability solution was proposed thomas bayes his paper essay towards solving problem the doctrine chances which was published some three years after his death the philosophical transactions the royal society fact bayes only formulated his theory for the case uniform prior and was pierre simon laplace who independently rediscovered the theory general form and who demonstrated its broad applicability introduction tion probability consider the example polynomial curve fitting discussed section seems reasonable apply the frequentist notion probability the random values the observed variables however would like address and quantify the uncertainty that surrounds the appropriate choice for the model parameters shall see that from bayesian perspective can use the machinery probability theory describe the uncertainty model parameters such indeed the choice model itself bayes theorem now acquires new significance recall that the boxes fruit example the observation the identity the fruit provided relevant information that altered the probability that the chosen box was the red one that example bayes theorem was used convert prior probability into posterior probability incorporating the evidence provided the observed data shall see detail later can adopt similar approach when making inferences about quantities such the parameters the polynomial curve fitting example capture our assumptions about before observing the data the form prior probability distribution the effect the observed data expressed through the conditional probability and shall see later section how this can represented explicitly bayes theorem which takes the form then allows evaluate the uncertainty after have observed the form the posterior probability the quantity the right hand side bayes theorem evaluated for the observed data set and can viewed function the parameter vector which case called the likelihood function expresses how probable the observed data set for different settings the parameter vector note that the likelihood not probability distribution over and its integral with respect does not necessarily equal one given this definition likelihood can state bayes theorem words posterior likelihood prior where all these quantities are viewed functions the denominator the normalization constant which ensures that the posterior distribution the left hand side valid probability density and integrates one indeed integrating both sides with respect can express the denominator bayes theorem terms the prior distribution and the likelihood function both the bayesian and frequentist paradigms the likelihood function plays central role however the manner which used fundamentally different the two approaches frequentist setting considered fixed parameter whose value determined some form estimator and error bars probability theory this estimate are obtained considering the distribution possible data sets contrast from the bayesian viewpoint there only single data set namely the one that actually observed and the uncertainty the parameters expressed through probability distribution over widely used frequentist estimator maximum likelihood which set the value that maximizes the likelihood function this corresponds choosing the value for which the probability the observed data set maximized the machine learning literature the negative log the likelihood function called error function because the negative logarithm monotonically decreasing function maximizing the likelihood equivalent minimizing the error one approach determining frequentist error bars the bootstrap efron hastie which multiple data sets are created follows suppose our original data set consists data points can create new data set drawing points random from with replacement that some points may replicated whereas other points may absent from this process can repeated times generate data sets each size and each obtained sampling from the original data set the statistical accuracy parameter estimates can then evaluated looking the variability predictions between the different bootstrap data sets one advantage the bayesian viewpoint that the inclusion prior knowledge arises naturally suppose for instance that fair looking coin tossed three times and lands heads each time classical maximum likelihood estimate the probability landing heads would give implying that all future tosses will land heads contrast bayesian approach with any reasonable prior will lead much less extreme conclusion there has been much controversy and debate associated with the relative merits the frequentist and bayesian paradigms which have not been helped the fact that there unique frequentist even bayesian viewpoint for instance one common criticism the bayesian approach that the prior distribution often selected the basis mathematical convenience rather than reflection any prior beliefs even the subjective nature the conclusions through their dependence the choice prior seen some source difficulty reducing the dependence the prior one motivation for called noninformative priors however these lead difficulties when comparing different models and indeed bayesian methods based poor choices prior can give poor results with high confidence frequentist evaluation methods offer some protection from such problems and techniques such cross validation remain useful areas such model comparison this book places strong emphasis the bayesian viewpoint reflecting the huge growth the practical importance bayesian methods the past few years while also discussing useful frequentist concepts required although the bayesian framework has its origins the century the practical application bayesian methods was for long time severely limited the difficulties carrying through the full bayesian procedure particularly the need marginalize sum integrate over the whole parameter space which shall section section section introduction see required order make predictions compare different models the development sampling methods such markov chain monte carlo discussed chapter along with dramatic improvements the speed and memory capacity computers opened the door the practical use bayesian techniques impressive range problem domains monte carlo methods are very flexible and can applied wide range models however they are computationally intensive and have mainly been used for small scale problems more recently highly efficient deterministic approximation schemes such variational bayes and expectation propagation discussed chapter have been developed these offer complementary alternative sampling methods and have allowed bayesian techniques used large scale applications blei\n",
      "\n",
      "\n",
      "\n",
      "shall devote the whole chapter study various probability distributions and their key properties convenient however introduce here one the most important probability distributions for continuous variables called the normal gaussian distribution shall make extensive use this distribution the remainder this chapter and indeed throughout much the book for the case single real valued variable the gaussian distribution defined exp which governed two parameters called the mean and called the variance the square root the variance given called the standard deviation and the reciprocal the variance written called the precision shall see the motivation for these terms shortly figure shows plot the gaussian distribution from the form see that the gaussian distribution satisfies exercise also straightforward show that the gaussian normalized that pierre simon laplace said that laplace was seriously lacking modesty and one point declared himself the best mathematician france the time claim that was arguably true well being prolific mathematics also made numerous contributions astronomy including the nebular hypothesis which the earth thought have formed from the condensation and cooling large rotating disk gas and dust published the first edition eorie analytique des probabilit which laplace states that probability theory nothing but common sense reduced calculation this work included discussion the inverse probability calculation later termed bayes theorem poincar which used solve problems life expectancy jurisprudence planetary masses triangulation and error estimation figure plot the univariate gaussian showing the mean and the standard deviation probability theory exercise exercise thus satisfies the two requirements for valid probability density can readily find expectations functions under the gaussian distribution particular the average value given because the parameter represents the average value under the distribution referred the mean similarly for the second order moment from and follows that the variance given var and hence referred the variance parameter the maximum distribution known its mode for gaussian the mode coincides with the mean are also interested the gaussian distribution defined over dimensional vector continuous variables which given exp where the dimensional vector called the mean the matrix called the covariance and denotes the determinant shall make use the multivariate gaussian distribution briefly this chapter although its properties will studied detail section introduction figure illustration the likelihood function for gaussian distribution shown the red curve here the black points denote data set values and the likelihood function given corresponds the product the blue values maximizing the likelihood involves adjusting the mean and variance the gaussian maximize this product now suppose that have data set observations representing observations the scalar variable note that are using the typeface distinguish this from single observation the vector valued variable which denote shall suppose that the observations are drawn independently from gaussian distribution whose mean and variance are unknown and would like determine these parameters from the data set data points that are drawn independently from the same distribution are said independent and identically distributed which often abbreviated have seen that the joint probability two independent events given the product the marginal probabilities for each event separately because our data set can therefore write the probability the data set given and the form section when viewed function and this the likelihood function for the gaussian and interpreted diagrammatically figure one common criterion for determining the parameters probability distribution using observed data set find the parameter values that maximize the likelihood function this might seem like strange criterion because from our foregoing discussion probability theory would seem more natural maximize the probability the parameters given the data not the probability the data given the parameters fact these two criteria are related shall discuss the context curve fitting for the moment however shall determine values for the unknown parameters and the gaussian maximizing the likelihood function practice more convenient maximize the log the likelihood function because the logarithm monotonically increasing function its argument maximization the log function equivalent maximization the function itself taking the log not only simplifies the subsequent mathematical analysis but also helps numerically because the product large number small probabilities can easily underflow the numerical precision the computer and this resolved computing instead the sum the log probabilities from and the log likelihood exercise section exercise probability theory function can written the form maximizing with respect obtain the maximum likelihood solution given which the sample mean the mean the observed values similarly maximizing with respect obtain the maximum likelihood solution for the variance the form which the sample variance measured with respect the sample mean note that are performing joint maximization with respect and but the case the gaussian distribution the solution for decouples from that for that can first evaluate and then subsequently use this result evaluate later this chapter and also subsequent chapters shall highlight the significant limitations the maximum likelihood approach here give indication the problem the context our solutions for the maximum likelihood parameter settings for the univariate gaussian distribution particular shall show that the maximum likelihood approach systematically underestimates the variance the distribution this example phenomenon called bias and related the problem over fitting encountered the context polynomial curve fitting first note that the maximum likelihood solutions and are functions the data set values consider the expectations these quantities with respect the data set values which themselves come from gaussian distribution with parameters and straightforward show that that average the maximum likelihood estimate will obtain the correct mean but will underestimate the true variance factor the intuition behind this result given figure from follows that the following estimate for the variance parameter unbiased introduction figure illustration how bias arises using maximum likelihood determine the variance gaussian the green curve shows the true gaussian distribution from which data generated and the three red curves show the gaussian distributions obtained fitting three data sets each consisting two data points shown blue using the maximum likelihood results and averaged across the three data sets the mean correct but the variance systematically under estimated because measured relative the sample mean and not relative the true mean section section shall see how this result arises automatically when adopt bayesian approach note that the bias the maximum likelihood solution becomes less significant the number data points increases and the limit the maximum likelihood solution for the variance equals the true variance the distribution that generated the data practice for anything other than small this bias will not prove serious problem however throughout this book shall interested more complex models with many parameters for which the bias problems associated with maximum likelihood will much more severe fact shall see the issue bias maximum likelihood lies the root the over fitting problem that encountered earlier the context polynomial curve fitting\n",
      "\n",
      "\n",
      "\n",
      "have seen how the problem polynomial curve fitting can expressed terms error minimization here return the curve fitting example and view from probabilistic perspective thereby gaining some insights into error functions and regularization well taking towards full bayesian treatment the goal the curve fitting problem able make predictions for the target variable given some new value the input variable the basis set training data comprising input values and their corresponding target values can express our uncertainty over the value the target variable using probability distribution for this purpose shall assume that given the value the corresponding value has gaussian distribution with mean equal the value the polynomial curve given thus have where for consistency with the notation later chapters have defined precision parameter corresponding the inverse variance the distribution this illustrated schematically figure figure schematic illustration gaussian conditional distribution for given given which the mean given the polynomial function and the precision given the parameter which related the variance probability theory now use the training data determine the values the unknown parameters and maximum likelihood the data are assumed drawn independently from the distribution then the likelihood function given did the case the simple gaussian distribution earlier convenient maximize the logarithm the likelihood function substituting for the form the gaussian distribution given obtain the log likelihood function the form consider first the determination the maximum likelihood solution for the polynomial coefficients which will denoted wml these are determined maximizing with respect for this purpose can omit the last two terms the right hand side because they not depend also note that scaling the log likelihood positive constant coefficient does not alter the location the maximum with respect and can replace the coefficient with finally instead maximizing the log likelihood can equivalently minimize the negative log likelihood therefore see that maximizing likelihood equivalent far determining concerned minimizing the sum squares error function defined thus the sum squares error function has arisen consequence maximizing likelihood under the assumption gaussian noise distribution can also use maximum likelihood determine the precision parameter the gaussian conditional distribution maximizing with respect gives wml introduction section again can first determine the parameter vector wml governing the mean and subsequently use this find the precision was the case for the simple gaussian distribution having determined the parameters and can now make predictions for new values because now have probabilistic model these are expressed terms the predictive distribution that gives the probability distribution over rather than simply point estimate and obtained substituting the maximum likelihood parameters into give wml wml now let take step towards more bayesian approach and introduce prior distribution over the polynomial coefficients for simplicity let consider gaussian distribution the form exp wtw where the precision the distribution and the total number elements the vector for order polynomial variables such which control the distribution model parameters are called hyperparameters using bayes theorem the posterior distribution for proportional the product the prior distribution and the likelihood function can now determine finding the most probable value given the data other words maximizing the posterior distribution this technique called maximum posterior simply map taking the negative logarithm and combining with and find that the maximum the posterior given the minimum wtw thus see that maximizing the posterior distribution equivalent minimizing the regularized sum squares error function encountered earlier the form with regularization parameter given\n",
      "\n",
      "\n",
      "\n",
      "although have included prior distribution are far still making point estimate and this does not yet amount bayesian treatment fully bayesian approach should consistently apply the sum and product rules probability which requires shall see shortly that integrate over all values such marginalizations lie the heart bayesian methods for pattern recognition probability theory the curve fitting problem are given the training data and along with new test point and our goal predict the value therefore wish evaluate the predictive distribution here shall assume that the parameters and are fixed and known advance later chapters shall discuss how such parameters can inferred from data bayesian setting bayesian treatment simply corresponds consistent application the sum and product rules probability which allow the predictive distribution written the form here given and have omitted the dependence and simplify the notation here the posterior distribution over parameters and can found normalizing the right hand side shall see section that for problems such the curve fitting example this posterior distribution gaussian and can evaluated analytically similarly the integration can also performed analytically with the result that the predictive distribution given gaussian the form where the mean and variance are given here the matrix given where the unit matrix and have defined the vector with elements for see that the variance well the mean the predictive distribution dependent the first term represents the uncertainty the predicted value due the noise the target variables and was expressed already the maximum likelihood predictive distribution through however the second term arises from the uncertainty the parameters and consequence the bayesian treatment the predictive distribution for the synthetic sinusoidal regression problem illustrated figure introduction figure the predictive distribution resulting from bayesian treatment polynomial curve fitting using polynomial with the fixed parameters and corresponding the known noise variance which the red curve denotes the mean the predictive distribution and the red region corresponds standard deviation around the mean\n",
      "\n",
      "\n",
      "\n",
      "our example polynomial curve fitting using least squares saw that there was optimal order polynomial that gave the best generalization the order the polynomial controls the number free parameters the model and thereby governs the model complexity with regularized least squares the regularization coefficient also controls the effective complexity the model whereas for more complex models such mixture distributions neural networks there may multiple parameters governing complexity practical application need determine the values such parameters and the principal objective doing usually achieve the best predictive performance new data furthermore well finding the appropriate values for complexity parameters within given model may wish consider range different types model order find the best one for our particular application have already seen that the maximum likelihood approach the performance the training set not good indicator predictive performance unseen data due the problem over fitting data plentiful then one approach simply use some the available data train range models given model with range values for its complexity parameters and then compare them independent data sometimes called validation set and select the one having the best predictive performance the model design iterated many times using limited size data set then some over fitting the validation data can occur and may necessary keep aside third test set which the performance the selected model finally evaluated many applications however the supply data for training and testing will limited and order build good models wish use much the available data possible for training however the validation set small will give relatively noisy estimate predictive performance one solution this dilemma use cross validation which illustrated figure this allows proportion the available data used for training while making use all the\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(book_data[i][\"Content\"])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data in JSON Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book_data.json', 'w') as file:\n",
    "    json.dump(book_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data in CSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Section', 'Title', 'Content'])\n",
    "\n",
    "for i in range(len(book_data)):\n",
    "    df = df.append(book_data[i], ignore_index=True)\n",
    "\n",
    "df.to_csv(\"book_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
