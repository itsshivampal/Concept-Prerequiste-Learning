{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Section', 'Title', 'Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_file = \"toc.txt\"\n",
    "chapter_data = \"booktext.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_array = []\n",
    "file = open(toc_file)\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    if line != \"\\n\" and re.search('[a-zA-Z0-9]', line):\n",
    "        toc_array.append(line)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_array = []\n",
    "file = open(chapter_data)\n",
    "for line in file:\n",
    "    line = line.strip()\n",
    "    if line != \"\\n\" and re.search('[a-zA-Z0-9]', line):\n",
    "        book_array.append(line)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_separation(line):\n",
    "    line = line.strip()\n",
    "    pos = line.find(\" \")\n",
    "    index_num = line[:pos]\n",
    "    title = line[pos+1:]\n",
    "    return (index_num, title)\n",
    "\n",
    "def create_file(line):\n",
    "    line = line.replace(\" \", \"_\")\n",
    "    file_name = \"book_data/\" + line + \".txt\"\n",
    "    file = open(file_name, \"w+\")\n",
    "    file.close()\n",
    "    return file_name\n",
    "\n",
    "def match_line(line1, line2):\n",
    "    line1 = line1.lower()\n",
    "    line2 = line2.lower()\n",
    "    index1, title = data_separation(line1)\n",
    "    if index1 in line2 or index1+\".\" in line2:\n",
    "        if title in line2: return 1\n",
    "        else: return 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def save_data(file_name, current_line):\n",
    "    index_num, title = data_separation(current_line)\n",
    "#     print(file_name)\n",
    "#     file_data = file_name.split(\"/\")[1].split(\".\")[0].split(\"_\")\n",
    "    file = open(file_name, \"r\")\n",
    "    content = file.read()\n",
    "    current_file.close()\n",
    "    data = {\n",
    "        'Section': index_num,\n",
    "        'Title': title,\n",
    "        'Content': content\n",
    "    }\n",
    "    df = df.append(data, ignore_index=True)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_len = len(toc_array)\n",
    "book_len = len(book_array)\n",
    "\n",
    "i = 0\n",
    "\n",
    "current_line = toc_array[i]\n",
    "next_line = toc_array[i + 1]\n",
    "\n",
    "file_name= create_file(current_line)\n",
    "current_file = open(file_name, \"a\")\n",
    "\n",
    "for j in range(book_len):\n",
    "    line = book_array[j]\n",
    "    if match_line(next_line, line):\n",
    "        save_data(file_name, current_line, df)\n",
    "        if i < toc_len - 2:\n",
    "            i += 1\n",
    "            current_line = toc_array[i]\n",
    "            next_line = toc_array[i + 1]\n",
    "        else:\n",
    "            current_line = toc_array[toc_len - 1]\n",
    "        current_file.close()\n",
    "        file_name = create_file(current_line)\n",
    "        current_file = open(file_name, \"a\")\n",
    "    else:\n",
    "        line += \"\\n\"\n",
    "        current_file.write(line)\n",
    "        \n",
    "current_file.close()\n",
    "save_data(file_name, current_line, df)\n",
    "\n",
    "df.to_csv(\"book_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1   Introduction\n",
      "1.1   Example: Polynomial Curve Fitting\n",
      "1.2   Probability Theory\n",
      "1.2.1   Probability densities\n",
      "1.2.2   Expectations and covariances\n",
      "1.2.3   Bayesian probabilities\n",
      "1.2.4   The Gaussian distribution\n",
      "1.2.5   Curve ﬁtting re-visited\n",
      "1.2.6   Bayesian curve ﬁtting\n",
      "1.3   Model Selection\n",
      "1.4   The Curse of Dimensionality\n",
      "1.5   Decision Theory\n",
      "1.5.1   Minimizing the misclassiﬁcation rate\n",
      "1.5.2   Minimizing the expected loss\n",
      "1.5.3   The reject option\n",
      "1.5.4   Inference and decision\n",
      "1.5.5   Loss functions for regression\n",
      "1.6   Information Theory\n",
      "1.6.1   Relative entropy and mutual information\n",
      "2   Probability Distributions\n",
      "2.1   Binary Variables\n",
      "2.1.1   The beta distribution\n",
      "2.2   Multinomial Variables\n",
      "2.2.1   The Dirichlet distribution\n",
      "2.3   The Gaussian Distribution\n",
      "2.3.1   Conditional Gaussian distributions\n",
      "2.3.2   Marginal Gaussian distributions\n",
      "2.3.3   Bayes’ theorem for Gaussian variables\n",
      "2.3.4   Maximum likelihood for the Gaussian\n",
      "2.3.5   Sequential estimation\n",
      "2.3.6   Bayesian inference for the Gaussian\n",
      "2.3.7   Student’s t-distribution\n",
      "2.3.8   Periodic variables\n",
      "2.3.9   Mixtures of Gaussians\n",
      "2.4   The Exponential Family\n",
      "2.4.1   Maximum likelihood and sufﬁcient statistics\n",
      "2.4.2   Conjugate priors\n",
      "2.4.3   Noninformative priors\n",
      "2.5   Nonparametric Methods\n",
      "2.5.1   Kernel density estimators\n",
      "2.5.2   Nearest-neighbour methods\n",
      "3.   Linear Models for Regression\n",
      "3.1   Linear Basis Function Models\n",
      "3.1.1   Maximum likelihood and least squares\n",
      "3.1.2   Geometry of least squares\n",
      "3.1.3   Sequential learning\n",
      "3.1.4   Regularized least squares\n",
      "3.1.5   Multiple outputs\n",
      "3.2   The Bias-Variance Decomposition\n",
      "3.3   Bayesian Linear Regression\n",
      "3.3.1   Parameter distribution\n",
      "3.3.2   Predictive distribution\n",
      "3.3.3   Equivalent kernel\n",
      "3.4   Bayesian Model Comparison\n",
      "3.5   The Evidence Approximation\n",
      "3.5.1   Evaluation of the evidence function\n",
      "3.5.2   Maximizing the evidence function\n",
      "3.5.3   Effective number of parameters\n",
      "3.6   Limitations of Fixed Basis Functions\n",
      "4.   Linear Models for Classification\n",
      "4.1   Discriminant Functions\n",
      "4.1.1   Two classes\n",
      "4.1.2   Multiple classes\n",
      "4.1.3   Least squares for classification\n",
      "4.1.4   Fisher’s linear discriminant\n",
      "4.1.5   Relation to least squares\n",
      "4.1.6   Fisher’s discriminant for multiple classes\n",
      "4.1.7   The perceptron algorithm\n",
      "4.2   Probabilistic Generative Models\n",
      "4.2.1   Continuous inputs\n",
      "4.2.2   Maximum likelihood solution\n",
      "4.2.3   Discrete features\n",
      "4.2.4   Exponential family\n",
      "4.3   Probabilistic Discriminative Models\n",
      "4.3.1   Fixed basis functions\n",
      "4.3.2   Logistic regression\n",
      "4.3.3   Iterative reweighted least squares\n",
      "4.3.4   Multiclass logistic regression\n",
      "4.3.5   Probit regression\n",
      "4.3.6   Canonical link functions\n",
      "4.4   The Laplace Approximation\n",
      "4.4.1   Model comparison and BIC\n",
      "4.5   Bayesian Logistic Regression\n",
      "4.5.1   Laplace approximation\n",
      "4.5.2   Predictive distribution\n",
      "5   Neural Networks\n",
      "5.1   Feed-forward Network Functions\n",
      "5.1.1   Weight-space symmetries\n",
      "5.2   Network Training\n",
      "5.2.1   Parameter optimization\n",
      "5.2.2   Local quadratic approximation\n",
      "5.2.3   Use of gradient information\n",
      "5.2.4   Gradient descent optimization\n",
      "5.3   Error Backpropagation\n",
      "5.3.1   Evaluation of error-function derivatives\n",
      "5.3.2   A simple example\n",
      "5.3.3   Efficiency of backpropagation\n",
      "5.3.4   The Jacobian matrix\n",
      "5.4   The Hessian Matrix\n",
      "5.4.1   Diagonal approximation\n",
      "5.4.2   Outer product approximation\n",
      "5.4.3   Inverse Hessian\n",
      "5.4.4   Finite differences\n",
      "5.4.5   Exact evaluation of the Hessian\n",
      "5.4.6   Fast multiplication by the Hessian\n",
      "5.5   Regularization in Neural Networks\n",
      "5.5.1   Consistent Gaussian priors\n",
      "5.5.2   Early stopping\n",
      "5.5.3   Invariances\n",
      "5.5.4   Tangent propagation\n",
      "5.5.5   Training with transformed data\n",
      "5.5.6   Convolutional networks\n",
      "5.5.7   Soft weight sharing\n",
      "5.6   Mixture Density Networks\n",
      "5.7   Bayesian Neural Networks\n",
      "5.7.1   Posterior parameter distribution\n",
      "5.7.2   Hyperparameter optimization\n",
      "5.7.3   Bayesian neural networks for classification\n",
      "6   Kernel Methods\n",
      "6.1   Dual Representations\n",
      "6.2   Constructing Kernels\n",
      "6.3   Radial Basis Function Networks\n",
      "6.3.1   Nadaraya-Watson model\n",
      "6.4   Gaussian Processes\n",
      "6.4.1   Linear regression revisited\n",
      "6.4.2   Gaussian processes for regression\n",
      "6.4.3   Learning the hyperparameters\n",
      "6.4.4   Automatic relevance determination\n",
      "6.4.5   Gaussian processes for classification\n",
      "6.4.6   Laplace approximation\n",
      "6.4.7   Connection to neural networks\n",
      "7   Sparse Kernel Machines\n",
      "7.1   Maximum Margin Classifiers\n",
      "7.1.1   Overlapping class distributions\n",
      "7.1.2   Relation to logistic regression\n",
      "7.1.3   Multiclass SVMs\n",
      "7.1.4   SVMs for regression\n",
      "7.1.5   Computational learning theory\n",
      "7.2   Relevance Vector Machines\n",
      "7.2.1   RVM for regression\n",
      "7.2.2   Analysis of sparsity\n",
      "7.2.3   RVM for classification\n",
      "8   Graphical Models\n",
      "8.1   Bayesian Networks\n",
      "8.1.1   Example: Polynomial regression\n",
      "8.1.2   Generative models\n",
      "8.1.3   Discrete variables\n",
      "8.1.4   Linear-Gaussian models\n",
      "8.2   Conditional Independence\n",
      "8.2.1   Three example graphs\n",
      "8.2.2   D-separation\n",
      "8.3   Markov Random Fields\n",
      "8.3.1   Conditional independence properties\n",
      "8.3.2   Factorization properties\n",
      "8.3.3   Illustration: Image de-noising\n",
      "8.3.4   Relation to directed graphs\n",
      "8.4   Inference in Graphical Models\n",
      "8.4.1   Inference on a chain\n",
      "8.4.2   Trees\n",
      "8.4.3   Factor graphs\n",
      "8.4.4   The sum-product algorithm\n",
      "8.4.5   The max-sum algorithm\n",
      "8.4.6   Exact inference in general graphs\n",
      "8.4.7   Loopy belief propagation\n",
      "8.4.8   Learning the graph structure\n",
      "9   Mixture Models and EM\n",
      "9.1   K-means Clustering . . . . . . . . . . . . .\n",
      "9.1.1   Image segmentation and compression\n",
      "9.2   Mixtures of Gaussians . . . . . . . . . . . .\n",
      "9.2.1   Maximum likelihood . . . . . . . . .\n",
      "9.2.2   EM for Gaussian mixtures . . . . . .\n",
      "9.3   An Alternative View of EM . . . . . . . . .\n",
      "9.3.1   Gaussian mixtures revisited . . . . .\n",
      "9.3.2   Relation to K-means . . . . . . . . .\n",
      "9.3.3   Mixtures of Bernoulli distributions . .\n",
      "9.3.4   EM for Bayesian linear regression . .\n",
      "9.4   The EM Algorithm in General\n",
      "10   Approximate Inference\n",
      "10.1   Variational Inference\n",
      "10.1.1   Factorized distributions\n",
      "10.1.2   Properties of factorized approximations\n",
      "10.1.3   Example: The univariate Gaussian\n",
      "10.1.4   Model comparison\n",
      "10.2   Illustration: Variational Mixture of Gaussians\n",
      "10.2.1   Variational distribution\n",
      "10.2.2   Variational lower bound\n",
      "10.2.3   Predictive density\n",
      "10.2.4   Determining the number of components\n",
      "10.2.5   Induced factorizations\n",
      "10.3   Variational Linear Regression\n",
      "10.3.1   Variational distribution\n",
      "10.3.2   Predictive distribution\n",
      "10.3.3   Lower bound\n",
      "10.4   Exponential Family Distributions\n",
      "10.4.1   Variational message passing\n",
      "10.5   Local Variational Methods\n",
      "10.6   Variational Logistic Regression\n",
      "10.6.1   Variational posterior distribution\n",
      "10.6.2   Optimizing the variational parameters\n",
      "10.6.3   Inference of hyperparameters\n",
      "10.7   Expectation Propagation\n",
      "10.7.1   Example: The clutter problem\n",
      "10.7.2   Expectation propagation on graphs\n",
      "11   Sampling Methods\n",
      "11.1   Basic Sampling Algorithms\n",
      "11.1.1   Standard distributions\n",
      "11.1.2   Rejection sampling\n",
      "11.1.3   Adaptive rejection sampling\n",
      "11.1.4   Importance sampling\n",
      "11.1.5   Sampling-importance-resampling\n",
      "11.1.6   Sampling and the EM algorithm\n",
      "11.2   Markov Chain Monte Carlo\n",
      "11.2.1   Markov chains\n",
      "11.2.2   The Metropolis-Hastings algorithm\n",
      "11.3   Gibbs Sampling\n",
      "11.4   Slice Sampling\n",
      "11.5   The Hybrid Monte Carlo Algorithm\n",
      "11.5.1   Dynamical systems\n",
      "11.5.2   Hybrid Monte Carlo\n",
      "11.6   Estimating the Partition Function\n",
      "12   Continuous Latent Variables\n",
      "12.1   Principal Component Analysis\n",
      "12.1.1   Maximum variance formulation\n",
      "12.1.2   Minimum-error formulation\n",
      "12.1.3   Applications of PCA\n",
      "12.1.4   PCA for high-dimensional data\n",
      "12.2   Probabilistic PCA\n",
      "12.2.1   Maximum likelihood PCA\n",
      "12.2.2   EM algorithm for PCA\n",
      "12.2.3   Bayesian PCA\n",
      "12.2.4   Factor analysis\n",
      "12.3   Kernel PCA\n",
      "12.4   Nonlinear Latent Variable Models\n",
      "12.4.1   Independent component analysis\n",
      "12.4.2   Autoassociative neural networks\n",
      "12.4.3   Modelling nonlinear manifolds\n",
      "13   Sequential Data\n",
      "13.1   Markov Models\n",
      "13.2   Hidden Markov Models\n",
      "13.2.1   Maximum likelihood for the HMM\n",
      "13.2.2   The forward-backward algorithm\n",
      "13.2.3   The sum-product algorithm for the HMM\n",
      "13.2.4   Scaling factors\n",
      "13.2.5   The Viterbi algorithm\n",
      "13.2.6   Extensions of the hidden Markov model\n",
      "13.3   Linear Dynamical Systems\n",
      "13.3.1   Inference in LDS\n",
      "13.3.2   Learning in LDS\n",
      "13.3.3   Extensions of LDS\n",
      "13.3.4   Particle filters\n",
      "14   Combining Models\n",
      "14.1   Bayesian Model Averaging\n",
      "14.2   Committees\n",
      "14.3   Boosting\n",
      "14.3.1   Minimizing exponential error\n",
      "14.3.2   Error functions for boosting\n",
      "14.4   Tree-based Models\n",
      "14.5   Conditional Mixture Models\n",
      "14.5.1   Mixtures of linear regression models\n",
      "14.5.2   Mixtures of logistic models\n",
      "14.5.3   Mixtures of experts\n"
     ]
    }
   ],
   "source": [
    "file = open(toc_file)\n",
    "write_data = open(\"dummy.txt\", \"a\")\n",
    "for line in file:\n",
    "    if line!= \"\\n\":\n",
    "        write_data.write(line)\n",
    "        index_num, title = data_separation(line)\n",
    "        print(index_num, \" \", title)\n",
    "file.close()\n",
    "write_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toc_file = file.open(toc_file)\n",
    "book_file = file.open(chapter_data)\n",
    "\n",
    "for index_line in toc_file:\n",
    "    \n",
    "    for book_line in book_file:\n",
    "\n",
    "\n",
    "toc_file.close()\n",
    "book_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
