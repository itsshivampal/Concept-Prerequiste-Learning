We have seen that the range of functions which can be represented by a neural
network is governed by the number M of hidden units, and that, for sufficiently
large M, a two-layer network can approximate any given function with arbitrary
accuracy.
In the framework of maximum likelihood, the number of hidden units
needs to be limited (to a level dependent on the size of the training set) in order
to avoid over-fitting. However, from a Bayesian perspective it makes little sense to
limit the number of parameters in the network according to the size of the training
set.
In a Bayesian neural network, the prior distribution over the parameter vector
w, in conjunction with the network function f(x, w), produces a prior distribution
over functions from y(x) where y is the vector of network outputs. Neal (1996)
has shown that, for a broad class of prior distributions over w, the distribution of
functions generated by a neural network will tend to a Gaussian process in the limit
M → ∞. It should be noted, however, that in this limit the output variables of the
neural network become independent. One of the great merits of neural networks is
that the outputs share the hidden units and so they can ‘borrow statistical strength’
from each other, that is, the weights associated with each hidden unit are influenced
by all of the output variables not just by one of them. This property is therefore lost
in the Gaussian process limit.
We have seen that a Gaussian process is determined by its covariance (kernel)
function. Williams (1998) has given explicit forms for the covariance in the case of
two specific choices for the hidden unit activation function (probit and Gaussian).
These kernel functions k(x, x) are nonstationary, i.e. they cannot be expressed as
a function of the difference x − x, as a consequence of the Gaussian weight prior
being centred on zero which breaks translation invariance in weight space.
320
6. KERNEL METHODS
Exercises
By working directly with the covariance function we have implicitly marginal-
ized over the distribution of weights. If the weight prior is governed by hyperpa-
rameters, then their values will determine the length scales of the distribution over
functions, as can be understood by studying the examples in Figure 5.11 for the case
of a finite number of hidden units. Note that we cannot marginalize out the hyperpa-
rameters analytically, and must instead resort to techniques of the kind discussed in
Section 6.4.
6.1 ( ) www Consider the dual formulation of the least squares linear regression
problem given in Section 6.1. Show that the solution for the components an of
the vector a can be expressed as a linear combination of the elements of the vector
φ(xn). Denoting these coefficients by the vector w, show that the dual of the dual
formulation is given by the original representation in terms of the parameter vector
w.
6.2 ( )
In this exercise, we develop a dual formulation of the perceptron learning
algorithm. Using the perceptron learning rule (4.55), show that the learned weight
vector w can be written as a linear combination of the vectors tnφ(xn) where tn ∈
{−1, +1}. Denote the coefficients of this linear combination by αn and derive a
formulation of the perceptron learning algorithm, and the predictive function for the
perceptron, in terms of the αn. Show that the feature vector φ(x) enters only in the
form of the kernel function k(x, x) = φ(x)Tφ(x).
6.3 () The nearest-neighbour classifier (Section 2.5.2) assigns a new input vector x
to the same class as that of the nearest input vector xn from the training set, where
in the simplest case, the distance is defined by the Euclidean metric x − xn2. By
expressing this rule in terms of scalar products and then making use of kernel sub-
stitution, formulate the nearest-neighbour classifier for a general nonlinear kernel.
6.4 ()
In Appendix C, we give an example of a matrix that has positive elements but
that has a negative eigenvalue and hence that is not positive definite. Find an example
of the converse property, namely a 2 × 2 matrix with positive eigenvalues yet that
has at least one negative element.
6.5 () www Verify the results (6.13) and (6.14) for constructing valid kernels.
6.6 () Verify the results (6.15) and (6.16) for constructing valid kernels.
6.7 () www Verify the results (6.17) and (6.18) for constructing valid kernels.
6.8 () Verify the results (6.19) and (6.20) for constructing valid kernels.
6.9 () Verify the results (6.21) and (6.22) for constructing valid kernels.
6.10 () Show that an excellent choice of kernel for learning a function f(x) is given
by k(x, x) = f(x)f(x) by showing that a linear learning machine based on this
kernel will always find a solution proportional to f(x).
N
(6.97)
(6.98)
w =
n=1
αnφ(xn) + w⊥
show that the value of w that minimizes J(w) takes the form of a linear combination
of the basis functions φ(xn) for n = 1, . . . , N.
6.17 ( ) www Consider the sum-of-squares error function (6.39) for data having
noisy inputs, where ν(ξ) is the distribution of the noise. Use the calculus of vari-
ations to minimize this error function with respect to the function y(x), and hence
show that the optimal solution is given by an expansion of the form (6.40) in which
the basis functions are given by (6.41).
Exercises
321
6.11 () By making use of the expansion (6.25), and then expanding the middle factor
as a power series, show that the Gaussian kernel (6.23) can be expressed as the inner
product of an infinite-dimensional feature vector.
6.12 ( ) www Consider the space of all possible subsets A of a given fixed set D.
Show that the kernel function (6.27) corresponds to an inner product in a feature
space of dimensionality 2|D| defined by the mapping φ(A) where A is a subset of D
and the element φU (A), indexed by the subset U, is given by
φU (A) =
1,
if U ⊆ A;
0, otherwise.
(6.95)
Here U ⊆ A denotes that U is either a subset of A or is equal to A.
6.13 () Show that the Fisher kernel, defined by (6.33), remains invariant if we make
a nonlinear transformation of the parameter vector θ → ψ(θ), where the function
ψ(·) is invertible and differentiable.
6.14 () www Write down the form of the Fisher kernel, defined by (6.33), for the
case of a distribution p(x|µ) = N (x|µ, S) that is Gaussian with mean µ and fixed
covariance S.
6.15 () By considering the determinant of a 2 × 2 Gram matrix, show that a positive-
definite kernel function k(x, x) satisfies the Cauchy-Schwartz inequality
k(x1, x2)2 � k(x1, x1)k(x2, x2).
(6.96)
6.16 ( ) Consider a parametric model governed by the parameter vector w together
with a data set of input values x1, . . . , xN and a nonlinear feature mapping φ(x).
Suppose that the dependence of the error function on w takes the form
J(w) = f(wTφ(x1), . . . , wTφ(xN )) + g(wTw)
where g(·) is a monotonically increasing function. By writing w in the form
N
E =
1
2
{y(xn − ξn) − tn}2 g(ξn) dξn.
n=1
(6.99)
By minimizing E with respect to the function y(z) using the calculus of variations
(Appendix D), show that optimal solution for y(x) is given by a Nadaraya-Watson
kernel regression solution of the form (6.45) with a kernel of the form (6.46).
6.20 ( ) www Verify the results (6.66) and (6.67).
6.21 ( ) www Consider a Gaussian process regression model in which the kernel
function is defined in terms of a fixed set of nonlinear basis functions. Show that the
predictive distribution is identical to the result (3.58) obtained in Section 3.3.2 for the
Bayesian linear regression model. To do this, note that both models have Gaussian
predictive distributions, and so it is only necessary to show that the conditional mean
and variance are the same. For the mean, make use of the matrix identity (C.6), and
for the variance, make use of the matrix identity (C.7).
6.22 ( ) Consider a regression problem with N training set input vectors x1, . . . , xN
and L test set input vectors xN +1, . . . , xN +L, and suppose we define a Gaussian
process prior over functions t(x). Derive an expression for the joint predictive dis-
tribution for t(xN +1), . . . , t(xN +L), given the values of t(x1), . . . , t(xN ). Show the
marginal of this distribution for one of the test observations tj where N + 1 � j �
N + L is given by the usual Gaussian process regression result (6.66) and (6.67).
6.23 ( ) www Consider a Gaussian process regression model in which the target
variable t has dimensionality D. Write down the conditional distribution of tN +1
for a test input vector xN +1, given a training set of input vectors x1, . . . , xN +1 and
corresponding target observations t1, . . . , tN .
6.24 () Show that a diagonal matrix W whose elements satisfy 0 < Wii < 1 is positive
definite. Show that the sum of two positive definite matrices is itself positive definite.
322
6. KERNEL METHODS
6.18 () Consider a Nadaraya-Watson model with one input variable x and one target
variable t having Gaussian components with isotropic covariances, so that the co-
variance matrix is given by σ2I where I is the unit matrix. Write down expressions
for the conditional density p(t|x) and for the conditional mean E[t|x] and variance
var[t|x], in terms of the kernel function k(x, xn).
6.19 ( ) Another viewpoint on kernel regression comes from a consideration of re-
gression problems in which the input variables as well as the target variables are
corrupted with additive noise. Suppose each target value tn is generated as usual
by taking a function y(zn) evaluated at a point zn, and adding Gaussian noise. The
value of zn is not directly observed, however, but only a noise corrupted version
xn = zn + ξn where the random variable ξ is governed by some distribution g(ξ).
Consider a set of observations {xn, tn}, where n = 1, . . . , N, together with a cor-
responding sum-of-squares error function defined by averaging over the distribution
of input noise to give
Exercises
323
6.25 () www Using the Newton-Raphson formula (4.92), derive the iterative update
N of the posterior distribution in the Gaussian
formula (6.83) for finding the mode a
process classification model.
6.26 () Using the result (2.115), derive the expressions (6.87) and (6.88) for the mean
and variance of the posterior distribution p(aN +1|tN ) in the Gaussian process clas-
sification model.
6.27 (  ) Derive the result (6.90) for the log likelihood function in the Laplace approx-
imation framework for Gaussian process classification. Similarly, derive the results
(6.91), (6.92), and (6.94) for the terms in the gradient of the log likelihood.
