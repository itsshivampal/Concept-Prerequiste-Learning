We have seen that the range of functions which can be represented by a neural
network is governed by the number M of hidden units, and that, for sufficiently
large M, a two-layer network can approximate any given function with arbitrary
accuracy.
In the framework of maximum likelihood, the number of hidden units
needs to be limited (to a level dependent on the size of the training set) in order
to avoid over-fitting. However, from a Bayesian perspective it makes little sense to
limit the number of parameters in the network according to the size of the training
set.
In a Bayesian neural network, the prior distribution over the parameter vector
w, in conjunction with the network function f(x, w), produces a prior distribution
over functions from y(x) where y is the vector of network outputs. Neal (1996)
has shown that, for a broad class of prior distributions over w, the distribution of
functions generated by a neural network will tend to a Gaussian process in the limit
M → ∞. It should be noted, however, that in this limit the output variables of the
neural network become independent. One of the great merits of neural networks is
that the outputs share the hidden units and so they can ‘borrow statistical strength’
from each other, that is, the weights associated with each hidden unit are influenced
by all of the output variables not just by one of them. This property is therefore lost
in the Gaussian process limit.
We have seen that a Gaussian process is determined by its covariance (kernel)
function. Williams (1998) has given explicit forms for the covariance in the case of
two specific choices for the hidden unit activation function (probit and Gaussian).
These kernel functions k(x, x) are nonstationary, i.e. they cannot be expressed as
a function of the difference x − x, as a consequence of the Gaussian weight prior
being centred on zero which breaks translation invariance in weight space.
320
6. KERNEL METHODS
Exercises
By working directly with the covariance function we have implicitly marginal-
ized over the distribution of weights. If the weight prior is governed by hyperpa-
rameters, then their values will determine the length scales of the distribution over
functions, as can be understood by studying the examples in Figure 5.11 for the case
of a finite number of hidden units. Note that we cannot marginalize out the hyperpa-
rameters analytically, and must instead resort to techniques of the kind discussed in
Section 6.4.
