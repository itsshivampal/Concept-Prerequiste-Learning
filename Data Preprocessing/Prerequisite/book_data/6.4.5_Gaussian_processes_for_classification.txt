In a probabilistic approach to classification, our goal is to model the posterior
probabilities of the target variable for a new input vector, given a set of training
data. These probabilities must lie in the interval (0, 1), whereas a Gaussian process
model makes predictions that lie on the entire real axis. However, we can easily
adapt Gaussian processes to classification problems by transforming the output of
the Gaussian process using an appropriate nonlinear activation function.
Consider first the two-class problem with a target variable t ∈ {0, 1}. If we de-
fine a Gaussian process over a function a(x) and then transform the function using
a logistic sigmoid y = σ(a), given by (4.59), then we will obtain a non-Gaussian
stochastic process over functions y(x) where y ∈ (0, 1). This is illustrated for the
case of a one-dimensional input space in Figure 6.11 in which the probability distri-
314
6. KERNEL METHODS
10
5
0
−5
−10
−1
−0.5
0
0.5
1
1
0.75
0.5
0.25
0
−1
−0.5
0
0.5
1
Figure 6.11 The left plot shows a sample from a Gaussian process prior over functions a(x), and the right plot
shows the result of transforming this sample using a logistic sigmoid function.
bution over the target variable t is then given by the Bernoulli distribution
p(t|a) = σ(a)t(1 − σ(a))1−t.
(6.73)
As usual, we denote the training set inputs by x1, . . . , xN with corresponding
observed target variables t = (t1, . . . , tN )T. We also consider a single test point
xN +1 with target value tN +1. Our goal is to determine the predictive distribution
p(tN +1|t), where we have left the conditioning on the input variables implicit. To do
this we introduce a Gaussian process prior over the vector aN +1, which has compo-
nents a(x1), . . . , a(xN +1). This in turn defines a non-Gaussian process over tN +1,
and by conditioning on the training data tN we obtain the required predictive distri-
bution. The Gaussian process prior for aN +1 takes the form
p(aN +1) = N (aN +1|0, CN +1).
(6.74)
Unlike the regression case, the covariance matrix no longer includes a noise term
because we assume that all of the training data points are correctly labelled. How-
ever, for numerical reasons it is convenient to introduce a noise-like term governed
by a parameter ν that ensures that the covariance matrix is positive definite. Thus
the covariance matrix CN +1 has elements given by
C(xn, xm) = k(xn, xm) + νδnm
(6.75)
where k(xn, xm) is any positive semidefinite kernel function of the kind considered
in Section 6.2, and the value of ν is typically fixed in advance. We shall assume that
the kernel function k(x, x) is governed by a vector θ of parameters, and we shall
later discuss how θ may be learned from the training data.
For two-class problems, it is sufficient to predict p(tN +1 = 1|tN ) because the
value of p(tN +1 = 0|tN ) is then given by 1 − p(tN +1 = 1|tN ). The required
predictive distribution is given by
p(tN +1 = 1|tN ) =
p(tN +1 = 1|aN +1)p(aN +1|tN ) daN +1
(6.76)
where p(tN +1 = 1|aN +1) = σ(aN +1).
This integral is analytically intractable, and so may be approximated using sam-
pling methods (Neal, 1997). Alternatively, we can consider techniques based on
an analytical approximation. In Section 4.5.2, we derived the approximate formula
(4.153) for the convolution of a logistic sigmoid with a Gaussian distribution. We
can use this result to evaluate the integral in (6.76) provided we have a Gaussian
approximation to the posterior distribution p(aN +1|tN ). The usual justification for a
Gaussian approximation to a posterior distribution is that the true posterior will tend
to a Gaussian as the number of data points increases as a consequence of the central
limit theorem. In the case of Gaussian processes, the number of variables grows with
the number of data points, and so this argument does not apply directly. However, if
we consider increasing the number of data points falling in a fixed region of x space,
then the corresponding uncertainty in the function a(x) will decrease, again leading
asymptotically to a Gaussian (Williams and Barber, 1998).
Three different approaches to obtaining a Gaussian approximation have been
considered. One technique is based on variational inference (Gibbs and MacKay,
2000) and makes use of the local variational bound (10.144) on the logistic sigmoid.
This allows the product of sigmoid functions to be approximated by a product of
Gaussians thereby allowing the marginalization over aN to be performed analyti-
cally. The approach also yields a lower bound on the likelihood function p(tN|θ).
The variational framework for Gaussian process classification can also be extended
to multiclass (K > 2) problems by using a Gaussian approximation to the softmax
function (Gibbs, 1997).
A second approach uses expectation propagation (Opper and Winther, 2000b;
Minka, 2001b; Seeger, 2003). Because the true posterior distribution is unimodal, as
we shall see shortly, the expectation propagation approach can give good results.
