So far in our general discussion of EP, we have allowed the factors fi(θ) in the
distribution p(θ) to be functions of all of the components of θ, and similarly for the
f(θ) in the approximating distribution q(θ). We now consider
approximating factors
situations in which the factors depend only on subsets of the variables. Such restric-
tions can be conveniently expressed using the framework of probabilistic graphical
models, as discussed in Chapter 8. Here we use a factor graph representation because
this encompasses both directed and undirected graphs.
10. APPROXIMATE INFERENCE
514
100
r
o
r
r
E
10−5
Posterior mean
laplace
vb
104
ep
106
FLOPS
10−200
r
o
r
r
E
10−202
laplace
10−204
104
Evidence
vb
ep
106
FLOPS
Figure 10.17 Comparison of expectation propagation, variational inference, and the Laplace approximation on
the clutter problem. The left-hand plot shows the error in the predicted posterior mean versus the number of
floating point operations, and the right-hand plot shows the corresponding results for the model evidence.
Section 8.4.4
We shall focus on the case in which the approximating distribution is fully fac-
torized, and we shall show that in this case expectation propagation reduces to loopy
belief propagation (Minka, 2001a). To start with, we show this in the context of a
simple example, and then we shall explore the general case.
First of all, recall from (10.17) that if we minimize the Kullback-Leibler diver-
gence KL(pq) with respect to a factorized distribution q, then the optimal solution
for each factor is simply the corresponding marginal of p.
Now consider the factor graph shown on the left in Figure 10.18, which was
introduced earlier in the context of the sum-product algorithm. The joint distribution
is given by
p(x) = fa(x1, x2)fb(x2, x3)fc(x2, x4).
We seek an approximation q(x) that has the same factorization, so that
(10.225)
(10.226)
q(x) ∝
fa(x1, x2)
fb(x2, x3)
fc(x2, x4).
Note that normalization constants have been omitted, and these can be re-instated at
the end by local normalization, as is generally done in belief propagation. Now sup-
pose we restrict attention to approximations in which the factors themselves factorize
with respect to the individual variables so that
q(x) ∝
fa1(x1)
fa2(x2)
fb2(x2)
fb3(x3)
fc2(x2)
fc4(x4)
(10.227)
which corresponds to the factor graph shown on the right in Figure 10.18. Because
the individual factors are factorized, the overall distribution q(x) is itself fully fac-
torized.
Now we apply the EP algorithm using the fully factorized approximation. Sup-
pose that we have initialized all of the factors and that we choose to refine factor
x3
10.7. Expectation Propagation
515
x2
x3
x1
x2
x3
x1
fa
fb
fc
x4
˜fa1
˜fa2
˜fb2
˜fb3
˜fc2
˜fc4
x4
Figure 10.18 On the left is a simple factor graph from Figure 8.51 and reproduced here for convenience. On
the right is the corresponding factorized approximation.
fb(x2, x3) =
distribution to give
fb2(x2)
fb3(x3). We first remove this factor from the approximating
q\b(x) =
fa1(x1)
fa2(x2)
fc2(x2)
fc4(x4)
(10.228)
and we then multiply this by the exact factor fb(x2, x3) to give
fa1(x1)
fa2(x2)
p(x) = q\b(x)fb(x2, x3) =
(10.229)
pqnew).
We now find qnew(x) by minimizing the Kullback-Leibler divergence KL(
The result, as noted above, is that qnew(z) comprises the product of factors, one for
each variable xi, in which each factor is given by the corresponding marginal of
p(x). These four marginals are given by
fc4(x4)fb(x2, x3).
fc2(x2)
p(x1) ∝
p(x2) ∝
p(x3) ∝
p(x4) ∝
fa1(x1)
fa2(x2)
x2
fc4(x4)
fc2(x2)
fb(x2, x3)
fb(x2, x3)
fc2(x2)
x3
fa2(x2)
and qnew(x) is obtained by multiplying these marginals together. We see that the
fb(x2, x3) are those that involve
only factors in q(x) that change when we update
fb(x2, x3) =
the variables in fb namely x2 and x3. To obtain the refined factor
fb2(x2)
fb3(x3) we simply divide qnew(x) by q\b(x), which gives
fb2(x2) ∝
fb(x2, x3)
fb3(x3) ∝
x2
fb(x2, x3)
fa2(x2)
fc2(x2)
(10.235)
(10.230)
(10.231)
(10.232)
(10.233)
(10.234)
i=j
k
i
k
i
516
10. APPROXIMATE INFERENCE
Section 8.4.4
These are precisely the messages obtained using belief propagation in which mes-
sages from variable nodes to factor nodes have been folded into the messages from
fb2(x2) corresponds to the message
factor nodes to variable nodes. In particular,
µfb→x2(x2) sent by factor node fb to variable node x2 and is given by (8.81). Simi-
fa2(x2) corre-
larly, if we substitute (8.78) into (8.79), we obtain (10.235) in which
sponds to µfa→x2(x2) and
fc2(x2) corresponds to µfc→x2(x2), giving the message
fb3(x3) which corresponds to µfb→x3(x3).
This result differs slightly from standard belief propagation in that messages are
passed in both directions at the same time. We can easily modify the EP procedure
to give the standard form of the sum-product algorithm by updating just one of the
fb2(x2) is unchanged
factors at a time, for instance if we refine only
fb3(x3) is again given by (10.235). If
by definition, while the refined version of
we are refining only one term at a time, then we can choose the order in which the
refinements are done as we wish. In particular, for a tree-structured graph we can
follow a two-pass update scheme, corresponding to the standard belief propagation
schedule, which will result in exact inference of the variable and factor marginals.
The initialization of the approximation factors in this case is unimportant.
fb3(x3), then
Now let us consider a general factor graph corresponding to the distribution
p(θ) =
fi(θi)
(10.236)
where θi represents the subset of variables associated with factor fi. We approximate
this using a fully factorized distribution of the form
q(θ) ∝
fik(θk)
(10.237)
where θk corresponds to an individual variable node. Suppose that we wish to refine
fjl(θl) keeping all other terms fixed. We first remove the term
the particular term
fj(θj) from q(θ) to give
q\j(θ) ∝
fik(θk)
(10.238)
fjl(θl),
and then multiply by the exact factor fj(θj). To determine the refined term
we need only consider the functional dependence on θl, and so we simply find the
corresponding marginal of
q\j(θ)fj(θj).
(10.239)
Up to a multiplicative constant, this involves taking the marginal of fj(θj) multiplied
by any terms from q\j(θ) that are functions of any of the variables in θj. Terms that
fi(θi) for i = j will cancel between numerator and
correspond to other factors
denominator when we subsequently divide by q\j(θ). We therefore obtain
fjl(θl) ∝
fj(θj)
fkm(θm).
(10.240)
θm=l∈θj
k
m=l
Exercises
517
We recognize this as the sum-product rule in the form in which messages from vari-
able nodes to factor nodes have been eliminated, as illustrated by the example shown
fjm(θm) corresponds to the message µfj→θm(θm),
in Figure 8.50. The quantity
which factor node j sends to variable node m, and the product over k in (10.240)
is over all factors that depend on the variables θm that have variables (other than
variable θl) in common with factor fj(θj). In other words, to compute the outgoing
message from a factor node, we take the product of all the incoming messages from
other factor nodes, multiply by the local factor, and then marginalize.
Thus, the sum-product algorithm arises as a special case of expectation propa-
gation if we use an approximating distribution that is fully factorized. This suggests
that more flexible approximating distributions, corresponding to partially discon-
nected graphs, could be used to achieve higher accuracy. Another generalization is
to group factors fi(θi) together into sets and to refine all the factors in a set together
at each iteration. Both of these approaches can lead to improvements in accuracy
(Minka, 2001b). In general, the problem of choosing the best combination of group-
ing and disconnection is an open research issue.
We have seen that variational message passing and expectation propagation op-
timize two different forms of the Kullback-Leibler divergence. Minka (2005) has
shown that a broad range of message passing algorithms can be derived from a com-
mon framework involving minimization of members of the alpha family of diver-
gences, given by (10.19). These include variational message passing, loopy belief
propagation, and expectation propagation, as well as a range of other algorithms,
which we do not have space to discuss here, such as tree-reweighted message pass-
ing (Wainwright et al., 2005), fractional belief propagation (Wiegerinck and Heskes,
2003), and power EP (Minka, 2004).
