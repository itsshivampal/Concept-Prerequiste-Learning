As we shall see in Section 5.3, it is possible to evaluate the gradient of an error
function efficiently by means of the backpropagation procedure. The use of this
gradient information can lead to significant improvements in the speed with which
the minima of the error function can be located. We can see why this is so, as follows.
In the quadratic approximation to the error function, given in (5.28), the error
surface is specified by the quantities b and H, which contain a total of W (W +
3)/2 independent elements (because the matrix H is symmetric), where W is the
dimensionality of w (i.e., the total number of adaptive parameters in the network).
The location of the minimum of this quadratic approximation therefore depends on
O(W 2) parameters, and we should not expect to be able to locate the minimum until
we have gathered O(W 2) independent pieces of information. If we do not make
use of gradient information, we would expect to have to perform O(W 2) function
Exercise 5.13
240
5. NEURAL NETWORKS
evaluations, each of which would require O(W ) steps. Thus, the computational
effort needed to find the minimum using such an approach would be O(W 3).
Now compare this with an algorithm that makes use of the gradient information.
Because each evaluation of âˆ‡E brings W items of information, we might hope to
find the minimum of the function in O(W ) gradient evaluations. As we shall see,
by using error backpropagation, each such evaluation takes only O(W ) steps and so
the minimum can now be found in O(W 2) steps. For this reason, the use of gradient
information forms the basis of practical algorithms for training neural networks.
N
