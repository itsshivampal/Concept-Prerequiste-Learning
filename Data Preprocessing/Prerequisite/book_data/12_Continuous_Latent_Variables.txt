In Chapter 9, we discussed probabilistic models having discrete latent variables, such
as the mixture of Gaussians. We now explore models in which some, or all, of the
latent variables are continuous. An important motivation for such models is that
many data sets have the property that the data points all lie close to a manifold of
much lower dimensionality than that of the original data space. To see why this
might arise, consider an artiﬁcial data set constructed by taking one of the off-line
digits, represented by a 64 × 64 pixel grey-level image, and embedding it in a larger
image of size 100×100 by padding with pixels having the value zero (corresponding
to white pixels) in which the location and orientation of the digit is varied at random,
as illustrated in Figure 12.1. Each of the resulting images is represented by a point in
the 100× 100 = 10, 000-dimensional data space. However, across a data set of such
images, there are only three degrees of freedom of variability, corresponding to the
vertical and horizontal translations and the rotations. The data points will therefore
live on a subspace of the data space whose intrinsic dimensionality is three. Note
559
560
12. CONTINUOUS LATENT VARIABLES
Figure 12.1 A synthetic data set obtained by taking one of the off-line digit images and creating multi-
ple copies in each of which the digit has undergone a random displacement and rotation
within some larger image ﬁeld. The resulting images each have 100 × 100 = 10, 000
pixels.
that the manifold will be nonlinear because, for instance, if we translate the digit
past a particular pixel, that pixel value will go from zero (white) to one (black) and
back to zero again, which is clearly a nonlinear function of the digit position. In
this example, the translation and rotation parameters are latent variables because we
observe only the image vectors and are not told which values of the translation or
rotation variables were used to create them.
For real digit image data, there will be a further degree of freedom arising from
scaling. Moreover there will be multiple additional degrees of freedom associated
with more complex deformations due to the variability in an individual’s writing
as well as the differences in writing styles between individuals. Nevertheless, the
number of such degrees of freedom will be small compared to the dimensionality of
the data set.
Another example is provided by the oil ﬂow data set, in which (for a given ge-
ometrical conﬁguration of the gas, water, and oil phases) there are only two degrees
of freedom of variability corresponding to the fraction of oil in the pipe and the frac-
tion of water (the fraction of gas then being determined). Although the data space
comprises 12 measurements, a data set of points will lie close to a two-dimensional
manifold embedded within this space. In this case, the manifold comprises several
distinct segments corresponding to different ﬂow regimes, each such segment being
a (noisy) continuous two-dimensional manifold. If our goal is data compression, or
density modelling, then there can be beneﬁts in exploiting this manifold structure.
In practice, the data points will not be conﬁned precisely to a smooth low-
dimensional manifold, and we can interpret the departures of data points from the
manifold as ‘noise’. This leads naturally to a generative view of such models in
which we ﬁrst select a point within the manifold according to some latent variable
distribution and then generate an observed data point by adding noise, drawn from
some conditional distribution of the data variables given the latent variables.
The simplest continuous latent variable model assumes Gaussian distributions
for both the latent and observed variables and makes use of a linear-Gaussian de-
pendence of the observed variables on the state of the latent variables. This leads
to a probabilistic formulation of the well-known technique of principal component
analysis (PCA), as well as to a related model called factor analysis.
In this chapter w will begin with a standard, nonprobabilistic treatment of PCA,
and then we show how PCA arises naturally as the maximum likelihood solution to
Appendix A
Section 8.1.4
Section 12.1
