In some applications of principal component analysis, the number of data points
is smaller than the dimensionality of the data space. For example, we might want to
apply PCA to a data set of a few hundred images, each of which corresponds to a
vector in a space of potentially several million dimensions (corresponding to three
colour values for each of the pixels in the image). Note that in a D-dimensional space
a set of N points, where N < D, deﬁnes a linear subspace whose dimensionality
is at most N − 1, and so there is little point in applying PCA for values of M
that are greater than N − 1. Indeed, if we perform PCA we will ﬁnd that at least
D − N + 1 of the eigenvalues are zero, corresponding to eigenvectors along whose
directions the data set has zero variance. Furthermore, typical algorithms for ﬁnding
the eigenvectors of a D×D matrix have a computational cost that scales like O(D3),
and so for applications such as the image example, a direct application of PCA will
be computationally infeasible.
We can resolve this problem as follows. First, let us deﬁne X to be the (N ×D)-
570
12. CONTINUOUS LATENT VARIABLES
dimensional centred data matrix, whose nth row is given by (xn − x)T. The covari-
−1XTX, and the corresponding
ance matrix (12.3) can then be written as S = N
eigenvector equation becomes
1
N
XTXui = λiui.
Now pre-multiply both sides by X to give
1
N
XXT(Xui) = λi(Xui).
If we now deﬁne vi = Xui, we obtain
(12.26)
(12.27)
XXTvi = λivi
(12.28)
which is an eigenvector equation for the N × N matrix N
−1XXT. We see that this
has the same N −1 eigenvalues as the original covariance matrix (which itself has an
additional D − N + 1 eigenvalues of value zero). Thus we can solve the eigenvector
problem in spaces of lower dimensionality with computational cost O(N 3) instead
of O(D3). In order to determine the eigenvectors, we multiply both sides of (12.28)
by XT to give
(cid:16)
XTX
(XTvi) = λi(XTvi)
(12.29)
1
N
(cid:15)
1
N
from which we see that (XTvi) is an eigenvector of S with eigenvalue λi. Note,
however, that these eigenvectors need not be normalized. To determine the appropri-
ate normalization, we re-scale ui ∝ XTvi by a constant such that (cid:5)ui(cid:5) = 1, which,
assuming vi has been normalized to unit length, gives
ui =
1
(N λi)1/2 XTvi.
(12.30)
In summary, to apply this approach we ﬁrst evaluate XXT and then ﬁnd its eigen-
vectors and eigenvalues and then compute the eigenvectors in the original data space
using (12.30).
