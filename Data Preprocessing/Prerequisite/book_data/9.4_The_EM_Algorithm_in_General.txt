Exercise 9.23
Section 10.1
9.4. The EM Algorithm in General
451
Figure 9.11 Illustration of the decomposition given
by (9.70), which holds for any choice
of distribution q(Z).
Because the
Kullback-Leibler divergence satisfies
KL(qp) � 0, we see that the quan-
tity L(q, θ) is a lower bound on the log
likelihood function ln p(X|θ).
KL(q||p)
L(q, θ)
ln p(X|θ)
Exercise 9.24
Section 1.6.1
carefully the forms of the expressions (9.71) and (9.72), and in particular noting that
they differ in sign and also that L(q, θ) contains the joint distribution of X and Z
while KL(qp) contains the conditional distribution of Z given X. To verify the
decomposition (9.70), we first make use of the product rule of probability to give
ln p(X, Z|θ) = ln p(Z|X, θ) + ln p(X|θ)
(9.73)
which we then substitute into the expression for L(q, θ). This gives rise to two terms,
one of which cancels KL(qp) while the other gives the required log likelihood
ln p(X|θ) after noting that q(Z) is a normalized distribution that sums to 1.
From (9.72), we see that KL(qp) is the Kullback-Leibler divergence between
q(Z) and the posterior distribution p(Z|X, θ). Recall that the Kullback-Leibler di-
vergence satisfies KL(qp) � 0, with equality if, and only if, q(Z) = p(Z|X, θ). It
therefore follows from (9.70) that L(q, θ) � ln p(X|θ), in other words that L(q, θ)
is a lower bound on ln p(X|θ). The decomposition (9.70) is illustrated in Fig-
ure 9.11.
The EM algorithm is a two-stage iterative optimization technique for finding
maximum likelihood solutions. We can use the decomposition (9.70) to define the
EM algorithm and to demonstrate that it does indeed maximize the log likelihood.
Suppose that the current value of the parameter vector is θold. In the E step, the
lower bound L(q, θold) is maximized with respect to q(Z) while holding θold fixed.
The solution to this maximization problem is easily seen by noting that the value
of ln p(X|θold) does not depend on q(Z) and so the largest value of L(q, θold) will
occur when the Kullback-Leibler divergence vanishes, in other words when q(Z) is
equal to the posterior distribution p(Z|X, θold). In this case, the lower bound will
equal the log likelihood, as illustrated in Figure 9.12.
In the subsequent M step, the distribution q(Z) is held fixed and the lower bound
L(q, θ) is maximized with respect to θ to give some new value θnew. This will
cause the lower bound L to increase (unless it is already at a maximum), which will
necessarily cause the corresponding log likelihood function to increase. Because the
distribution q is determined using the old parameter values rather than the new values
and is held fixed during the M step, it will not equal the new posterior distribution
p(Z|X, θnew), and hence there will be a nonzero KL divergence. The increase in the
log likelihood function is therefore greater than the increase in the lower bound, as
452
9. MIXTURE MODELS AND EM
KL(q||p) = 0
Figure 9.12 Illustration of the E step of
the EM algorithm. The q
distribution is set equal to
the posterior distribution for
the current parameter val-
ues θold, causing the lower
bound to move up to the
same value as the log like-
lihood function, with the KL
divergence vanishing.
L(q, θold)
ln p(X|θold)
shown in Figure 9.13. If we substitute q(Z) = p(Z|X, θold) into (9.71), we see that,
after the E step, the lower bound takes the form
L(q, θ) =
Z
p(Z|X, θold) ln p(X, Z|θ) −
= Q(θ, θold) + const
Z
p(Z|X, θold) ln p(Z|X, θold)
(9.74)
where the constant is simply the negative entropy of the q distribution and is there-
fore independent of θ. Thus in the M step, the quantity that is being maximized is the
expectation of the complete-data log likelihood, as we saw earlier in the case of mix-
tures of Gaussians. Note that the variable θ over which we are optimizing appears
only inside the logarithm. If the joint distribution p(Z, X|θ) comprises a member of
the exponential family, or a product of such members, then we see that the logarithm
will cancel the exponential and lead to an M step that will be typically much simpler
than the maximization of the corresponding incomplete-data log likelihood function
p(X|θ).
The operation of the EM algorithm can also be viewed in the space of parame-
ters, as illustrated schematically in Figure 9.14. Here the red curve depicts the (in-
Figure 9.13 Illustration of the M step of the EM
algorithm.
The distribution q(Z)
is held fixed and the lower bound
L(q, θ) is maximized with respect
to the parameter vector θ to give
a revised value θnew. Because the
KL divergence is nonnegative, this
causes the log likelihood ln p(X|θ)
to increase by at least as much as
the lower bound does.
KL(q||p)
L(q, θnew)
ln p(X|θnew)
Z
Figure 9.14 The EM algorithm involves alter-
nately computing a lower bound
on the log likelihood for the cur-
rent parameter values and then
maximizing this bound to obtain
the new parameter values. See
the text for a full discussion.
9.4. The EM Algorithm in General
453
ln p(X|θ)
L (q, θ)
θold θnew
Exercise 9.25
complete data) log likelihood function whose value we wish to maximize. We start
with some initial parameter value θold, and in the first E step we evaluate the poste-
rior distribution over latent variables, which gives rise to a lower bound L(θ, θ(old))
whose value equals the log likelihood at θ(old), as shown by the blue curve. Note that
the bound makes a tangential contact with the log likelihood at θ(old), so that both
curves have the same gradient. This bound is a convex function having a unique
maximum (for mixture components from the exponential family). In the M step, the
bound is maximized giving the value θ(new), which gives a larger value of log likeli-
hood than θ(old). The subsequent E step then constructs a bound that is tangential at
θ(new) as shown by the green curve.
For the particular case of an independent, identically distributed data set, X
will comprise N data points {xn} while Z will comprise N corresponding latent
variables {zn}, where n = 1, . . . , N. From the independence assumption, we have
n p(xn, zn) and, by marginalizing over the {zn} we have p(X) =
p(X, Z) =
n p(xn). Using the sum and product rules, we see that the posterior probability
that is evaluated in the E step takes the form
p(Z|X, θ) = p(X, Z|θ)
p(X, Z|θ)
n=1
p(zn|xn, θ)
(9.75)
p(xn, zn|θ)
p(xn, zn|θ)
Z
n=1
In the case of
and so the posterior distribution also factorizes with respect to n.
the Gaussian mixture model this simply says that the responsibility that each of the
mixture components takes for a particular data point xn depends only on the value
of xn and on the parameters θ of the mixture components, not on the values of the
other data points.
We have seen that both the E and the M steps of the EM algorithm are increas-
ing the value of a well-defined bound on the log likelihood function and that the
n=1
N
N
N
454
9. MIXTURE MODELS AND EM
complete EM cycle will change the model parameters in such a way as to cause
the log likelihood to increase (unless it is already at a maximum, in which case the
parameters remain unchanged).
We can also use the EM algorithm to maximize the posterior distribution p(θ|X)
for models in which we have introduced a prior p(θ) over the parameters. To see this,
we note that as a function of θ, we have p(θ|X) = p(θ, X)/p(X) and so
ln p(θ|X) = ln p(θ, X) − ln p(X).
Making use of the decomposition (9.70), we have
ln p(θ|X) = L(q, θ) + KL(qp) + ln p(θ) − ln p(X)
� L(q, θ) + ln p(θ) − ln p(X).
(9.76)
(9.77)
where ln p(X) is a constant. We can again optimize the right-hand side alternately
with respect to q and θ. The optimization with respect to q gives rise to the same E-
step equations as for the standard EM algorithm, because q only appears in L(q, θ).
The M-step equations are modified through the introduction of the prior term ln p(θ),
which typically requires only a small modification to the standard maximum likeli-
hood M-step equations.
The EM algorithm breaks down the potentially difficult problem of maximizing
the likelihood function into two stages, the E step and the M step, each of which will
often prove simpler to implement. Nevertheless, for complex models it may be the
case that either the E step or the M step, or indeed both, remain intractable. This
leads to two possible extensions of the EM algorithm, as follows.
The generalized EM, or GEM, algorithm addresses the problem of an intractable
M step. Instead of aiming to maximize L(q, θ) with respect to θ, it seeks instead
to change the parameters in such a way as to increase its value. Again, because
L(q, θ) is a lower bound on the log likelihood function, each complete EM cycle of
the GEM algorithm is guaranteed to increase the value of the log likelihood (unless
the parameters already correspond to a local maximum). One way to exploit the
GEM approach would be to use one of the nonlinear optimization strategies, such
as the conjugate gradients algorithm, during the M step. Another form of GEM
algorithm, known as the expectation conditional maximization, or ECM, algorithm,
involves making several constrained optimizations within each M step (Meng and
Rubin, 1993). For instance, the parameters might be partitioned into groups, and the
M step is broken down into multiple steps each of which involves optimizing one of
the subset with the remainder held fixed.
We can similarly generalize the E step of the EM algorithm by performing a
partial, rather than complete, optimization of L(q, θ) with respect to q(Z) (Neal and
Hinton, 1999). As we have seen, for any given value of θ there is a unique maximum
of L(q, θ) with respect to q(Z) that corresponds to the posterior distribution qθ(Z) =
p(Z|X, θ) and that for this choice of q(Z) the bound L(q, θ) is equal to the log
likelihood function ln p(X|θ). It follows that any algorithm that converges to the
global maximum of L(q, θ) will find a value of θ that is also a global maximum
of the log likelihood ln p(X|θ). Provided p(X, Z|θ) is a continuous function of θ
Exercises
455
then, by continuity, any local maximum of L(q, θ) will also be a local maximum of
ln p(X|θ).
Consider the case of N independent data points x1, . . . , xN with corresponding
latent variables z1, . . . , zN . The joint distribution p(X, Z|θ) factorizes over the data
points, and this structure can be exploited in an incremental form of EM in which
at each EM cycle only one data point is processed at a time. In the E step, instead
of recomputing the responsibilities for all of the data points, we just re-evaluate the
responsibilities for one data point. It might appear that the subsequent M step would
require computation involving the responsibilities for all of the data points. How-
ever, if the mixture components are members of the exponential family, then the
responsibilities enter only through simple sufficient statistics, and these can be up-
dated efficiently. Consider, for instance, the case of a Gaussian mixture, and suppose
we perform an update for data point m in which the corresponding old and new
values of the responsibilities are denoted γold(zmk) and γnew(zmk). In the M step,
the required sufficient statistics can be updated incrementally. For instance, for the
means the sufficient statistics are defined by (9.17) and (9.18) from which we obtain
(9.78)
(9.79)
Exercise 9.26
Exercises
µnew
k = µold
k +
γnew(zmk) − γold(zmk)
N new
k
xm − µold
k
together with
N new
k = N old
k + γnew(zmk) − γold(zmk).
The corresponding results for the covariances and the mixing coefficients are analo-
gous.
Thus both the E step and the M step take fixed time that is independent of the
total number of data points. Because the parameters are revised after each data point,
rather than waiting until after the whole data set is processed, this incremental ver-
sion can converge faster than the batch version. Each E or M step in this incremental
algorithm is increasing the value of L(q, θ) and, as we have shown above, if the
algorithm converges to a local (or global) maximum of L(q, θ), this will correspond
to a local (or global) maximum of the log likelihood function ln p(X|θ).
