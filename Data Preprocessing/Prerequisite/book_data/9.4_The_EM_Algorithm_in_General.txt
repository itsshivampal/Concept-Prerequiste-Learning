Exercise 9.23
Section 10.1
9.4. The EM Algorithm in General
451
Figure 9.11 Illustration of the decomposition given
by (9.70), which holds for any choice
of distribution q(Z).
Because the
Kullback-Leibler divergence satisfies
KL(qp) � 0, we see that the quan-
tity L(q, θ) is a lower bound on the log
likelihood function ln p(X|θ).
KL(q||p)
L(q, θ)
ln p(X|θ)
Exercise 9.24
Section 1.6.1
carefully the forms of the expressions (9.71) and (9.72), and in particular noting that
they differ in sign and also that L(q, θ) contains the joint distribution of X and Z
while KL(qp) contains the conditional distribution of Z given X. To verify the
decomposition (9.70), we first make use of the product rule of probability to give
ln p(X, Z|θ) = ln p(Z|X, θ) + ln p(X|θ)
(9.73)
which we then substitute into the expression for L(q, θ). This gives rise to two terms,
one of which cancels KL(qp) while the other gives the required log likelihood
ln p(X|θ) after noting that q(Z) is a normalized distribution that sums to 1.
From (9.72), we see that KL(qp) is the Kullback-Leibler divergence between
q(Z) and the posterior distribution p(Z|X, θ). Recall that the Kullback-Leibler di-
vergence satisfies KL(qp) � 0, with equality if, and only if, q(Z) = p(Z|X, θ). It
therefore follows from (9.70) that L(q, θ) � ln p(X|θ), in other words that L(q, θ)
is a lower bound on ln p(X|θ). The decomposition (9.70) is illustrated in Fig-
ure 9.11.
The EM algorithm is a two-stage iterative optimization technique for finding
maximum likelihood solutions. We can use the decomposition (9.70) to define the
EM algorithm and to demonstrate that it does indeed maximize the log likelihood.
Suppose that the current value of the parameter vector is θold. In the E step, the
lower bound L(q, θold) is maximized with respect to q(Z) while holding θold fixed.
The solution to this maximization problem is easily seen by noting that the value
of ln p(X|θold) does not depend on q(Z) and so the largest value of L(q, θold) will
occur when the Kullback-Leibler divergence vanishes, in other words when q(Z) is
equal to the posterior distribution p(Z|X, θold). In this case, the lower bound will
equal the log likelihood, as illustrated in Figure 9.12.
In the subsequent M step, the distribution q(Z) is held fixed and the lower bound
L(q, θ) is maximized with respect to θ to give some new value θnew. This will
cause the lower bound L to increase (unless it is already at a maximum), which will
necessarily cause the corresponding log likelihood function to increase. Because the
distribution q is determined using the old parameter values rather than the new values
and is held fixed during the M step, it will not equal the new posterior distribution
p(Z|X, θnew), and hence there will be a nonzero KL divergence. The increase in the
log likelihood function is therefore greater than the increase in the lower bound, as
452
9. MIXTURE MODELS AND EM
KL(q||p) = 0
Figure 9.12 Illustration of the E step of
the EM algorithm. The q
distribution is set equal to
the posterior distribution for
the current parameter val-
ues θold, causing the lower
bound to move up to the
same value as the log like-
lihood function, with the KL
divergence vanishing.
L(q, θold)
ln p(X|θold)
shown in Figure 9.13. If we substitute q(Z) = p(Z|X, θold) into (9.71), we see that,
after the E step, the lower bound takes the form
L(q, θ) =
Z
p(Z|X, θold) ln p(X, Z|θ) −
= Q(θ, θold) + const
Z
p(Z|X, θold) ln p(Z|X, θold)
(9.74)
where the constant is simply the negative entropy of the q distribution and is there-
fore independent of θ. Thus in the M step, the quantity that is being maximized is the
expectation of the complete-data log likelihood, as we saw earlier in the case of mix-
tures of Gaussians. Note that the variable θ over which we are optimizing appears
only inside the logarithm. If the joint distribution p(Z, X|θ) comprises a member of
the exponential family, or a product of such members, then we see that the logarithm
will cancel the exponential and lead to an M step that will be typically much simpler
than the maximization of the corresponding incomplete-data log likelihood function
p(X|θ).
The operation of the EM algorithm can also be viewed in the space of parame-
ters, as illustrated schematically in Figure 9.14. Here the red curve depicts the (in-
Figure 9.13 Illustration of the M step of the EM
algorithm.
The distribution q(Z)
is held fixed and the lower bound
L(q, θ) is maximized with respect
to the parameter vector θ to give
a revised value θnew. Because the
KL divergence is nonnegative, this
causes the log likelihood ln p(X|θ)
to increase by at least as much as
the lower bound does.
KL(q||p)
L(q, θnew)
ln p(X|θnew)
Z
Figure 9.14 The EM algorithm involves alter-
nately computing a lower bound
on the log likelihood for the cur-
rent parameter values and then
maximizing this bound to obtain
the new parameter values. See
the text for a full discussion.
9.4. The EM Algorithm in General
453
ln p(X|θ)
L (q, θ)
θold θnew
Exercise 9.25
complete data) log likelihood function whose value we wish to maximize. We start
with some initial parameter value θold, and in the first E step we evaluate the poste-
rior distribution over latent variables, which gives rise to a lower bound L(θ, θ(old))
whose value equals the log likelihood at θ(old), as shown by the blue curve. Note that
the bound makes a tangential contact with the log likelihood at θ(old), so that both
curves have the same gradient. This bound is a convex function having a unique
maximum (for mixture components from the exponential family). In the M step, the
bound is maximized giving the value θ(new), which gives a larger value of log likeli-
hood than θ(old). The subsequent E step then constructs a bound that is tangential at
θ(new) as shown by the green curve.
For the particular case of an independent, identically distributed data set, X
will comprise N data points {xn} while Z will comprise N corresponding latent
variables {zn}, where n = 1, . . . , N. From the independence assumption, we have
n p(xn, zn) and, by marginalizing over the {zn} we have p(X) =
p(X, Z) =
n p(xn). Using the sum and product rules, we see that the posterior probability
that is evaluated in the E step takes the form
p(Z|X, θ) = p(X, Z|θ)
p(X, Z|θ)
n=1
p(zn|xn, θ)
(9.75)
p(xn, zn|θ)
p(xn, zn|θ)
Z
n=1
In the case of
and so the posterior distribution also factorizes with respect to n.
the Gaussian mixture model this simply says that the responsibility that each of the
mixture components takes for a particular data point xn depends only on the value
of xn and on the parameters θ of the mixture components, not on the values of the
other data points.
We have seen that both the E and the M steps of the EM algorithm are increas-
ing the value of a well-defined bound on the log likelihood function and that the
n=1
N
N
N
454
9. MIXTURE MODELS AND EM
complete EM cycle will change the model parameters in such a way as to cause
the log likelihood to increase (unless it is already at a maximum, in which case the
parameters remain unchanged).
We can also use the EM algorithm to maximize the posterior distribution p(θ|X)
for models in which we have introduced a prior p(θ) over the parameters. To see this,
we note that as a function of θ, we have p(θ|X) = p(θ, X)/p(X) and so
ln p(θ|X) = ln p(θ, X) − ln p(X).
Making use of the decomposition (9.70), we have
ln p(θ|X) = L(q, θ) + KL(qp) + ln p(θ) − ln p(X)
� L(q, θ) + ln p(θ) − ln p(X).
(9.76)
(9.77)
where ln p(X) is a constant. We can again optimize the right-hand side alternately
with respect to q and θ. The optimization with respect to q gives rise to the same E-
step equations as for the standard EM algorithm, because q only appears in L(q, θ).
The M-step equations are modified through the introduction of the prior term ln p(θ),
which typically requires only a small modification to the standard maximum likeli-
hood M-step equations.
The EM algorithm breaks down the potentially difficult problem of maximizing
the likelihood function into two stages, the E step and the M step, each of which will
often prove simpler to implement. Nevertheless, for complex models it may be the
case that either the E step or the M step, or indeed both, remain intractable. This
leads to two possible extensions of the EM algorithm, as follows.
The generalized EM, or GEM, algorithm addresses the problem of an intractable
M step. Instead of aiming to maximize L(q, θ) with respect to θ, it seeks instead
to change the parameters in such a way as to increase its value. Again, because
L(q, θ) is a lower bound on the log likelihood function, each complete EM cycle of
the GEM algorithm is guaranteed to increase the value of the log likelihood (unless
the parameters already correspond to a local maximum). One way to exploit the
GEM approach would be to use one of the nonlinear optimization strategies, such
as the conjugate gradients algorithm, during the M step. Another form of GEM
algorithm, known as the expectation conditional maximization, or ECM, algorithm,
involves making several constrained optimizations within each M step (Meng and
Rubin, 1993). For instance, the parameters might be partitioned into groups, and the
M step is broken down into multiple steps each of which involves optimizing one of
the subset with the remainder held fixed.
We can similarly generalize the E step of the EM algorithm by performing a
partial, rather than complete, optimization of L(q, θ) with respect to q(Z) (Neal and
Hinton, 1999). As we have seen, for any given value of θ there is a unique maximum
of L(q, θ) with respect to q(Z) that corresponds to the posterior distribution qθ(Z) =
p(Z|X, θ) and that for this choice of q(Z) the bound L(q, θ) is equal to the log
likelihood function ln p(X|θ). It follows that any algorithm that converges to the
global maximum of L(q, θ) will find a value of θ that is also a global maximum
of the log likelihood ln p(X|θ). Provided p(X, Z|θ) is a continuous function of θ
Exercises
455
then, by continuity, any local maximum of L(q, θ) will also be a local maximum of
ln p(X|θ).
Consider the case of N independent data points x1, . . . , xN with corresponding
latent variables z1, . . . , zN . The joint distribution p(X, Z|θ) factorizes over the data
points, and this structure can be exploited in an incremental form of EM in which
at each EM cycle only one data point is processed at a time. In the E step, instead
of recomputing the responsibilities for all of the data points, we just re-evaluate the
responsibilities for one data point. It might appear that the subsequent M step would
require computation involving the responsibilities for all of the data points. How-
ever, if the mixture components are members of the exponential family, then the
responsibilities enter only through simple sufficient statistics, and these can be up-
dated efficiently. Consider, for instance, the case of a Gaussian mixture, and suppose
we perform an update for data point m in which the corresponding old and new
values of the responsibilities are denoted γold(zmk) and γnew(zmk). In the M step,
the required sufficient statistics can be updated incrementally. For instance, for the
means the sufficient statistics are defined by (9.17) and (9.18) from which we obtain
(9.78)
(9.79)
Exercise 9.26
Exercises
µnew
k = µold
k +
γnew(zmk) − γold(zmk)
N new
k
xm − µold
k
together with
N new
k = N old
k + γnew(zmk) − γold(zmk).
The corresponding results for the covariances and the mixing coefficients are analo-
gous.
Thus both the E step and the M step take fixed time that is independent of the
total number of data points. Because the parameters are revised after each data point,
rather than waiting until after the whole data set is processed, this incremental ver-
sion can converge faster than the batch version. Each E or M step in this incremental
algorithm is increasing the value of L(q, θ) and, as we have shown above, if the
algorithm converges to a local (or global) maximum of L(q, θ), this will correspond
to a local (or global) maximum of the log likelihood function ln p(X|θ).
9.1 () www Consider the K-means algorithm discussed in Section 9.1. Show that as
a consequence of there being a finite number of possible assignments for the set of
discrete indicator variables rnk, and that for each such assignment there is a unique
optimum for the {µk}, the K-means algorithm must converge after a finite number
of iterations.
9.2 () Apply the Robbins-Monro sequential estimation procedure described in Sec-
tion 2.3.5 to the problem of finding the roots of the regression function given by
the derivatives of J in (9.1) with respect to µk. Show that this leads to a stochastic
K-means algorithm in which, for each data point xn, the nearest prototype µk is
updated using (9.5).
N
K
456
9. MIXTURE MODELS AND EM
9.3 () www Consider a Gaussian mixture model in which the marginal distribution
p(z) for the latent variable is given by (9.10), and the conditional distribution p(x|z)
for the observed variable is given by (9.11). Show that the marginal distribution
p(x), obtained by summing p(z)p(x|z) over all possible values of z, is a Gaussian
mixture of the form (9.7).
9.4 () Suppose we wish to use the EM algorithm to maximize the posterior distri-
bution over parameters p(θ|X) for a model containing latent variables, where X is
the observed data set. Show that the E step remains the same as in the maximum
likelihood case, whereas in the M step the quantity to be maximized is given by
Q(θ, θold) + ln p(θ) where Q(θ, θold) is defined by (9.30).
9.5 () Consider the directed graph for a Gaussian mixture model shown in Figure 9.6.
By making use of the d-separation criterion discussed in Section 8.2, show that the
posterior distribution of the latent variables factorizes with respect to the different
data points so that
p(Z|X, µ, Σ, π) =
p(zn|xn, µ, Σ, π).
(9.80)
n=1
9.6 ( ) Consider a special case of a Gaussian mixture model in which the covari-
ance matrices Σk of the components are all constrained to have a common value
Σ. Derive the EM equations for maximizing the likelihood function under such a
model.
9.7 () www Verify that maximization of the complete-data log likelihood (9.36) for
a Gaussian mixture model leads to the result that the means and covariances of each
component are fitted independently to the corresponding group of data points, and
the mixing coefficients are given by the fractions of points in each group.
9.8 () www Show that if we maximize (9.40) with respect to µk while keeping the
responsibilities γ(znk) fixed, we obtain the closed form solution given by (9.17).
9.9 () Show that if we maximize (9.40) with respect to Σk and πk while keeping the
responsibilities γ(znk) fixed, we obtain the closed form solutions given by (9.19)
and (9.22).
9.10 ( ) Consider a density model given by a mixture distribution
p(x) =
πkp(x|k)
k=1
(9.81)
and suppose that we partition the vector x into two parts so that x = (xa, xb).
Show that the conditional density p(xb|xa) is itself a mixture distribution and find
expressions for the mixing coefficients and for the component densities.
k=1
K
Exercises
457
9.11 ()
In Section 9.3.2, we obtained a relationship between K means and EM for
Gaussian mixtures by considering a mixture model in which all components have
covariance 	I. Show that in the limit 	 → 0, maximizing the expected complete-
data log likelihood for this model, given by (9.40), is equivalent to minimizing the
distortion measure J for the K-means algorithm given by (9.1).
9.12 () www Consider a mixture distribution of the form
p(x) =
πkp(x|k)
(9.82)
where the elements of x could be discrete or continuous or a combination of these.
Denote the mean and covariance of p(x|k) by µk and Σk, respectively. Show that
the mean and covariance of the mixture distribution are given by (9.49) and (9.50).
9.13 ( ) Using the re-estimation equations for the EM algorithm, show that a mix-
ture of Bernoulli distributions, with its parameters set to values corresponding to a
maximum of the likelihood function, has the property that
E[x] =
N
1
N
xn ≡ x.
n=1
(9.83)
Hence show that if the parameters of this model are initialized such that all compo-
nents have the same mean µk =
µ for k = 1, . . . , K, then the EM algorithm will
converge after one iteration, for any choice of the initial mixing coefficients, and that
this solution has the property µk = x. Note that this represents a degenerate case of
the mixture model in which all of the components are identical, and in practice we
try to avoid such solutions by using an appropriate initialization.
9.14 () Consider the joint distribution of latent and observed variables for the Bernoulli
distribution obtained by forming the product of p(x|z, µ) given by (9.52) and p(z|π)
given by (9.53). Show that if we marginalize this joint distribution with respect to z,
then we obtain (9.47).
9.15 () www Show that if we maximize the expected complete-data log likelihood
function (9.55) for a mixture of Bernoulli distributions with respect to µk, we obtain
the M step equation (9.59).
9.16 () Show that if we maximize the expected complete-data log likelihood function
(9.55) for a mixture of Bernoulli distributions with respect to the mixing coefficients
πk, using a Lagrange multiplier to enforce the summation constraint, we obtain the
M step equation (9.60).
9.17 () www Show that as a consequence of the constraint 0 � p(xn|µk) � 1 for
the discrete variable xn, the incomplete-data log likelihood function for a mixture
of Bernoulli distributions is bounded above, and hence that there are no singularities
for which the likelihood goes to infinity.
k=1
i=1
j=1
K
p(x) =
where
πkp(x|µk)
p(x|µk) =
D
M
µxij
kij.
(9.84)
(9.85)
458
9. MIXTURE MODELS AND EM
9.18 ( ) Consider a Bernoulli mixture model as discussed in Section 9.3.3, together
with a prior distribution p(µk|ak, bk) over each of the parameter vectors µk given
by the beta distribution (2.13), and a Dirichlet prior p(π|α) given by (2.38). Derive
the EM algorithm for maximizing the posterior probability p(µ, π|X).
9.19 ( ) Consider a D-dimensional variable x each of whose components i is itself a
multinomial variable of degree M so that x is a binary vector with components xij
where i = 1, . . . , D and j = 1, . . . , M, subject to the constraint that
j xij = 1 for
all i. Suppose that the distribution of these variables is described by a mixture of the
discrete multinomial distributions considered in Section 2.2 so that
The parameters µkij represent the probabilities p(xij = 1|µk) and must satisfy
0 � µkij � 1 together with the constraint
j µkij = 1 for all values of k and i.
Given an observed data set {xn}, where n = 1, . . . , N, derive the E and M step
equations of the EM algorithm for optimizing the mixing coefficients πk and the
component parameters µkij of this distribution by maximum likelihood.
9.20 () www Show that maximization of the expected complete-data log likelihood
function (9.62) for the Bayesian linear regression model leads to the M step re-
estimation result (9.63) for α.
9.21 ( ) Using the evidence framework of Section 3.5, derive the M-step re-estimation
equations for the parameter β in the Bayesian linear regression model, analogous to
the result (9.63) for α.
9.22 ( ) By maximization of the expected complete-data log likelihood defined by
(9.66), derive the M step equations (9.67) and (9.68) for re-estimating the hyperpa-
rameters of the relevance vector machine for regression.
9.23 ( ) www In Section 7.2.1 we used direct maximization of the marginal like-
lihood to derive the re-estimation equations (7.87) and (7.88) for finding values of
the hyperparameters α and β for the regression RVM. Similarly, in Section 9.3.4
we used the EM algorithm to maximize the same marginal likelihood, giving the
re-estimation equations (9.67) and (9.68). Show that these two sets of re-estimation
equations are formally equivalent.
9.24 () Verify the relation (9.70) in which L(q, θ) and KL(qp) are defined by (9.71)
and (9.72), respectively.
Exercises
459
9.25 () www Show that the lower bound L(q, θ) given by (9.71), with q(Z) =
p(Z|X, θ(old)), has the same gradient with respect to θ as the log likelihood function
ln p(X|θ) at the point θ = θ(old).
9.26 () www Consider the incremental form of the EM algorithm for a mixture of
Gaussians, in which the responsibilities are recomputed only for a specific data point
xm. Starting from the M-step formulae (9.17) and (9.18), derive the results (9.78)
and (9.79) for updating the component means.
9.27 ( ) Derive M-step formulae for updating the covariance matrices and mixing
coefficients in a Gaussian mixture model when the responsibilities are updated in-
crementally, analogous to the result (9.78) for updating the means.
