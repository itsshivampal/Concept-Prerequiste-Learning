493
10.5. Local Variational Methods
Section 1.6.1
The variational framework discussed in Sections 10.1 and 10.2 can be considered a
‘global’ method in the sense that it directly seeks an approximation to the full poste-
rior distribution over all random variables. An alternative ‘local’ approach involves
finding bounds on functions over individual variables or groups of variables within
a model. For instance, we might seek a bound on a conditional distribution p(y|x),
which is itself just one factor in a much larger probabilistic model specified by a
directed graph. The purpose of introducing the bound of course is to simplify the
resulting distribution. This local approximation can be applied to multiple variables
in turn until a tractable approximation is obtained, and in Section 10.6.1 we shall
give a practical example of this approach in the context of logistic regression. Here
we focus on developing the bounds themselves.
We have already seen in our discussion of the Kullback-Leibler divergence that
the convexity of the logarithm function played a key role in developing the lower
bound in the global variational approach. We have defined a (strictly) convex func-
tion as one for which every chord lies above the function. Convexity also plays a
central role in the local variational framework. Note that our discussion will ap-
ply equally to concave functions with ‘min’ and ‘max’ interchanged and with lower
bounds replaced by upper bounds.
Let us begin by considering a simple example, namely the function f(x) =
exp(−x), which is a convex function of x, and which is shown in the left-hand plot
of Figure 10.10. Our goal is to approximate f(x) by a simpler function, in particular
a linear function of x. From Figure 10.10, we see that this linear function will be a
lower bound on f(x) if it corresponds to a tangent. We can obtain the tangent line
y(x) at a specific value of x, say x = ξ, by making a first order Taylor expansion
so that y(x) � f(x) with equality when x = ξ. For our example function f(x) =
y(x) = f(ξ) + f(ξ)(x − ξ)
(10.125)
Figure 10.10 In the left-hand fig-
ure the red curve shows the function
exp(−x), and the blue line shows
the tangent at x = ξ defined by
(10.125) with ξ = 1. This line has
slope λ = f(ξ) = − exp(−ξ). Note
that any other tangent line, for ex-
ample the ones shown in green, will
have a smaller value of y at x =
ξ. The right-hand figure shows the
corresponding plot of
the function
λξ − g(λ), where g(λ) is given by
(10.131), versus λ for ξ = 1,
in
which the maximum corresponds to
λ = − exp(−ξ) = −1/e.
1
0.5
λξ − g(λ)
0.4
0.2
0
0
1.5
x
3
0
−1
−0.5
0
494
10. APPROXIMATE INFERENCE
y
f(x)
λx
y
−g(λ)
f(x)
x
x
λx − g(λ)
Figure 10.11 In the left-hand plot the red curve shows a convex function f (x), and the blue line represents the
linear function λx, which is a lower bound on f (x) because f (x) > λx for all x. For the given value of slope λ the
contact point of the tangent line having the same slope is found by minimizing with respect to x the discrepancy
(shown by the green dashed lines) given by f (x) − λx. This defines the dual function g(λ), which corresponds
to the (negative of the) intercept of the tangent line having slope λ.
exp(−x), we therefore obtain the tangent line in the form
y(x) = exp(−ξ) − exp(−ξ)(x − ξ)
which is a linear function parameterized by ξ. For consistency with subsequent
discussion, let us define λ = − exp(−ξ) so that
(10.126)
(10.127)
y(x, λ) = λx − λ + λ ln(−λ).
Different values of λ correspond to different tangent lines, and because all such lines
are lower bounds on the function, we have f(x) � y(x, λ). Thus we can write the
function in the form
f(x) = max
λ {λx − λ + λ ln(−λ)} .
(10.128)
We have succeeded in approximating the convex function f(x) by a simpler, lin-
ear function y(x, λ). The price we have paid is that we have introduced a variational
parameter λ, and to obtain the tightest bound we must optimize with respect to λ.
We can formulate this approach more generally using the framework of convex
duality (Rockafellar, 1972; Jordan et al., 1999). Consider the illustration of a convex
function f(x) shown in the left-hand plot in Figure 10.11.
In this example, the
function λx is a lower bound on f(x) but it is not the best lower bound that can
be achieved by a linear function having slope λ, because the tightest bound is given
by the tangent line. Let us write the equation of the tangent line, having slope λ as
λx − g(λ) where the (negative) intercept g(λ) clearly depends on the slope λ of the
tangent. To determine the intercept, we note that the line must be moved vertically by
an amount equal to the smallest vertical distance between the line and the function,
as shown in Figure 10.11. Thus
g(λ) = − min
= max
x {f(x) − λx}
x {λx − f(x)} .
(10.129)
10.5. Local Variational Methods
495
Now, instead of fixing λ and varying x, we can consider a particular x and then
adjust λ until the tangent plane is tangent at that particular x. Because the y value
of the tangent line at a particular x is maximized when that value coincides with its
contact point, we have
f(x) = max
λ {λx − g(λ)} .
(10.130)
We see that the functions f(x) and g(λ) play a dual role, and are related through
(10.129) and (10.130).
Let us apply these duality relations to our simple example f(x) = exp(−x).
From (10.129) we see that the maximizing value of x is given by ξ = − ln(−λ), and
back-substituting we obtain the conjugate function g(λ) in the form
g(λ) = λ − λ ln(−λ)
(10.131)
as obtained previously. The function λξ − g(λ) is shown, for ξ = 1 in the right-hand
plot in Figure 10.10. As a check, we can substitute (10.131) into (10.130), which
gives the maximizing value of λ = − exp(−x), and back-substituting then recovers
the original function f(x) = exp(−x).
For concave functions, we can follow a similar argument to obtain upper bounds,
in which max’ is replaced with ‘min’, so that
f(x) = min
g(λ) = min
λ {λx − g(λ)}
x {λx − f(x)} .
(10.132)
(10.133)
If the function of interest is not convex (or concave), then we cannot directly
apply the method above to obtain a bound. However, we can first seek invertible
transformations either of the function or of its argument which change it into a con-
vex form. We then calculate the conjugate function and then transform back to the
original variables.
An important example, which arises frequently in pattern recognition, is the
logistic sigmoid function defined by
σ(x) =
1
1 + e−x .
(10.134)
Exercise 10.30
As it stands this function is neither convex nor concave. However, if we take the
logarithm we obtain a function which is concave, as is easily verified by finding the
second derivative. From (10.133) the corresponding conjugate function then takes
the form
g(λ) = min
x {λx − f(x)} = −λ ln λ − (1 − λ) ln(1 − λ)
(10.135)
Appendix B
which we recognize as the binary entropy function for a variable whose probability
of having the value 1 is λ. Using (10.132), we then obtain an upper bound on the log
sigmoid
ln σ(x) � λx − g(λ)
(10.136)
0
496
10. APPROXIMATE INFERENCE
1
0.5
0
−6
λ = 0.2
λ = 0.7
0
6
1
0.5
ξ = 2.5
0
−6
6
Figure 10.12 The left-hand plot shows the logistic sigmoid function σ(x) defined by (10.134) in red, together
with two examples of the exponential upper bound (10.137) shown in blue. The right-hand plot shows the logistic
sigmoid again in red together with the Gaussian lower bound (10.144) shown in blue. Here the parameter
ξ = 2.5, and the bound is exact at x = ξ and x = −ξ, denoted by the dashed green lines.
Exercise 10.31
and taking the exponential, we obtain an upper bound on the logistic sigmoid itself
of the form
σ(x) � exp(λx − g(λ))
(10.137)
which is plotted for two values of λ on the left-hand plot in Figure 10.12.
We can also obtain a lower bound on the sigmoid having the functional form of
a Gaussian. To do this, we follow Jaakkola and Jordan (2000) and make transforma-
tions both of the input variable and of the function itself. First we take the log of the
logistic function and then decompose it so that
ln σ(x) = − ln(1 + e−x) = − ln
e−x/2(ex/2 + e−x/2)
= x/2 − ln(ex/2 + e−x/2).
(10.138)
We now note that the function f(x) = − ln(ex/2 + e−x/2) is a convex function of
the variable x2, as can again be verified by finding the second derivative. This leads
to a lower bound on f(x), which is a linear function of x2 whose conjugate function
is given by
g(λ) = max
x2
λx2 − f
√x2
The stationarity condition leads to
0 = λ −
dx
dx2
d
dx
f(x) = λ +
tanh
1
4x
x
2
(10.139)
(10.140)
If we denote this value of x, corresponding to the contact point of the tangent line
for this particular value of λ, by ξ, then we have
1
4ξ
tanh
2
1
2ξ
1
2
(10.141)
(10.142)
(10.143)
(10.144)
Section 4.5
Section 4.3
10.5. Local Variational Methods
497
Instead of thinking of λ as the variational parameter, we can let ξ play this role as
this leads to simpler expressions for the conjugate function, which is then given by
g(λ) = λ(ξ)ξ2 − f(ξ) = λ(ξ)ξ2 + ln(eξ/2 + e−ξ/2).
Hence the bound on f(x) can be written as
f(x) � λx2 − g(λ) = λx2 − λξ2 − ln(eξ/2 + e−ξ/2).
The bound on the sigmoid then becomes
σ(x) � σ(ξ) exp
(x − ξ)/2 − λ(ξ)(x2 − ξ2)
where λ(ξ) is defined by (10.141). This bound is illustrated in the right-hand plot of
Figure 10.12. We see that the bound has the form of the exponential of a quadratic
function of x, which will prove useful when we seek Gaussian representations of
posterior distributions defined through logistic sigmoid functions.
The logistic sigmoid arises frequently in probabilistic models over binary vari-
ables because it is the function that transforms a log odds ratio into a posterior prob-
ability. The corresponding transformation for a multiclass distribution is given by
the softmax function. Unfortunately, the lower bound derived here for the logistic
sigmoid does not directly extend to the softmax. Gibbs (1997) proposes a method
for constructing a Gaussian distribution that is conjectured to be a bound (although
no rigorous proof is given), which may be used to apply local variational methods to
multiclass problems.
We shall see an example of the use of local variational bounds in Sections 10.6.1.
For the moment, however, it is instructive to consider in general terms how these
bounds can be used. Suppose we wish to evaluate an integral of the form
I =
σ(a)p(a) da
(10.145)
where σ(a) is the logistic sigmoid, and p(a) is a Gaussian probability density. Such
integrals arise in Bayesian models when, for instance, we wish to evaluate the pre-
dictive distribution, in which case p(a) represents a posterior parameter distribution.
Because the integral is intractable, we employ the variational bound (10.144), which
we write in the form σ(a) � f(a, ξ) where ξ is a variational parameter. The inte-
gral now becomes the product of two exponential-quadratic functions and so can be
integrated analytically to give a bound on I
I �
f(a, ξ)p(a) da = F (ξ).
(10.146)
We now have the freedom to choose the variational parameter ξ, which we do by
finding the value ξ that maximizes the function F (ξ). The resulting value F (ξ)
represents the tightest bound within this family of bounds and can be used as an
approximation to I. This optimized bound, however, will in general not be exact.
n=1
N
498
10. APPROXIMATE INFERENCE
Although the bound σ(a) � f(a, ξ) on the logistic sigmoid can be optimized exactly,
the required choice for ξ depends on the value of a, so that the bound is exact for one
value of a only. Because the quantity F (ξ) is obtained by integrating over all values
of a, the value of ξ represents a compromise, weighted by the distribution p(a).
