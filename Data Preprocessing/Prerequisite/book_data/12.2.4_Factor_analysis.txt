Factor analysis is a linear-Gaussian latent variable model that is closely related
to probabilistic PCA. Its deﬁnition differs from that of probabilistic PCA only in that
the conditional distribution of the observed variable x given the latent variable z is
584
12. CONTINUOUS LATENT VARIABLES
Figure 12.14 ‘Hinton’ diagrams of the matrix W in which each element of the matrix is depicted as
a square (white for positive and black for negative values) whose area is proportional
to the magnitude of that element. The synthetic data set comprises 300 data points in
D = 10 dimensions sampled from a Gaussian distribution having standard deviation 1.0
in 3 directions and standard deviation 0.5 in the remaining 7 directions for a data set in
D = 10 dimensions having M = 3 directions with larger variance than the remaining 7
directions. The left-hand plot shows the result from maximum likelihood probabilistic PCA,
and the left-hand plot shows the corresponding result from Bayesian PCA. We see how
the Bayesian model is able to discover the appropriate dimensionality by suppressing the
6 surplus degrees of freedom.
taken to have a diagonal rather than an isotropic covariance so that
p(x|z) = N (x|Wz + µ, Ψ)
(12.64)
where Ψ is a D×D diagonal matrix. Note that the factor analysis model, in common
with probabilistic PCA, assumes that the observed variables x1, . . . , xD are indepen-
dent, given the latent variable z. In essence, the factor analysis model is explaining
the observed covariance structure of the data by representing the independent vari-
ance associated with each coordinate in the matrix Ψ and capturing the covariance
between variables in the matrix W. In the factor analysis literature, the columns
of W, which capture the correlations between observed variables, are called factor
loadings, and the diagonal elements of Ψ, which represent the independent noise
variances for each of the variables, are called uniquenesses.
The origins of factor analysis are as old as those of PCA, and discussions of
factor analysis can be found in the books by Everitt (1984), Bartholomew (1987),
and Basilevsky (1994). Links between factor analysis and PCA were investigated
by Lawley (1953) and Anderson (1963) who showed that at stationary points of
the likelihood function, for a factor analysis model with Ψ = σ2I, the columns of
W are scaled eigenvectors of the sample covariance matrix, and σ2 is the average
of the discarded eigenvalues. Later, Tipping and Bishop (1999b) showed that the
maximum of the log likelihood function occurs when the eigenvectors comprising
W are chosen to be the principal eigenvectors.
Making use of (2.115), we see that the marginal distribution for the observed
12.2. Probabilistic PCA
585
Figure 12.15 Gibbs sampling for Bayesian
ln αi
PCA showing plots of
for
versus iteration number
three α values,
showing
transitions
the
three modes of the posterior
distribution.
between
10
5
0
10
5
0
10
5
0
Exercise 12.19
Section 12.4
Exercise 12.21
variable is given by p(x) = N (x|µ, C) where now
C = WWT + Ψ.
(12.65)
As with probabilistic PCA, this model is invariant to rotations in the latent space.
Historically, factor analysis has been the subject of controversy when attempts
have been made to place an interpretation on the individual factors (the coordinates
in z-space), which has proven problematic due to the nonidentiﬁability of factor
analysis associated with rotations in this space. From our perspective, however, we
shall view factor analysis as a form of latent variable density model, in which the
form of the latent space is of interest but not the particular choice of coordinates
used to describe it. If we wish to remove the degeneracy associated with latent space
rotations, we must consider non-Gaussian latent variable distributions, giving rise to
independent component analysis (ICA) models.
We can determine the parameters µ, W, and Ψ in the factor analysis model by
maximum likelihood. The solution for µ is again given by the sample mean. How-
ever, unlike probabilistic PCA, there is no longer a closed-form maximum likelihood
solution for W, which must therefore be found iteratively. Because factor analysis is
a latent variable model, this can be done using an EM algorithm (Rubin and Thayer,
1982) that is analogous to the one used for probabilistic PCA. Speciﬁcally, the E-step
equations are given by
E[zn] = GWTΨ
−1(xn − x)
E[znzT
n] = G + E[zn]E[zn]T
(12.66)
(12.67)
where we have deﬁned
(12.68)
Note that this is expressed in a form that involves inversion of matrices of size M×M
rather than D × D (except for the D × D diagonal matrix Ψ whose inverse is trivial
G = (I + WTΨ
−1W)−1.
586
12. CONTINUOUS LATENT VARIABLES
Exercise 12.22
Exercise 12.25
to compute in O(D) steps), which is convenient because often M (cid:13) D. Similarly,
the M-step equations take the form
−1
Wnew =
(xn − x)E[zn]T
Ψnew = diag
S − Wnew
1
N
E[znzT
n]
n=1
E[zn](xn − x)T
(cid:25)
(12.69)
(12.70)
(cid:31)
N(cid:2)
(cid:24)
n=1
N(cid:2)
(cid:31)
N(cid:2)
n=1
where the ‘diag’ operator sets all of the nondiagonal elements of a matrix to zero. A
Bayesian treatment of the factor analysis model can be obtained by a straightforward
application of the techniques discussed in this book.
Another difference between probabilistic PCA and factor analysis concerns their
different behaviour under transformations of the data set. For PCA and probabilis-
tic PCA, if we rotate the coordinate system in data space, then we obtain exactly
the same ﬁt to the data but with the W matrix transformed by the corresponding
rotation matrix. However, for factor analysis, the analogous property is that if we
make a component-wise re-scaling of the data vectors, then this is absorbed into a
corresponding re-scaling of the elements of Ψ.
