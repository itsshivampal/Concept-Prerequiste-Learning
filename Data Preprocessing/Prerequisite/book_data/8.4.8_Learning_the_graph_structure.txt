In our discussion of inference in graphical models, we have assumed that the
structure of the graph is known and fixed. However, there is also interest in go-
ing beyond the inference problem and learning the graph structure itself from data
(Friedman and Koller, 2003). This requires that we define a space of possible struc-
tures as well as a measure that can be used to score each structure.
From a Bayesian viewpoint, we would ideally like to compute a posterior dis-
tribution over graph structures and to make predictions by averaging with respect
to this distribution. If we have a prior p(m) over graphs indexed by m, then the
posterior distribution is given by
p(m|D) ‚àù p(m)p(D|m)
(8.103)
where D is the observed data set. The model evidence p(D|m) then provides the
score for each model. However, evaluation of the evidence involves marginalization
over the latent variables and presents a challenging computational problem for many
models.
Exploring the space of structures can also be problematic. Because the number
of different graph structures grows exponentially with the number of nodes, it is
often necessary to resort to heuristics to find good candidates.
