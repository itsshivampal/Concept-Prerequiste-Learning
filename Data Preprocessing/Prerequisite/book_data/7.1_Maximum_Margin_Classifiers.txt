We begin our discussion of support vector machines by returning to the two-class
classification problem using linear models of the form
y(x) = wTφ(x) + b
(7.1)
where φ(x) denotes a fixed feature-space transformation, and we have made the
bias parameter b explicit. Note that we shall shortly introduce a dual representation
expressed in terms of kernel functions, which avoids having to work explicitly in
feature space. The training data set comprises N input vectors x1, . . . , xN , with
corresponding target values t1, . . . , tN where tn ∈ {−1, 1}, and new data points x
are classified according to the sign of y(x).
We shall assume for the moment that the training data set is linearly separable in
feature space, so that by definition there exists at least one choice of the parameters
w and b such that a function of the form (7.1) satisfies y(xn) > 0 for points having
tn = +1 and y(xn) < 0 for points having tn = −1, so that tny(xn) > 0 for all
training data points.
There may of course exist many such solutions that separate the classes exactly.
In Section 4.1.7, we described the perceptron algorithm that is guaranteed to find
a solution in a finite number of steps. The solution that it finds, however, will be
dependent on the (arbitrary) initial values chosen for w and b as well as on the
order in which the data points are presented. If there are multiple solutions all of
which classify the training data set exactly, then we should try to find the one that
will give the smallest generalization error. The support vector machine approaches
this problem through the concept of the margin, which is defined to be the smallest
distance between the decision boundary and any of the samples, as illustrated in
Figure 7.1.
In support vector machines the decision boundary is chosen to be the one for
which the margin is maximized. The maximum margin solution can be motivated us-
ing computational learning theory, also known as statistical learning theory. How-
ever, a simple insight into the origins of maximum margin has been given by Tong
and Koller (2000) who consider a framework for classification based on a hybrid of
generative and discriminative approaches. They first model the distribution over in-
put vectors x for each class using a Parzen density estimator with Gaussian kernels
Section 7.1.5
7.1. Maximum Margin Classifiers
327
y = −1
y = 0
y = 1
y = 1
y = 0
y = −1
margin
Figure 7.1 The margin is defined as the perpendicular distance between the decision boundary and the closest
of the data points, as shown on the left figure. Maximizing the margin leads to a particular choice of decision
boundary, as shown on the right. The location of this boundary is determined by a subset of the data points,
known as support vectors, which are indicated by the circles.
having a common parameter σ2. Together with the class priors, this defines an opti-
mal misclassification-rate decision boundary. However, instead of using this optimal
boundary, they determine the best hyperplane by minimizing the probability of error
relative to the learned density model. In the limit σ2 → 0, the optimal hyperplane
is shown to be the one having maximum margin. The intuition behind this result is
that as σ2 is reduced, the hyperplane is increasingly dominated by nearby data points
relative to more distant ones. In the limit, the hyperplane becomes independent of
data points that are not support vectors.
We shall see in Figure 10.13 that marginalization with respect to the prior distri-
bution of the parameters in a Bayesian approach for a simple linearly separable data
set leads to a decision boundary that lies in the middle of the region separating the
data points. The large margin solution has similar behaviour.
Recall from Figure 4.1 that the perpendicular distance of a point x from a hyper-
plane defined by y(x) = 0 where y(x) takes the form (7.1) is given by |y(x)|/w.
Furthermore, we are only interested in solutions for which all data points are cor-
rectly classified, so that tny(xn) > 0 for all n. Thus the distance of a point xn to the
decision surface is given by
tny(xn)
w
= tn(wTφ(xn) + b)
w
(7.2)
The margin is given by the perpendicular distance to the closest point xn from the
data set, and we wish to optimize the parameters w and b in order to maximize this
distance. Thus the maximum margin solution is found by solving
arg max
w,b
1
w
min
n
tn
wTφ(xn) + b
(7.3)
where we have taken the factor 1/w outside the optimization over n because w
n=1
n=1
N
N
N
328
Appendix E
7. SPARSE KERNEL MACHINES
does not depend on n. Direct solution of this optimization problem would be very
complex, and so we shall convert it into an equivalent problem that is much easier
to solve. To do this we note that if we make the rescaling w → κw and b → κb,
then the distance from any point xn to the decision surface, given by tny(xn)/w,
is unchanged. We can use this freedom to set
tn
wTφ(xn) + b
= 1
(7.4)
tn
for the point that is closest to the surface. In this case, all data points will satisfy the
constraints
� 1,
n = 1, . . . , N.
wTφ(xn) + b
(7.5)
This is known as the canonical representation of the decision hyperplane.
In the
case of data points for which the equality holds, the constraints are said to be active,
whereas for the remainder they are said to be inactive. By definition, there will
always be at least one active constraint, because there will always be a closest point,
and once the margin has been maximized there will be at least two active constraints.
The optimization problem then simply requires that we maximize w−1, which is
equivalent to minimizing w2, and so we have to solve the optimization problem
(7.6)
arg min
1
2w2
w,b
subject to the constraints given by (7.5). The factor of 1/2 in (7.6) is included for
later convenience. This is an example of a quadratic programming problem in which
we are trying to minimize a quadratic function subject to a set of linear inequality
constraints. It appears that the bias parameter b has disappeared from the optimiza-
tion. However, it is determined implicitly via the constraints, because these require
that changes to w be compensated by changes to b. We shall see how this works
shortly.
In order to solve this constrained optimization problem, we introduce Lagrange
multipliers an � 0, with one multiplier an for each of the constraints in (7.5), giving
the Lagrangian function
L(w, b, a) =
1
2w2 −
an
tn(wTφ(xn) + b) − 1
(7.7)
where a = (a1, . . . , aN )T. Note the minus sign in front of the Lagrange multiplier
term, because we are minimizing with respect to w and b, and maximizing with
respect to a. Setting the derivatives of L(w, b, a) with respect to w and b equal to
zero, we obtain the following two conditions
w =
antnφ(xn)
0 =
antn.
n=1
(7.8)
(7.9)
n=1
N
N
n=1
N
1
2
N
N
7.1. Maximum Margin Classifiers
329
Eliminating w and b from L(w, b, a) using these conditions then gives the dual
representation of the maximum margin problem in which we maximize
L(a) =
an −
n=1
with respect to a subject to the constraints
m=1
anamtntmk(xn, xm)
(7.10)
n = 1, . . . , N,
(7.11)
(7.12)
an � 0,
antn = 0.
Here the kernel function is defined by k(x, x) = φ(x)Tφ(x). Again, this takes the
form of a quadratic programming problem in which we optimize a quadratic function
of a subject to a set of inequality constraints. We shall discuss techniques for solving
such quadratic programming problems in Section 7.1.1.
The solution to a quadratic programming problem in M variables in general has
computational complexity that is O(M 3). In going to the dual formulation we have
turned the original optimization problem, which involved minimizing (7.6) over M
variables, into the dual problem (7.10), which has N variables. For a fixed set of
basis functions whose number M is smaller than the number N of data points, the
move to the dual problem appears disadvantageous. However, it allows the model to
be reformulated using kernels, and so the maximum margin classifier can be applied
efficiently to feature spaces whose dimensionality exceeds the number of data points,
including infinite feature spaces. The kernel formulation also makes clear the role
of the constraint that the kernel function k(x, x) be positive definite, because this
L(a) is bounded below, giving rise to a well-
ensures that the Lagrangian function
defined optimization problem.
In order to classify new data points using the trained model, we evaluate the sign
of y(x) defined by (7.1). This can be expressed in terms of the parameters {an} and
the kernel function by substituting for w using (7.8) to give
y(x) =
antnk(x, xn) + b.
(7.13)
n=1
Joseph-Louis Lagrange
1736–1813
Although widely considered to be
a French mathematician, Lagrange
was born in Turin in Italy. By the age
of nineteen, he had already made
important contributions mathemat-
ics and had been appointed as Pro-
fessor at the Royal Artillery School in Turin. For many
years, Euler worked hard to persuade Lagrange to
move to Berlin, which he eventually did in 1766 where
he succeeded Euler as Director of Mathematics at
the Berlin Academy. Later he moved to Paris, nar-
rowly escaping with his life during the French revo-
lution thanks to the personal intervention of Lavoisier
(the French chemist who discovered oxygen) who him-
self was later executed at the guillotine. Lagrange
made key contributions to the calculus of variations
and the foundations of dynamics.
m∈S
n∈S
N
330
7. SPARSE KERNEL MACHINES
In Appendix E, we show that a constrained optimization of this form satisfies the
Karush-Kuhn-Tucker (KKT) conditions, which in this case require that the following
three properties hold
an � 0
tny(xn) − 1 � 0
an {tny(xn) − 1} = 0.
(7.14)
(7.15)
(7.16)
Thus for every data point, either an = 0 or tny(xn) = 1. Any data point for
which an = 0 will not appear in the sum in (7.13) and hence plays no role in making
predictions for new data points. The remaining data points are called support vectors,
and because they satisfy tny(xn) = 1, they correspond to points that lie on the
maximum margin hyperplanes in feature space, as illustrated in Figure 7.1. This
property is central to the practical applicability of support vector machines. Once
the model is trained, a significant proportion of the data points can be discarded and
only the support vectors retained.
Having solved the quadratic programming problem and found a value for a, we
can then determine the value of the threshold parameter b by noting that any support
vector xn satisfies tny(xn) = 1. Using (7.13) this gives
tn
amtmk(xn, xm) + b
= 1
(7.17)
where S denotes the set of indices of the support vectors. Although we can solve
this equation for b using an arbitrarily chosen support vector xn, a numerically more
n = 1,
stable solution is obtained by first multiplying through by tn, making use of t2
and then averaging these equations over all support vectors and solving for b to give
b =
1
NS
amtmk(xn, xm)
(7.18)
tn −
m∈S
where NS is the total number of support vectors.
For later comparison with alternative models, we can express the maximum-
margin classifier in terms of the minimization of an error function, with a simple
quadratic regularizer, in the form
E∞(y(xn)tn − 1) + λw2
n=1
(7.19)
where E∞(z) is a function that is zero if z � 0 and ∞ otherwise and ensures that
the constraints (7.5) are satisfied. Note that as long as the regularization parameter
satisfies λ > 0, its precise value plays no role.
Figure 7.2 shows an example of the classification resulting from training a sup-
port vector machine on a simple synthetic data set using a Gaussian kernel of the
7.1. Maximum Margin Classifiers
331
Figure 7.2 Example of synthetic data from
two classes in two dimensions
showing contours of constant
y(x) obtained from a support
vector machine having a Gaus-
sian kernel function. Also shown
are the decision boundary,
the
margin boundaries, and the sup-
port vectors.
form (6.23). Although the data set is not linearly separable in the two-dimensional
data space x, it is linearly separable in the nonlinear feature space defined implicitly
by the nonlinear kernel function. Thus the training data points are perfectly separated
in the original data space.
This example also provides a geometrical insight into the origin of sparsity in
the SVM. The maximum margin hyperplane is defined by the location of the support
vectors. Other data points can be moved around freely (so long as they remain out-
side the margin region) without changing the decision boundary, and so the solution
will be independent of such data points.
