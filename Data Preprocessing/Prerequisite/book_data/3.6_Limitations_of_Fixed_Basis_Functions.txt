Throughout this chapter, we have focussed on models comprising a linear combina-
tion of fixed, nonlinear basis functions. We have seen that the assumption of linearity
in the parameters led to a range of useful properties including closed-form solutions
to the least-squares problem, as well as a tractable Bayesian treatment. Furthermore,
for a suitable choice of basis functions, we can model arbitrary nonlinearities in the
j=1
M
M
Exercises
173
mapping from input variables to targets. In the next chapter, we shall study an anal-
ogous class of models for classification.
It might appear, therefore, that such linear models constitute a general purpose
framework for solving problems in pattern recognition. Unfortunately, there are
some significant shortcomings with linear models, which will cause us to turn in
later chapters to more complex models such as support vector machines and neural
networks.
The difficulty stems from the assumption that the basis functions φj(x) are fixed
before the training data set is observed and is a manifestation of the curse of dimen-
sionality discussed in Section 1.4. As a consequence, the number of basis functions
needs to grow rapidly, often exponentially, with the dimensionality D of the input
space.
Fortunately, there are two properties of real data sets that we can exploit to help
alleviate this problem. First of all, the data vectors {xn} typically lie close to a non-
linear manifold whose intrinsic dimensionality is smaller than that of the input space
as a result of strong correlations between the input variables. We will see an example
of this when we consider images of handwritten digits in Chapter 12. If we are using
localized basis functions, we can arrange that they are scattered in input space only
in regions containing data. This approach is used in radial basis function networks
and also in support vector and relevance vector machines. Neural network models,
which use adaptive basis functions having sigmoidal nonlinearities, can adapt the
parameters so that the regions of input space over which the basis functions vary
corresponds to the data manifold. The second property is that target variables may
have significant dependence on only a small number of possible directions within the
data manifold. Neural networks can exploit this property by choosing the directions
in input space to which the basis functions respond.
Exercises
3.1 () www Show that the ‘tanh’ function and the logistic sigmoid function (3.6)
are related by
Hence show that a general linear combination of logistic sigmoid functions of the
form
tanh(a) = 2σ(2a) − 1.
is equivalent to a linear combination of ‘tanh’ functions of the form
y(x, w) = w0 +
wjσ
x − µj
s
y(x, u) = u0 +
uj tanh
j=1
x − µj
s
and find expressions to relate the new parameters {u1, . . . , uM} to the original pa-
rameters {w1, . . . , wM}.
(3.100)
(3.101)
(3.102)
n=1
N
N
D
174
3. LINEAR MODELS FOR REGRESSION
3.2 ( ) Show that the matrix
Φ(ΦTΦ)−1ΦT
(3.103)
takes any vector v and projects it onto the space spanned by the columns of Φ. Use
this result to show that the least-squares solution (3.15) corresponds to an orthogonal
projection of the vector t onto the manifold S as shown in Figure 3.2.
3.3 () Consider a data set in which each data point tn is associated with a weighting
factor rn > 0, so that the sum-of-squares error function becomes
ED(w) =
1
2
rn
tn − wTφ(xn)
2
(3.104)
Find an expression for the solution w that minimizes this error function. Give two
alternative interpretations of the weighted sum-of-squares error function in terms of
(i) data dependent noise variance and (ii) replicated data points.
3.4 () www Consider a linear model of the form
y(x, w) = w0 +
wixi
i=1
together with a sum-of-squares error function of the form
ED(w) =
1
2
{y(xn, w) − tn}2 .
n=1
(3.105)
(3.106)
Now suppose that Gaussian noise 	i with zero mean and variance σ2 is added in-
dependently to each of the input variables xi. By making use of E[	i] = 0 and
E[	i	j] = δijσ2, show that minimizing ED averaged over the noise distribution is
equivalent to minimizing the sum-of-squares error for noise-free input variables with
the addition of a weight-decay regularization term, in which the bias parameter w0
is omitted from the regularizer.
3.5 () www Using the technique of Lagrange multipliers, discussed in Appendix E,
show that minimization of the regularized error function (3.29) is equivalent to mini-
mizing the unregularized sum-of-squares error (3.12) subject to the constraint (3.30).
Discuss the relationship between the parameters η and λ.
3.6 () www Consider a linear basis function regression model for a multivariate
target variable t having a Gaussian distribution of the form
p(t|W, Σ) = N (t|y(x, W), Σ)
where
y(x, W) = WTφ(x)
(3.107)
(3.108)
n=1
N
Exercises
175
together with a training data set comprising input basis vectors φ(xn) and corre-
sponding target vectors tn, with n = 1, . . . , N. Show that the maximum likelihood
solution WML for the parameter matrix W has the property that each column is
given by an expression of the form (3.15), which was the solution for an isotropic
noise distribution. Note that this is independent of the covariance matrix Σ. Show
that the maximum likelihood solution for Σ is given by
1
N
tn − WT
MLφ(xn)
tn − WT
MLφ(xn)
T
(3.109)
3.7 () By using the technique of completing the square, verify the result (3.49) for the
posterior distribution of the parameters w in the linear basis function model in which
mN and SN are defined by (3.50) and (3.51) respectively.
3.8 ( ) www Consider the linear basis function model in Section 3.1, and suppose
that we have already observed N data points, so that the posterior distribution over
w is given by (3.49). This posterior can be regarded as the prior for the next obser-
vation. By considering an additional data point (xN +1, tN +1), and by completing
the square in the exponential, show that the resulting posterior distribution is again
given by (3.49) but with SN replaced by SN +1 and mN replaced by mN +1.
3.9 ( ) Repeat the previous exercise but instead of completing the square by hand,
make use of the general result for linear-Gaussian models given by (2.116).
3.10 ( ) www By making use of the result (2.115) to evaluate the integral in (3.57),
verify that the predictive distribution for the Bayesian linear regression model is
given by (3.58) in which the input-dependent variance is given by (3.59).
3.11 ( ) We have seen that, as the size of a data set increases, the uncertainty associated
with the posterior distribution over model parameters decreases. Make use of the
matrix identity (Appendix C)
to show that the uncertainty σ2
given by (3.59) satisfies
M + vvT
(M−1v)
vTM−1
1 + vTM−1v
−1 = M−1 −
N (x) associated with the linear regression function
(3.110)
N +1(x) � σ2
σ2
N (x).
(3.111)
3.12 ( ) We saw in Section 2.3.6 that the conjugate prior for a Gaussian distribution
with unknown mean and unknown precision (inverse variance) is a normal-gamma
distribution. This property also holds for the case of the conditional Gaussian dis-
tribution p(t|x, w, β) of the linear regression model. If we consider the likelihood
function (3.10), then the conjugate prior for w and β is given by
p(w, β) = N (w|m0, β−1S0)Gam(β|a0, b0).
(3.112)
N
n=1
N
Show that the corresponding posterior distribution takes the same functional form,
so that
p(w, β|t) = N (w|mN , β−1SN )Gam(β|aN , bN )
(3.113)
and find expressions for the posterior parameters mN , SN , aN , and bN .
3.13 ( ) Show that the predictive distribution p(t|x, t) for the model discussed in Ex-
ercise 3.12 is given by a Student’s t-distribution of the form
p(t|x, t) = St(t|µ, λ, ν)
(3.114)
and obtain expressions for µ, λ and ν.
3.14 ( )
In this exercise, we explore in more detail the properties of the equivalent
kernel defined by (3.62), where SN is defined by (3.54). Suppose that the basis
functions φj(x) are linearly independent and that the number N of data points is
greater than the number M of basis functions. Furthermore, let one of the basis
functions be constant, say φ0(x) = 1. By taking suitable linear combinations of
these basis functions, we can construct a new basis set ψj(x) spanning the same
space but that are orthonormal, so that
176
3. LINEAR MODELS FOR REGRESSION
ψj(xn)ψk(xn) = Ijk
(3.115)
where Ijk is defined to be 1 if j = k and 0 otherwise, and we take ψ0(x) = 1. Show
that for α = 0, the equivalent kernel can be written as k(x, x) = ψ(x)Tψ(x)
where ψ = (ψ1, . . . , ψM )T. Use this result to show that the kernel satisfies the
summation constraint
k(x, xn) = 1.
n=1
(3.116)
3.15 () www Consider a linear basis function model for regression in which the pa-
rameters α and β are set using the evidence framework. Show that the function
E(mN ) defined by (3.82) satisfies the relation 2E(mN ) = N.
3.16 ( ) Derive the result (3.86) for the log evidence function p(t|α, β) of the linear
regression model by making use of (2.115) to evaluate the integral (3.77) directly.
3.17 () Show that the evidence function for the Bayesian linear regression model can
be written in the form (3.78) in which E(w) is defined by (3.79).
3.18 ( ) www By completing the square over w, show that the error function (3.79)
in Bayesian linear regression can be written in the form (3.80).
3.19 ( ) Show that the integration over w in the Bayesian linear regression model gives
the result (3.85). Hence show that the log marginal likelihood is given by (3.86).
3.20 ( ) www Starting from (3.86) verify all of the steps needed to show that maxi-
mization of the log marginal likelihood function (3.86) with respect to α leads to the
re-estimation equation (3.92).
Exercises
177
3.21 ( ) An alternative way to derive the result (3.92) for the optimal value of α in the
evidence framework is to make use of the identity
d
dα
ln|A| = Tr
A−1 d
dα
A
(3.117)
Prove this identity by considering the eigenvalue expansion of a real, symmetric
matrix A, and making use of the standard results for the determinant and trace of
A expressed in terms of its eigenvalues (Appendix C). Then make use of (3.117) to
derive (3.92) starting from (3.86).
3.22 ( ) Starting from (3.86) verify all of the steps needed to show that maximiza-
tion of the log marginal likelihood function (3.86) with respect to β leads to the
re-estimation equation (3.95).
3.23 ( ) www Show that the marginal probability of the data, in other words the
model evidence, for the model described in Exercise 3.12 is given by
p(t) =
1
(2π)N/2
ba0
0
baN
N
Γ(aN )
Γ(a0)
|SN|1/2
|S0|1/2
by first marginalizing with respect to w and then with respect to β.
3.24 ( ) Repeat the previous exercise but now use Bayes’ theorem in the form
p(t) = p(t|w, β)p(w, β)
p(w, β|t)
(3.118)
(3.119)
and then substitute for the prior and posterior distributions and the likelihood func-
tion in order to derive the result (3.118).
4 Linear Models for Classification
In the previous chapter, we explored a class of regression models having particularly
simple analytical and computational properties. We now discuss an analogous class
of models for solving classification problems. The goal in classification is to take an
input vector x and to assign it to one of K discrete classes Ck where k = 1, . . . , K.
In the most common scenario, the classes are taken to be disjoint, so that each input is
assigned to one and only one class. The input space is thereby divided into decision
regions whose boundaries are called decision boundaries or decision surfaces. In
this chapter, we consider linear models for classification, by which we mean that the
decision surfaces are linear functions of the input vector x and hence are defined
by (D − 1)-dimensional hyperplanes within the D-dimensional input space. Data
sets whose classes can be separated exactly by linear decision surfaces are said to be
linearly separable.
For regression problems, the target variable t was simply the vector of real num-
bers whose values we wish to predict. In the case of classification, there are various
179
180
