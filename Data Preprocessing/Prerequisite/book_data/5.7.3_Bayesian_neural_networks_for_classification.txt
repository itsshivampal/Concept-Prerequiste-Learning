So far, we have used the Laplace approximation to develop a Bayesian treat-
ment of neural network regression models. We now discuss the modifications to
N
this framework that arise when it is applied to classification. Here we shall con-
sider a network having a single logistic sigmoid output corresponding to a two-class
classification problem. The extension to networks with multiclass softmax outputs
is straightforward. We shall build extensively on the analogous results for linear
classification models discussed in Section 4.5, and so we encourage the reader to
familiarize themselves with that material before studying this section.
The log likelihood function for this model is given by
ln p(D|w) =
= 1N {tn ln yn + (1 − tn) ln(1 − yn)}
n
(5.181)
where tn ∈ {0, 1} are the target values, and yn ≡ y(xn, w). Note that there is no
hyperparameter β, because the data points are assumed to be correctly labelled. As
before, the prior is taken to be an isotropic Gaussian of the form (5.162).
The first stage in applying the Laplace framework to this model is to initialize
the hyperparameter α, and then to determine the parameter vector w by maximizing
the log posterior distribution. This is equivalent to minimizing the regularized error
function
E(w) = − ln p(D|w) + α
2
wTw
(5.182)
and can be achieved using error backpropagation combined with standard optimiza-
tion algorithms, as discussed in Section 5.3.
Having found a solution wMAP for the weight vector, the next step is to eval-
uate the Hessian matrix H comprising the second derivatives of the negative log
likelihood function. This can be done, for instance, using the exact method of Sec-
tion 5.4.5, or using the outer product approximation given by (5.85). The second
derivatives of the negative log posterior can again be written in the form (5.166), and
the Gaussian approximation to the posterior is then given by (5.167).
To optimize the hyperparameter α, we again maximize the marginal likelihood,
which is easily shown to take the form
ln p(D|α)  −E(wMAP) −
1
2
ln|A| + W
2
ln α + const
(5.183)
where the regularized error function is defined by
E(wMAP) = −
n=1
{tn ln yn + (1 − tn) ln(1 − yn)} + α
2
wT
MAPwMAP (5.184)
in which yn ≡ y(xn, wMAP). Maximizing this evidence function with respect to α
again leads to the re-estimation equation given by (5.178).
The use of the evidence procedure to determine α is illustrated in Figure 5.22
for the synthetic two-dimensional data discussed in Appendix A.
Finally, we need the predictive distribution, which is defined by (5.168). Again,
this integration is intractable due to the nonlinearity of the network function. The
282
5. NEURAL NETWORKS
Exercise 5.40
Exercise 5.41
5.7. Bayesian Neural Networks
283
Figure 5.22 Illustration of
the evidence framework
applied to a synthetic two-class data set.
The green curve shows the optimal de-
cision boundary, the black curve shows
the result of fitting a two-layer network
with 8 hidden units by maximum likeli-
hood, and the red curve shows the re-
sult of including a regularizer in which
α is optimized using the evidence pro-
cedure, starting from the initial value
α = 0. Note that the evidence proce-
dure greatly reduces the over-fitting of
the network.
3
2
1
0
−1
−2
−2
−1
0
1
2
simplest approximation is to assume that the posterior distribution is very narrow
and hence make the approximation
p(t|x,D)  p(t|x, wMAP).
(5.185)
We can improve on this, however, by taking account of the variance of the posterior
distribution. In this case, a linear approximation for the network outputs, as was used
in the case of regression, would be inappropriate due to the logistic sigmoid output-
unit activation function that constrains the output to lie in the range (0, 1). Instead,
we make a linear approximation for the output unit activation in the form
a(x, w)  aMAP(x) + bT(w − wMAP)
(5.186)
where aMAP(x) = a(x, wMAP), and the vector b ≡ ∇a(x, wMAP) can be found by
backpropagation.
Because we now have a Gaussian approximation for the posterior distribution
over w, and a model for a that is a linear function of w, we can now appeal to the
results of Section 4.5.2. The distribution of output unit activation values, induced by
the distribution over network weights, is given by
p(a|x,D) =
a − aMAP(x) − bT(x)(w − wMAP)
q(w|D) dw (5.187)
where q(w|D) is the Gaussian approximation to the posterior distribution given by
(5.167). From Section 4.5.2, we see that this distribution is Gaussian with mean
aMAP ≡ a(x, wMAP), and variance
a(x) = bT(x)A−1b(x).
σ2
(5.188)
Finally, to obtain the predictive distribution, we must marginalize over a using
p(t = 1|x,D) =
σ(a)p(a|x,D) da.
(5.189)
284
5. NEURAL NETWORKS
3
2
1
0
−1
−2
−2
−1
0
1
2
3
2
1
0
−1
−2
−2
−1
0
1
2
Figure 5.23 An illustration of the Laplace approximation for a Bayesian neural network having 8 hidden units
with ‘tanh’ activation functions and a single logistic-sigmoid output unit. The weight parameters were found using
scaled conjugate gradients, and the hyperparameter α was optimized using the evidence framework. On the left
is the result of using the simple approximation (5.185) based on a point estimate wMAP of the parameters,
in which the green curve shows the y = 0.5 decision boundary, and the other contours correspond to output
probabilities of y = 0.1, 0.3, 0.7, and 0.9. On the right is the corresponding result obtained using (5.190). Note
that the effect of marginalization is to spread out the contours and to make the predictions less confident, so
that at each input point x, the posterior probabilities are shifted towards 0.5, while the y = 0.5 contour itself is
unaffected.
The convolution of a Gaussian with a logistic sigmoid is intractable. We therefore
apply the approximation (4.153) to (5.189) giving
p(t = 1|x,D) = σ
κ(σ2
a)bTwMAP
(5.190)
where κ(·) is defined by (4.154). Recall that both σ2
fication data set described in Appendix A.
Figure 5.23 shows an example of this framework applied to the synthetic classi-
a and b are functions of x.
