So far, we have used the Laplace approximation to develop a Bayesian treat-
ment of neural network regression models. We now discuss the modifications to
N
this framework that arise when it is applied to classification. Here we shall con-
sider a network having a single logistic sigmoid output corresponding to a two-class
classification problem. The extension to networks with multiclass softmax outputs
is straightforward. We shall build extensively on the analogous results for linear
classification models discussed in Section 4.5, and so we encourage the reader to
familiarize themselves with that material before studying this section.
The log likelihood function for this model is given by
ln p(D|w) =
= 1N {tn ln yn + (1 − tn) ln(1 − yn)}
n
(5.181)
where tn ∈ {0, 1} are the target values, and yn ≡ y(xn, w). Note that there is no
hyperparameter β, because the data points are assumed to be correctly labelled. As
before, the prior is taken to be an isotropic Gaussian of the form (5.162).
The first stage in applying the Laplace framework to this model is to initialize
the hyperparameter α, and then to determine the parameter vector w by maximizing
the log posterior distribution. This is equivalent to minimizing the regularized error
function
E(w) = − ln p(D|w) + α
2
wTw
(5.182)
and can be achieved using error backpropagation combined with standard optimiza-
tion algorithms, as discussed in Section 5.3.
Having found a solution wMAP for the weight vector, the next step is to eval-
uate the Hessian matrix H comprising the second derivatives of the negative log
likelihood function. This can be done, for instance, using the exact method of Sec-
tion 5.4.5, or using the outer product approximation given by (5.85). The second
derivatives of the negative log posterior can again be written in the form (5.166), and
the Gaussian approximation to the posterior is then given by (5.167).
To optimize the hyperparameter α, we again maximize the marginal likelihood,
which is easily shown to take the form
ln p(D|α)  −E(wMAP) −
1
2
ln|A| + W
2
ln α + const
(5.183)
where the regularized error function is defined by
E(wMAP) = −
n=1
{tn ln yn + (1 − tn) ln(1 − yn)} + α
2
wT
MAPwMAP (5.184)
in which yn ≡ y(xn, wMAP). Maximizing this evidence function with respect to α
again leads to the re-estimation equation given by (5.178).
The use of the evidence procedure to determine α is illustrated in Figure 5.22
for the synthetic two-dimensional data discussed in Appendix A.
Finally, we need the predictive distribution, which is defined by (5.168). Again,
this integration is intractable due to the nonlinearity of the network function. The
282
5. NEURAL NETWORKS
Exercise 5.40
Exercise 5.41
5.7. Bayesian Neural Networks
283
Figure 5.22 Illustration of
the evidence framework
applied to a synthetic two-class data set.
The green curve shows the optimal de-
cision boundary, the black curve shows
the result of fitting a two-layer network
with 8 hidden units by maximum likeli-
hood, and the red curve shows the re-
sult of including a regularizer in which
α is optimized using the evidence pro-
cedure, starting from the initial value
α = 0. Note that the evidence proce-
dure greatly reduces the over-fitting of
the network.
3
2
1
0
−1
−2
−2
−1
0
1
2
simplest approximation is to assume that the posterior distribution is very narrow
and hence make the approximation
p(t|x,D)  p(t|x, wMAP).
(5.185)
We can improve on this, however, by taking account of the variance of the posterior
distribution. In this case, a linear approximation for the network outputs, as was used
in the case of regression, would be inappropriate due to the logistic sigmoid output-
unit activation function that constrains the output to lie in the range (0, 1). Instead,
we make a linear approximation for the output unit activation in the form
a(x, w)  aMAP(x) + bT(w − wMAP)
(5.186)
where aMAP(x) = a(x, wMAP), and the vector b ≡ ∇a(x, wMAP) can be found by
backpropagation.
Because we now have a Gaussian approximation for the posterior distribution
over w, and a model for a that is a linear function of w, we can now appeal to the
results of Section 4.5.2. The distribution of output unit activation values, induced by
the distribution over network weights, is given by
p(a|x,D) =
a − aMAP(x) − bT(x)(w − wMAP)
q(w|D) dw (5.187)
where q(w|D) is the Gaussian approximation to the posterior distribution given by
(5.167). From Section 4.5.2, we see that this distribution is Gaussian with mean
aMAP ≡ a(x, wMAP), and variance
a(x) = bT(x)A−1b(x).
σ2
(5.188)
Finally, to obtain the predictive distribution, we must marginalize over a using
p(t = 1|x,D) =
σ(a)p(a|x,D) da.
(5.189)
284
5. NEURAL NETWORKS
3
2
1
0
−1
−2
−2
−1
0
1
2
3
2
1
0
−1
−2
−2
−1
0
1
2
Figure 5.23 An illustration of the Laplace approximation for a Bayesian neural network having 8 hidden units
with ‘tanh’ activation functions and a single logistic-sigmoid output unit. The weight parameters were found using
scaled conjugate gradients, and the hyperparameter α was optimized using the evidence framework. On the left
is the result of using the simple approximation (5.185) based on a point estimate wMAP of the parameters,
in which the green curve shows the y = 0.5 decision boundary, and the other contours correspond to output
probabilities of y = 0.1, 0.3, 0.7, and 0.9. On the right is the corresponding result obtained using (5.190). Note
that the effect of marginalization is to spread out the contours and to make the predictions less confident, so
that at each input point x, the posterior probabilities are shifted towards 0.5, while the y = 0.5 contour itself is
unaffected.
The convolution of a Gaussian with a logistic sigmoid is intractable. We therefore
apply the approximation (4.153) to (5.189) giving
p(t = 1|x,D) = σ
κ(σ2
a)bTwMAP
(5.190)
where κ(·) is defined by (4.154). Recall that both σ2
fication data set described in Appendix A.
Figure 5.23 shows an example of this framework applied to the synthetic classi-
a and b are functions of x.
Exercises
5.1 ( ) Consider a two-layer network function of the form (5.7) in which the hidden-
unit nonlinear activation functions g(·) are given by logistic sigmoid functions of the
form
(5.191)
σ(a) = {1 + exp(−a)}−1 .
Show that there exists an equivalent network, which computes exactly the same func-
tion, but with hidden unit activation functions given by tanh(a) where the tanh func-
tion is defined by (5.59). Hint: first find the relation between σ(a) and tanh(a), and
then show that the parameters of the two networks differ by linear transformations.
5.2 () www Show that maximizing the likelihood function under the conditional
distribution (5.16) for a multioutput neural network is equivalent to minimizing the
sum-of-squares error function (5.11).
Exercises
285
5.3 ( ) Consider a regression problem involving multiple target variables in which it
is assumed that the distribution of the targets, conditioned on the input vector x, is a
Gaussian of the form
p(t|x, w) = N (t|y(x, w), Σ)
(5.192)
where y(x, w) is the output of a neural network with input vector x and weight
vector w, and Σ is the covariance of the assumed Gaussian noise on the targets.
Given a set of independent observations of x and t, write down the error function
that must be minimized in order to find the maximum likelihood solution for w, if
we assume that Σ is fixed and known. Now assume that Σ is also to be determined
from the data, and write down an expression for the maximum likelihood solution
for Σ. Note that the optimizations of w and Σ are now coupled, in contrast to the
case of independent target variables discussed in Section 5.2.
5.4 ( ) Consider a binary classification problem in which the target values are t ∈
{0, 1}, with a network output y(x, w) that represents p(t = 1|x), and suppose that
there is a probability 	 that the class label on a training data point has been incorrectly
set. Assuming independent and identically distributed data, write down the error
function corresponding to the negative log likelihood. Verify that the error function
(5.21) is obtained when 	 = 0. Note that this error function makes the model robust
to incorrectly labelled data, in contrast to the usual error function.
5.5 () www Show that maximizing likelihood for a multiclass neural network model
in which the network outputs have the interpretation yk(x, w) = p(tk = 1|x) is
equivalent to the minimization of the cross-entropy error function (5.24).
5.6 () www Show the derivative of the error function (5.21) with respect to the
activation ak for an output unit having a logistic sigmoid activation function satisfies
(5.18).
5.7 () Show the derivative of the error function (5.24) with respect to the activation ak
for output units having a softmax activation function satisfies (5.18).
5.8 () We saw in (4.88) that the derivative of the logistic sigmoid activation function
can be expressed in terms of the function value itself. Derive the corresponding result
for the ‘tanh’ activation function defined by (5.59).
5.9 () www The error function (5.21) for binary classification problems was de-
rived for a network having a logistic-sigmoid output activation function, so that
0 � y(x, w) � 1, and data having target values t ∈ {0, 1}. Derive the correspond-
ing error function if we consider a network having an output −1 � y(x, w) � 1
and target values t = 1 for class C1 and t = −1 for class C2. What would be the
appropriate choice of output unit activation function?
5.10 () www Consider a Hessian matrix H with eigenvector equation (5.33). By
setting the vector v in (5.39) equal to each of the eigenvectors ui in turn, show that
H is positive definite if, and only if, all of its eigenvalues are positive.
286
5. NEURAL NETWORKS
5.11 ( ) www Consider a quadratic error function defined by (5.32), in which the
Hessian matrix H has an eigenvalue equation given by (5.33). Show that the con-
tours of constant error are ellipses whose axes are aligned with the eigenvectors ui,
with lengths that are inversely proportional to the square root of the corresponding
eigenvalues λi.
5.12 ( ) www By considering the local Taylor expansion (5.32) of an error function
about a stationary point w, show that the necessary and sufficient condition for the
stationary point to be a local minimum of the error function is that the Hessian matrix
H, defined by (5.30) with
w = w, be positive definite.
5.13 ()
Show that as a consequence of the symmetry of the Hessian matrix H, the
number of independent elements in the quadratic error function (5.28) is given by
W (W + 3)/2.
5.14 () By making a Taylor expansion, verify that the terms that are O(	) cancel on the
right-hand side of (5.69).
5.15 ( ) In Section 5.3.4, we derived a procedure for evaluating the Jacobian matrix of a
neural network using a backpropagation procedure. Derive an alternative formalism
for finding the Jacobian based on forward propagation equations.
5.16 () The outer product approximation to the Hessian matrix for a neural network
using a sum-of-squares error function is given by (5.84). Extend this result to the
case of multiple outputs.
5.17 () Consider a squared loss function of the form
E =
1
2
{y(x, w) − t}2 p(x, t) dx dt
(5.193)
where y(x, w) is a parametric function such as a neural network. The result (1.89)
shows that the function y(x, w) that minimizes this error is given by the conditional
expectation of t given x. Use this result to show that the second derivative of E with
respect to two elements wr and ws of the vector w, is given by
∂2E
∂wr∂ws
∂y
∂wr
∂y
∂ws
p(x) dx.
(5.194)
Note that, for a finite sample from p(x), we obtain (5.84).
5.18 () Consider a two-layer network of the form shown in Figure 5.1 with the addition
of extra parameters corresponding to skip-layer connections that go directly from
the inputs to the outputs. By extending the discussion of Section 5.3.2, write down
the equations for the derivatives of the error function with respect to these additional
parameters.
5.19 () www Derive the expression (5.85) for the outer product approximation to
the Hessian matrix for a network having a single output with a logistic sigmoid
output-unit activation function and a cross-entropy error function, corresponding to
the result (5.84) for the sum-of-squares error function.
Exercises
287
5.20 () Derive an expression for the outer product approximation to the Hessian matrix
for a network having K outputs with a softmax output-unit activation function and
a cross-entropy error function, corresponding to the result (5.84) for the sum-of-
squares error function.
5.21 (  ) Extend the expression (5.86) for the outer product approximation of the Hes-
sian matrix to the case of K > 1 output units. Hence, derive a recursive expression
analogous to (5.87) for incrementing the number N of patterns and a similar expres-
sion for incrementing the number K of outputs. Use these results, together with the
identity (5.88), to find sequential update expressions analogous to (5.89) for finding
the inverse of the Hessian by incrementally including both extra patterns and extra
outputs.
5.22 ( ) Derive the results (5.93), (5.94), and (5.95) for the elements of the Hessian
matrix of a two-layer feed-forward network by application of the chain rule of cal-
culus.
5.23 ( ) Extend the results of Section 5.4.5 for the exact Hessian of a two-layer network
to include skip-layer connections that go directly from inputs to outputs.
5.24 () Verify that the network function defined by (5.113) and (5.114) is invariant un-
der the transformation (5.115) applied to the inputs, provided the weights and biases
are simultaneously transformed using (5.116) and (5.117). Similarly, show that the
network outputs can be transformed according (5.118) by applying the transforma-
tion (5.119) and (5.120) to the second-layer weights and biases.
5.25 (  ) www Consider a quadratic error function of the form
E = E0 +
1
2
(w − w)TH(w − w)
(5.195)
where w represents the minimum, and the Hessian matrix H is positive definite and
constant. Suppose the initial weight vector w(0) is chosen to be at the origin and is
updated using simple gradient descent
w(τ ) = w(τ−1) − ρ∇E
(5.196)
where τ denotes the step number, and ρ is the learning rate (which is assumed to be
small). Show that, after τ steps, the components of the weight vector parallel to the
eigenvectors of H can be written
w(τ )
j = {1 − (1 − ρηj)τ} w
j
(5.197)
where wj = wTuj, and uj and ηj are the eigenvectors and eigenvalues, respectively,
of H so that
(5.198)
Show that as τ → ∞, this gives w(τ ) → w as expected, provided |1 − ρηj| < 1.
Now suppose that training is halted after a finite number τ of steps. Show that the
Huj = ηjuj.
k
1
2
i
(5.201)
(5.202)
(5.203)
(5.204)
(5.205)
288
5. NEURAL NETWORKS
components of the weight vector parallel to the eigenvectors of the Hessian satisfy
w(τ )
j  w
|  |w
j when ηj
(ρτ)−1
j| when ηj  (ρτ)−1.
|w(τ )
j
(5.199)
(5.200)
Compare this result with the discussion in Section 3.5.3 of regularization with simple
weight decay, and hence show that (ρτ)−1 is analogous to the regularization param-
eter λ. The above results also show that the effective number of parameters in the
network, as defined by (3.91), grows as the training progresses.
5.26 ( ) Consider a multilayer perceptron with arbitrary feed-forward topology, which
is to be trained by minimizing the tangent propagation error function (5.127) in
which the regularizing function is given by (5.128). Show that the regularization
term Ω can be written as a sum over patterns of terms of the form
Ωn =
(Gyk)2
where G is a differential operator defined by
τi
G ≡
i
∂xi
By acting on the forward propagation equations
zj = h(aj),
aj =
wjizi
with the operator G, show that Ωn can be evaluated by forward propagation using
the following equations:
αj = h(aj)βj,
βj =
wjiαi.
i
where we have defined the new variables
αj ≡ Gzj,
βj ≡ Gaj.
Now show that the derivatives of Ωn with respect to a weight wrs in the network can
be written in the form
where we have defined
∂Ωn
∂wrs
k
αk {φkrzs + δkrαs}
δkr ≡
∂yk
∂ar
φkr ≡ Gδkr.
(5.206)
(5.207)
Write down the backpropagation equations for δkr, and hence derive a set of back-
propagation equations for the evaluation of the φkr.
5.27 ( ) www Consider the framework for training with transformed data in the
special case in which the transformation consists simply of the addition of random
noise x → x + ξ where ξ has a Gaussian distribution with zero mean and unit
covariance. By following an argument analogous to that of Section 5.5.5, show that
the resulting regularizer reduces to the Tikhonov form (5.135).
5.28 () www Consider a neural network, such as the convolutional network discussed
in Section 5.5.6, in which multiple weights are constrained to have the same value.
Discuss how the standard backpropagation algorithm must be modified in order to
ensure that such constraints are satisfied when evaluating the derivatives of an error
function with respect to the adjustable parameters in the network.
5.29 () www Verify the result (5.141).
5.30 () Verify the result (5.142).
5.31 () Verify the result (5.143).
5.32 ( ) Show that the derivatives of the mixing coefficients {πk}, defined by (5.146),
with respect to the auxiliary parameters {ηj} are given by
Exercises
289
∂πk
∂ηj
= δjkπj − πjπk.
(5.208)
Hence, by making use of the constraint
k πk = 1, derive the result (5.147).
5.33 () Write down a pair of equations that express the Cartesian coordinates (x1, x2)
for the robot arm shown in Figure 5.18 in terms of the joint angles θ1 and θ2 and
the lengths L1 and L2 of the links. Assume the origin of the coordinate system is
given by the attachment point of the lower arm. These equations define the ‘forward
kinematics’ of the robot arm.
5.34 () www Derive the result (5.155) for the derivative of the error function with
respect to the network output activations controlling the mixing coefficients in the
mixture density network.
5.35 () Derive the result (5.156) for the derivative of the error function with respect
to the network output activations controlling the component means in the mixture
density network.
5.36 () Derive the result (5.157) for the derivative of the error function with respect to
the network output activations controlling the component variances in the mixture
density network.
5.37 () Verify the results (5.158) and (5.160) for the conditional mean and variance of
the mixture density network model.
5.38 () Using the general result (2.115), derive the predictive distribution (5.172) for
the Laplace approximation to the Bayesian neural network model.
290
5. NEURAL NETWORKS
5.39 () www Make use of the Laplace approximation result (4.135) to show that the
evidence function for the hyperparameters α and β in the Bayesian neural network
model can be approximated by (5.175).
5.40 () www Outline the modifications needed to the framework for Bayesian neural
networks, discussed in Section 5.7.3, to handle multiclass problems using networks
having softmax output-unit activation functions.
5.41 ( ) By following analogous steps to those given in Section 5.7.1 for regression
networks, derive the result (5.183) for the marginal likelihood in the case of a net-
work having a cross-entropy error function and logistic-sigmoid output-unit activa-
tion function.
