In the case of the linear regression models discussed in Chapter 3, the maxi-
mum likelihood solution, on the assumption of a Gaussian noise model, leads to a
closed-form solution. This was a consequence of the quadratic dependence of the
log likelihood function on the parameter vector w. For logistic regression, there
is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid
function. However, the departure from a quadratic form is not substantial. To be
precise, the error function is concave, as we shall see shortly, and hence has a unique
minimum. Furthermore, the error function can be minimized by an efficient iterative
technique based on the Newton-Raphson iterative optimization scheme, which uses a
local quadratic approximation to the log likelihood function. The Newton-Raphson
update, for minimizing a function E(w), takes the form (Fletcher, 1987; Bishop and
Nabney, 2008)
(4.92)
where H is the Hessian matrix whose elements comprise the second derivatives of
E(w) with respect to the components of w.
w(new) = w(old) − H−1∇E(w).
Let us first of all apply the Newton-Raphson method to the linear regression
model (3.3) with the sum-of-squares error function (3.12). The gradient and Hessian
of this error function are given by
∇E(w) =
(wTφn − tn)φn = ΦTΦw − ΦTt
H = ∇∇E(w) =
φnφT
n = ΦTΦ
w(new) = w(old) − (ΦTΦ)−1
= (ΦTΦ)−1ΦTt
ΦTΦw(old) − ΦTt
(4.95)
which we recognize as the standard least-squares solution. Note that the error func-
tion in this case is quadratic and hence the Newton-Raphson formula gives the exact
solution in one step.
Now let us apply the Newton-Raphson update to the cross-entropy error function
(4.90) for the logistic regression model. From (4.91) we see that the gradient and
Hessian of this error function are given by
∇E(w) =
n=1
(yn − tn)φn = ΦT(y − t)
H = ∇∇E(w) =
n=1
yn(1 − yn)φnφT
n = ΦTRΦ
(4.93)
(4.94)
(4.96)
(4.97)
Section 3.1.1
where Φ is the N × M design matrix, whose nth row is given by φT
Raphson update then takes the form
n. The Newton-
208
4. LINEAR MODELS FOR CLASSIFICATION
Rnn = yn(1 − yn).
where we have made use of (4.88). Also, we have introduced the N × N diagonal
matrix R with elements
(4.98)
We see that the Hessian is no longer constant but depends on w through the weight-
ing matrix R, corresponding to the fact that the error function is no longer quadratic.
Using the property 0 < yn < 1, which follows from the form of the logistic sigmoid
function, we see that uTHu > 0 for an arbitrary vector u, and so the Hessian matrix
H is positive definite. It follows that the error function is a concave function of w
and hence has a unique minimum.
The Newton-Raphson update formula for the logistic regression model then be-
Exercise 4.15
comes
w(new) = w(old) − (ΦTRΦ)−1ΦT(y − t)
= (ΦTRΦ)−1
= (ΦTRΦ)−1ΦTRz
ΦTRΦw(old) − ΦT(y − t)
where z is an N-dimensional vector with elements
z = Φw(old) − R−1(y − t).
(4.99)
(4.100)
We see that the update formula (4.99) takes the form of a set of normal equations for a
weighted least-squares problem. Because the weighing matrix R is not constant but
depends on the parameter vector w, we must apply the normal equations iteratively,
each time using the new weight vector w to compute a revised weighing matrix
R. For this reason, the algorithm is known as iterative reweighted least squares, or
IRLS (Rubin, 1983). As in the weighted least-squares problem, the elements of the
diagonal weighting matrix R can be interpreted as variances because the mean and
variance of t in the logistic regression model are given by
E[t] = σ(x) = y
var[t] = E[t2] − E[t]2 = σ(x) − σ(x)2 = y(1 − y)
(4.101)
(4.102)
where we have used the property t2 = t for t ∈ {0, 1}. In fact, we can interpret IRLS
as the solution to a linearized problem in the space of the variable a = wTφ. The
quantity zn, which corresponds to the nth element of z, can then be given a simple
interpretation as an effective target value in this space obtained by making a local
linear approximation to the logistic sigmoid function around the current operating
point w(old)
an(w)  an(w(old)) +
nw(old) −
= φT
w(old)
dan
dyn
(yn − tn)
yn(1 − yn)
(tn − yn)
= zn.
(4.103)
N
n=1
K
N
N
K
k=1
N
K
4.3. Probabilistic Discriminative Models
209
