Now consider the extension of linear discriminants to K > 2 classes. We might
be tempted be to build a K-class discriminant by combining a number of two-class
discriminant functions. However, this leads to some serious difficulties (Duda and
Hart, 1973) as we now show.
Consider the use of K−1 classifiers each of which solves a two-class problem of
separating points in a particular class Ck from points not in that class. This is known
as a one-versus-the-rest classifier. The left-hand example in Figure 4.2 shows an
4.1. Discriminant Functions
183
C1
R1
C3
R3
R2
C3
C2
C1
C2
R1
not C1
R3
not C2
R2
C2
C1
Figure 4.2 Attempting to construct a K class discriminant from a set of two class discriminants leads to am-
biguous regions, shown in green. On the left is an example involving the use of two discriminants designed to
distinguish points in class Ck from points not in class Ck. On the right is an example involving three discriminant
functions each of which is used to separate a pair of classes Ck and Cj.
example involving three classes where this approach leads to regions of input space
that are ambiguously classified.
An alternative is to introduce K(K − 1)/2 binary discriminant functions, one
for every possible pair of classes. This is known as a one-versus-one classifier. Each
point is then classified according to a majority vote amongst the discriminant func-
tions. However, this too runs into the problem of ambiguous regions, as illustrated
in the right-hand diagram of Figure 4.2.
We can avoid these difficulties by considering a single K-class discriminant
comprising K linear functions of the form
yk(x) = wT
(4.9)
and then assigning a point x to class Ck if yk(x) > yj(x) for all j = k. The decision
boundary between class Ck and class Cj is therefore given by yk(x) = yj(x) and
hence corresponds to a (D − 1)-dimensional hyperplane defined by
k x + wk0
(wk − wj)Tx + (wk0 − wj0) = 0.
(4.10)
This has the same form as the decision boundary for the two-class case discussed in
Section 4.1.1, and so analogous geometrical properties apply.
The decision regions of such a discriminant are always singly connected and
convex. To see this, consider two points xA and xB both of which lie inside decision
x that lies on the line connecting
region Rk, as illustrated in Figure 4.3. Any point
xA and xB can be expressed in the form
x = λxA + (1 − λ)xB
(4.11)
b
184
4. LINEAR MODELS FOR CLASSIFICATION
Figure 4.3 Illustration of the decision regions for a mul-
ticlass linear discriminant, with the decision
boundaries shown in red.
If two points xA
and xB both lie inside the same decision re-
gion Rk, then any point
x that lies on the line
connecting these two points must also lie in
Rk, and hence the decision region must be
singly connected and convex.
Ri
xA
Rj
Rk
ˆx
xB
where 0 � λ � 1. From the linearity of the discriminant functions, it follows that
yk(
x) = λyk(xA) + (1 − λ)yk(xB).
(4.12)
Because both xA and xB lie inside Rk, it follows that yk(xA) > yj(xA), and
yk(xB) > yj(xB), for all j = k, and hence yk(
x also lies
inside Rk. Thus Rk is singly connected and convex.
Note that for two classes, we can either employ the formalism discussed here,
based on two discriminant functions y1(x) and y2(x), or else use the simpler but
equivalent formulation described in Section 4.1.1 based on a single discriminant
function y(x).
x), and so
x) > yj(
We now explore three approaches to learning the parameters of linear discrimi-
nant functions, based on least squares, Fisher’s linear discriminant, and the percep-
tron algorithm.
