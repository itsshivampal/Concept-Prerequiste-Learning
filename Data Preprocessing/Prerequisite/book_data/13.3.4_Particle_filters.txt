For dynamical systems which do not have a linear-Gaussian, for example, if
they use a non-Gaussian emission density, we can turn to sampling methods in order
to find a tractable inference algorithm. In particular, we can apply the sampling-
importance-resampling formalism of Section 11.1.5 to obtain a sequential Monte
Carlo algorithm known as the particle filter.
Consider the class of distributions represented by the graphical model in Fig-
ure 13.5, and suppose we are given the observed values Xn = (x1, . . . , xn) and
we wish to draw L samples from the posterior distribution p(zn|Xn). Using Bayes’
theorem, we have
f(zn)p(zn|Xn) dzn
f(zn)p(zn|xn, Xn−1) dzn
f(zn)p(xn|zn)p(zn|Xn−1) dzn
p(xn|zn)p(zn|Xn−1) dzn
E[f(zn)] =
w(l)
n f(z(l)
n )
(13.117)
where {z(l)
n } is a set of samples drawn from p(zn|Xn−1) and we have made use of
the conditional independence property p(xn|zn, Xn−1) = p(xn|zn), which follows
from the graph in Figure 13.5. The sampling weights {w(l)
n } are defined by
w(l)
n =
p(xn|z(l)
n )
m=1 p(xn|z(m)
n )
L
(13.118)
l w(l)
where the same samples are used in the numerator as in the denominator. Thus the
posterior distribution p(zn|xn) is represented by the set of samples {z(l)
n } together
n }. Note that these weights satisfy 0 � w(l)
with the corresponding weights {w(l)
n 1
and
Because we wish to find a sequential sampling scheme, we shall suppose that
a set of samples and weights have been obtained at time step n, and that we have
subsequently observed the value of xn+1, and we wish to find the weights and sam-
ples at time step n + 1. We first sample from the distribution p(zn+1|Xn). This is
n = 1.
p(zn+1|Xn) =
p(zn+1|zn, Xn)p(zn|Xn) dzn
p(zn+1|zn)p(zn|Xn) dzn
p(zn+1|zn)p(zn|xn, Xn−1) dzn
p(zn+1|zn)p(xn|zn)p(zn|Xn−1) dzn
p(xn|zn)p(zn|Xn−1) dzn
n p(zn+1|z(l)
n )
w(l)
l
(13.119)
(13.120)
(13.121)
646
13. SEQUENTIAL DATA
straightforward since, again using Bayes’ theorem
where we have made use of the conditional independence properties
p(zn+1|zn, Xn) = p(zn+1|zn)
p(xn|zn, Xn−1) = p(xn|zn)
which follow from the application of the d-separation criterion to the graph in Fig-
ure 13.5. The distribution given by (13.119) is a mixture distribution, and samples
can be drawn by choosing a component l with probability given by the mixing coef-
ficients w(l) and then drawing a sample from the corresponding component.
In summary, we can view each step of the particle filter algorithm as comprising
two stages. At time step n, we have a sample representation of the posterior dis-
n } with corresponding weights {w(l)
tribution p(zn|Xn) expressed as samples {z(l)
n }.
This can be viewed as a mixture representation of the form (13.119). To obtain the
corresponding representation for the next time step, we first draw L samples from
the mixture distribution (13.119), and then for each sample we use the new obser-
vation xn+1 to evaluate the corresponding weights w(l)
n+1). This is
illustrated, for the case of a single variable z, in Figure 13.23.
n+1 ∝ p(xn+1|z(l)
The particle filtering, or sequential Monte Carlo, approach has appeared in the
literature under various names including the bootstrap filter (Gordon et al., 1993),
survival of the fittest (Kanazawa et al., 1995), and the condensation algorithm (Isard
and Blake, 1998).
