Consider a data set of observations {xn} where n = 1, . . . , N, and xn is a
Euclidean variable with dimensionality D. Our goal is to project the data onto a
space having dimensionality M < D while maximizing the variance of the projected
data. For the moment, we shall assume that the value of M is given. Later in this
562
12. CONTINUOUS LATENT VARIABLES
chapter, we shall consider techniques to determine an appropriate value of M from
the data.
To begin with, consider the projection onto a one-dimensional space (M = 1).
We can deﬁne the direction of this space using a D-dimensional vector u1, which
for convenience (and without loss of generality) we shall choose to be a unit vector
1 u1 = 1 (note that we are only interested in the direction deﬁned by u1,
so that uT
not in the magnitude of u1 itself). Each data point xn is then projected onto a scalar
value uT
1 x where x is the sample set mean
given by
1 xn. The mean of the projected data is uT
N(cid:2)
n=1
xn
x =
1
N
1 xn − uT
uT
1 x
N(cid:2)
N(cid:2)
(cid:26)
n=1
1
N
(cid:27)2 = uT
1 Su1
(12.1)
(12.2)
(12.3)
and the variance of the projected data is given by
where S is the data covariance matrix deﬁned by
S =
1
N
n=1
(xn − x)(xn − x)T.
Appendix E
We now maximize the projected variance uT
1 Su1 with respect to u1. Clearly, this has
to be a constrained maximization to prevent (cid:5)u1(cid:5) → ∞. The appropriate constraint
1 u1 = 1. To enforce this constraint,
comes from the normalization condition uT
we introduce a Lagrange multiplier that we shall denote by λ1, and then make an
unconstrained maximization of
uT
1 Su1 + λ1
1 − uT
1 u1
(12.4)
(cid:10)
(cid:11)
By setting the derivative with respect to u1 equal to zero, we see that this quantity
will have a stationary point when
Su1 = λ1u1
(12.5)
which says that u1 must be an eigenvector of S. If we left-multiply by uT
use of uT
1 u1 = 1, we see that the variance is given by
1 and make
uT
1 Su1 = λ1
(12.6)
and so the variance will be a maximum when we set u1 equal to the eigenvector
having the largest eigenvalue λ1. This eigenvector is known as the ﬁrst principal
component.
We can deﬁne additional principal components in an incremental fashion by
choosing each new direction to be that which maximizes the projected variance
12.1. Principal Component Analysis
563
amongst all possible directions orthogonal to those already considered. If we con-
sider the general case of an M-dimensional projection space, the optimal linear pro-
jection for which the variance of the projected data is maximized is now deﬁned by
the M eigenvectors u1, . . . , uM of the data covariance matrix S corresponding to the
M largest eigenvalues λ1, . . . , λM . This is easily shown using proof by induction.
To summarize, principal component analysis involves evaluating the mean x
and the covariance matrix S of the data set and then ﬁnding the M eigenvectors of S
corresponding to the M largest eigenvalues. Algorithms for ﬁnding eigenvectors and
eigenvalues, as well as additional theorems related to eigenvector decomposition,
can be found in Golub and Van Loan (1996). Note that the computational cost of
computing the full eigenvector decomposition for a matrix of size D × D is O(D3).
If we plan to project our data onto the ﬁrst M principal components, then we only
need to ﬁnd the ﬁrst M eigenvalues and eigenvectors. This can be done with more
efﬁcient techniques, such as the power method (Golub and Van Loan, 1996), that
scale like O(M D2), or alternatively we can make use of the EM algorithm.
