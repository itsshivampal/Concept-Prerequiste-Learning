The predictive distribution for class C1, given a new feature vector φ(x), is
obtained by marginalizing with respect to the posterior distribution p(w|t), which is
itself approximated by a Gaussian distribution q(w) so that
p(C1|φ, t) =
p(C1|φ, w)p(w|t) dw 
σ(wTφ)q(w) dw
(4.145)
with the corresponding probability for class C2 given by p(C2|φ, t) = 1− p(C1|φ, t).
To evaluate the predictive distribution, we first note that the function σ(wTφ) de-
pends on w only through its projection onto φ. Denoting a = wTφ, we have
σ(wTφ) =
δ(a − wTφ)σ(a) da
where δ(·) is the Dirac delta function. From this we obtain
(4.146)
σ(wTφ)q(w) dw =
σ(a)p(a) da
(4.147)
4.5. Bayesian Logistic Regression
219
(4.148)
where
p(a) =
δ(a − wTφ)q(w) dw.
We can evaluate p(a) by noting that the delta function imposes a linear constraint
on w and so forms a marginal distribution from the joint distribution q(w) by inte-
grating out all directions orthogonal to φ. Because q(w) is Gaussian, we know from
Section 2.3.2 that the marginal distribution will also be Gaussian. We can evaluate
the mean and covariance of this distribution by taking moments, and interchanging
the order of integration over a and w, so that
µa = E[a] =
p(a)a da =
q(w)wTφ dw = wT
MAPφ
(4.149)
where we have used the result (4.144) for the variational posterior distribution q(w).
Similarly
a = var[a] =
σ2
p(a)
da
a2 − E[a]2
N φ)2
q(w)
(wTφ)2 − (mT
dw = φTSN φ.
(4.150)
Note that the distribution of a takes the same form as the predictive distribution
(3.58) for the linear regression model, with the noise variance set to zero. Thus our
variational approximation to the predictive distribution becomes
p(C1|t) =
σ(a)p(a) da =
σ(a)N (a|µa, σ2
a) da.
(4.151)
This result can also be derived directly by making use of the results for the marginal
of a Gaussian distribution given in Section 2.3.2.
The integral over a represents the convolution of a Gaussian with a logistic sig-
moid, and cannot be evaluated analytically. We can, however, obtain a good approx-
imation (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b; Barber and Bishop,
1998a) by making use of the close similarity between the logistic sigmoid function
σ(a) defined by (4.59) and the probit function Φ(a) defined by (4.114). In order to
obtain the best approximation to the logistic function we need to re-scale the hori-
zontal axis, so that we approximate σ(a) by Φ(λa). We can find a suitable value of
λ by requiring that the two functions have the same slope at the origin, which gives
λ2 = π/8. The similarity of the logistic sigmoid and the probit function, for this
choice of λ, is illustrated in Figure 4.9.
The advantage of using a probit function is that its convolution with a Gaussian
can be expressed analytically in terms of another probit function. Specifically we
can show that
Φ(λa)N (a|µ, σ2) da = Φ
(λ−2 + σ2)1/2
(4.152)
Exercise 4.24
Exercise 4.25
Exercise 4.26
n
220
4. LINEAR MODELS FOR CLASSIFICATION
We now apply the approximation σ(a)  Φ(λa) to the probit functions appearing
on both sides of this equation, leading to the following approximation for the convo-
lution of a logistic sigmoid with a Gaussian
σ(a)N (a|µ, σ2) da  σ
κ(σ2)µ
(4.153)
where we have defined
(4.154)
Applying this result to (4.151) we obtain the approximate predictive distribution
κ(σ2) = (1 + πσ2/8)−1/2.
in the form
κ(σ2
a)µa
p(C1|φ, t) = σ
where µa and σ2
fined by (4.154).
a are defined by (4.149) and (4.150), respectively, and κ(σ2
(4.155)
a) is de-
Note that the decision boundary corresponding to p(C1|φ, t) = 0.5 is given by
µa = 0, which is the same as the decision boundary obtained by using the MAP
value for w. Thus if the decision criterion is based on minimizing misclassifica-
tion rate, with equal prior probabilities, then the marginalization over w has no ef-
fect. However, for more complex decision criteria it will play an important role.
Marginalization of the logistic sigmoid model under a Gaussian approximation to
the posterior distribution will be illustrated in the context of variational inference in
Figure 10.13.
