At this point, it is instructive to consider the geometrical interpretation of the
least-squares solution. To do this we consider an N-dimensional space whose axes
are given by the tn, so that t = (t1, . . . , tN )T is a vector in this space. Each basis
function φj(xn), evaluated at the N data points, can also be represented as a vector in
the same space, denoted by ϕj, as illustrated in Figure 3.2. Note that ϕj corresponds
to the jth column of Φ, whereas φ(xn) corresponds to the nth row of Φ. If the
number M of basis functions is smaller than the number N of data points, then the
M vectors φj(xn) will span a linear subspace S of dimensionality M. We define
y to be an N-dimensional vector whose nth element is given by y(xn, w), where
n = 1, . . . , N. Because y is an arbitrary linear combination of the vectors ϕj, it can
live anywhere in the M-dimensional subspace. The sum-of-squares error (3.12) is
then equal (up to a factor of 1/2) to the squared Euclidean distance between y and
t. Thus the least-squares solution for w corresponds to that choice of y that lies in
subspace S and that is closest to t. Intuitively, from Figure 3.2, we anticipate that
this solution corresponds to the orthogonal projection of t onto the subspace S. This
is indeed the case, as can easily be verified by noting that the solution for y is given
by ΦwML, and then confirming that this takes the form of an orthogonal projection.
In practice, a direct solution of the normal equations can lead to numerical diffi-
culties when ΦTΦ is close to singular. In particular, when two or more of the basis
vectors ϕj are co-linear, or nearly so, the resulting parameter values can have large
magnitudes. Such near degeneracies will not be uncommon when dealing with real
data sets. The resulting numerical difficulties can be addressed using the technique
of singular value decomposition, or SVD (Press et al., 1992; Bishop and Nabney,
2008). Note that the addition of a regularization term ensures that the matrix is non-
singular, even in the presence of degeneracies.
