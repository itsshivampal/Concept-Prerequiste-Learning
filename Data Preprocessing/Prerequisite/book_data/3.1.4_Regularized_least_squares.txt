In Section 1.1, we introduced the idea of adding a regularization term to an
error function in order to control over-fitting, so that the total error function to be
minimized takes the form
ED(w) + λEW (w)
(3.24)
where λ is the regularization coefficient that controls the relative importance of the
data-dependent error ED(w) and the regularization term EW (w). One of the sim-
plest forms of regularizer is given by the sum-of-squares of the weight vector ele-
ments
If we also consider the sum-of-squares error function given by
EW (w) =
wTw.
1
2
then the total error function becomes
E(w) =
1
2
{tn − wTφ(xn)}2
n=1
1
2
{tn − wTφ(xn)}2 + λ
2
wTw.
This particular choice of regularizer is known in the machine learning literature as
weight decay because in sequential learning algorithms, it encourages weight values
to decay towards zero, unless supported by the data. In statistics, it provides an ex-
ample of a parameter shrinkage method because it shrinks parameter values towards
(3.25)
(3.26)
(3.27)
M
N
n=1
M
3.1. Linear Basis Function Models
145
q = 0.5
q = 1
q = 2
q = 4
Figure 3.3 Contours of the regularization term in (3.29) for various values of the parameter q.
zero. It has the advantage that the error function remains a quadratic function of
w, and so its exact minimizer can be found in closed form. Specifically, setting the
gradient of (3.27) with respect to w to zero, and solving for w as before, we obtain
w =
λI + ΦTΦ
−1 ΦTt.
(3.28)
This represents a simple extension of the least-squares solution (3.15).
A more general regularizer is sometimes used, for which the regularized error
takes the form
1
2
{tn − wTφ(xn)}2 + λ
2
|wj|q
j=1
(3.29)
where q = 2 corresponds to the quadratic regularizer (3.27). Figure 3.3 shows con-
tours of the regularization function for different values of q.
The case of q = 1 is know as the lasso in the statistics literature (Tibshirani,
1996).
It has the property that if λ is sufficiently large, some of the coefficients
wj are driven to zero, leading to a sparse model in which the corresponding basis
functions play no role. To see this, we first note that minimizing (3.29) is equivalent
to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint
|wj|q � η
j=1
(3.30)
for an appropriate value of the parameter η, where the two approaches can be related
using Lagrange multipliers. The origin of the sparsity can be seen from Figure 3.4,
which shows that the minimum of the error function, subject to the constraint (3.30).
As λ is increased, so an increasing number of parameters are driven to zero.
Regularization allows complex models to be trained on data sets of limited size
without severe over-fitting, essentially by limiting the effective model complexity.
However, the problem of determining the optimal model complexity is then shifted
from one of finding the appropriate number of basis functions to one of determining
a suitable value of the regularization coefficient λ. We shall return to the issue of
model complexity later in this chapter.
Exercise 3.5
Appendix E
N
N
146
3. LINEAR MODELS FOR REGRESSION
w2
Figure 3.4 Plot of
the contours
of the unregularized error function
(blue) along with the constraint re-
gion (3.30) for the quadratic regular-
izer q = 2 on the left and the lasso
regularizer q = 1 on the right,
in
which the optimum value for the pa-
rameter vector w is denoted by w.
The lasso gives a sparse solution in
which w
1 = 0.
w
w2
w
w1
w1
For the remainder of this chapter we shall focus on the quadratic regularizer
(3.27) both for its practical importance and its analytical tractability.
