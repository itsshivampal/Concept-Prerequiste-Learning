In Chapters 3 and 4 we considered models for regression and classification that com-
prised linear combinations of fixed basis functions. We saw that such models have
useful analytical and computational properties but that their practical applicability
was limited by the curse of dimensionality. In order to apply such models to large-
scale problems, it is necessary to adapt the basis functions to the data.
Support vector machines (SVMs), discussed in Chapter 7, address this by first
defining basis functions that are centred on the training data points and then selecting
a subset of these during training. One advantage of SVMs is that, although the
training involves nonlinear optimization, the objective function is convex, and so the
solution of the optimization problem is relatively straightforward. The number of
basis functions in the resulting models is generally much smaller than the number of
training points, although it is often still relatively large and typically increases with
the size of the training set. The relevance vector machine, discussed in Section 7.2,
also chooses a subset from a fixed set of basis functions and typically results in much
225
226
5. NEURAL NETWORKS
sparser models. Unlike the SVM it also produces probabilistic outputs, although this
is at the expense of a nonconvex optimization during training.
An alternative approach is to fix the number of basis functions in advance but
allow them to be adaptive, in other words to use parametric forms for the basis func-
tions in which the parameter values are adapted during training. The most successful
model of this type in the context of pattern recognition is the feed-forward neural
network, also known as the multilayer perceptron, discussed in this chapter. In fact,
‘multilayer perceptron’ is really a misnomer, because the model comprises multi-
ple layers of logistic regression models (with continuous nonlinearities) rather than
multiple perceptrons (with discontinuous nonlinearities). For many applications, the
resulting model can be significantly more compact, and hence faster to evaluate, than
a support vector machine having the same generalization performance. The price to
be paid for this compactness, as with the relevance vector machine, is that the like-
lihood function, which forms the basis for network training, is no longer a convex
function of the model parameters. In practice, however, it is often worth investing
substantial computational resources during the training phase in order to obtain a
compact model that is fast at processing new data.
The term ‘neural network’ has its origins in attempts to find mathematical rep-
resentations of information processing in biological systems (McCulloch and Pitts,
1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986). Indeed,
it has been used very broadly to cover a wide range of different models, many of
which have been the subject of exaggerated claims regarding their biological plau-
sibility. From the perspective of practical applications of pattern recognition, how-
ever, biological realism would impose entirely unnecessary constraints. Our focus in
this chapter is therefore on neural networks as efficient models for statistical pattern
recognition. In particular, we shall restrict our attention to the specific class of neu-
ral networks that have proven to be of greatest practical value, namely the multilayer
perceptron.
We begin by considering the functional form of the network model, including
the specific parameterization of the basis functions, and we then discuss the prob-
lem of determining the network parameters within a maximum likelihood frame-
work, which involves the solution of a nonlinear optimization problem. This requires
the evaluation of derivatives of the log likelihood function with respect to the net-
work parameters, and we shall see how these can be obtained efficiently using the
technique of error backpropagation. We shall also show how the backpropagation
framework can be extended to allow other derivatives to be evaluated, such as the
Jacobian and Hessian matrices. Next we discuss various approaches to regulariza-
tion of neural network training and the relationships between them. We also consider
some extensions to the neural network model, and in particular we describe a gen-
eral framework for modelling conditional probability distributions known as mixture
density networks. Finally, we discuss the use of Bayesian treatments of neural net-
works. Additional background on neural network models can be found in Bishop
(1995a).
M
i=1
D
M
