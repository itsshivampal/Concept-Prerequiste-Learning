We now discuss an alternative formulation of PCA based on projection error
minimization. To do this, we introduce a complete orthonormal set of D-dimensional
basis vectors {ui} where i = 1, . . . , D that satisfy
uT
i uj = δij.
(12.7)
Exercise 12.1
Section 12.2.2
Appendix C
Because this basis is complete, each data point can be represented exactly by a linear
combination of the basis vectors
xn =
αniui
(12.8)
where the coefﬁcients αni will be different for different data points. This simply
corresponds to a rotation of the coordinate system to a new system deﬁned by the
{ui}, and the original D components {xn1, . . . , xnD} are replaced by an equivalent
set {αn1, . . . , αnD}. Taking the inner product with uj, and making use of the or-
thonormality property, we obtain αnj = xT
nuj, and so without loss of generality we
can write
xn =
xT
nui
ui.
(12.9)
Our goal, however, is to approximate this data point using a representation in-
volving a restricted number M < D of variables corresponding to a projection onto
a lower-dimensional subspace. The M-dimensional linear subspace can be repre-
sented, without loss of generality, by the ﬁrst M of the basis vectors, and so we
approximate each data point xn by
D(cid:2)
i=1
D(cid:2)
(cid:10)
i=1
(cid:11)
D(cid:2)
M(cid:2)
(cid:4)xn =
zniui +
biui
(12.10)
i=1
i=M +1
564
12. CONTINUOUS LATENT VARIABLES
N(cid:2)
1
N
(cid:5)xn −(cid:4)xn(cid:5)2.
where the {zni} depend on the particular data point, whereas the {bi} are constants
that are the same for all data points. We are free to choose the {ui}, the {zni}, and
the {bi} so as to minimize the distortion introduced by the reduction in dimensional-
ity. As our distortion measure, we shall use the squared distance between the original
data point xn and its approximation(cid:4)xn, averaged over the data set, so that our goal
is to minimize
J =
(12.11)
Consider ﬁrst of all the minimization with respect to the quantities {zni}. Sub-
stituting for(cid:4)xn, setting the derivative with respect to znj to zero, and making use of
n=1
the orthonormality conditions, we obtain
znj = xT
nuj
(12.12)
where j = 1, . . . , M. Similarly, setting the derivative of J with respect to bi to zero,
and again making use of the orthonormality relations, gives
bj = xTuj
(12.13)
where j = M +1, . . . , D. If we substitute for zni and bi, and make use of the general
expansion (12.9), we obtain
D(cid:2)
(cid:26)
xn −(cid:4)xn =
(cid:27)
(xn − x)Tui
ui
(12.14)
from which we see that the displacement vector from xn to (cid:4)xn lies in the space
projected points(cid:4)xn must lie within the principal subspace, but we can move them
orthogonal to the principal subspace, because it is a linear combination of {ui} for
i = M + 1, . . . , D, as illustrated in Figure 12.2. This is to be expected because the
i=M +1
We therefore obtain an expression for the distortion measure J as a function
freely within that subspace, and so the minimum error is given by the orthogonal
projection.
purely of the {ui} in the form
D(cid:2)
N(cid:2)
D(cid:2)
(cid:10)
(cid:11)2 =
nui − xTui
xT
uT
i Sui.
(12.15)
n=1
i=M +1
i=M +1
There remains the task of minimizing J with respect to the {ui}, which must
be a constrained minimization otherwise we will obtain the vacuous result ui = 0.
The constraints arise from the orthonormality conditions and, as we shall see, the
solution will be expressed in terms of the eigenvector expansion of the covariance
matrix. Before considering a formal solution, let us try to obtain some intuition about
the result by considering the case of a two-dimensional data space D = 2 and a one-
dimensional principal subspace M = 1. We have to choose a direction u2 so as to
J =
1
N
12.1. Principal Component Analysis
565
(cid:10)
1 − uT
2 u2
(cid:11)
(cid:4)J = uT
minimize J = uT
Lagrange multiplier λ2 to enforce the constraint, we consider the minimization of
2 Su2, subject to the normalization constraint uT
2 u2 = 1. Using a
2 Su2 + λ2
(12.16)
Setting the derivative with respect to u2 to zero, we obtain Su2 = λ2u2 so that u2
is an eigenvector of S with eigenvalue λ2. Thus any eigenvector will deﬁne a sta-
tionary point of the distortion measure. To ﬁnd the value of J at the minimum, we
back-substitute the solution for u2 into the distortion measure to give J = λ2. We
therefore obtain the minimum value of J by choosing u2 to be the eigenvector corre-
sponding to the smaller of the two eigenvalues. Thus we should choose the principal
subspace to be aligned with the eigenvector having the larger eigenvalue. This result
accords with our intuition that, in order to minimize the average squared projection
distance, we should choose the principal component subspace to pass through the
mean of the data points and to be aligned with the directions of maximum variance.
For the case when the eigenvalues are equal, any choice of principal direction will
give rise to the same value of J.
The general solution to the minimization of J for arbitrary D and arbitrary M <
D is obtained by choosing the {ui} to be eigenvectors of the covariance matrix given
by
(12.17)
where i = 1, . . . , D, and as usual the eigenvectors {ui} are chosen to be orthonor-
mal. The corresponding value of the distortion measure is then given by
Sui = λiui
D(cid:2)
J =
λi
i=M +1
(12.18)
which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal
to the principal subspace. We therefore obtain the minimum value of J by selecting
these eigenvectors to be those having the D − M smallest eigenvalues, and hence
the eigenvectors deﬁning the principal subspace are those corresponding to the M
largest eigenvalues.
Although we have considered M < D, the PCA analysis still holds if M =
D, in which case there is no dimensionality reduction but simply a rotation of the
coordinate axes to align with principal components.
Finally, it is worth noting that there exists a closely related linear dimensionality
reduction technique called canonical correlation analysis, or CCA (Hotelling, 1936;
Bach and Jordan, 2002). Whereas PCA works with a single random variable, CCA
considers two (or more) variables and tries to ﬁnd a corresponding pair of linear
subspaces that have high cross-correlation, so that each component within one of the
subspaces is correlated with a single component from the other subspace. Its solution
can be expressed in terms of a generalized eigenvector problem.
