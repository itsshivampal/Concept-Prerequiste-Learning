Batch techniques, such as the maximum likelihood solution (3.15), which in-
volve processing the entire training set in one go, can be computationally costly for
large data sets. As we have discussed in Chapter 1, if the data set is sufficiently large,
it may be worthwhile to use sequential algorithms, also known as on-line algorithms,
Exercise 3.2
144
3. LINEAR MODELS FOR REGRESSION
N
N
n=1
in which the data points are considered one at a time, and the model parameters up-
dated after each such presentation. Sequential learning is also appropriate for real-
time applications in which the data observations are arriving in a continuous stream,
and predictions must be made before all of the data points are seen.
We can obtain a sequential learning algorithm by applying the technique of
stochastic gradient descent, also known as sequential gradient descent, as follows. If
the error function comprises a sum over data points E =
n En, then after presen-
tation of pattern n, the stochastic gradient descent algorithm updates the parameter
vector w using
(3.22)
where τ denotes the iteration number, and η is a learning rate parameter. We shall
discuss the choice of value for η shortly. The value of w is initialized to some starting
vector w(0). For the case of the sum-of-squares error function (3.12), this gives
w(τ +1) = w(τ ) − η∇En
w(τ +1) = w(τ ) + η(tn − w(τ )Tφn)φn
(3.23)
where φn = φ(xn). This is known as least-mean-squares or the LMS algorithm.
The value of η needs to be chosen with care to ensure that the algorithm converges
(Bishop and Nabney, 2008).
