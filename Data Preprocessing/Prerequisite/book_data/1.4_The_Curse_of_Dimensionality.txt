33
Figure 1.18 The technique of S-fold cross-validation,
illus-
trated here for the case of S = 4, involves tak-
ing the available data and partitioning it into S
groups (in the simplest case these are of equal
size). Then S − 1 of the groups are used to train
a set of models that are then evaluated on the re-
maining group. This procedure is then repeated
for all S possible choices for the held-out group,
indicated here by the red blocks, and the perfor-
mance scores from the S runs are then averaged.
run 1
run 2
run 3
run 4
data to assess performance. When data is particularly scarce, it may be appropriate
to consider the case S = N, where N is the total number of data points, which gives
the leave-one-out technique.
One major drawback of cross-validation is that the number of training runs that
must be performed is increased by a factor of S, and this can prove problematic for
models in which the training is itself computationally expensive. A further problem
with techniques such as cross-validation that use separate data to assess performance
is that we might have multiple complexity parameters for a single model (for in-
stance, there might be several regularization parameters). Exploring combinations
of settings for such parameters could, in the worst case, require a number of training
runs that is exponential in the number of parameters. Clearly, we need a better ap-
proach. Ideally, this should rely only on the training data and should allow multiple
hyperparameters and model types to be compared in a single training run. We there-
fore need to find a measure of performance which depends only on the training data
and which does not suffer from bias due to over-fitting.
Historically various ‘information criteria’ have been proposed that attempt to
correct for the bias of maximum likelihood by the addition of a penalty term to
compensate for the over-fitting of more complex models. For example, the Akaike
information criterion, or AIC (Akaike, 1974), chooses the model for which the quan-
tity
ln p(D|wML) − M
(1.73)
is largest. Here p(D|wML) is the best-fit log likelihood, and M is the number of
adjustable parameters in the model. A variant of this quantity, called the Bayesian
information criterion, or BIC, will be discussed in Section 4.4.1. Such criteria do
not take account of the uncertainty in the model parameters, however, and in practice
they tend to favour overly simple models. We therefore turn in Section 3.4 to a fully
Bayesian approach where we shall see how complexity penalties arise in a natural
and principled way.
1.4. The Curse of Dimensionality
In the polynomial curve fitting example we had just one input variable x. For prac-
tical applications of pattern recognition, however, we will have to deal with spaces
34
1. INTRODUCTION
Figure 1.19 Scatter plot of the oil flow data
for input variables x6 and x7, in
which red denotes the ‘homoge-
nous’ class, green denotes the
‘annular’ class, and blue denotes
the ‘laminar’ class. Our goal
is
to classify the new test point de-
noted by ‘×’.
2
1.5
x7
1
0.5
0
0
0.25
0.5
x6
0.75
1
of high dimensionality comprising many input variables. As we now discuss, this
poses some serious challenges and is an important factor influencing the design of
pattern recognition techniques.
In order to illustrate the problem we consider a synthetically generated data set
representing measurements taken from a pipeline containing a mixture of oil, wa-
ter, and gas (Bishop and James, 1993). These three materials can be present in one
of three different geometrical configurations known as ‘homogenous’, ‘annular’, and
‘laminar’, and the fractions of the three materials can also vary. Each data point com-
prises a 12-dimensional input vector consisting of measurements taken with gamma
ray densitometers that measure the attenuation of gamma rays passing along nar-
row beams through the pipe. This data set is described in detail in Appendix A.
Figure 1.19 shows 100 points from this data set on a plot showing two of the mea-
surements x6 and x7 (the remaining ten input values are ignored for the purposes of
this illustration). Each data point is labelled according to which of the three geomet-
rical classes it belongs to, and our goal is to use this data as a training set in order to
be able to classify a new observation (x6, x7), such as the one denoted by the cross
in Figure 1.19. We observe that the cross is surrounded by numerous red points, and
so we might suppose that it belongs to the red class. However, there are also plenty
of green points nearby, so we might think that it could instead belong to the green
class. It seems unlikely that it belongs to the blue class. The intuition here is that the
identity of the cross should be determined more strongly by nearby points from the
training set and less strongly by more distant points. In fact, this intuition turns out
to be reasonable and will be discussed more fully in later chapters.
How can we turn this intuition into a learning algorithm? One very simple ap-
proach would be to divide the input space into regular cells, as indicated in Fig-
ure 1.20. When we are given a test point and we wish to predict its class, we first
decide which cell it belongs to, and we then find all of the training data points that
1.4. The Curse of Dimensionality
35
Figure 1.20 Illustration of a simple approach
to the solution of a classification
problem in which the input space
is divided into cells and any new
test point is assigned to the class
that has a majority number of rep-
resentatives in the same cell as
the test point. As we shall see
shortly,
this simplistic approach
has some severe shortcomings.
2
1.5
x7
1
0.5
0
0
0.25
0.5
x6
0.75
1
fall in the same cell. The identity of the test point is predicted as being the same
as the class having the largest number of training points in the same cell as the test
point (with ties being broken at random).
There are numerous problems with this naive approach, but one of the most se-
vere becomes apparent when we consider its extension to problems having larger
numbers of input variables, corresponding to input spaces of higher dimensionality.
The origin of the problem is illustrated in Figure 1.21, which shows that, if we divide
a region of a space into regular cells, then the number of such cells grows exponen-
tially with the dimensionality of the space. The problem with an exponentially large
number of cells is that we would need an exponentially large quantity of training data
in order to ensure that the cells are not empty. Clearly, we have no hope of applying
such a technique in a space of more than a few variables, and so we need to find a
more sophisticated approach.
We can gain further insight into the problems of high-dimensional spaces by
returning to the example of polynomial curve fitting and considering how we would
Section 1.1
of
Figure 1.21 Illustration
the
curse of dimensionality, showing
how the number of regions of a
regular grid grows exponentially
with the dimensionality D of
the
space. For clarity, only a subset of
the cubical regions are shown for
D = 3.
x2
x2
x1
D = 1
x1
x3
D = 2
D = 3
x1
D
D
D
D
D
D
36
1. INTRODUCTION
extend this approach to deal with input spaces having several variables. If we have
D input variables, then a general polynomial with coefficients up to order 3 would
take the form
Exercise 1.16
y(x, w) = w0 +
wixi +
wijxixj +
wijkxixjxk.
(1.74)
i=1
i=1
j=1
i=1
j=1
k=1
As D increases, so the number of independent coefficients (not all of the coefficients
are independent due to interchange symmetries amongst the x variables) grows pro-
portionally to D3. In practice, to capture complex dependencies in the data, we may
need to use a higher-order polynomial. For a polynomial of order M, the growth in
the number of coefficients is like DM . Although this is now a power law growth,
rather than an exponential growth, it still points to the method becoming rapidly
unwieldy and of limited practical utility.
Our geometrical intuitions, formed through a life spent in a space of three di-
mensions, can fail badly when we consider spaces of higher dimensionality. As a
simple example, consider a sphere of radius r = 1 in a space of D dimensions, and
ask what is the fraction of the volume of the sphere that lies between radius r = 1−
and r = 1. We can evaluate this fraction by noting that the volume of a sphere of
radius r in D dimensions must scale as rD, and so we write
VD(r) = KDrD
(1.75)
Exercise 1.18
where the constant KD depends only on D. Thus the required fraction is given by
Exercise 1.20
VD(1) − VD(1 − 	)
VD(1)
= 1 − (1 − 	)D
(1.76)
which is plotted as a function of 	 for various values of D in Figure 1.22. We see
that, for large D, this fraction tends to 1 even for small values of 	. Thus, in spaces
of high dimensionality, most of the volume of a sphere is concentrated in a thin shell
near the surface!
As a further example, of direct relevance to pattern recognition, consider the
behaviour of a Gaussian distribution in a high-dimensional space. If we transform
from Cartesian to polar coordinates, and then integrate out the directional variables,
we obtain an expression for the density p(r) as a function of radius r from the origin.
Thus p(r)δr is the probability mass inside a thin shell of thickness δr located at
radius r. This distribution is plotted, for various values of D, in Figure 1.23, and we
see that for large D the probability mass of the Gaussian is concentrated in a thin
shell.
The severe difficulty that can arise in spaces of many dimensions is sometimes
called the curse of dimensionality (Bellman, 1961). In this book, we shall make ex-
tensive use of illustrative examples involving input spaces of one or two dimensions,
because this makes it particularly easy to illustrate the techniques graphically. The
reader should be warned, however, that not all intuitions developed in spaces of low
dimensionality will generalize to spaces of many dimensions.
1.4. The Curse of Dimensionality
37
Figure 1.22 Plot of the fraction of the volume of
a sphere lying in the range r = 1−
to r = 1 for various values of the
dimensionality D.
n
o
i
t
c
a
r
f
e
m
u
o
v
l
1
0.8
0.6
0.4
0.2
0
0
D = 20
D = 5
D = 2
D = 1
0.2
0.4
0.6
0.8
1
Although the curse of dimensionality certainly raises important issues for pat-
tern recognition applications, it does not prevent us from finding effective techniques
applicable to high-dimensional spaces. The reasons for this are twofold. First, real
data will often be confined to a region of the space having lower effective dimension-
ality, and in particular the directions over which important variations in the target
variables occur may be so confined. Second, real data will typically exhibit some
smoothness properties (at least locally) so that for the most part small changes in the
input variables will produce small changes in the target variables, and so we can ex-
ploit local interpolation-like techniques to allow us to make predictions of the target
variables for new values of the input variables. Successful pattern recognition tech-
niques exploit one or both of these properties. Consider, for example, an application
in manufacturing in which images are captured of identical planar objects on a con-
veyor belt, in which the goal is to determine their orientation. Each image is a point
Figure 1.23 Plot of the probability density with
respect
to radius r of a Gaus-
sian distribution for various values
of
In a
high-dimensional space, most of the
probability mass of a Gaussian is lo-
cated within a thin shell at a specific
radius.
the dimensionality D.
2
r
p
1
D = 1
D = 2
D = 20
0
0
2
r
4
38
1. INTRODUCTION
in a high-dimensional space whose dimensionality is determined by the number of
pixels. Because the objects can occur at different positions within the image and
in different orientations, there are three degrees of freedom of variability between
images, and a set of images will live on a three dimensional manifold embedded
within the high-dimensional space. Due to the complex relationships between the
object position or orientation and the pixel intensities, this manifold will be highly
nonlinear. If the goal is to learn a model that can take an input image and output the
orientation of the object irrespective of its position, then there is only one degree of
freedom of variability within the manifold that is significant.
