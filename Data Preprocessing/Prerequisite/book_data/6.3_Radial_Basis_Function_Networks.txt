299
Section 12.1.3
Section 6.4.7
This is the covariance matrix of the Fisher scores, and so the Fisher kernel corre-
sponds to a whitening of these scores. More simply, we can just omit the Fisher
information matrix altogether and use the noninvariant kernel
k(x, x) = g(θ, x)Tg(θ, x).
(6.36)
An application of Fisher kernels to document retrieval is given by Hofmann (2000).
A final example of a kernel function is the sigmoidal kernel given by
k(x, x) = tanh
axTx + b
(6.37)
whose Gram matrix in general is not positive semidefinite. This form of kernel
has, however, been used in practice (Vapnik, 1995), possibly because it gives kernel
expansions such as the support vector machine a superficial resemblance to neural
network models. As we shall see, in the limit of an infinite number of basis functions,
a Bayesian neural network with an appropriate prior reduces to a Gaussian process,
thereby providing a deeper link between neural networks and kernel methods.
6.3. Radial Basis Function Networks
In Chapter 3, we discussed regression models based on linear combinations of fixed
basis functions, although we did not discuss in detail what form those basis functions
might take. One choice that has been widely used is that of radial basis functions,
which have the property that each basis function depends only on the radial distance
(typically Euclidean) from a centre µj, so that φj(x) = h(x − µj).
Historically, radial basis functions were introduced for the purpose of exact func-
tion interpolation (Powell, 1987). Given a set of input vectors {x1, . . . , xN} along
with corresponding target values {t1, . . . , tN}, the goal is to find a smooth function
f(x) that fits every target value exactly, so that f(xn) = tn for n = 1, . . . , N. This
is achieved by expressing f(x) as a linear combination of radial basis functions, one
centred on every data point
f(x) =
wnh(x − xn).
n=1
(6.38)
The values of the coefficients {wn} are found by least squares, and because there
are the same number of coefficients as there are constraints, the result is a function
that fits every target value exactly. In pattern recognition applications, however, the
target values are generally noisy, and exact interpolation is undesirable because this
corresponds to an over-fitted solution.
Expansions in radial basis functions also arise from regularization theory (Pog-
gio and Girosi, 1990; Bishop, 1995a). For a sum-of-squares error function with a
regularizer defined in terms of a differential operator, the optimal solution is given
by an expansion in the Green’s functions of the operator (which are analogous to the
eigenvectors of a discrete matrix), again with one basis function centred on each data
N
n=1
n=1
N
300
6. KERNEL METHODS
point. If the differential operator is isotropic then the Green’s functions depend only
on the radial distance from the corresponding data point. Due to the presence of the
regularizer, the solution no longer interpolates the training data exactly.
Another motivation for radial basis functions comes from a consideration of
the interpolation problem when the input (rather than the target) variables are noisy
If the noise on the input variable x is described
(Webb, 1994; Bishop, 1995a).
by a variable ξ having a distribution ν(ξ), then the sum-of-squares error function
becomes
E =
1
2
{y(xn + ξ) − tn}2 ν(ξ) dξ.
Appendix D
Exercise 6.17
Using the calculus of variations, we can optimize with respect to the function f(x)
to give
(6.39)
(6.40)
(6.41)
where the basis functions are given by
y(xn) =
tnh(x − xn)
h(x − xn) = ν(x − xn)
ν(x − xn)
N
n=1
We see that there is one basis function centred on every data point. This is known as
the Nadaraya-Watson model and will be derived again from a different perspective
in Section 6.3.1. If the noise distribution ν(ξ) is isotropic, so that it is a function
only of ξ, then the basis functions will be radial.
n h(x − xn) = 1
for any value of x. The effect of such normalization is shown in Figure 6.2. Normal-
ization is sometimes used in practice as it avoids having regions of input space where
all of the basis functions take small values, which would necessarily lead to predic-
tions in such regions that are either small or controlled purely by the bias parameter.
Note that the basis functions (6.41) are normalized, so that
Another situation in which expansions in normalized radial basis functions arise
is in the application of kernel density estimation to the problem of regression, as we
shall discuss in Section 6.3.1.
Because there is one basis function associated with every data point, the corre-
sponding model can be computationally costly to evaluate when making predictions
for new data points. Models have therefore been proposed (Broomhead and Lowe,
1988; Moody and Darken, 1989; Poggio and Girosi, 1990), which retain the expan-
sion in radial basis functions but where the number M of basis functions is smaller
than the number N of data points. Typically, the number of basis functions, and the
locations µi of their centres, are determined based on the input data {xn} alone. The
basis functions are then kept fixed and the coefficients {wi} are determined by least
squares by solving the usual set of linear equations, as discussed in Section 3.1.1.
6.3. Radial Basis Function Networks
301
1
0.8
0.6
0.4
0.2
0
−1
−0.5
0
0.5
1
1
0.8
0.6
0.4
0.2
0
−1
Figure 6.2 Plot of a set of Gaussian basis functions on the left, together with the corresponding normalized
basis functions on the right.
−0.5
0
0.5
1
N
Section 9.1
Section 2.5.1
One of the simplest ways of choosing basis function centres is to use a randomly
chosen subset of the data points. A more systematic approach is called orthogonal
least squares (Chen et al., 1991). This is a sequential selection process in which at
each step the next data point to be chosen as a basis function centre corresponds to
the one that gives the greatest reduction in the sum-of-squares error. Values for the
expansion coefficients are determined as part of the algorithm. Clustering algorithms
such as K-means have also been used, which give a set of basis function centres that
no longer coincide with training data points.
