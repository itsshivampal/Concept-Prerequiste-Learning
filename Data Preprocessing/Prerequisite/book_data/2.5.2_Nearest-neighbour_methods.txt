One of the difficulties with the kernel approach to density estimation is that the
parameter h governing the kernel width is fixed for all kernels. In regions of high
data density, a large value of h may lead to over-smoothing and a washing out of
structure that might otherwise be extracted from the data. However, reducing h may
lead to noisy estimates elsewhere in data space where the density is smaller. Thus
the optimal choice for h may be dependent on location within the data space. This
issue is addressed by nearest-neighbour methods for density estimation.
We therefore return to our general result (2.246) for local density estimation,
and instead of fixing V and determining the value of K from the data, we consider
a fixed value of K and use the data to find an appropriate value for V . To do this,
we consider a small sphere centred on the point x at which we wish to estimate the
2.5. Nonparametric Methods
Figure 2.26 Illustration of K-nearest-neighbour den-
sity estimation using the same data set
as in Figures 2.25 and 2.24. We see
that the parameter K governs the degree
of smoothing, so that a small value of
K leads to a very noisy density model
(top panel), whereas a large value (bot-
tom panel) smoothes out the bimodal na-
ture of the true distribution (shown by the
green curve) from which the data set was
generated.
K = 1
K = 5
K = 30
5
0
5
0
0
5
0
0
0
125
1
1
1
0.5
0.5
0.5
Exercise 2.61
density p(x), and we allow the radius of the sphere to grow until it contains precisely
K data points. The estimate of the density p(x) is then given by (2.246) with V set to
the volume of the resulting sphere. This technique is known as K nearest neighbours
and is illustrated in Figure 2.26, for various choices of the parameter K, using the
same data set as used in Figure 2.24 and Figure 2.25. We see that the value of K
now governs the degree of smoothing and that again there is an optimum choice for
K that is neither too large nor too small. Note that the model produced by K nearest
neighbours is not a true density model because the integral over all space diverges.
We close this chapter by showing how the K-nearest-neighbour technique for
density estimation can be extended to the problem of classification. To do this, we
apply the K-nearest-neighbour density estimation technique to each class separately
and then make use of Bayes’ theorem. Let us suppose that we have a data set com-
k Nk = N. If we
prising Nk points in class Ck with N points in total, so that
wish to classify a new point x, we draw a sphere centred on x containing precisely
K points irrespective of their class. Suppose this sphere has volume V and contains
Kk points from class Ck. Then (2.246) provides an estimate of the density associated
with each class
(2.253)
p(x|Ck) = Kk
NkV
Similarly, the unconditional density is given by
p(x) = K
N V
while the class priors are given by
p(Ck) = Nk
N
(2.254)
(2.255)
We can now combine (2.253), (2.254), and (2.255) using Bayes’ theorem to obtain
the posterior probability of class membership
p(Ck|x) = p(x|Ck)p(Ck)
p(x)
= Kk
K
(2.256)
126
2. PROBABILITY DISTRIBUTIONS
x2
x2
Figure 2.27 (a) In the K-nearest-
neighbour classifier, a new point,
shown by the black diamond, is clas-
sified according to the majority class
membership of the K closest train-
ing data points, in this case K =
3.
In the nearest-neighbour
(K = 1) approach to classification,
the resulting decision boundary is
composed of hyperplanes that form
perpendicular bisectors of pairs of
points from different classes.
(b)
x1
x1
(a)
(b)
If we wish to minimize the probability of misclassification, this is done by assigning
the test point x to the class having the largest posterior probability, corresponding to
the largest value of Kk/K. Thus to classify a new point, we identify the K nearest
points from the training data set and then assign the new point to the class having the
largest number of representatives amongst this set. Ties can be broken at random.
The particular case of K = 1 is called the nearest-neighbour rule, because a test
point is simply assigned to the same class as the nearest point from the training set.
These concepts are illustrated in Figure 2.27.
In Figure 2.28, we show the results of applying the K-nearest-neighbour algo-
rithm to the oil flow data, introduced in Chapter 1, for various values of K. As
expected, we see that K controls the degree of smoothing, so that small K produces
many small regions of each class, whereas large K leads to fewer larger regions.
K = 1
x7
2
1
K = 3
x7
2
1
x7
2
1
K = 3 1
0
0
1
x6
2
0
0
1
x6
2
0
0
1
x6
2
Figure 2.28 Plot of 200 data points from the oil data set showing values of x6 plotted against x7, where the
red, green, and blue points correspond to the ‘laminar’, ‘annular’, and ‘homogeneous’ classes, respectively. Also
shown are the classifications of the input space given by the K-nearest-neighbour algorithm for various values
of K.
x=0
1
Exercises
127
An interesting property of the nearest-neighbour (K = 1) classifier is that, in the
limit N → ∞, the error rate is never more than twice the minimum achievable error
rate of an optimal classifier, i.e., one that uses the true class distributions (Cover and
Hart, 1967) .
As discussed so far, both the K-nearest-neighbour method, and the kernel den-
sity estimator, require the entire training data set to be stored, leading to expensive
computation if the data set is large. This effect can be offset, at the expense of some
additional one-off computation, by constructing tree-based search structures to allow
(approximate) near neighbours to be found efficiently without doing an exhaustive
search of the data set. Nevertheless, these nonparametric methods are still severely
limited. On the other hand, we have seen that simple parametric models are very
restricted in terms of the forms of distribution that they can represent. We therefore
need to find density models that are very flexible and yet for which the complexity
of the models can be controlled independently of the size of the training set, and we
shall see in subsequent chapters how to achieve this.
Exercises
erties
2.1 () www Verify that the Bernoulli distribution (2.2) satisfies the following prop-
(2.257)
(2.258)
(2.259)
(2.260)
(2.261)
p(x|µ) = 1
E[x] = µ
var[x] = µ(1 − µ).
Show that the entropy H[x] of a Bernoulli distributed random binary variable x is
given by
H[x] = −µ ln µ − (1 − µ) ln(1 − µ).
2.2 ( ) The form of the Bernoulli distribution given by (2.2) is not symmetric be-
tween the two values of x. In some situations, it will be more convenient to use an
equivalent formulation for which x ∈ {−1, 1}, in which case the distribution can be
written
p(x|µ) =
1 − µ
2
(1−x)/2
1 + µ
(1+x)/2
2
where µ ∈ [−1, 1]. Show that the distribution (2.261) is normalized, and evaluate its
mean, variance, and entropy.
2.3 ( ) www In this exercise, we prove that the binomial distribution (2.9) is nor-
malized. First use the definition (2.10) of the number of combinations of m identical
objects chosen from a total of N to show that
N
m
N
m − 1
N + 1
m
(2.262)
N
N
128
2. PROBABILITY DISTRIBUTIONS
Use this result to prove by induction the following result
(1 + x)N =
xm
(2.263)
N
m
m=0
which is known as the binomial theorem, and which is valid for all real values of x.
Finally, show that the binomial distribution is normalized, so that
N
m
µm(1 − µ)N−m = 1
m=0
(2.264)
which can be done by first pulling out a factor (1 − µ)N out of the summation and
then making use of the binomial theorem.
2.4 ( ) Show that the mean of the binomial distribution is given by (2.11). To do this,
differentiate both sides of the normalization condition (2.264) with respect to µ and
then rearrange to obtain an expression for the mean of n. Similarly, by differentiating
(2.264) twice with respect to µ and making use of the result (2.11) for the mean of
the binomial distribution prove the result (2.12) for the variance of the binomial.
2.5 ( ) www In this exercise, we prove that the beta distribution, given by (2.13), is
correctly normalized, so that (2.14) holds. This is equivalent to showing that
1
0
µa−1(1 − µ)b−1 dµ =
Γ(a)Γ(b)
Γ(a + b) .
From the definition (1.141) of the gamma function, we have
Γ(a)Γ(b) =
0
exp(−x)xa−1 dx
0
exp(−y)yb−1 dy.
(2.265)
(2.266)
Use this expression to prove (2.265) as follows. First bring the integral over y inside
the integrand of the integral over x, next make the change of variable t = y + x
where x is fixed, then interchange the order of the x and t integrations, and finally
make the change of variable x = tµ where t is fixed.
2.6 () Make use of the result (2.265) to show that the mean, variance, and mode of the
beta distribution (2.13) are given respectively by
E[µ] =
var[µ] =
mode[µ] =
a
a + b
ab
(a + b)2(a + b + 1)
a − 1
a + b − 2 .
(2.267)
(2.268)
(2.269)
Exercises
129
2.7 ( ) Consider a binomial random variable x given by (2.9), with prior distribution
for µ given by the beta distribution (2.13), and suppose we have observed m occur-
rences of x = 1 and l occurrences of x = 0. Show that the posterior mean value of x
lies between the prior mean and the maximum likelihood estimate for µ. To do this,
show that the posterior mean can be written as λ times the prior mean plus (1 − λ)
times the maximum likelihood estimate, where 0 � λ � 1. This illustrates the con-
cept of the posterior distribution being a compromise between the prior distribution
and the maximum likelihood solution.
2.8 () Consider two variables x and y with joint distribution p(x, y). Prove the follow-
ing two results
E[x] = Ey [Ex[x|y]]
var[x] = Ey [varx[x|y]] + vary [Ex[x|y]] .
(2.270)
(2.271)
Here Ex[x|y] denotes the expectation of x under the conditional distribution p(x|y),
with a similar notation for the conditional variance.
2.9 (  ) www . In this exercise, we prove the normalization of the Dirichlet dis-
tribution (2.38) using induction. We have already shown in Exercise 2.5 that the
beta distribution, which is a special case of the Dirichlet for M = 2, is normalized.
We now assume that the Dirichlet distribution is normalized for M − 1 variables
and prove that it is normalized for M variables. To do this, consider the Dirichlet
k=1 µk = 1 by
distribution over M variables, and take account of the constraint
eliminating µM , so that the Dirichlet is written
M
pM (µ1, . . . , µM−1) = CM
µαk−1
k
1 −
M−1
k=1
αM−1
µj
(2.272)
M−1
j=1
and our goal is to find an expression for CM . To do this, integrate over µM−1, taking
care over the limits of integration, and then make a change of variable so that this
integral has limits 0 and 1. By assuming the correct result for CM−1 and making use
of (2.265), derive the expression for CM .
2.10 ( ) Using the property Γ(x + 1) = xΓ(x) of the gamma function, derive the
following results for the mean, variance, and covariance of the Dirichlet distribution
given by (2.38)
E[µj] = αj
α0
var[µj] = αj(α0 − αj)
0(α0 + 1)
α2
αjαl
cov[µjµl] = −
0(α0 + 1) ,
α2
j = l
(2.273)
(2.274)
(2.275)
where α0 is defined by (2.39).
130
2. PROBABILITY DISTRIBUTIONS
2.11 () www By expressing the expectation of ln µj under the Dirichlet distribution
(2.38) as a derivative with respect to αj, show that
where α0 is given by (2.39) and
E[ln µj] = ψ(αj) − ψ(α0)
ψ(a) ≡
d
da
ln Γ(a)
(2.276)
(2.277)
is the digamma function.
2.12 () The uniform distribution for a continuous variable x is defined by
U(x|a, b) =
1
b − a
a � x � b.
(2.278)
Verify that this distribution is normalized, and find expressions for its mean and
variance.
2.13 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians
p(x) = N (x|µ, Σ) and q(x) = N (x|m, L).
2.14 ( ) www This exercise demonstrates that the multivariate distribution with max-
imum entropy, for a given covariance, is a Gaussian. The entropy of a distribution
p(x) is given by
H[x] = −
p(x) ln p(x) dx.
(2.279)
We wish to maximize H[x] over all distributions p(x) subject to the constraints that
p(x) be normalized and that it have a specific mean and covariance, so that
p(x) dx = 1
p(x)x dx = µ
p(x)(x − µ)(x − µ)T dx = Σ.
(2.280)
(2.281)
(2.282)
By performing a variational maximization of (2.279) and using Lagrange multipliers
to enforce the constraints (2.280), (2.281), and (2.282), show that the maximum
likelihood distribution is given by the Gaussian (2.43).
2.15 ( ) Show that the entropy of the multivariate Gaussian N (x|µ, Σ) is given by
H[x] =
1
2
ln|Σ| + D
2
(1 + ln(2π))
(2.283)
where D is the dimensionality of x.
Exercises
131
2.16 (  ) www Consider two random variables x1 and x2 having Gaussian distri-
butions with means µ1, µ2 and precisions τ1, τ2 respectively. Derive an expression
for the differential entropy of the variable x = x1 + x2. To do this, first find the
distribution of x by using the relation
p(x) =
p(x|x2)p(x2) dx2
(2.284)
and completing the square in the exponent. Then observe that this represents the
convolution of two Gaussian distributions, which itself will be Gaussian, and finally
make use of the result (1.110) for the entropy of the univariate Gaussian.
2.17 () www Consider the multivariate Gaussian distribution given by (2.43). By
writing the precision matrix (inverse covariance matrix) Σ−1 as the sum of a sym-
metric and an anti-symmetric matrix, show that the anti-symmetric term does not
appear in the exponent of the Gaussian, and hence that the precision matrix may be
taken to be symmetric without loss of generality. Because the inverse of a symmetric
matrix is also symmetric (see Exercise 2.22), it follows that the covariance matrix
may also be chosen to be symmetric without loss of generality.
2.18 (  ) Consider a real, symmetric matrix Σ whose eigenvalue equation is given
by (2.45). By taking the complex conjugate of this equation and subtracting the
original equation, and then forming the inner product with eigenvector ui, show that
the eigenvalues λi are real. Similarly, use the symmetry property of Σ to show that
two eigenvectors ui and uj will be orthogonal provided λj = λi. Finally, show that
without loss of generality, the set of eigenvectors can be chosen to be orthonormal,
so that they satisfy (2.46), even if some of the eigenvalues are zero.
2.19 ( ) Show that a real, symmetric matrix Σ having the eigenvector equation (2.45)
can be expressed as an expansion in the eigenvectors, with coefficients given by the
eigenvalues, of the form (2.48). Similarly, show that the inverse matrix Σ−1 has a
representation of the form (2.49).
2.20 ( ) www A positive definite matrix Σ can be defined as one for which the
quadratic form
(2.285)
is positive for any real value of the vector a. Show that a necessary and sufficient
condition for Σ to be positive definite is that all of the eigenvalues λi of Σ, defined
by (2.45), are positive.
aTΣa
2.21 () Show that a real, symmetric matrix of size D× D has D(D + 1)/2 independent
parameters.
2.22 () www Show that the inverse of a symmetric matrix is itself symmetric.
2.23 ( ) By diagonalizing the coordinate system using the eigenvector expansion (2.45),
show that the volume contained within the hyperellipsoid corresponding to a constant
132
2. PROBABILITY DISTRIBUTIONS
Mahalanobis distance ∆ is given by
where VD is the volume of the unit sphere in D dimensions, and the Mahalanobis
distance is defined by (2.44).
2.24 ( ) www Prove the identity (2.76) by multiplying both sides by the matrix
VD|Σ|1/2∆D
A B
C D
(2.286)
(2.287)
and making use of the definition (2.77).
2.25 ( )
In Sections 2.3.1 and 2.3.2, we considered the conditional and marginal distri-
butions for a multivariate Gaussian. More generally, we can consider a partitioning
of the components of x into three groups xa, xb, and xc, with a corresponding par-
titioning of the mean vector µ and of the covariance matrix Σ in the form
µa
µb
µc
Σaa Σab Σac
Σba Σbb Σbc
Σca Σcb Σcc
(2.288)
By making use of the results of Section 2.3, find an expression for the conditional
distribution p(xa|xb) in which xc has been marginalized out.
2.26 ( ) A very useful result from linear algebra is the Woodbury matrix inversion
formula given by
(A + BCD)−1 = A−1 − A−1B(C−1 + DA−1B)−1DA−1.
(2.289)
By multiplying both sides by (A + BCD) prove the correctness of this result.
2.27 () Let x and z be two independent random vectors, so that p(x, z) = p(x)p(z).
Show that the mean of their sum y = x + z is given by the sum of the means of each
of the variable separately. Similarly, show that the covariance matrix of y is given by
the sum of the covariance matrices of x and z. Confirm that this result agrees with
that of Exercise 1.10.
2.28 (  ) www Consider a joint distribution over the variable
z =
x
y
(2.290)
whose mean and covariance are given by (2.108) and (2.105) respectively. By mak-
ing use of the results (2.92) and (2.93) show that the marginal distribution p(x) is
given (2.99). Similarly, by making use of the results (2.81) and (2.82) show that the
conditional distribution p(y|x) is given by (2.100).
Exercises
133
2.29 ( ) Using the partitioned matrix inversion formula (2.76), show that the inverse of
the precision matrix (2.104) is given by the covariance matrix (2.105).
2.30 () By starting from (2.107) and making use of the result (2.105), verify the result
(2.108).
2.31 ( ) Consider two multidimensional random vectors x and z having Gaussian
distributions p(x) = N (x|µx, Σx) and p(z) = N (z|µz, Σz) respectively, together
with their sum y = x+z. Use the results (2.109) and (2.110) to find an expression for
the marginal distribution p(y) by considering the linear-Gaussian model comprising
the product of the marginal distribution p(x) and the conditional distribution p(y|x).
2.32 (  ) www This exercise and the next provide practice at manipulating the
quadratic forms that arise in linear-Gaussian models, as well as giving an indepen-
dent check of results derived in the main text. Consider a joint distribution p(x, y)
defined by the marginal and conditional distributions given by (2.99) and (2.100).
By examining the quadratic form in the exponent of the joint distribution, and using
the technique of ‘completing the square’ discussed in Section 2.3, find expressions
for the mean and covariance of the marginal distribution p(y) in which the variable
x has been integrated out. To do this, make use of the Woodbury matrix inversion
formula (2.289). Verify that these results agree with (2.109) and (2.110) obtained
using the results of Chapter 2.
2.33 (  ) Consider the same joint distribution as in Exercise 2.32, but now use the
technique of completing the square to find expressions for the mean and covariance
of the conditional distribution p(x|y). Again, verify that these agree with the corre-
sponding expressions (2.111) and (2.112).
2.34 ( ) www To find the maximum likelihood solution for the covariance matrix
of a multivariate Gaussian, we need to maximize the log likelihood function (2.118)
with respect to Σ, noting that the covariance matrix must be symmetric and positive
definite. Here we proceed by ignoring these constraints and doing a straightforward
maximization. Using the results (C.21), (C.26), and (C.28) from Appendix C, show
that the covariance matrix Σ that maximizes the log likelihood function (2.118) is
given by the sample covariance (2.122). We note that the final result is necessarily
symmetric and positive definite (provided the sample covariance is nonsingular).
2.35 ( ) Use the result (2.59) to prove (2.62). Now, using the results (2.59), and (2.62),
show that
E[xnxm] = µµT + InmΣ
(2.291)
where xn denotes a data point sampled from a Gaussian distribution with mean µ
and covariance Σ, and Inm denotes the (n, m) element of the identity matrix. Hence
prove the result (2.124).
2.36 ( ) www Using an analogous procedure to that used to obtain (2.126), derive
an expression for the sequential estimation of the variance of a univariate Gaussian
N
n=1
134
2. PROBABILITY DISTRIBUTIONS
distribution, by starting with the maximum likelihood expression
ML =
σ2
1
N
(xn − µ)2.
(2.292)
Verify that substituting the expression for a Gaussian distribution into the Robbins-
Monro sequential estimation formula (2.135) gives a result of the same form, and
hence obtain an expression for the corresponding coefficients aN .
2.37 ( ) Using an analogous procedure to that used to obtain (2.126), derive an ex-
pression for the sequential estimation of the covariance of a multivariate Gaussian
distribution, by starting with the maximum likelihood expression (2.122). Verify that
substituting the expression for a Gaussian distribution into the Robbins-Monro se-
quential estimation formula (2.135) gives a result of the same form, and hence obtain
an expression for the corresponding coefficients aN .
2.38 () Use the technique of completing the square for the quadratic form in the expo-
nent to derive the results (2.141) and (2.142).
2.39 ( )
Starting from the results (2.141) and (2.142) for the posterior distribution
of the mean of a Gaussian random variable, dissect out the contributions from the
first N − 1 data points and hence obtain expressions for the sequential update of
µN and σ2
N . Now derive the same results starting from the posterior distribution
p(µ|x1, . . . , xN−1) = N (µ|µN−1, σ2
N−1) and multiplying by the likelihood func-
tion p(xN|µ) = N (xN|µ, σ2) and then completing the square and normalizing to
obtain the posterior distribution after N observations.
2.40 ( ) www Consider a D-dimensional Gaussian random variable x with distribu-
tion N (x|µ, Σ) in which the covariance Σ is known and for which we wish to infer
the mean µ from a set of observations X = {x1, . . . , xN}. Given a prior distribution
p(µ) = N (µ|µ0, Σ0), find the corresponding posterior distribution p(µ|X).
2.41 () Use the definition of the gamma function (1.141) to show that the gamma dis-
tribution (2.146) is normalized.
2.42 ( ) Evaluate the mean, variance, and mode of the gamma distribution (2.146).
2.43 () The following distribution
p(x|σ2, q) =
q
2(2σ2)1/qΓ(1/q)
exp
−|x|q
2σ2
(2.293)
is a generalization of the univariate Gaussian distribution. Show that this distribution
is normalized so that
p(x|σ2, q) dx = 1
(2.294)
and that it reduces to the Gaussian when q = 2. Consider a regression model in
which the target variable is given by t = y(x, w) + 	 and 	 is a random noise
N
ln p(t|X, w, σ2) = −
1
2σ2
n=1
where ‘const’ denotes terms independent of both w and σ2. Note that, as a function
of w, this is the Lq error function considered in Section 1.5.5.
2.44 ( ) Consider a univariate Gaussian distribution N (x|µ, τ−1) having conjugate
Gaussian-gamma prior given by (2.154), and a data set x = {x1, . . . , xN} of i.i.d.
observations. Show that the posterior distribution is also a Gaussian-gamma distri-
bution of the same functional form as the prior, and write down expressions for the
parameters of this posterior distribution.
2.45 () Verify that the Wishart distribution defined by (2.155) is indeed a conjugate
prior for the precision matrix of a multivariate Gaussian.
2.46 () www Verify that evaluating the integral in (2.158) leads to the result (2.159).
2.47 () www Show that in the limit ν → ∞, the t-distribution (2.159) becomes a
Gaussian. Hint: ignore the normalization coefficient, and simply look at the depen-
dence on x.
2.48 () By following analogous steps to those used to derive the univariate Student’s
t-distribution (2.159), verify the result (2.162) for the multivariate form of the Stu-
dent’s t-distribution, by marginalizing over the variable η in (2.161). Using the
definition (2.161), show by exchanging integration variables that the multivariate
t-distribution is correctly normalized.
2.49 ( ) By using the definition (2.161) of the multivariate Student’s t-distribution as a
convolution of a Gaussian with a gamma distribution, verify the properties (2.164),
(2.165), and (2.166) for the multivariate t-distribution defined by (2.162).
2.50 () Show that in the limit ν → ∞, the multivariate Student’s t-distribution (2.162)
reduces to a Gaussian with mean µ and precision Λ.
2.51 () www The various trigonometric identities used in the discussion of periodic
variables in this chapter can be proven easily from the relation
exp(iA) = cos A + i sin A
in which i is the square root of minus one. By considering the identity
exp(iA) exp(−iA) = 1
prove the result (2.177). Similarly, using the identity
cos(A − B) =
exp{i(A − B)}
(2.296)
(2.297)
(2.298)
Exercises
135
variable drawn from the distribution (2.293). Show that the log likelihood function
over w and σ2, for an observed data set of input vectors X = {x1, . . . , xN} and
corresponding target variables t = (t1, . . . , tN )T, is given by
|y(xn, w) − tn|q −
N
q
ln(2σ2) + const
(2.295)
136
2. PROBABILITY DISTRIBUTIONS
2.52 ( )
where
denotes the real part, prove (2.178). Finally, by using sin(A − B) =
exp{i(A − B)}, where  denotes the imaginary part, prove the result (2.183).
For large m, the von Mises distribution (2.179) becomes sharply peaked
around the mode θ0. By defining ξ = m1/2(θ − θ0) and making the Taylor ex-
pansion of the cosine function given by
cos α = 1 −
α2
2
+ O(α4)
(2.299)
show that as m → ∞, the von Mises distribution tends to a Gaussian.
2.53 () Using the trigonometric identity (2.183), show that solution of (2.182) for θ0 is
given by (2.184).
2.54 () By computing first and second derivatives of the von Mises distribution (2.179),
and using I0(m) > 0 for m > 0, show that the maximum of the distribution occurs
when θ = θ0 and that the minimum occurs when θ = θ0 + π (mod 2π).
2.55 () By making use of the result (2.168), together with (2.184) and the trigonometric
identity (2.178), show that the maximum likelihood solution mML for the concentra-
tion of the von Mises distribution satisfies A(mML) = r where r is the radius of the
mean of the observations viewed as unit vectors in the two-dimensional Euclidean
plane, as illustrated in Figure 2.17.
2.56 ( ) www Express the beta distribution (2.13), the gamma distribution (2.146),
and the von Mises distribution (2.179) as members of the exponential family (2.194)
and thereby identify their natural parameters.
2.57 () Verify that the multivariate Gaussian distribution can be cast in exponential
family form (2.194) and derive expressions for η, u(x), h(x) and g(η) analogous to
(2.220)–(2.223).
2.58 () The result (2.226) showed that the negative gradient of ln g(η) for the exponen-
tial family is given by the expectation of u(x). By taking the second derivatives of
(2.195), show that
−∇∇ ln g(η) = E[u(x)u(x)T] − E[u(x)]E[u(x)T] = cov[u(x)].
(2.300)
2.59 () By changing variables using y = x/σ, show that the density (2.236) will be
correctly normalized, provided f(x) is correctly normalized.
2.60 ( ) www Consider a histogram-like density model in which the space x is di-
vided into fixed regions for which the density p(x) takes the constant value hi over
the ith region, and that the volume of region i is denoted ∆i. Suppose we have a set
of N observations of x such that ni of these observations fall in region i. Using a
Lagrange multiplier to enforce the normalization constraint on the density, derive an
expression for the maximum likelihood estimator for the {hi}.
2.61 () Show that the K-nearest-neighbour density model defines an improper distribu-
tion whose integral over all space is divergent.
3 Linear Models for Regression
The focus so far in this book has been on unsupervised learning, including topics
such as density estimation and data clustering. We turn now to a discussion of super-
vised learning, starting with regression. The goal of regression is to predict the value
of one or more continuous target variables t given the value of a D-dimensional vec-
tor x of input variables. We have already encountered an example of a regression
problem when we considered polynomial curve fitting in Chapter 1. The polynomial
is a specific example of a broad class of functions called linear regression models,
which share the property of being linear functions of the adjustable parameters, and
which will form the focus of this chapter. The simplest form of linear regression
models are also linear functions of the input variables. However, we can obtain a
much more useful class of functions by taking linear combinations of a fixed set of
nonlinear functions of the input variables, known as basis functions. Such models
are linear functions of the parameters, which gives them simple analytical properties,
and yet can be nonlinear with respect to the input variables.
137
138
