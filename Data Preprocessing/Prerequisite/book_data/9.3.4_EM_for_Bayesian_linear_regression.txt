As a third example of the application of EM, we return to the evidence ap-
proximation for Bayesian linear regression. In Section 3.5.2, we obtained the re-
estimation equations for the hyperparameters α and β by evaluation of the evidence
and then setting the derivatives of the resulting expression to zero. We now turn to
an alternative approach for finding α and β based on the EM algorithm. Recall that
our goal is to maximize the evidence function p(t|α, β) given by (3.77) with respect
to α and β. Because the parameter vector w is marginalized out, we can regard it as
a latent variable, and hence we can optimize this marginal likelihood function using
EM. In the E step, we compute the posterior distribution of w given the current set-
ting of the parameters α and β and then use this to find the expected complete-data
log likelihood. In the M step, we maximize this quantity with respect to α and β. We
have already derived the posterior distribution of w because this is given by (3.49).
The complete-data log likelihood function is then given by
ln p(t, w|α, β) = ln p(t|w, β) + ln p(w|α)
(9.61)
N
2
M
9.3. An Alternative View of EM
449
where the likelihood p(t|w, β) and the prior p(w|α) are given by (3.10) and (3.52),
respectively, and y(x, w) is given by (3.3). Taking the expectation with respect to
the posterior distribution of w then gives
E [ln p(t, w|α, β)] = M
2
ln
2π
2 E
wTw
+ N
2
ln
2π
E
(tn − wTφn)2
n=1
(9.62)
Setting the derivatives with respect to α to zero, we obtain the M step re-estimation
equation
α = M
E [wTw]
M
N mN + Tr(SN ) .
mT
(9.63)
An analogous result holds for β.
Note that this re-estimation equation takes a slightly different form from the
corresponding result (3.92) derived by direct evaluation of the evidence function.
However, they each involve computation and inversion (or eigen decomposition) of
an M × M matrix and hence will have comparable computational cost per iteration.
These two approaches to determining α should of course converge to the same
result (assuming they find the same local maximum of the evidence function). This
can be verified by first noting that the quantity γ is defined by
γ = M − α
= M − αTr(SN ).
(9.64)
1
λi + α
i=1
Exercise 9.20
Exercise 9.21
At a stationary point of the evidence function, the re-estimation equation (3.92) will
be self-consistently satisfied, and hence we can substitute for γ to give
αmT
N mN = γ = M − αTr(SN )
(9.65)
and solving for α we obtain (9.63), which is precisely the EM re-estimation equation.
As a final example, we consider a closely related model, namely the relevance
vector machine for regression discussed in Section 7.2.1. There we used direct max-
imization of the marginal likelihood to derive re-estimation equations for the hyper-
parameters α and β. Here we consider an alternative approach in which we view the
weight vector w as a latent variable and apply the EM algorithm. The E step involves
finding the posterior distribution over the weights, and this is given by (7.81). In the
M step we maximize the expected complete-data log likelihood, which is defined by
Exercise 9.22
where the expectation is taken with respect to the posterior distribution computed
using the ‘old’ parameter values. To compute the new parameter values we maximize
with respect to α and β to give
Ew [ln p(t|X, w, β)p(w|α)]
(9.66)
1
m2
Z
Z
N
The expectation maximization algorithm, or EM algorithm, is a general technique for
finding maximum likelihood solutions for probabilistic models having latent vari-
ables (Dempster et al., 1977; McLachlan and Krishnan, 1997). Here we give a very
general treatment of the EM algorithm and in the process provide a proof that the
EM algorithm derived heuristically in Sections 9.2 and 9.3 for Gaussian mixtures
does indeed maximize the likelihood function (Csisz`ar and Tusn`ady, 1984; Hath-
away, 1986; Neal and Hinton, 1999). Our discussion will also form the basis for the
derivation of the variational inference framework.
Consider a probabilistic model in which we collectively denote all of the ob-
served variables by X and all of the hidden variables by Z. The joint distribution
p(X, Z|θ) is governed by a set of parameters denoted θ. Our goal is to maximize
the likelihood function that is given by
p(X|θ) =
p(X, Z|θ).
(9.69)
Here we are assuming Z is discrete, although the discussion is identical if Z com-
prises continuous variables or a combination of discrete and continuous variables,
with summation replaced by integration as appropriate.
We shall suppose that direct optimization of p(X|θ) is difficult, but that opti-
mization of the complete-data likelihood function p(X, Z|θ) is significantly easier.
Next we introduce a distribution q(Z) defined over the latent variables, and we ob-
serve that, for any choice of q(Z), the following decomposition holds
ln p(X|θ) = L(q, θ) + KL(qp)
where we have defined
L(q, θ) =
q(Z) ln
p(X, Z|θ)
q(Z)
p(Z|X, θ)
q(Z)
KL(qp) = −
Z
q(Z) ln
(9.70)
(9.71)
(9.72)
Note that L(q, θ) is a functional (see Appendix D for a discussion of functionals)
of the distribution q(Z), and a function of the parameters θ. It is worth studying
450
9. MIXTURE MODELS AND EM
αnew
i
i + Σii
(βnew)−1 = t − ΦmN2 + β−1
i γi
(9.67)
(9.68)
These re-estimation equations are formally equivalent to those obtained by direct
maxmization.
