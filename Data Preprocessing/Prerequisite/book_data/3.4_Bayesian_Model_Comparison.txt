161
y(x) = 1, from which we obtain (3.64). Note that the kernel function can
be simply
be negative as well as positive, so although it satisfies a summation constraint, the
corresponding predictions are not necessarily convex combinations of the training
set target variables.
Chapter 6
Finally, we note that the equivalent kernel (3.62) satisfies an important property
shared by kernel functions in general, namely that it can be expressed in the form an
inner product with respect to a vector ψ(x) of nonlinear functions, so that
k(x, z) = ψ(x)Tψ(z)
(3.65)
where ψ(x) = β1/2S1/2
N φ(x).
3.4. Bayesian Model Comparison
In Chapter 1, we highlighted the problem of over-fitting as well as the use of cross-
validation as a technique for setting the values of regularization parameters or for
choosing between alternative models. Here we consider the problem of model se-
lection from a Bayesian perspective.
In this section, our discussion will be very
general, and then in Section 3.5 we shall see how these ideas can be applied to the
determination of regularization parameters in linear regression.
As we shall see, the over-fitting associated with maximum likelihood can be
avoided by marginalizing (summing or integrating) over the model parameters in-
stead of making point estimates of their values. Models can then be compared di-
rectly on the training data, without the need for a validation set. This allows all
available data to be used for training and avoids the multiple training runs for each
model associated with cross-validation. It also allows multiple complexity parame-
ters to be determined simultaneously as part of the training process. For example,
in Chapter 7 we shall introduce the relevance vector machine, which is a Bayesian
model having one complexity parameter for every training data point.
The Bayesian view of model comparison simply involves the use of probabilities
to represent uncertainty in the choice of model, along with a consistent application
of the sum and product rules of probability. Suppose we wish to compare a set of L
models {Mi} where i = 1, . . . , L. Here a model refers to a probability distribution
over the observed data D. In the case of the polynomial curve-fitting problem, the
distribution is defined over the set of target values t, while the set of input values X
is assumed to be known. Other types of model define a joint distributions over X
and t. We shall suppose that the data is generated from one of these models but we
are uncertain which one. Our uncertainty is expressed through a prior probability
distribution p(Mi). Given a training set D, we then wish to evaluate the posterior
distribution
(3.66)
The prior allows us to express a preference for different models. Let us simply
assume that all models are given equal prior probability. The interesting term is
the model evidence p(D|Mi) which expresses the preference shown by the data for
p(Mi|D) ∝ p(Mi)p(D|Mi).
Section 1.5.4
i=1
L
3. LINEAR MODELS FOR REGRESSION
different models, and we shall examine this term in more detail shortly. The model
evidence is sometimes also called the marginal likelihood because it can be viewed
as a likelihood function over the space of models, in which the parameters have been
marginalized out. The ratio of model evidences p(D|Mi)/p(D|Mj) for two models
is known as a Bayes factor (Kass and Raftery, 1995).
Once we know the posterior distribution over models, the predictive distribution
is given, from the sum and product rules, by
p(t|x,D) =
p(t|x,Mi,D)p(Mi|D).
(3.67)
This is an example of a mixture distribution in which the overall predictive distribu-
tion is obtained by averaging the predictive distributions p(t|x,Mi,D) of individual
models, weighted by the posterior probabilities p(Mi|D) of those models. For in-
stance, if we have two models that are a-posteriori equally likely and one predicts
a narrow distribution around t = a while the other predicts a narrow distribution
around t = b, the overall predictive distribution will be a bimodal distribution with
modes at t = a and t = b, not a single model at t = (a + b)/2.
A simple approximation to model averaging is to use the single most probable
model alone to make predictions. This is known as model selection.
For a model governed by a set of parameters w, the model evidence is given,
from the sum and product rules of probability, by
p(D|Mi) =
p(D|w,Mi)p(w|Mi) dw.
(3.68)
From a sampling perspective, the marginal likelihood can be viewed as the proba-
bility of generating the data set D from a model whose parameters are sampled at
random from the prior. It is also interesting to note that the evidence is precisely the
normalizing term that appears in the denominator in Bayes’ theorem when evaluating
the posterior distribution over parameters because
p(w|D,Mi) = p(D|w,Mi)p(w|Mi)
p(D|Mi)
(3.69)
We can obtain some insight into the model evidence by making a simple approx-
imation to the integral over parameters. Consider first the case of a model having a
single parameter w. The posterior distribution over parameters is proportional to
p(D|w)p(w), where we omit the dependence on the model Mi to keep the notation
uncluttered. If we assume that the posterior distribution is sharply peaked around the
most probable value wMAP, with width ∆wposterior, then we can approximate the in-
tegral by the value of the integrand at its maximum times the width of the peak. If we
further assume that the prior is flat with width ∆wprior so that p(w) = 1/∆wprior,
then we have
p(D) =
p(D|w)p(w) dw  p(D|wMAP)
∆wposterior
∆wprior
(3.70)
162
Chapter 11
3.4. Bayesian Model Comparison
∆wposterior
163
w
(3.71)
Figure 3.12 We can obtain a rough approximation to
the model evidence if we assume that
the posterior distribution over parame-
ters is sharply peaked around its mode
wMAP.
and so taking logs we obtain
ln p(D)  ln p(D|wMAP) + ln
wMAP
∆wprior
∆wposterior
∆wprior
This approximation is illustrated in Figure 3.12. The first term represents the fit to
the data given by the most probable parameter values, and for a flat prior this would
correspond to the log likelihood. The second term penalizes the model according to
its complexity. Because ∆wposterior < ∆wprior this term is negative, and it increases
in magnitude as the ratio ∆wposterior/∆wprior gets smaller. Thus, if parameters are
finely tuned to the data in the posterior distribution, then the penalty term is large.
For a model having a set of M parameters, we can make a similar approximation
for each parameter in turn. Assuming that all parameters have the same ratio of
∆wposterior/∆wprior, we obtain
ln p(D)  ln p(D|wMAP) + M ln
∆wposterior
∆wprior
(3.72)
Thus, in this very simple approximation, the size of the complexity penalty increases
linearly with the number M of adaptive parameters in the model. As we increase
the complexity of the model, the first term will typically decrease, because a more
complex model is better able to fit the data, whereas the second term will increase
due to the dependence on M. The optimal model complexity, as determined by
the maximum evidence, will be given by a trade-off between these two competing
terms. We shall later develop a more refined version of this approximation, based on
a Gaussian approximation to the posterior distribution.
We can gain further insight into Bayesian model comparison and understand
how the marginal likelihood can favour models of intermediate complexity by con-
sidering Figure 3.13. Here the horizontal axis is a one-dimensional representation
of the space of possible data sets, so that each point on this axis corresponds to a
specific data set. We now consider three models M1, M2 and M3 of successively
increasing complexity. Imagine running these models generatively to produce exam-
ple data sets, and then looking at the distribution of data sets that result. Any given
Section 4.4.1
164
3. LINEAR MODELS FOR REGRESSION
Figure 3.13 Schematic illustration of
the
distribution of data sets for
three models of different com-
in which M1 is the
plexity,
simplest and M3 is the most
complex. Note that the dis-
tributions are normalized.
In
this example,
for the partic-
ular observed data set D0,
the model M2 with intermedi-
ate complexity has the largest
evidence.
Bayes factor in the form
p(D)
M1
M2
D0
M3
D
model can generate a variety of different data sets since the parameters are governed
by a prior probability distribution, and for any choice of the parameters there may
be random noise on the target variables. To generate a particular data set from a spe-
cific model, we first choose the values of the parameters from their prior distribution
p(w), and then for these parameter values we sample the data from p(D|w). A sim-
ple model (for example, based on a first order polynomial) has little variability and
so will generate data sets that are fairly similar to each other. Its distribution p(D)
is therefore confined to a relatively small region of the horizontal axis. By contrast,
a complex model (such as a ninth order polynomial) can generate a great variety of
different data sets, and so its distribution p(D) is spread over a large region of the
space of data sets. Because the distributions p(D|Mi) are normalized, we see that
the particular data set D0 can have the highest value of the evidence for the model
of intermediate complexity. Essentially, the simpler model cannot fit the data well,
whereas the more complex model spreads its predictive probability over too broad a
range of data sets and so assigns relatively small probability to any one of them.
Implicit in the Bayesian model comparison framework is the assumption that
the true distribution from which the data are generated is contained within the set of
models under consideration. Provided this is so, we can show that Bayesian model
comparison will on average favour the correct model. To see this, consider two
models M1 and M2 in which the truth corresponds to M1. For a given finite data
set, it is possible for the Bayes factor to be larger for the incorrect model. However, if
we average the Bayes factor over the distribution of data sets, we obtain the expected
Section 1.6.1
p(D|M1) ln p(D|M1)
p(D|M2)
dD
(3.73)
where the average has been taken with respect to the true distribution of the data.
This quantity is an example of the Kullback-Leibler divergence and satisfies the prop-
erty of always being positive unless the two distributions are equal in which case it
is zero. Thus on average the Bayes factor will always favour the correct model.
We have seen that the Bayesian framework avoids the problem of over-fitting
and allows models to be compared on the basis of the training data alone. However,
