Now consider a more complex problem involving the chain of nodes of the form
shown in Figure 8.32. This example will lay the foundation for a discussion of exact
inference in more general graphs later in this section.
Specifically, we shall consider the undirected graph in Figure 8.32(b). We have
already seen that the directed chain can be transformed into an equivalent undirected
chain. Because the directed graph does not have any nodes with more than one
parent, this does not require the addition of any extra links, and the directed and
undirected versions of this graph express exactly the same set of conditional inde-
pendence statements.
x1
xN
8.4. Inference in Graphical Models
The joint distribution for this graph takes the form
p(x) =
1
Z
ψ1,2(x1, x2)ψ2,3(x2, x3)··· ψN−1,N (xN−1, xN ).
We shall consider the specific case in which the N nodes represent discrete vari-
ables each having K states, in which case each potential function ψn−1,n(xn−1, xn)
comprises an K × K table, and so the joint distribution has (N − 1)K 2 parameters.
Let us consider the inference problem of finding the marginal distribution p(xn)
for a specific node xn that is part way along the chain. Note that, for the moment,
there are no observed nodes. By definition, the required marginal is obtained by
summing the joint distribution over all variables except xn, so that
395
(8.49)
p(xn) =
xn−1
xn+1
p(x).
xN
(8.50)
In a naive implementation, we would first evaluate the joint distribution and
then perform the summations explicitly. The joint distribution can be represented as
a set of numbers, one for each possible value for x. Because there are N variables
each with K states, there are K N values for x and so evaluation and storage of the
joint distribution, as well as marginalization to obtain p(xn), all involve storage and
computation that scale exponentially with the length N of the chain.
We can, however, obtain a much more efficient algorithm by exploiting the con-
ditional independence properties of the graphical model. If we substitute the factor-
ized expression (8.49) for the joint distribution into (8.50), then we can rearrange the
order of the summations and the multiplications to allow the required marginal to be
evaluated much more efficiently. Consider for instance the summation over xN . The
potential ψN−1,N (xN−1, xN ) is the only one that depends on xN , and so we can
perform the summation
(8.51)
ψN−1,N (xN−1, xN )
first to give a function of xN−1. We can then use this to perform the summation
over xN−1, which will involve only this new function together with the potential
ψN−2,N−1(xN−2, xN−1), because this is the only other place that xN−1 appears.
Similarly, the summation over x1 involves only the potential ψ1,2(x1, x2) and so
can be performed separately to give a function of x2, and so on. Because each
summation effectively removes a variable from the distribution, this can be viewed
as the removal of a node from the graph.
If we group the potentials and summations together in this way, we can express
x1
ψ2,3(x2, x3)
x2
xN
xn+1
xn−1
396
8. GRAPHICAL MODELS
the desired marginal in the form
p(xn) =
1
Z
ψ1,2(x1, x2)
ψn−1,n(xn−1, xn)···
ψn,n+1(xn, xn+1)···
µα(xn)
ψN−1,N (xN−1, xN )
(8.52)
µβ(xn)
The reader is encouraged to study this re-ordering carefully as the underlying idea
forms the basis for the later discussion of the general sum-product algorithm. Here
the key concept that we are exploiting is that multiplication is distributive over addi-
tion, so that
ab + ac = a(b + c)
(8.53)
in which the left-hand side involves three arithmetic operations whereas the right-
hand side reduces this to two operations.
Let us work out the computational cost of evaluating the required marginal using
this re-ordered expression. We have to perform N − 1 summations each of which is
over K states and each of which involves a function of two variables. For instance,
the summation over x1 involves only the function ψ1,2(x1, x2), which is a table of
K × K numbers. We have to sum this table over x1 for each value of x2 and so this
has O(K 2) cost. The resulting vector of K numbers is multiplied by the matrix of
numbers ψ2,3(x2, x3) and so is again O(K 2). Because there are N − 1 summations
and multiplications of this kind, the total cost of evaluating the marginal p(xn) is
O(N K 2). This is linear in the length of the chain, in contrast to the exponential cost
of a naive approach. We have therefore been able to exploit the many conditional
independence properties of this simple graph in order to obtain an efficient calcula-
tion. If the graph had been fully connected, there would have been no conditional
independence properties, and we would have been forced to work directly with the
full joint distribution.
We now give a powerful interpretation of this calculation in terms of the passing
of local messages around on the graph. From (8.52) we see that the expression for the
marginal p(xn) decomposes into the product of two factors times the normalization
constant
p(xn) =
µα(xn)µβ(xn).
(8.54)
1
Z
We shall interpret µα(xn) as a message passed forwards along the chain from node
xn−1 to node xn. Similarly, µβ(xn) can be viewed as a message passed backwards
x1
xn−1
xn−1
Figure 8.38 The marginal distribution
p(xn) for a node xn along the chain is ob-
tained by multiplying the two messages
µα(xn) and µβ(xn), and then normaliz-
ing. These messages can themselves
be evaluated recursively by passing mes-
sages from both ends of the chain to-
wards node xn.
x1
8.4. Inference in Graphical Models
µα(xn−1)
µα(xn)
µβ(xn)
µβ(xn+1)
xn−1
xn
xn+1
397
xN
(8.55)
(8.56)
along the chain to node xn from node xn+1. Note that each of the messages com-
prises a set of K values, one for each choice of xn, and so the product of two mes-
sages should be interpreted as the point-wise multiplication of the elements of the
two messages to give another set of K values.
The message µα(xn) can be evaluated recursively because
µα(xn) =
We therefore first evaluate
ψn−1,n(xn−1, xn)
xn−2
ψn−1,n(xn−1, xn)µα(xn−1).
µα(x2) =
ψ1,2(x1, x2)
and then apply (8.55) repeatedly until we reach the desired node. Note carefully the
structure of the message passing equation. The outgoing message µα(xn) in (8.55)
is obtained by multiplying the incoming message µα(xn−1) by the local potential
involving the node variable and the outgoing variable and then summing over the
node variable.
Similarly, the message µβ(xn) can be evaluated recursively by starting with
node xN and using
µβ(xn) =
ψn+1,n(xn+1, xn)
xn+2
xn+1
ψn+1,n(xn+1, xn)µβ(xn+1).
(8.57)
xn+1
This recursive message passing is illustrated in Figure 8.38. The normalization con-
stant Z is easily evaluated by summing the right-hand side of (8.54) over all states
of xn, an operation that requires only O(K) computation.
Graphs of the form shown in Figure 8.38 are called Markov chains, and the
corresponding message passing equations represent an example of the Chapman-
Kolmogorov equations for Markov processes (Papoulis, 1984).
398
8. GRAPHICAL MODELS
Now suppose we wish to evaluate the marginals p(xn) for every node n ∈
{1, . . . , N} in the chain. Simply applying the above procedure separately for each
node will have computational cost that is O(N 2M 2). However, such an approach
would be very wasteful of computation. For instance, to find p(x1) we need to prop-
agate a message µβ(·) from node xN back to node x2. Similarly, to evaluate p(x2)
we need to propagate a messages µβ(·) from node xN back to node x3. This will
involve much duplicated computation because most of the messages will be identical
in the two cases.
Suppose instead we first launch a message µβ(xN−1) starting from node xN
and propagate corresponding messages all the way back to node x1, and suppose we
similarly launch a message µα(x2) starting from node x1 and propagate the corre-
sponding messages all the way forward to node xN . Provided we store all of the
intermediate messages along the way, then any node can evaluate its marginal sim-
ply by applying (8.54). The computational cost is only twice that for finding the
marginal of a single node, rather than N times as much. Observe that a message
has passed once in each direction across each link in the graph. Note also that the
normalization constant Z need be evaluated only once, using any convenient node.
If some of the nodes in the graph are observed, then the corresponding variables
are simply clamped to their observed values and there is no summation. To see
this, note that the effect of clamping a variable xn to an observed value
xn can
be expressed by multiplying the joint distribution by (one or more copies of) an
additional function I(xn,
xn and the value
0 otherwise. One such function can then be absorbed into each of the potentials that
contain xn. Summations over xn then contain only one term in which xn =
xn), which takes the value 1 when xn =
xn.
Exercise 8.15
Chapter 9
Now suppose we wish to calculate the joint distribution p(xn−1, xn) for two
neighbouring nodes on the chain. This is similar to the evaluation of the marginal
for a single node, except that there are now two variables that are not summed out.
A few moments thought will show that the required joint distribution can be written
in the form
p(xn−1, xn) =
1
Z
µα(xn−1)ψn−1,n(xn−1, xn)µβ(xn).
(8.58)
Thus we can obtain the joint distributions over all of the sets of variables in each
of the potentials directly once we have completed the message passing required to
obtain the marginals.
This is a useful result because in practice we may wish to use parametric forms
for the clique potentials, or equivalently for the conditional distributions if we started
from a directed graph. In order to learn the parameters of these potentials in situa-
tions where not all of the variables are observed, we can employ the EM algorithm,
and it turns out that the local joint distributions of the cliques, conditioned on any
observed data, is precisely what is needed in the E step. We shall consider some
examples of this in detail in Chapter 13.
