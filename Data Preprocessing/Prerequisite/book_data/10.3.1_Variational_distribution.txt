Our first goal is to find an approximation to the posterior distribution p(w, α|t).
To do this, we employ the variational framework of Section 10.1, with a variational
10.3. Variational Linear Regression
Figure 10.8 Probabilistic graphical model representing the joint dis-
regression
the Bayesian linear
for
tribution (10.90)
model.
φn
tn
N
n=1
N
487
w
(10.91)
(10.92)
(10.93)
(10.94)
(10.95)
posterior distribution given by the factorized expression
q(w, α) = q(w)q(α).
We can find re-estimation equations for the factors in this distribution by making use
of the general result (10.9). Recall that for each factor, we take the log of the joint
distribution over all variables and then average with respect to those variables not in
that factor. Consider first the distribution over α. Keeping only terms that have a
functional dependence on α, we have
ln q(α) = ln p(α) + Ew [ln p(w|α)] + const
= (a0 − 1) ln α − b0α + M
2
ln α −
2 E[wTw] + const.
We recognize this as the log of a gamma distribution, and so identifying the coeffi-
cients of α and ln α we obtain
where
q(α) = Gam(α|aN , bN )
aN = a0 + M
2
1
2E[wTw].
bN = b0 +
Similarly, we can find the variational re-estimation equation for the posterior
distribution over w. Again, using the general result (10.9), and keeping only those
terms that have a functional dependence on w, we have
ln q(w) = ln p(t|w) + Eα [ln p(w|α)] + const
(10.96)
{wTφn − tn}2 −
1
2E[α]wTw + const
(10.97)
wT
E[α]I + βΦTΦ
w + βwTΦTt + const.
(10.98)
Because this is a quadratic form, the distribution q(w) is Gaussian, and so we can
complete the square in the usual way to identify the mean and covariance, giving
q(w) = N (w|mN , SN )
(10.99)
2
1
2
−1
(10.100)
(10.101)
488
10. APPROXIMATE INFERENCE
where
mN = βSN ΦTt
SN =
E[α]I + βΦTΦ
Note the close similarity to the posterior distribution (3.52) obtained when α was
treated as a fixed parameter. The difference is that here α is replaced by its expecta-
tion E[α] under the variational distribution. Indeed, we have chosen to use the same
notation for the covariance matrix SN in both cases.
Using the standard results (B.27), (B.38), and (B.39), we can obtain the required
moments as follows
E[α] = aN /bN
E[wwT] = mN mT
N + SN .
(10.102)
(10.103)
The evaluation of the variational posterior distribution begins by initializing the pa-
rameters of one of the distributions q(w) or q(α), and then alternately re-estimates
these factors in turn until a suitable convergence criterion is satisfied (usually speci-
fied in terms of the lower bound to be discussed shortly).
It is instructive to relate the variational solution to that found using the evidence
framework in Section 3.5. To do this consider the case a0 = b0 = 0, corresponding
to the limit of an infinitely broad prior over α. The mean of the variational posterior
distribution q(α) is then given by
E[α] = aN
bN
= M/2
E[wTw]/2
M
N mN + Tr(SN ) .
mT
(10.104)
Comparison with (9.63) shows that in the case of this particularly simple model,
the variational approach gives precisely the same expression as that obtained by
maximizing the evidence function using EM except that the point estimate for α
is replaced by its expected value. Because the distribution q(w) depends on q(α)
only through the expectation E[α], we see that the two approaches will give identical
results for the case of an infinitely broad prior.
