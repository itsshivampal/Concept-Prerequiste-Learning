While the Gaussian distribution has some important analytical properties, it suf-
fers from significant limitations when it comes to modelling real data sets. Consider
the example shown in Figure 2.21. This is known as the ‘Old Faithful’ data set,
and comprises 272 measurements of the eruption of the Old Faithful geyser at Yel-
lowstone National Park in the USA. Each measurement comprises the duration of
Appendix A
k=1
K
K
the eruption in minutes (horizontal axis) and the time in minutes to the next erup-
tion (vertical axis). We see that the data set forms two dominant clumps, and that
a simple Gaussian distribution is unable to capture this structure, whereas a linear
superposition of two Gaussians gives a better characterization of the data set.
Such superpositions, formed by taking linear combinations of more basic dis-
tributions such as Gaussians, can be formulated as probabilistic models known as
mixture distributions (McLachlan and Basford, 1988; McLachlan and Peel, 2000).
In Figure 2.22 we see that a linear combination of Gaussians can give rise to very
complex densities. By using a sufficient number of Gaussians, and by adjusting their
means and covariances as well as the coefficients in the linear combination, almost
any continuous density can be approximated to arbitrary accuracy.
We therefore consider a superposition of K Gaussian densities of the form
p(x) =
πkN (x|µk, Σk)
(2.188)
which is called a mixture of Gaussians. Each Gaussian density N (x|µk, Σk) is
called a component of the mixture and has its own mean µk and covariance Σk.
Contour and surface plots for a Gaussian mixture having 3 components are shown in
Figure 2.23.
In this section we shall consider Gaussian components to illustrate the frame-
work of mixture models. More generally, mixture models can comprise linear com-
binations of other distributions. For instance, in Section 9.3.3 we shall consider
mixtures of Bernoulli distributions as an example of a mixture model for discrete
variables.
The parameters πk in (2.188) are called mixing coefficients. If we integrate both
sides of (2.188) with respect to x, and note that both p(x) and the individual Gaussian
components are normalized, we obtain
πk = 1.
k=1
(2.189)
Also, the requirement that p(x) � 0, together with N (x|µk, Σk) � 0, implies
πk � 0 for all k. Combining this with the condition (2.189) we obtain
0 � πk � 1.
(2.190)
2.3. The Gaussian Distribution
Figure 2.22 Example of a Gaussian mixture distribution
in one dimension showing three Gaussians
(each scaled by a coefficient) in blue and
their sum in red.
p(x)
111
x
Section 9.3.3
k=1
K
N
1
K
112
2. PROBABILITY DISTRIBUTIONS
1
(a)
1
(b)
0.5
0
0.5
0.3
0.2
0.5
0
0
0.5
1
0
0.5
Figure 2.23 Illustration of a mixture of 3 Gaussians in a two-dimensional space.
(a) Contours of constant
density for each of the mixture components, in which the 3 components are denoted red, blue and green, and
the values of the mixing coefficients are shown below each component. (b) Contours of the marginal probability
density p(x) of the mixture distribution. (c) A surface plot of the distribution p(x).
We therefore see that the mixing coefficients satisfy the requirements to be probabil-
ities.
From the sum and product rules, the marginal density is given by
p(x) =
p(k)p(x|k)
(2.191)
which is equivalent to (2.188) in which we can view πk = p(k) as the prior prob-
ability of picking the kth component, and the density N (x|µk, Σk) = p(x|k) as
the probability of x conditioned on k. As we shall see in later chapters, an impor-
tant role is played by the posterior probabilities p(k|x), which are also known as
responsibilities. From Bayes’ theorem these are given by
γk(x) ≡ p(k|x)
p(k)p(x|k)
l p(l)p(x|l)
πkN (x|µk, Σk)
l πlN (x|µl, Σl) .
(2.192)
We shall discuss the probabilistic interpretation of the mixture distribution in greater
detail in Chapter 9.
The form of the Gaussian mixture distribution is governed by the parameters π,
µ and Σ, where we have used the notation π ≡ {π1, . . . , πK}, µ ≡ {µ1, . . . , µK}
and Σ ≡ {Σ1, . . . ΣK}. One way to set the values of these parameters is to use
maximum likelihood. From (2.188) the log of the likelihood function is given by
ln p(X|π, µ, Σ) =
ln
n=1
k=1
πkN (xn|µk, Σk)
(2.193)
