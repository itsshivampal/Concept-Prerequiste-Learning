537
Now suppose we move from a maximum likelihood approach to a full Bayesian
treatment in which we wish to sample from the posterior distribution over the param-
eter vector θ. In principle, we would like to draw samples from the joint posterior
p(θ, Z|X), but we shall suppose that this is computationally difficult. Suppose fur-
ther that it is relatively straightforward to sample from the complete-data parameter
posterior p(θ|Z, X). This inspires the data augmentation algorithm, which alter-
nates between two steps known as the I-step (imputation step, analogous to an E
step) and the P-step (posterior step, analogous to an M step).
IP Algorithm
I-step. We wish to sample from p(Z|X) but we cannot do this directly. We
therefore note the relation
p(Z|X) =
p(Z|θ, X)p(θ|X) dθ
(11.30)
and hence for l = 1, . . . , L we first draw a sample θ(l) from the current esti-
mate for p(θ|X), and then use this to draw a sample Z(l) from p(Z|θ(l), X).
P-step. Given the relation
p(θ|X) =
p(θ|Z, X)p(Z|X) dZ
(11.31)
we use the samples {Z(l)} obtained from the I-step to compute a revised
estimate of the posterior distribution over θ given by
p(θ|X) 
1
L
p(θ|Z(l), X).
l=1
(11.32)
By assumption, it will be feasible to sample from this approximation in the
I-step.
Note that we are making a (somewhat artificial) distinction between parameters θ
and hidden variables Z. From now on, we blur this distinction and focus simply on
the problem of drawing samples from a given posterior distribution.
11.2. Markov Chain Monte Carlo
In the previous section, we discussed the rejection sampling and importance sam-
pling strategies for evaluating expectations of functions, and we saw that they suffer
from severe limitations particularly in spaces of high dimensionality. We therefore
turn in this section to a very general and powerful framework called Markov chain
Monte Carlo (MCMC), which allows sampling from a large class of distributions,
538
11. SAMPLING METHODS
Section 11.2.1
A(z, z(τ )) = min
1,
(11.33)
p(z)
p(z(τ ))
and which scales well with the dimensionality of the sample space. Markov chain
Monte Carlo methods have their origins in physics (Metropolis and Ulam, 1949),
and it was only towards the end of the 1980s that they started to have a significant
impact in the field of statistics.
As with rejection and importance sampling, we again sample from a proposal
distribution. This time, however, we maintain a record of the current state z(τ ), and
the proposal distribution q(z|z(τ )) depends on this current state, and so the sequence
of samples z(1), z(2), . . . forms a Markov chain. Again, if we write p(z) =
p(z)/Zp,
p(z) can readily be evaluated for any given value of z, although
we will assume that
the value of Zp may be unknown. The proposal distribution itself is chosen to be
sufficiently simple that it is straightforward to draw samples from it directly. At
each cycle of the algorithm, we generate a candidate sample z from the proposal
distribution and then accept the sample according to an appropriate criterion.
In the basic Metropolis algorithm (Metropolis et al., 1953), we assume that the
proposal distribution is symmetric, that is q(zA|zB) = q(zB|zA) for all values of
zA and zB. The candidate sample is then accepted with probability
This can be achieved by choosing a random number u with uniform distribution over
the unit interval (0, 1) and then accepting the sample if A(z, z(τ )) > u. Note that
if the step from z(τ ) to z causes an increase in the value of p(z), then the candidate
point is certain to be kept.
If the candidate sample is accepted, then z(τ +1) = z, otherwise the candidate
point z is discarded, z(τ +1) is set to z(τ ) and another candidate sample is drawn
from the distribution q(z|z(τ +1)). This is in contrast to rejection sampling, where re-
jected samples are simply discarded. In the Metropolis algorithm when a candidate
point is rejected, the previous sample is included instead in the final list of samples,
leading to multiple copies of samples. Of course, in a practical implementation,
only a single copy of each retained sample would be kept, along with an integer
weighting factor recording how many times that state appears. As we shall see, as
long as q(zA|zB) is positive for any values of zA and zB (this is a sufficient but
not necessary condition), the distribution of z(τ ) tends to p(z) as τ → ∞. It should
be emphasized, however, that the sequence z(1), z(2), . . . is not a set of independent
samples from p(z) because successive samples are highly correlated. If we wish to
obtain independent samples, then we can discard most of the sequence and just re-
tain every M th sample. For M sufficiently large, the retained samples will for all
practical purposes be independent. Figure 11.9 shows a simple illustrative exam-
ple of sampling from a two-dimensional Gaussian distribution using the Metropolis
algorithm in which the proposal distribution is an isotropic Gaussian.
Further insight into the nature of Markov chain Monte Carlo algorithms can be
gleaned by looking at the properties of a specific example, namely a simple random
11.2. Markov Chain Monte Carlo
539
Figure 11.9 A simple illustration using Metropo-
lis algorithm to sample from a
Gaussian distribution whose one
standard-deviation contour is shown
by the ellipse. The proposal distribu-
tion is an isotropic Gaussian distri-
bution whose standard deviation is
0.2. Steps that are accepted are
shown as green lines, and rejected
steps are shown in red. A total of
150 candidate samples are gener-
ated, of which 43 are rejected.
3
2.5
2
1.5
1
0.5
0
0
0.5
1
1.5
2
2.5
3
Exercise 11.10
walk. Consider a state space z consisting of the integers, with probabilities
p(z(τ +1) = z(τ )) = 0.5
p(z(τ +1) = z(τ ) + 1) = 0.25
p(z(τ +1) = z(τ ) − 1) = 0.25
(11.34)
(11.35)
(11.36)
where z(τ ) denotes the state at step τ . If the initial state is z(1) = 0, then by sym-
metry the expected state at time τ will also be zero E[z(τ )] = 0, and similarly it is
easily seen that E[(z(τ ))2] = τ /2. Thus after τ steps, the random walk has only trav-
elled a distance that on average is proportional to the square root of τ . This square
root dependence is typical of random walk behaviour and shows that random walks
are very inefficient in exploring the state space. As we shall see, a central goal in
designing Markov chain Monte Carlo methods is to avoid random walk behaviour.
