We begin our treatment of generalized linear models by considering the problem
of two-class classification. In our discussion of generative approaches in Section 4.2,
we saw that under rather general assumptions, the posterior probability of class C1
can be written as a logistic sigmoid acting on a linear function of the feature vector
φ so that
p(C1|φ) = y(φ) = σ
(4.87)
with p(C2|φ) = 1 − p(C1|φ). Here σ(·) is the logistic sigmoid function defined by
(4.59). In the terminology of statistics, this model is known as logistic regression,
although it should be emphasized that this is a model for classification rather than
regression.
wTφ
For an M-dimensional feature space φ, this model has M adjustable parameters.
By contrast, if we had fitted Gaussian class conditional densities using maximum
likelihood, we would have used 2M parameters for the means and M(M + 1)/2
parameters for the (shared) covariance matrix. Together with the class prior p(C1),
this gives a total of M(M +5)/2+1 parameters, which grows quadratically with M,
in contrast to the linear dependence on M of the number of parameters in logistic
regression. For large values of M, there is a clear advantage in working with the
logistic regression model directly.
We now use maximum likelihood to determine the parameters of the logistic
regression model. To do this, we shall make use of the derivative of the logistic sig-
moid function, which can conveniently be expressed in terms of the sigmoid function
itself
dσ
da
= σ(1 − σ).
(4.88)
Section 3.6
Exercise 4.12
n=1
N
N
n=1
N
Exercise 4.13
Section 3.1.1
Exercise 4.14
E(w) = − ln p(t|w) = −
{tn ln yn + (1 − tn) ln(1 − yn)}
(4.90)
where yn = σ(an) and an = wTφn. Taking the gradient of the error function with
respect to w, we obtain
∇E(w) =
(yn − tn)φn
n=1
(4.91)
where we have made use of (4.88). We see that the factor involving the derivative
of the logistic sigmoid has cancelled, leading to a simplified form for the gradient
of the log likelihood. In particular, the contribution to the gradient from data point
n is given by the ‘error’ yn − tn between the target value and the prediction of the
model, times the basis function vector φn. Furthermore, comparison with (3.13)
shows that this takes precisely the same form as the gradient of the sum-of-squares
error function for the linear regression model.
If desired, we could make use of the result (4.91) to give a sequential algorithm
in which patterns are presented one at a time, in which each of the weight vectors is
updated using (3.22) in which ∇En is the nth term in (4.91).
It is worth noting that maximum likelihood can exhibit severe over-fitting for
data sets that are linearly separable. This arises because the maximum likelihood so-
lution occurs when the hyperplane corresponding to σ = 0.5, equivalent to wTφ =
0, separates the two classes and the magnitude of w goes to infinity. In this case, the
logistic sigmoid function becomes infinitely steep in feature space, corresponding to
a Heaviside step function, so that every training point from each class k is assigned
a posterior probability p(Ck|x) = 1. Furthermore, there is typically a continuum
of such solutions because any separating hyperplane will give rise to the same pos-
terior probabilities at the training data points, as will be seen later in Figure 10.13.
Maximum likelihood provides no way to favour one such solution over another, and
which solution is found in practice will depend on the choice of optimization algo-
rithm and on the parameter initialization. Note that the problem will arise even if
the number of data points is large compared with the number of parameters in the
model, so long as the training data set is linearly separable. The singularity can be
avoided by inclusion of a prior and finding a MAP solution for w, or equivalently by
adding a regularization term to the error function.
206
4. LINEAR MODELS FOR CLASSIFICATION
For a data set {φn, tn}, where tn ∈ {0, 1} and φn = φ(xn), with n =
1, . . . , N, the likelihood function can be written
p(t|w) =
n {1 − yn}1−tn
ytn
(4.89)
where t = (t1, . . . , tN )T and yn = p(C1|φn). As usual, we can define an error
function by taking the negative logarithm of the likelihood, which gives the cross-
entropy error function in the form
n=1
N
N
n=1
N
N
4.3. Probabilistic Discriminative Models
207
