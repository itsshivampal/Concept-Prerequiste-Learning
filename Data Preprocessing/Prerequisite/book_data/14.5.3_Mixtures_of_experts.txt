In Section 14.5.1, we considered a mixture of linear regression models, and in
Section 14.5.2 we discussed the analogous mixture of linear classifiers. Although
these simple mixtures extend the flexibility of linear models to include more com-
plex (e.g., multimodal) predictive distributions, they are still very limited. We can
further increase the capability of such models by allowing the mixing coefficients
themselves to be functions of the input variable, so that
p(t|x) =
k=1
πk(x)pk(t|x).
(14.53)
This is known as a mixture of experts model (Jacobs et al., 1991) in which the mix-
ing coefficients πk(x) are known as gating functions and the individual component
densities pk(t|x) are called experts. The notion behind the terminology is that differ-
ent components can model the distribution in different regions of input space (they
are ‘experts’ at making predictions in their own regions), and the gating functions
determine which components are dominant in which region.
The gating functions πk(x) must satisfy the usual constraints for mixing co-
efficients, namely 0 � πk(x) � 1 and
k πk(x) = 1. They can therefore be
represented, for example, by linear softmax models of the form (4.104) and (4.105).
If the experts are also linear (regression or classification) models, then the whole
model can be fitted efficiently using the EM algorithm, with iterative reweighted
least squares being employed in the M step (Jordan and Jacobs, 1994).
Exercise 14.17
Section 4.3.3
Such a model still has significant limitations due to the use of linear models
for the gating and expert functions. A much more flexible model is obtained by
using a multilevel gating function to give the hierarchical mixture of experts, or
HME model (Jordan and Jacobs, 1994). To understand the structure of this model,
imagine a mixture distribution in which each component in the mixture is itself a
mixture distribution. For simple unconditional mixtures, this hierarchical mixture is
trivially equivalent to a single flat mixture distribution. However, when the mixing
coefficients are input dependent, this hierarchical model becomes nontrivial. The
HME model can also be viewed as a probabilistic version of decision trees discussed
in Section 14.4 and can again be trained efficiently by maximum likelihood using an
EM algorithm with IRLS in the M step. A Bayesian treatment of the HME has been
given by Bishop and Svens´en (2003) based on variational inference.
We shall not discuss the HME in detail here. However, it is worth pointing out
the close connection with the mixture density network discussed in Section 5.6. The
principal advantage of the mixtures of experts model is that it can be optimized by
EM in which the M step for each mixture component and gating model involves
a convex optimization (although the overall optimization is nonconvex). By con-
trast, the advantage of the mixture density network approach is that the component
14.5. Conditional Mixture Models
673
Figure 14.10 Illustration of a mixture of logistic regression models. The left plot shows data points drawn
from two classes denoted red and blue, in which the background colour (which varies from pure red to pure blue)
denotes the true probability of the class label. The centre plot shows the result of fitting a single logistic regression
model using maximum likelihood, in which the background colour denotes the corresponding probability of the
class label. Because the colour is a near-uniform purple, we see that the model assigns a probability of around
0.5 to each of the classes over most of input space. The right plot shows the result of fitting a mixture of two
logistic regression models, which now gives much higher probability to the correct labels for many of the points
in the blue class.
674
14. COMBINING MODELS
Exercises
M
M
densities and the mixing coefficients share the hidden units of the neural network.
Furthermore, in the mixture density network, the splits of the input space are further
relaxed compared to the hierarchical mixture of experts in that they are not only soft,
and not constrained to be axis aligned, but they can also be nonlinear.
14.1 ( ) www Consider a set models of the form p(t|x, zh, θh, h) in which x is the
input vector, t is the target vector, h indexes the different models, zh is a latent vari-
able for model h, and θh is the set of parameters for model h. Suppose the models
have prior probabilities p(h) and that we are given a training set X = {x1, . . . , xN}
and T = {t1, . . . , tN}. Write down the formulae needed to evaluate the predic-
tive distribution p(t|x, X, T) in which the latent variables and the model index are
marginalized out. Use these formulae to highlight the difference between Bayesian
averaging of different models and the use of latent variables within a single model.
14.2 () The expected sum-of-squares error EAV for a simple committee model can
be defined by (14.10), and the expected error of the committee itself is given by
(14.11). Assuming that the individual errors satisfy (14.12) and (14.13), derive the
result (14.14).
14.3 () www By making use of Jensen’s inequality (1.115), for the special case of
the convex function f(x) = x2, show that the average expected sum-of-squares
error EAV of the members of a simple committee model, given by (14.10), and the
expected error ECOM of the committee itself, given by (14.11), satisfy
ECOM � EAV.
(14.54)
14.4 ( ) By making use of Jensen’s in equality (1.115), show that the result (14.54)
derived in the previous exercise hods for any error function E(y), not just sum-of-
squares, provided it is a convex function of y.
14.5 ( ) www Consider a committee in which we allow unequal weighting of the
constituent models, so that
yCOM(x) =
m=1
αmym(x).
(14.55)
In order to ensure that the predictions yCOM(x) remain within sensible limits, sup-
pose that we require that they be bounded at each value of x by the minimum and
maximum values given by any of the members of the committee, so that
ymin(x) � yCOM(x) � ymax(x).
(14.56)
Show that a necessary and sufficient condition for this constraint is that the coeffi-
cients αm satisfy
αm � 0,
αm = 1.
m=1
(14.57)
Exercises
675
14.6 () www By differentiating the error function (14.23) with respect to αm, show
that the parameters αm in the AdaBoost algorithm are updated using (14.17) in
which 	m is defined by (14.16).
14.7 () By making a variational minimization of the expected exponential error function
given by (14.27) with respect to all possible functions y(x), show that the minimizing
function is given by (14.28).
14.8 () Show that the exponential error function (14.20), which is minimized by the
AdaBoost algorithm, does not correspond to the log likelihood of any well-behaved
probabilistic model. This can be done by showing that the corresponding conditional
distribution p(t|x) cannot be correctly normalized.
14.9 () www Show that the sequential minimization of the sum-of-squares error func-
tion for an additive model of the form (14.21) in the style of boosting simply involves
fitting each new base classifier to the residual errors tn−fm−1(xn) from the previous
model.
14.10 () Verify that if we minimize the sum-of-squares error between a set of training
values {tn} and a single predictive value t, then the optimal solution for t is given
by the mean of the {tn}.
14.11 ( ) Consider a data set comprising 400 data points from class C1 and 400 data
points from class C2. Suppose that a tree model A splits these into (300, 100) at
the first leaf node and (100, 300) at the second leaf node, where (n, m) denotes that
n points are assigned to C1 and m points are assigned to C2. Similarly, suppose
that a second tree model B splits them into (200, 400) and (200, 0). Evaluate the
misclassification rates for the two trees and hence show that they are equal. Similarly,
evaluate the cross-entropy (14.32) and Gini index (14.33) for the two trees and show
that they are both lower for tree B than for tree A.
14.12 ( ) Extend the results of Section 14.5.1 for a mixture of linear regression models
to the case of multiple target values described by a vector t. To do this, make use of
the results of Section 3.1.5.
14.13 () www Verify that the complete-data log likelihood function for the mixture of
linear regression models is given by (14.36).
14.14 () Use the technique of Lagrange multipliers (Appendix E) to show that the M-step
re-estimation equation for the mixing coefficients in the mixture of linear regression
models trained by maximum likelihood EM is given by (14.38).
14.15 () www We have already noted that if we use a squared loss function in a regres-
sion problem, the corresponding optimal prediction of the target variable for a new
input vector is given by the conditional mean of the predictive distribution. Show
that the conditional mean for the mixture of linear regression models discussed in
Section 14.5.1 is given by a linear combination of the means of each component dis-
tribution. Note that if the conditional distribution of the target data is multimodal,
the conditional mean can give poor predictions.
K
676
14. COMBINING MODELS
14.16 (  ) Extend the logistic regression mixture model of Section 14.5.2 to a mixture
of softmax classifiers representing C � 2 classes. Write down the EM algorithm for
determining the parameters of this model through maximum likelihood.
14.17 ( ) www Consider a mixture model for a conditional distribution p(t|x) of the
form
p(t|x) =
k=1
πkψk(t|x)
(14.58)
in which each mixture component ψk(t|x) is itself a mixture model. Show that this
two-level hierarchical mixture is equivalent to a conventional single-level mixture
model. Now suppose that the mixing coefficients in both levels of such a hierar-
chical model are arbitrary functions of x. Again, show that this hierarchical model
is again equivalent to a single-level model with x-dependent mixing coefficients.
Finally, consider the case in which the mixing coefficients at both levels of the hi-
erarchical mixture are constrained to be linear classification (logistic or softmax)
models. Show that the hierarchical mixture cannot in general be represented by a
single-level mixture having linear classification models for the mixing coefficients.
Hint: to do this it is sufficient to construct a single counter-example, so consider a
mixture of two components in which one of those components is itself a mixture of
two components, with mixing coefficients given by linear-logistic models. Show that
this cannot be represented by a single-level mixture of 3 components having mixing
coefficients determined by a linear-softmax model.
