Let us first consider the maximization of p(t|α, β) with respect to α. This can
be done by first defining the following eigenvector equation
(3.87)
From (3.81), it then follows that A has eigenvalues α + λi. Now consider the deriva-
tive of the term involving ln|A| in (3.86) with respect to α. We have
ui = λiui.
βΦTΦ
d
dα
ln|A| = d
dα
ln
i
(λi + α) = d
dα
ln(λi + α) =
1
λi + α
i
Thus the stationary points of (3.86) with respect to α satisfy
0 = M
2α −
1
2
mT
N mN −
1
2
1
λi + α
i
(3.88)
(3.89)
1
2
n=1
N
i
i
N
1
i
169
(3.90)
(3.91)
(3.92)
(3.94)
(3.95)
3.5. The Evidence Approximation
Multiplying through by 2α and rearranging, we obtain
αmT
N mN = M − α
1
λi + α
i
Since there are M terms in the sum over i, the quantity γ can be written
Exercise 3.20
The interpretation of the quantity γ will be discussed shortly. From (3.90) we see
that the value of α that maximizes the marginal likelihood satisfies
λi
α + λi
mT
N mN
Note that this is an implicit solution for α not only because γ depends on α, but also
because the mode mN of the posterior distribution itself depends on the choice of
α. We therefore adopt an iterative procedure in which we make an initial choice for
α and use this to find mN , which is given by (3.53), and also to evaluate γ, which
is given by (3.91). These values are then used to re-estimate α using (3.92), and the
process repeated until convergence. Note that because the matrix ΦTΦ is fixed, we
can compute its eigenvalues once at the start and then simply multiply these by β to
obtain the λi.
It should be emphasized that the value of α has been determined purely by look-
ing at the training data. In contrast to maximum likelihood methods, no independent
data set is required in order to optimize the model complexity.
We can similarly maximize the log marginal likelihood (3.86) with respect to β.
To do this, we note that the eigenvalues λi defined by (3.87) are proportional to β,
and hence dλi/dβ = λi/β giving
d
dβ
ln|A| = d
dβ
ln(λi + α) =
1
λi
λi + α
(3.93)
The stationary point of the marginal likelihood therefore satisfies
Exercise 3.22
and rearranging we obtain
0 = N
2β −
tn − mT
N φ(xn)
2
2β
1
N − γ
n=1
tn − mT
N φ(xn)
2
Again, this is an implicit solution for β and can be solved by choosing an initial
value for β and then using this to calculate mN and γ and then re-estimate β using
(3.95), repeating until convergence. If both α and β are to be determined from the
data, then their values can be re-estimated together after each update of γ.
170
3. LINEAR MODELS FOR REGRESSION
Figure 3.15 Contours of the likelihood function (red)
and the prior (green) in which the axes in parameter
space have been rotated to align with the eigenvectors
ui of the Hessian. For α = 0, the mode of the poste-
rior is given by the maximum likelihood solution wML,
whereas for nonzero α the mode is at wMAP = mN . In
the direction w1 the eigenvalue λ1, defined by (3.87), is
small compared with α and so the quantity λ1/(λ1 + α)
is close to zero, and the corresponding MAP value of
w1 is also close to zero. By contrast, in the direction w2
the eigenvalue λ2 is large compared with α and so the
quantity λ2/(λ2 +α) is close to unity, and the MAP value
of w2 is close to its maximum likelihood value.
w2
u2
wML
wMAP
u1
w1
