227
5.1. Feed-forward Network Functions
The linear models for regression and classification discussed in Chapters 3 and 4, re-
spectively, are based on linear combinations of fixed nonlinear basis functions φj(x)
and take the form
y(x, w) = f
wjφj(x)
j=1
(5.1)
where f(·) is a nonlinear activation function in the case of classification and is the
identity in the case of regression. Our goal is to extend this model by making the
basis functions φj(x) depend on parameters and then to allow these parameters to
be adjusted, along with the coefficients {wj}, during training. There are, of course,
many ways to construct parametric nonlinear basis functions. Neural networks use
basis functions that follow the same form as (5.1), so that each basis function is itself
a nonlinear function of a linear combination of the inputs, where the coefficients in
the linear combination are adaptive parameters.
This leads to the basic neural network model, which can be described a series
of functional transformations. First we construct M linear combinations of the input
variables x1, . . . , xD in the form
aj =
w(1)
ji xi + w(1)
j0
(5.2)
where j = 1, . . . , M, and the superscript (1) indicates that the corresponding param-
eters are in the first ‘layer’ of the network. We shall refer to the parameters w(1)
ji as
weights and the parameters w(1)
j0 as biases, following the nomenclature of Chapter 3.
The quantities aj are known as activations. Each of them is then transformed using
a differentiable, nonlinear activation function h(·) to give
zj = h(aj).
(5.3)
These quantities correspond to the outputs of the basis functions in (5.1) that, in the
context of neural networks, are called hidden units. The nonlinear functions h(·) are
generally chosen to be sigmoidal functions such as the logistic sigmoid or the ‘tanh’
function. Following (5.1), these values are again linearly combined to give output
unit activations
ak =
w(2)
kj zj + w(2)
k0
j=1
(5.4)
where k = 1, . . . , K, and K is the total number of outputs. This transformation cor-
responds to the second layer of the network, and again the w(2)
k0 are bias parameters.
Finally, the output unit activations are transformed using an appropriate activation
function to give a set of network outputs yk. The choice of activation function is
determined by the nature of the data and the assumed distribution of target variables
Exercise 5.1
M
D
228
5. NEURAL NETWORKS
Figure 5.1 Network diagram for the two-
layer neural network corre-
sponding to (5.7). The input,
hidden, and output variables
are represented by nodes, and
the weight parameters are rep-
resented by links between the
nodes,
in which the bias pa-
rameters are denoted by links
coming from additional
input
and hidden variables x0 and
z0. Arrows denote the direc-
tion of information flow through
the network during forward
propagation.
xD
inputs
x1
x0
hidden units
w(1)
M D
w(2)
KM
zM
z1
z0
yK
outputs
y1
w(2)
10
and follows the same considerations as for linear models discussed in Chapters 3 and
4. Thus for standard regression problems, the activation function is the identity so
that yk = ak. Similarly, for multiple binary classification problems, each output unit
activation is transformed using a logistic sigmoid function so that
where
yk = σ(ak)
σ(a) =
1
1 + exp(−a) .
(5.5)
(5.6)
Finally, for multiclass problems, a softmax activation function of the form (4.62)
is used. The choice of output unit activation function is discussed in detail in Sec-
tion 5.2.
We can combine these various stages to give the overall network function that,
for sigmoidal output unit activation functions, takes the form
yk(x, w) = σ
w(2)
kj h
w(1)
ji xi + w(1)
j0
+ w(2)
k0
(5.7)
j=1
i=1
where the set of all weight and bias parameters have been grouped together into a
vector w. Thus the neural network model is simply a nonlinear function from a set
of input variables {xi} to a set of output variables {yk} controlled by a vector w of
adjustable parameters.
This function can be represented in the form of a network diagram as shown
in Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward
propagation of information through the network. It should be emphasized that these
diagrams do not represent probabilistic graphical models of the kind to be consid-
ered in Chapter 8 because the internal nodes represent deterministic variables rather
than stochastic ones. For this reason, we have adopted a slightly different graphical
i=0
D
M
D
5.1. Feed-forward Network Functions
229
notation for the two kinds of model. We shall see later how to give a probabilistic
interpretation to a neural network.
As discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into
the set of weight parameters by defining an additional input variable x0 whose value
is clamped at x0 = 1, so that (5.2) takes the form
aj =
w(1)
ji xi.
(5.8)
We can similarly absorb the second-layer biases into the second-layer weights, so
that the overall network function becomes
yk(x, w) = σ
w(2)
kj h
w(1)
ji xi
(5.9)
j=0
i=0
As can be seen from Figure 5.1, the neural network model comprises two stages
of processing, each of which resembles the perceptron model of Section 4.1.7, and
for this reason the neural network is also known as the multilayer perceptron, or
MLP. A key difference compared to the perceptron, however, is that the neural net-
work uses continuous sigmoidal nonlinearities in the hidden units, whereas the per-
ceptron uses step-function nonlinearities. This means that the neural network func-
tion is differentiable with respect to the network parameters, and this property will
play a central role in network training.
If the activation functions of all the hidden units in a network are taken to be
linear, then for any such network we can always find an equivalent network without
hidden units. This follows from the fact that the composition of successive linear
transformations is itself a linear transformation. However, if the number of hidden
units is smaller than either the number of input or output units, then the transforma-
tions that the network can generate are not the most general possible linear trans-
formations from inputs to outputs because information is lost in the dimensionality
reduction at the hidden units. In Section 12.4.2, we show that networks of linear
units give rise to principal component analysis. In general, however, there is little
interest in multilayer networks of linear units.
The network architecture shown in Figure 5.1 is the most commonly used one
in practice. However, it is easily generalized, for instance by considering additional
layers of processing each consisting of a weighted linear combination of the form
(5.4) followed by an element-wise transformation using a nonlinear activation func-
tion. Note that there is some confusion in the literature regarding the terminology
for counting the number of layers in such networks. Thus the network in Figure 5.1
may be described as a 3-layer network (which counts the number of layers of units,
and treats the inputs as units) or sometimes as a single-hidden-layer network (which
counts the number of layers of hidden units). We recommend a terminology in which
Figure 5.1 is called a two-layer network, because it is the number of layers of adap-
tive weights that is important for determining the network properties.
Another generalization of the network architecture is to include skip-layer con-
nections, each of which is associated with a corresponding adaptive parameter. For
inputs
x2
x1
230
5. NEURAL NETWORKS
Figure 5.2 Example of a neural network having a
general feed-forward topology. Note that
each hidden and output unit has an
associated bias parameter (omitted for
clarity).
z1
z2
z3
y2
outputs
y1
instance, in a two-layer network these would go directly from inputs to outputs. In
principle, a network with sigmoidal hidden units can always mimic skip layer con-
nections (for bounded input values) by using a sufficiently small first-layer weight
that, over its operating range, the hidden unit is effectively linear, and then com-
pensating with a large weight value from the hidden unit to the output. In practice,
however, it may be advantageous to include skip-layer connections explicitly.
Furthermore, the network can be sparse, with not all possible connections within
a layer being present. We shall see an example of a sparse network architecture when
we consider convolutional neural networks in Section 5.5.6.
Because there is a direct correspondence between a network diagram and its
mathematical function, we can develop more general network mappings by con-
sidering more complex network diagrams. However, these must be restricted to a
feed-forward architecture, in other words to one having no closed directed cycles, to
ensure that the outputs are deterministic functions of the inputs. This is illustrated
with a simple example in Figure 5.2. Each (hidden or output) unit in such a network
computes a function given by
zk = h
wkjzj
j
(5.10)
where the sum runs over all units that send connections to unit k (and a bias param-
eter is included in the summation). For a given set of values applied to the inputs of
the network, successive application of (5.10) allows the activations of all units in the
network to be evaluated including those of the output units.
The approximation properties of feed-forward networks have been widely stud-
ied (Funahashi, 1989; Cybenko, 1989; Hornik et al., 1989; Stinchecombe and White,
1989; Cotter, 1990; Ito, 1991; Hornik, 1991; Kreinovich, 1991; Ripley, 1996) and
found to be very general. Neural networks are therefore said to be universal ap-
proximators. For example, a two-layer network with linear outputs can uniformly
approximate any continuous function on a compact input domain to arbitrary accu-
racy provided the network has a sufficiently large number of hidden units. This result
holds for a wide range of hidden unit activation functions, but excluding polynomi-
als. Although such theorems are reassuring, the key problem is how to find suitable
parameter values given a set of training data, and in later sections of this chapter we
Figure 5.3 Illustration of
the ca-
pability of a multilayer perceptron
to approximate four different func-
tions comprising (a) f (x) = x2, (b)
f (x) = sin(x), (c), f (x) = |x|,
and (d) f (x) = H(x) where H(x)
is the Heaviside step function.
In
each case, N = 50 data points,
shown as blue dots, have been sam-
pled uniformly in x over the interval
(−1, 1) and the corresponding val-
ues of f (x) evaluated. These data
points are then used to train a two-
layer network having 3 hidden units
with ‘tanh’ activation functions and
linear output units. The resulting
network functions are shown by the
red curves, and the outputs of the
three hidden units are shown by the
three dashed curves.
5.1. Feed-forward Network Functions
231
(a)
(c)
(b)
(d)
will show that there exist effective solutions to this problem based on both maximum
likelihood and Bayesian approaches.
The capability of a two-layer network to model a broad range of functions is
illustrated in Figure 5.3. This figure also shows how individual hidden units work
collaboratively to approximate the final function. The role of hidden units in a simple
classification problem is illustrated in Figure 5.4 using the synthetic classification
data set described in Appendix A.
