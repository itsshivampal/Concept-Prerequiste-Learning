Our discussion of the maximum likelihood solution for the parameters of a Gaus-
sian distribution provides a convenient opportunity to give a more general discussion
of the topic of sequential estimation for maximum likelihood. Sequential methods
allow data points to be processed one at a time and then discarded and are important
for on-line applications, and also where large data sets are involved so that batch
processing of all data points at once is infeasible.
Consider the result (2.121) for the maximum likelihood estimator of the mean
ML when it is based on N observations. If we
µML, which we will denote by µ(N )
N
z
Figure 2.10 A schematic illustration of two correlated ran-
dom variables z and θ,
together with the
regression function f (θ) given by the con-
The Robbins-
ditional expectation E[z|θ].
Monro algorithm provides a general sequen-
tial procedure for finding the root θ of such
functions.
2.3. The Gaussian Distribution
95
f(θ)
dissect out the contribution from the final data point xN , we obtain
µ(N )
ML =
1
N
xn
n=1
N−1
n=1
1
N
xN +
1
N
1
N
ML +
xN + N − 1
N
1
N
xn
µ(N−1)
ML
= µ(N−1)
(xN − µ(N−1)
ML
(2.126)
This result has a nice interpretation, as follows. After observing N − 1 data points
we have estimated µ by µ(N−1)
. We now observe data point xN , and we obtain our
revised estimate µ(N )
ML by moving the old estimate a small amount, proportional to
1/N, in the direction of the ‘error signal’ (xN − µ(N−1)
). Note that, as N increases,
so the contribution from successive data points gets smaller.
ML
ML
The result (2.126) will clearly give the same answer as the batch result (2.121)
because the two formulae are equivalent. However, we will not always be able to de-
rive a sequential algorithm by this route, and so we seek a more general formulation
of sequential learning, which leads us to the Robbins-Monro algorithm. Consider a
pair of random variables θ and z governed by a joint distribution p(z, θ). The con-
ditional expectation of z given θ defines a deterministic function f(θ) that is given
by
f(θ) ≡ E[z|θ] =
zp(z|θ) dz
(2.127)
and is illustrated schematically in Figure 2.10. Functions defined in this way are
called regression functions.
Our goal is to find the root θ at which f(θ) = 0. If we had a large data set
of observations of z and θ, then we could model the regression function directly and
then obtain an estimate of its root. Suppose, however, that we observe values of
z one at a time and we wish to find a corresponding sequential estimation scheme
for θ. The following general procedure for solving such problems was given by
N
N =1
N =1
N
n=1
96
2. PROBABILITY DISTRIBUTIONS
Robbins and Monro (1951). We shall assume that the conditional variance of z is
finite so that
(2.128)
and we shall also, without loss of generality, consider the case where f(θ) > 0 for
θ > θ and f(θ) < 0 for θ < θ, as is the case in Figure 2.10. The Robbins-Monro
procedure then defines a sequence of successive estimates of the root θ given by
(z − f)2 | θ
E
θ(N ) = θ(N−1) + aN−1z(θ(N−1))
where z(θ(N )) is an observed value of z when θ takes the value θ(N ). The coefficients
{aN} represent a sequence of positive numbers that satisfy the conditions
(2.129)
(2.130)
(2.131)
(2.132)
lim
N→∞
aN = 0
aN = ∞
a2
N < ∞.
It can then be shown (Robbins and Monro, 1951; Fukunaga, 1990) that the sequence
of estimates given by (2.129) does indeed converge to the root with probability one.
Note that the first condition (2.130) ensures that the successive corrections decrease
in magnitude so that the process can converge to a limiting value. The second con-
dition (2.131) is required to ensure that the algorithm does not converge short of the
root, and the third condition (2.132) is needed to ensure that the accumulated noise
has finite variance and hence does not spoil convergence.
Now let us consider how a general maximum likelihood problem can be solved
sequentially using the Robbins-Monro algorithm. By definition, the maximum like-
lihood solution θML is a stationary point of the log likelihood function and hence
satisfies
1
N
ln p(xn|θ)
n=1
= 0.
θML
(2.133)
Exchanging the derivative and the summation, and taking the limit N → ∞ we have
1
N
lim
N→∞
ln p(xn|θ) = Ex
ln p(x|θ)
(2.134)
and so we see that finding the maximum likelihood solution corresponds to find-
ing the root of a regression function. We can therefore apply the Robbins-Monro
procedure, which now takes the form
θ(N ) = θ(N−1) + aN−1
∂θ(N−1) ln p(xN|θ(N−1)).
(2.135)
N
z
N
2.3. The Gaussian Distribution
Figure 2.11 In the case of a Gaussian distribution, with θ
corresponding to the mean µ, the regression
function illustrated in Figure 2.10 takes the form
of a straight line, as shown in red.
In this
case, the random variable z corresponds to the
derivative of the log likelihood function and is
given by (x − µML)/σ2, and its expectation that
defines the regression function is a straight line
given by (µ − µML)/σ2. The root of the regres-
sion function corresponds to the maximum like-
lihood estimator µML.
µML
97
p(z|µ)
As a specific example, we consider once again the sequential estimation of the
mean of a Gaussian distribution, in which case the parameter θ(N ) is the estimate
µ(N )
ML of the mean of the Gaussian, and the random variable z is given by
z = ∂
∂µML
ln p(x|µML, σ2) =
1
σ2 (x − µML).
(2.136)
Thus the distribution of z is Gaussian with mean µ − µML, as illustrated in Fig-
ure 2.11. Substituting (2.136) into (2.135), we obtain the univariate form of (2.126),
provided we choose the coefficients aN to have the form aN = σ2/N. Note that
although we have focussed on the case of a single variable, the same technique,
together with the same restrictions (2.130)–(2.132) on the coefficients aN , apply
equally to the multivariate case (Blum, 1965).
