Chapter 1

Vectors

In this chapter we introduce vectors and some common operations on them. We
describe some settings in which vectors are used.

1.1 Vectors

A vector is an ordered ﬁnite list of numbers. Vectors are usually written as vertical
arrays, surrounded by square or curved brackets, as in



−1.1
0.0
3.6
−7.2

 or 

−1.1
0.0
3.6
−7.2

 .

They can also be written as numbers separated by commas and surrounded by
parentheses. In this notation style, the vector above is written as

(−1.1, 0.0, 3.6,−7.2).

The elements (or entries, coeﬃcients, components) of a vector are the values in the
array. The size (also called dimension or length) of the vector is the number of
elements it contains. The vector above, for example, has size four; its third entry
is 3.6. A vector of size n is called an n-vector. A 1-vector is considered to be the
same as a number, i.e., we do not distinguish between the 1-vector [ 1.3 ] and the
number 1.3.

We often use symbols to denote vectors. If we denote an n-vector using the
symbol a, the ith element of the vector a is denoted ai, where the subscript i is an
integer index that runs from 1 to n, the size of the vector.

Two vectors a and b are equal, which we denote a = b, if they have the same
size, and each of the corresponding entries is the same. If a and b are n-vectors,
then a = b means a1 = b1, . . . , an = bn.

4

1 Vectors

The numbers or values of the elements in a vector are called scalars. We will
focus on the case that arises in most applications, where the scalars are real num-
bers. In this case we refer to vectors as real vectors. (Occasionally other types of
scalars arise, for example, complex numbers, in which case we refer to the vector
as a complex vector.) The set of all real numbers is written as R, and the set of all
real n-vectors is denoted Rn, so a ∈ Rn is another way to say that a is an n-vector
with real entries. Here we use set notation: a ∈ Rn means that a is an element of
the set Rn; see appendix A.

Block or stacked vectors.
ing or stacking two or more vectors, as in

It is sometimes useful to deﬁne vectors by concatenat-

a =

b
c

d  ,

where a, b, c, and d are vectors. If b is an m-vector, c is an n-vector, and d is a
p-vector, this deﬁnes the (m + n + p)-vector

a = (b1, b2, . . . , bm, c1, c2, . . . , cn, d1, d2, . . . , dp).

The stacked vector a is also written as a = (b, c, d).

Stacked vectors can include scalars (numbers). For example if a is a 3-vector,

(1, a) is the 4-vector (1, a1, a2, a3).

Subvectors.
In the equation above, we say that b, c, and d are subvectors or
slices of a, with sizes m, n, and p, respectively. Colon notation is used to denote
subvectors. If a is a vector, then ar:s is the vector of size s − r + 1, with entries
ar, . . . , as:

ar:s = (ar, . . . , as).

The subscript r : s is called the index range. Thus, in our example above, we have

b = a1:m,

c = a(m+1):(m+n),

d = a(m+n+1):(m+n+p).

As a more concrete example, if z is the 4-vector (1,−1, 2, 0), the slice z2:3 is z2:3 =
(−1, 2). Colon notation is not completely standard, but it is growing in popularity.

Notational conventions. Some authors try to use notation that helps the reader
distinguish between vectors and scalars (numbers). For example, Greek letters
(α, β, . . . ) might be used for numbers, and lower-case letters (a, x, f , . . . ) for
vectors. Other notational conventions include vectors given in bold font (g), or
vectors written with arrows above them (�a). These notational conventions are not
standardized, so you should be prepared to ﬁgure out what things are (i.e., scalars
or vectors) despite the author’s notational scheme (if any exists).

1.1 Vectors

5

Indexing. We should give a couple of warnings concerning the subscripted index
notation ai. The ﬁrst warning concerns the range of the index. In many computer
languages, arrays of length n are indexed from i = 0 to i = n − 1. But in standard
mathematical notation, n-vectors are indexed from i = 1 to i = n, so in this book,
vectors will be indexed from i = 1 to i = n.

The next warning concerns an ambiguity in the notation ai, used for the ith
element of a vector a. The same notation will occasionally refer to the ith vector
in a collection or list of k vectors a1, . . . , ak. Whether a3 means the third element
of a vector a (in which case a3 is a number), or the third vector in some list of
vectors (in which case a3 is a vector) should be clear from the context. When we
need to refer to an element of a vector that is in an indexed collection of vectors,
we can write (ai)j to refer to the jth entry of ai, the ith vector in our list.

Zero vectors. A zero vector is a vector with all elements equal to zero. Sometimes
the zero vector of size n is written as 0n, where the subscript denotes the size.
But usually a zero vector is denoted just 0, the same symbol used to denote the
number 0. In this case you have to ﬁgure out the size of the zero vector from the
context. As a simple example, if a is a 9-vector, and we are told that a = 0, the 0
vector on the right-hand side must be the one of size 9.

Even though zero vectors of diﬀerent sizes are diﬀerent vectors, we use the same
symbol 0 to denote them.
In computer programming this is called overloading:
The symbol 0 is overloaded because it can mean diﬀerent things depending on the
context (e.g., the equation it appears in).

Unit vectors. A (standard) unit vector is a vector with all elements equal to zero,
except one element which is equal to one. The ith unit vector (of size n) is the
unit vector with ith element one, and denoted ei. For example, the vectors

e1 =

1
0

0  ,

e2 =

0
1

0  ,

e3 =

0
0

1 

are the three unit vectors of size 3. The notation for unit vectors is an example of
the ambiguity in notation noted above. Here, ei denotes the ith unit vector, and
not the ith element of a vector e. Thus we can describe the ith unit n-vector ei as

(ei)j =� 1

0

j = i
j �= i,

for i, j = 1, . . . , n. On the left-hand side ei is an n-vector; (ei)j is a number, its jth
entry. As with zero vectors, the size of ei is usually determined from the context.

Ones vector. We use the notation 1n for the n-vector with all its elements equal
to one. We also write 1 if the size of the vector can be determined from the
context. (Some authors use e to denote a vector of all ones, but we will not use
this notation.) The vector 1 is sometimes called the ones vector.

6

x2

1 Vectors

x

x1

x2

x

x1

Figure 1.1 Left. The 2-vector x speciﬁes the position (shown as a dot)
with coordinates x1 and x2 in a plane. Right. The 2-vector x represents a
displacement in the plane (shown as an arrow) by x1 in the ﬁrst axis and x2
in the second.

Sparsity. A vector is said to be sparse if many of its entries are zero; its sparsity
pattern is the set of indices of nonzero entries. The number of the nonzero entries
of an n-vector x is denoted nnz(x). Unit vectors are sparse, since they have only
one nonzero entry. The zero vector is the sparsest possible vector, since it has no
nonzero entries. Sparse vectors arise in many applications.

Examples

An n-vector can be used to represent n quantities or values in an application. In
some cases the values are similar in nature (for example, they are given in the same
physical units); in others, the quantities represented by the entries of the vector are
quite diﬀerent from each other. We brieﬂy describe below some typical examples,
many of which we will see throughout the book.

Location and displacement. A 2-vector can be used to represent a position or
location in a 2-dimensional (2-D) space, i.e., a plane, as shown in ﬁgure 1.1. A
3-vector is used to represent a location or position of some point in 3-dimensional
(3-D) space. The entries of the vector give the coordinates of the position or
location.

A vector can also be used to represent a displacement in a plane or 3-D space,
in which case it is typically drawn as an arrow, as shown in ﬁgure 1.1. A vector can
also be used to represent the velocity or acceleration, at a given time, of a point
that moves in a plane or 3-D space.

Color. A 3-vector can represent a color, with its entries giving the Red, Green,
and Blue (RGB) intensity values (often between 0 and 1). The vector (0, 0, 0)
represents black, the vector (0, 1, 0) represents a bright pure green color, and the
vector (1, 0.5, 0.5) represents a shade of pink. This is illustrated in ﬁgure 1.2.

1.1 Vectors

7

(1, 0, 0)

(0, 1, 0)

(0, 0, 1)

(1, 1, 0)

(1, 0.5, 0.5)

(0.5, 0.5, 0.5)

Figure 1.2 Six colors and their RGB vectors.

Quantities. An n-vector q can represent the amounts or quantities of n diﬀerent
resources or products held (or produced, or required) by an entity such as a com-
pany. Negative entries mean an amount of the resource owed to another party (or
consumed, or to be disposed of). For example, a bill of materials is a vector that
gives the amounts of n resources required to create a product or carry out a task.

Portfolio. An n-vector s can represent a stock portfolio or investment in n dif-
ferent assets, with si giving the number of shares of asset i held. The vector
(100, 50, 20) represents a portfolio consisting of 100 shares of asset 1, 50 shares of
asset 2, and 20 shares of asset 3. Short positions (i.e., shares that you owe another
party) are represented by negative entries in a portfolio vector. The entries of the
portfolio vector can also be given in dollar values, or fractions of the total dollar
amount invested.

Values across a population. An n-vector can give the values of some quantity
across a population of individuals or entities. For example, an n-vector b can
give the blood pressure of a collection of n patients, with bi the blood pressure of
patient i, for i = 1, . . . , n.

Proportions. A vector w can be used to give fractions or proportions out of n
choices, outcomes, or options, with wi the fraction with choice or outcome i. In
this case the entries are nonnegative and add up to one. Such vectors can also be
interpreted as the recipes for a mixture of n items, an allocation across n entities,
or as probability values in a probability space with n outcomes. For example, a
uniform mixture of 4 outcomes is represented as the 4-vector (1/4, 1/4, 1/4, 1/4).

Time series. An n-vector can represent a time series or signal, that is, the value
of some quantity at diﬀerent times. (The entries in a vector that represents a time
series are sometimes called samples, especially when the quantity is something

8

1 Vectors

)
F
◦
(

i

x

90

85

80

75

70

65

0

10

20

30

40

50

i

Figure 1.3 Hourly temperature in downtown Los Angeles on August 5 and
6, 2015 (starting at 12:47AM, ending at 11:47PM).

measured.) An audio (sound) signal can be represented as a vector whose entries
give the value of acoustic pressure at equally spaced times (typically 48000 or 44100
per second). A vector might give the hourly rainfall (or temperature, or barometric
pressure) at some location, over some time period. When a vector represents a time
series, it is natural to plot xi versus i with lines connecting consecutive time series
values. (These lines carry no information; they are added only to make the plot
easier to understand visually.) An example is shown in ﬁgure 1.3, where the 48-
vector x gives the hourly temperature in downtown Los Angeles over two days.

Daily return. A vector can represent the daily return of a stock, i.e., its fractional
increase (or decrease if negative) in value over the day. For example the return time
series vector (−0.022, +0.014, +0.004) means the stock price went down 2.2% on
the ﬁrst day, then up 1.4% the next day, and up again 0.4% on the third day. In
this example, the samples are not uniformly spaced in time; the index refers to
trading days, and does not include weekends or market holidays. A vector can
represent the daily (or quarterly, hourly, or minute-by-minute) value of any other
quantity of interest for an asset, such as price or volume.

Cash ﬂow. A cash ﬂow into and out of an entity (say, a company) can be repre-
sented by a vector, with positive entries representing payments to the entity, and
negative entries representing payments by the entity. For example, with entries
giving cash ﬂow each quarter, the vector (1000,−10,−10,−10,−1010) represents
a one year loan of $1000, with 1% interest only payments made each quarter, and
the principal and last interest payment at the end.

1.1 Vectors

9

0.65 0.05 0.20

0.28 0.00 0.90

Figure 1.4 8 × 8 image and the grayscale levels at six pixels.

Images. A monochrome (black and white) image is an array of M × N pixels
(square patches with uniform grayscale level) with M rows and N columns. Each
of the M N pixels has a grayscale or intensity value, with 0 corresponding to black
and 1 corresponding to bright white. (Other ranges are also used.) An image can
be represented by a vector of length M N , with the elements giving grayscale levels
at the pixel locations, typically ordered column-wise or row-wise.

Figure 1.4 shows a simple example, an 8×8 image. (This is a very low resolution;
typical values of M and N are in the hundreds or thousands.) With the vector
entries arranged row-wise, the associated 64-vector is

x = (0.65, 0.05, 0.20, . . . , 0.28, 0.00, 0.90).

A color M × N pixel image is described by a vector of length 3M N , with the
entries giving the R, G, and B values for each pixel, in some agreed-upon order.

Video. A monochrome video, i.e., a sequence of length K of images with M × N
pixels, can be represented by a vector of length KM N (again, in some particular
order).

Word count and histogram. A vector of length n can represent the number of
times each word in a dictionary of n words appears in a document. For example,
(25, 2, 0) means that the ﬁrst dictionary word appears 25 times, the second one
twice, and the third one not at all. (Typical dictionaries used for document word
counts have many more than 3 elements.) A small example is shown in ﬁgure 1.5. A
variation is to have the entries of the vector give the histogram of word frequencies
in the document, so that, e.g., x5 = 0.003 means that 0.3% of all the words in the
document are the ﬁfth word in the dictionary.

It is common practice to count variations of a word (say, the same word stem
with diﬀerent endings) as the same word; for example, ‘rain’, ‘rains’, ‘raining’, and

10

1 Vectors

Word count vectors are used in computer based document analysis.
Each entry of the word count vector is the number of times the as-
sociated dictionary word appears in the document.

word
in
number
horse
the
document



3
2
1
0
4
2



Figure 1.5 A snippet of text (top), the dictionary (bottom left), and word
count vector (bottom right).

‘rained’ might all be counted as ‘rain’. Reducing each word to its stem is called
stemming. It is also common practice to exclude words that are too common (such
as ‘a’ or ‘the’), which are referred to as stop words, as well as words that are
extremely rare.

Customer purchases. An n-vector p can be used to record a particular customer’s
purchases from some business over some period of time, with pi the quantity of
item i the customer has purchased, for i = 1, . . . , n. (Unless n is small, we would
expect many of these entries to be zero, meaning the customer has not purchased
those items.) In one variation, pi represents the total dollar value of item i the
customer has purchased.

Occurrence or subsets. An n-vector o can be used to record whether or not
each of n diﬀerent events has occurred, with oi = 0 meaning that event i did not
occur, and oi = 1 meaning that it did occur. Such a vector encodes a subset of
a collection of n objects, with oi = 1 meaning that object i is contained in the
subset, and oi = 0 meaning that object i is not in the subset. Each entry of the
vector a is either 0 or 1; such vectors are called Boolean, after the mathematician
George Boole, a pioneer in the study of logic.

Features or attributes.
In many applications a vector collects together n diﬀerent
quantities that pertain to a single thing or object. The quantities can be measure-
ments, or quantities that can be measured or derived from the object. Such a
vector is sometimes called a feature vector, and its entries are called the features
or attributes. For example, a 6-vector f could give the age, height, weight, blood
pressure, temperature, and gender of a patient admitted to a hospital. (The last
entry of the vector could be encoded as f6 = 0 for male, f6 = 1 for female.) In this
example, the quantities represented by the entries of the vector are quite diﬀerent,
with diﬀerent physical units.

1.2 Vector addition

11

Vector entry labels.
In applications such as the ones described above, each entry
of a vector has a meaning, such as the count of a speciﬁc word in a document, the
number of shares of a speciﬁc stock held in a portfolio, or the rainfall in a speciﬁc
hour. It is common to keep a separate list of labels or tags that explain or annotate
the meaning of the vector entries. As an example, we might associate the portfolio
vector (100, 50, 20) with the list of ticker symbols (AAPL, INTC, AMZN), so we
know that assets 1, 2, and 3 are Apple, Intel, and Amazon. In some applications,
such as an image, the meaning or ordering of the entries follow known conventions
or standards.

1.2 Vector addition

Two vectors of the same size can be added together by adding the corresponding
elements, to form another vector of the same size, called the sum of the vectors.
Vector addition is denoted by the symbol +. (Thus the symbol + is overloaded
to mean scalar addition when scalars appear on its left- and right-hand sides, and
vector addition when vectors appear on its left- and right-hand sides.) For example,

0
7

1
2

Vector subtraction is similar. As an example,

1
9


3  +
� 1
9 � −� 1

0  =
3  .
1 � =� 0
8 � .

The result of vector subtraction is called the diﬀerence of the two vectors.

Properties. Several properties of vector addition are easily veriﬁed. For any vec-
tors a, b, and c of the same size we have the following.

• Vector addition is commutative: a + b = b + a.
• Vector addition is associative: (a + b) + c = a + (b + c). We can therefore

write both as a + b + c.

• a + 0 = 0 + a = a. Adding the zero vector to a vector has no eﬀect. (This
is an example where the size of the zero vector follows from the context: It
must be the same as the size of a.)

• a − a = 0. Subtracting a vector from itself yields the zero vector. (Here too

the size of 0 is the size of a.)

To show that these properties hold, we argue using the deﬁnition of vector
addition and vector equality. As an example, let us show that for any n-vectors a
and b, we have a + b = b + a. The ith entry of a + b is, by the deﬁnition of vector

12

1 Vectors

a

b

b + a

a + b

b

a

Figure 1.6 Left. The lower blue arrow shows the displacement a; the dis-
placement b, shown as the shorter blue arrow, starts from the head of the
displacement a and ends at the sum displacement a + b, shown as the red
arrow. Right. The displacement b + a.

addition, ai + bi. The ith entry of b + a is bi + ai. For any two numbers we have
ai + bi = bi + ai, so the ith entries of the vectors a + b and b + a are the same.
This is true for all of the entries, so by the deﬁnition of vector equality, we have
a + b = b + a.

Verifying identities like the ones above, and many others we will encounter
later, can be tedious. But it is important to understand that the various properties
we will list can be derived using elementary arguments like the one above. We
recommend that the reader select a few of the properties we will see, and attempt
to derive them, just to see that it can be done. (Deriving all of them is overkill.)

Examples.

• Displacements. When vectors a and b represent displacements, the sum a + b
is the net displacement found by ﬁrst displacing by a, then displacing by b,
as shown in ﬁgure 1.6. Note that we arrive at the same vector if we ﬁrst
displace by b and then a. If the vector p represents a position and the vector
a represents a displacement, then p+a is the position of the point p, displaced
by a, as shown in ﬁgure 1.7.

• Displacements between two points. If the vectors p and q represent the posi-
tions of two points in 2-D or 3-D space, then p− q is the displacement vector
from q to p, as illustrated in ﬁgure 1.8.

• Word counts. If a and b are word count vectors (using the same dictionary)
for two documents, the sum a + b is the word count vector of a new document
created by combining the original two (in either order). The word count
diﬀerence vector a − b gives the number of times more each word appears in
the ﬁrst document than the second.

• Bill of materials. Suppose q1, . . . , qN are n-vectors that give the quantities of
n diﬀerent resources required to accomplish N tasks. Then the sum n-vector
q1 + ··· + qN gives the bill of materials for completing all N tasks.

1.2 Vector addition

13

p + a

a

p

Figure 1.7 The vector p + a is the position of the point represented by p
displaced by the displacement represented by a.

q

p − q

p

Figure 1.8 The vector p − q represents the displacement from the point
represented by q to the point represented by p.

14

1 Vectors

• Market clearing. Suppose the n-vector qi represents the quantities of n
goods or resources produced (when positive) or consumed (when negative)
by agent i, for i = 1, . . . , N , so (q5)4 = −3.2 means that agent 5 consumes
3.2 units of resource 4. The sum s = q1 +··· + qN is the n-vector of total net
surplus of the resources (or shortfall, when the entries are negative). When
s = 0, we have a closed market, which means that the total quantity of each
resource produced by the agents balances the total quantity consumed. In
other words, the n resources are exchanged among the agents. In this case
we say that the market clears (with the resource vectors q1, . . . , qN ).

• Audio addition. When a and b are vectors representing audio signals over
the same period of time, the sum a + b is an audio signal that is perceived as
containing both audio signals combined into one. If a represents a recording of
a voice, and b a recording of music (of the same length), the audio signal a + b
will be perceived as containing both the voice recording and, simultaneously,
the music.

• Feature diﬀerences. If f and g are n-vectors that give n feature values for two
items, the diﬀerence vector d = f − g gives the diﬀerence in feature values for
the two objects. For example, d7 = 0 means that the two objects have the
same value for feature 7; d3 = 1.67 means that the ﬁrst object’s third feature
value exceeds the second object’s third feature value by 1.67.

• Time series. If a and b represent time series of the same quantity, such as
daily proﬁt at two diﬀerent stores, then a + b represents a time series which is
the total daily proﬁt at the two stores. An example (with monthly rainfall)
is shown in ﬁgure 1.9.

• Portfolio trading. Suppose s is an n-vector giving the number of shares of n
assets in a portfolio, and b is an n-vector giving the number of shares of the
assets that we buy (when bi is positive) or sell (when bi is negative). After
the asset purchases and sales, our portfolio is given by s + b, the sum of the
original portfolio vector and the purchase vector b, which is also called the
trade vector or trade list. (The same interpretation works when the portfolio
and trade vectors are given in dollar value.)

Addition notation in computer languages. Some computer languages for manip-
ulating vectors deﬁne the sum of a vector and a scalar as the vector obtained by
adding the scalar to each element of the vector. This is not standard mathematical
notation, however, so we will not use it. Even more confusing, in some computer
languages the plus symbol is used to denote concatenation of arrays, which means
putting one array after another, as in (1, 2) + (3, 4, 5) = (1, 2, 3, 4, 5). While this
notation might give a valid expression in some computer languages, it is not stan-
dard mathematical notation, and we will not use it in this book. In general, it is
very important to distinguish between mathematical notation for vectors (which
we use) and the syntax of speciﬁc computer languages or software packages for
manipulating vectors.

1.3 Scalar-vector multiplication

15

Los Angeles
San Francisco

Sum

)
s
e
h
c
n
i
(

l
l
a
f
n

i
a
R

8

6

4

2

0

1

2

3

4

5

6

7

8

9 10 11 12

k

Figure 1.9 Average monthly rainfall in inches measured in downtown Los
Angeles and San Francisco International Airport, and their sum. Averages
are 30-year averages (1981–2010).

1.3 Scalar-vector multiplication

Another operation is scalar multiplication or scalar-vector multiplication, in which
a vector is multiplied by a scalar (i.e., number), which is done by multiplying
every element of the vector by the scalar. Scalar multiplication is denoted by
juxtaposition, typically with the scalar on the left, as in

1
9

6  =
(−2)

6  (1.5) =

1
9

−2
−18

−12  .
9  .

1.5
13.5

Scalar-vector multiplication can also be written with the scalar on the right, as in

The meaning is the same: It is the vector obtained by multiplying each element
by the scalar. A similar notation is a/2, where a is a vector, meaning (1/2)a. The
scalar-vector product (−1)a is written simply as −a. Note that 0 a = 0 (where the
left-hand zero is the scalar zero, and the right-hand zero is a vector zero of the
same size as a).

Properties. By deﬁnition, we have αa = aα, for any scalar α and any vector a.
This is called the commutative property of scalar-vector multiplication; it means
that scalar-vector multiplication can be written in either order.

16

1 Vectors

Scalar multiplication obeys several other laws that are easy to ﬁgure out from
the deﬁnition. For example, it satisﬁes the associative property: If a is a vector
and β and γ are scalars, we have

(βγ)a = β(γa).

On the left-hand side we see scalar-scalar multiplication (βγ) and scalar-vector
multiplication; on the right-hand side we see two scalar-vector products. As a
consequence, we can write the vector above as βγa, since it does not matter whether
we interpret this as β(γa) or (βγ)a.

The associative property holds also when we denote scalar-vector multiplication
with the scalar on the right. For example, we have β(γa) = (βa)γ, and consequently
we can write both as βaγ. As a convention, however, this vector is normally written
as βγa or as (βγ)a.

If a is a vector and β, γ are scalars, then

(β + γ)a = βa + γa.

(This is the left-distributive property of scalar-vector multiplication.) Scalar mul-
tiplication, like ordinary multiplication, has higher precedence in equations than
vector addition, so the right-hand side here, βa+γa, means (βa)+(γa). It is useful
to identify the symbols appearing in this formula above. The + symbol on the left
is addition of scalars, while the + symbol on the right denotes vector addition.
When scalar multiplication is written with the scalar on the right, we have the
right-distributive property:

a(β + γ) = aβ + aγ.

Scalar-vector multiplication also satisﬁes another version of the right-distributive

property:

β(a + b) = βa + βb

for any scalar β and any n-vectors a and b. In this equation, both of the + symbols
refer to the addition of n-vectors.

Examples.

• Displacements. When a vector a represents a displacement, and β > 0, βa
is a displacement in the same direction of a, with its magnitude scaled by β.
When β < 0, βa represents a displacement in the opposite direction of a,
with magnitude scaled by |β|. This is illustrated in ﬁgure 1.10.

• Materials requirements. Suppose the n-vector q is the bill of materials for
producing one unit of some product, i.e., qi is the amount of raw material
required to produce one unit of product. To produce α units of the product
will then require raw materials given by αq. (Here we assume that α ≥ 0.)
• Audio scaling. If a is a vector representing an audio signal, the scalar-vector
product βa is perceived as the same audio signal, but changed in volume
(loudness) by the factor |β|. For example, when β = 1/2 (or β = −1/2), βa
is perceived as the same audio signal, but quieter.

1.3 Scalar-vector multiplication

17

a

0.75a

−1.5a

Figure 1.10 The vector 0.75a represents the displacement in the direction of
the displacement a, with magnitude scaled by 0.75; (−1.5)a represents the
displacement in the opposite direction, with magnitude scaled by 1.5.

Linear combinations.
n-vector

If a1, . . . , am are n-vectors, and β1, . . . , βm are scalars, the

is called a linear combination of the vectors a1, . . . , an. The scalars β1, . . . , βm are
called the coeﬃcients of the linear combination.

β1a1 + ··· + βmam

Linear combination of unit vectors. We can write any n-vector b as a linear
combination of the standard unit vectors, as

b = b1e1 + ··· + bnen.

(1.1)

In this equation bi is the ith entry of b (i.e., a scalar), and ei is the ith unit vector.
In the linear combination of e1, . . . , en given in (1.1), the coeﬃcients are the entries
of the vector b. A speciﬁc example is



−1
3

5  = (−1)

1
0

0  + 3

0
1

0  + 5

0
0

1  .

Special linear combinations. Some linear combinations of the vectors a1, . . . , am
have special names. For example, the linear combination with β1 = ··· = βm = 1,
given by a1 + ··· + am, is the sum of the vectors, and the linear combination with
β1 = ··· = βm = 1/m, given by (1/m)(a1 +··· + am), is the average of the vectors.
When the coeﬃcients sum to one, i.e., β1 + ··· + βm = 1, the linear combination
is called an aﬃne combination. When the coeﬃcients in an aﬃne combination are
nonnegative, it is called a convex combination, a mixture, or a weighted average. The
coeﬃcients in an aﬃne or convex combination are sometimes given as percentages,
which add up to 100%.

18

1 Vectors

a2

a1

b

1.5a2

0.75a1

Figure 1.11 Left. Two 2-vectors a1 and a2. Right. The linear combination
b = 0.75a1 + 1.5a2.

Examples.

• Displacements. When the vectors represent displacements, a linear combina-
tion is the sum of the scaled displacements. This is illustrated in ﬁgure 1.11.

• Audio mixing. When a1, . . . , am are vectors representing audio signals (over
the same period of time, for example, simultaneously recorded), they are
called tracks. The linear combination β1a1 + ··· + βmam is perceived as a
mixture (also called a mix ) of the audio tracks, with relative loudness given
by |β1|, . . . , |βm|. A producer in a studio, or a sound engineer at a live show,
chooses values of β1, . . . , βm to give a good balance between the diﬀerent
instruments, vocals, and drums.

• Cash ﬂow replication. Suppose that c1, . . . , cm are vectors that represent cash
ﬂows, such as particular types of loans or investments. The linear combination
f = β1c1 + ··· + βmcm represents another cash ﬂow. We say that the cash
ﬂow f has been replicated by the (linear combination of the) original cash
ﬂows c1, . . . , cm. As an example, c1 = (1,−1.1, 0) represents a $1 loan from
period 1 to period 2 with 10% interest, and c2 = (0, 1,−1.1) represents a $1
loan from period 2 to period 3 with 10% interest. The linear combination

d = c1 + 1.1c2 = (1, 0,−1.21)

represents a two period loan of $1 in period 1, with compounded 10% interest.
Here we have replicated a two period loan from two one period loans.

• Line and segment. When a and b are diﬀerent n-vectors, the aﬃne combi-
nation c = (1 − θ)a + θb, where θ is a scalar, describes a point on the line
passing through a and b. When 0 ≤ θ ≤ 1, c is a convex combination of a
and b, and is said to lie on the segment between a and b. For n = 2 and
n = 3, with the vectors representing coordinates of 2-D or 3-D points, this
agrees with the usual geometric notion of line and segment. But we can also
talk about the line passing through two vectors of dimension 100. This is
illustrated in ﬁgure 1.12.

1.4

Inner product

19

b

θ = 1.2

a

θ = 0.4

θ = −0.4

Figure 1.12 The aﬃne combination (1 − θ)a + θb for diﬀerent values of θ.
These points are on the line passing through a and b; for θ between 0 and 1,
the points are on the line segment between a and b.

1.4

Inner product

The (standard) inner product (also called dot product) of two n-vectors is deﬁned
as the scalar

aT b = a1b1 + a2b2 + ··· + anbn,

the sum of the products of corresponding entries. (The origin of the superscript
‘T’ in the inner product notation aT b will be explained in chapter 6.) Some other
notations for the inner product (that we will not use in this book) are �a, b�, �a|b�,
(a, b), and a · b. (In the notation used in this book, (a, b) denotes a stacked vector
of length 2n.) As you might guess, there is also a vector outer product, which we
will encounter later, in §10.1. As a speciﬁc example of the inner product, we have

−1
2



2 

T

1
0

−3  = (−1)(1) + (2)(0) + (2)(−3) = −7.

When n = 1, the inner product reduces to the usual product of two numbers.

Properties. The inner product satisﬁes some simple properties that are easily
veriﬁed from the deﬁnition. If a, b, and c are vectors of the same size, and γ is a
scalar, we have the following.

• Commutativity. aT b = bT a. The order of the two vector arguments in the

inner product does not matter.

• Associativity with scalar multiplication. (γa)T b = γ(aT b), so we can write

both as γaT b.

• Distributivity with vector addition. (a + b)T c = aT c + bT c. The inner product

can be distributed across vector addition.

These can be combined to obtain other identities, such as aT (γb) = γ(aT b), or
aT (b + γc) = aT b + γaT c. As another useful example, we have, for any vectors
a, b, c, d of the same size,

(a + b)T (c + d) = aT c + aT d + bT c + bT d.

20

1 Vectors

This formula expresses an inner product on the left-hand side as a sum of four
inner products on the right-hand side, and is analogous to expanding a product of
sums in algebra. Note that on the left-hand side, the two addition symbols refer to
vector addition, whereas on the right-hand side, the three addition symbols refer
to scalar (number) addition.

General examples.
• Unit vector. eT

i a = ai. The inner product of a vector with the ith standard

unit vector gives (or ‘picks out’) the ith element a.

• Sum. 1T a = a1 + ··· + an. The inner product of a vector with the vector of

ones gives the sum of the elements of the vector.

• Average. (1/n)T a = (a1 +··· + an)/n. The inner product of an n-vector with
the vector 1/n gives the average or mean of the elements of the vector. The
average of the entries of a vector is denoted by avg(x). The Greek letter µ
is a traditional symbol used to denote the average or mean.

• Sum of squares. aT a = a2

1 + ··· + a2

itself gives the sum of the squares of the elements of the vector.

n. The inner product of a vector with

• Selective sum. Let b be a vector all of whose entries are either 0 or 1. Then

bT a is the sum of the elements in a for which bi = 1.

Block vectors.
blocks have the same sizes (in which case we say they conform), then we have

If the vectors a and b are block vectors, and the corresponding

aT b =

a1
...
ak



T

b1
...
bk

 = aT

1 b1 + ··· + aT

k bk.

The inner product of block vectors is the sum of the inner products of the blocks.

Applications. The inner product is useful in many applications, a few of which
we list here.

• Co-occurrence. If a and b are n-vectors that describe occurrence, i.e., each
of their elements is either 0 or 1, then aT b gives the total number of indices
for which ai and bi are both one, that is, the total number of co-occurrences.
If we interpret the vectors a and b as describing subsets of n objects, then
aT b gives the number of objects in the intersection of the two subsets. This
is illustrated in ﬁgure 1.13, for two subsets A and B of 7 objects, labeled
1, . . . , 7, with corresponding occurrence vectors

a = (0, 1, 1, 1, 1, 1, 1),

b = (1, 0, 1, 0, 1, 0, 0).

Here we have aT b = 2, which is the number of objects in both A and B (i.e.,
objects 3 and 5).

1.4

Inner product

21

A

6

B

2

7

4

3

5

1

Figure 1.13 Two sets A and B, containing seven objects.

• Weights, features, and score. When the vector f represents a set of features of
an object, and w is a vector of the same size (often called a weight vector ), the
inner product wT f is the sum of the feature values, scaled (or weighted) by
the weights, and is sometimes called a score. For example, if the features are
associated with a loan applicant (e.g., age, income, . . . ), we might interpret
s = wT f as a credit score. In this example we can interpret wi as the weight
given to feature i in forming the score.

• Price-quantity. If p represents a vector of prices of n goods, and q is a vector
of quantities of the n goods (say, the bill of materials for a product), then
their inner product pT q is the total cost of the goods given by the vector q.

• Speed-time. A vehicle travels over n segments with constant speed in each
segment. Suppose the n-vector s gives the speed in the segments, and the
n-vector t gives the times taken to traverse the segments. Then sT t is the
total distance traveled.

• Probability and expected values. Suppose the n-vector p has nonnegative
entries that sum to one, so it describes a set of proportions among n items,
or a set of probabilities of n outcomes, one of which must occur. Suppose
f is another n-vector, where we interpret fi as the value of some quantity if
outcome i occurs. Then f T p gives the expected value or mean of the quantity,
under the probabilities (or fractions) given by p.

• Polynomial evaluation. Suppose the n-vector c represents the coeﬃcients of

a polynomial p of degree n − 1 or less:

p(x) = c1 + c2x + ··· + cn−1xn−2 + cnxn−1.

Let t be a number, and let z = (1, t, t2, . . . , tn−1) be the n-vector of powers
of t. Then cT z = p(t), the value of the polynomial p at the point t. So the
inner product of a polynomial coeﬃcient vector and vector of powers of a
number evaluates the polynomial at the number.

22

1 Vectors

• Discounted total. Let c be an n-vector representing a cash ﬂow, with ci the

cash received (when ci > 0) in period i. Let d be the n-vector deﬁned as

d = (1, 1/(1 + r), . . . , 1/(1 + r)n−1),

where r ≥ 0 is an interest rate. Then

dT c = c1 + c2/(1 + r) + ··· + cn/(1 + r)n−1

is the discounted total of the cash ﬂow, i.e., its net present value (NPV), with
interest rate r.

• Portfolio value. Suppose s is an n-vector representing the holdings in shares of
a portfolio of n diﬀerent assets, with negative values meaning short positions.
If p is an n-vector giving the prices of the assets, then pT s is the total (or
net) value of the portfolio.

• Portfolio return. Suppose r is the vector of (fractional) returns of n assets

over some time period, i.e., the asset relative price changes

ri =

pﬁnal
i − pinitial

i
pinitial
i

,

i = 1, . . . , n,

i

i

and pﬁnal

where pinitial
are the (positive) prices of asset i at the beginning and
end of the investment period. If h is an n-vector giving our portfolio, with hi
denoting the dollar value of asset i held, then the inner product rT h is the
total return of the portfolio, in dollars, over the period. If w represents the
fractional (dollar) holdings of our portfolio, then rT w gives the total return
of the portfolio. For example, if rT w = 0.09, then our portfolio return is 9%.
If we had invested $10000 initially, we would have earned $900.

• Document sentiment analysis. Suppose the n-vector x represents the his-
togram of word occurrences in a document, from a dictionary of n words.
Each word in the dictionary is assigned to one of three sentiment categories:
Positive, Negative, and Neutral. The list of positive words might include
‘nice’ and ‘superb’; the list of negative words might include ‘bad’ and ‘ter-
rible’. Neutral words are those that are neither positive nor negative. We
encode the word categories as an n-vector w, with wi = 1 if word i is posi-
tive, with wi = −1 if word i is negative, and wi = 0 if word i is neutral. The
number wT x gives a (crude) measure of the sentiment of the document.

1.5 Complexity of vector computations

Computer representation of numbers and vectors. Real numbers are stored in
computers using ﬂoating point format, which represents a real number using a
block of 64 bits (0s and 1s), or 8 bytes (groups of 8 bits). Each of the 264 possible
sequences of bits corresponds to a speciﬁc real number. The ﬂoating point numbers

1.5 Complexity of vector computations

23

span a very wide range of values, and are very closely spaced, so numbers that
arise in applications can be approximated as ﬂoating point numbers to an accuracy
of around 10 digits, which is good enough for almost all practical applications.
Integers are stored in a more compact format, and are represented exactly.

Vectors are stored as arrays of ﬂoating point numbers (or integers, when the
entries are all integers). Storing an n-vector requires 8n bytes to store. Current
memory and storage devices, with capacities measured in many gigabytes (109
bytes), can easily store vectors with dimensions in the millions or billions. Sparse
vectors are stored in a more eﬃcient way that keeps track of indices and values of
the nonzero entries.

Floating point operations. When computers carry out addition, subtraction,
multiplication, division, or other arithmetic operations on numbers represented
in ﬂoating point format, the result is rounded to the nearest ﬂoating point number.
These operations are called ﬂoating point operations. The very small error in the
computed result is called (ﬂoating point) round-oﬀ error.
In most applications,
these very small errors have no practical eﬀect. Floating point round-oﬀ errors,
and methods to mitigate their eﬀect, are studied in a ﬁeld called numerical analy-
sis. In this book we will not consider ﬂoating point round-oﬀ error, but you should
be aware that it exists. For example, when a computer evaluates the left-hand and
right-hand sides of a mathematical identity, you should not be surprised if the two
numbers are not equal. They should, however, be very close.

Flop counts and complexity. So far we have seen only a few vector operations, like
scalar multiplication, vector addition, and the inner product. How quickly these
operations can be carried out by a computer depends very much on the computer
hardware and software, and the size of the vector.

A very rough estimate of the time required to carry out some computation, such
as an inner product, can be found by counting the total number of ﬂoating point
operations, or FLOPs. This term is in such common use that the acronym is now
written in lower case letters, as ﬂops, and the speed with which a computer can
carry out ﬂops is expressed in Gﬂop/s (gigaﬂops per second, i.e., billions of ﬂops
per second). Typical current values are in the range of 1–10 Gﬂop/s, but this can
vary by several orders of magnitude. The actual time it takes a computer to carry
out some computation depends on many other factors beyond the total number of
ﬂops required, so time estimates based on counting ﬂops are very crude, and are
not meant to be more accurate than a factor of ten or so. For this reason, gross
approximations (such as ignoring a factor of 2) can be used when counting the ﬂops
required in a computation.

The complexity of an operation is the number of ﬂops required to carry it out, as
a function of the size or sizes of the input to the operation. Usually the complexity
is highly simpliﬁed, dropping terms that are small or negligible (compared to other
terms) when the sizes of the inputs are large. In theoretical computer science, the
term ‘complexity’ is used in a diﬀerent way, to mean the number of ﬂops of the best
method to carry out the computation, i.e., the one that requires the fewest ﬂops.
In this book, we use the term complexity to mean the number of ﬂops required by
a speciﬁc method.

24

1 Vectors

Complexity of vector operations. Scalar-vector multiplication ax, where x is an
n-vector, requires n multiplications, i.e., axi for i = 1, . . . , n. Vector addition x + y
of two n-vectors takes n additions, i.e., xi + yi for i = 1, . . . , n. Computing the
inner product xT y = x1y1 +··· + xnyn of two n-vectors takes 2n− 1 ﬂops, n scalar
multiplications and n−1 scalar additions. So scalar multiplication, vector addition,
and the inner product of n-vectors require n, n, and 2n − 1 ﬂops, respectively.
We only need an estimate, so we simplify the last to 2n ﬂops, and say that the
complexity of scalar multiplication, vector addition, and the inner product of n-
vectors is n, n, and 2n ﬂops, respectively. We can guess that a 1 Gﬂop/s computer
can compute the inner product of two vectors of size one million in around one
thousandth of a second, but we should not be surprised if the actual time diﬀers
by a factor of 10 from this value.

The order of the computation is obtained by ignoring any constant that multi-
plies a power of the dimension. So we say that the three vector operations scalar
multiplication, vector addition, and inner product have order n. Ignoring the fac-
tor of 2 dropped in the actual complexity of the inner product is reasonable, since
we do not expect ﬂop counts to predict the running time with an accuracy better
than a factor of 2. The order is useful in understanding how the time to execute
the computation will scale when the size of the operands changes. An order n
computation should take around 10 times longer to carry out its computation on
an input that is 10 times bigger.

Complexity of sparse vector operations.
If x is sparse, then computing ax re-
quires nnz(x) ﬂops. If x and y are sparse, computing x + y requires no more than
min{nnz(x), nnz(y)} ﬂops (since no arithmetic operations are required to compute
(x + y)i when either xi or yi is zero). If the sparsity patterns of x and y do not
overlap (intersect), then zero ﬂops are needed to compute x + y. The inner product
calculation is similar: computing xT y requires no more than 2 min{nnz(x), nnz(y)}
ﬂops. When the sparsity patterns of x and y do not overlap, computing xT y re-
quires zero ﬂops, since xT y = 0 in this case.

Exercises

Exercises

25

1.1 Vector equations. Determine whether each of the equations below is true, false, or contains

bad notation (and therefore does not make sense).

2

(a) � 1
(b) � 1

1 � = (1, 2, 1).
1 � =� 1,

(c) (1, (2, 1)) = ((1, 2), 1).

2

2,

1 �.

1.2 Vector notation. Which of the following expressions uses correct notation? When the
expression does make sense, give its length. In the following, a and b are 10-vectors, and
c is a 20-vector.
(a) a + b − c3:12.
(b) (a, b, c3:13).
(c) 2a + c.

(d) (a, 1) + (c1, b).
(e) ((a, b), a).

(f) [ a b ] + 4c.

(g) � a

b � + 4c.

1.3 Overloading. Which of the following expressions uses correct notation? If the notation is

correct, is it also unambiguous? Assume that a is a 10-vector and b is a 20-vector.

(a) b = (0, a).

(b) a = (0, b).

(c) b = (0, a, 0).

(d) a = 0 = b.

1.4 Periodic energy usage. The 168-vector w gives the hourly electricity consumption of
a manufacturing plant, starting on Sunday midnight to 1AM, over one week, in MWh
(megawatt-hours). The consumption pattern is the same each day, i.e., it is 24-periodic,
which means that wt+24 = wt for t = 1, . . . , 144. Let d be the 24-vector that gives the
energy consumption over one day, starting at midnight.

(a) Use vector notation to express w in terms of d.

(b) Use vector notation to express d in terms of w.

1.5 Interpreting sparsity. Suppose the n-vector x is sparse, i.e., has only a few nonzero entries.
Give a short sentence or two explaining what this means in each of the following contexts.

(a) x represents the daily cash ﬂow of some business over n days.

(b) x represents the annual dollar value purchases by a customer of n products or ser-

vices.

(c) x represents a portfolio, say, the dollar value holdings of n stocks.

(d) x represents a bill of materials for a project, i.e., the amounts of n materials needed.

(e) x represents a monochrome image, i.e., the brightness values of n pixels.

(f) x is the daily rainfall in a location over one year.

26

1 Vectors

1.6 Vector of diﬀerences. Suppose x is an n-vector. The associated vector of diﬀerences is the
(n − 1)-vector d given by d = (x2 − x1, x3 − x2, . . . , xn − xn−1). Express d in terms of x
using vector operations (e.g., slicing notation, sum, diﬀerence, linear combinations, inner
product). The diﬀerence vector has a simple interpretation when x represents a time
series. For example, if x gives the daily value of some quantity, d gives the day-to-day
changes in the quantity.

1.7 Transforming between two encodings for Boolean vectors. A Boolean n-vector is one
for which all entries are either 0 or 1. Such vectors are used to encode whether each
of n conditions holds, with ai = 1 meaning that condition i holds. Another common
encoding of the same information uses the two values −1 and +1 for the entries. For
example the Boolean vector (0, 1, 1, 0) would be written using this alternative encoding
as (−1, +1, +1,−1). Suppose that x is a Boolean vector with entries that are 0 or 1, and
y is a vector encoding the same information using the values −1 and +1. Express y in
terms of x using vector notation. Also, express x in terms of y using vector notation.

1.8 Proﬁt and sales vectors. A company sells n diﬀerent products or items. The n-vector p
gives the proﬁt, in dollars per unit, for each of the n items. (The entries of p are typically
positive, but a few items might have negative entries. These items are called loss leaders,
and are used to increase customer engagement in the hope that the customer will make
other, proﬁtable purchases.) The n-vector s gives the total sales of each of the items, over
some period (such as a month), i.e., si is the total number of units of item i sold. (These
are also typically nonnegative, but negative entries can be used to reﬂect items that were
purchased in a previous time period and returned in this one.) Express the total proﬁt in
terms of p and s using vector notation.

1.9 Symptoms vector. A 20-vector s records whether each of 20 diﬀerent symptoms is present
in a medical patient, with si = 1 meaning the patient has the symptom and si = 0
meaning she does not. Express the following using vector notation.

(a) The total number of symptoms the patient has.

(b) The patient exhibits ﬁve out of the ﬁrst ten symptoms.

1.10 Total score from course record. The record for each student in a class is given as a 10-
vector r, where r1, . . . , r8 are the grades for the 8 homework assignments, each on a 0–10
scale, r9 is the midterm exam grade on a 0–120 scale, and r10 is ﬁnal exam score on a
0–160 scale. The student’s total course score s, on a 0–100 scale, is based 25% on the
homework, 35% on the midterm exam, and 40% on the ﬁnal exam. Express s in the form
s = wT r. (That is, determine the 10-vector w.) You can give the coeﬃcients of w to 4
digits after the decimal point.

1.11 Word count and word count histogram vectors. Suppose the n-vector w is the word count
vector associated with a document and a dictionary of n words. For simplicity we will
assume that all words in the document appear in the dictionary.

(a) What is 1T w?

(b) What does w282 = 0 mean?

(c) Let h be the n-vector that gives the histogram of the word counts, i.e., hi is the
fraction of the words in the document that are word i. Use vector notation to express
h in terms of w. (You can assume that the document contains at least one word.)

1.12 Total cash value. An international company holds cash in ﬁve currencies: USD (US
dollar), RMB (Chinese yuan), EUR (euro), GBP (British pound), and JPY (Japanese
yen), in amounts given by the 5-vector c. For example, c2 gives the number of RMB held.
Negative entries in c represent liabilities or amounts owed. Express the total (net) value
of the cash in USD, using vector notation. Be sure to give the size and deﬁne the entries
of any vectors that you introduce in your solution. Your solution can refer to currency
exchange rates.

Exercises

27

1.13 Average age in a population. Suppose the 100-vector x represents the distribution of ages
in some population of people, with xi being the number of i−1 year olds, for i = 1, . . . , 100.
(You can assume that x �= 0, and that there is no one in the population over age 99.)
Find expressions, using vector notation, for the following quantities.

(a) The total number of people in the population.

(b) The total number of people in the population age 65 and over.

(c) The average age of the population. (You can use ordinary division of numbers in

your expression.)

1.14 Industry or sector exposure. Consider a set of n assets or stocks that we invest in. Let
f be an n-vector that encodes whether each asset is in some speciﬁc industry or sector,
e.g., pharmaceuticals or consumer electronics. Speciﬁcally, we take fi = 1 if asset i is in
the sector, and fi = 0 if it is not. Let the n-vector h denote a portfolio, with hi the dollar
value held in asset i (with negative meaning a short position). The inner product f T h
is called the (dollar value) exposure of our portfolio to the sector. It gives the net dollar
value of the portfolio that is invested in assets from the sector. A portfolio h is called
neutral (to a sector or industry) if f T h = 0.
A portfolio h is called long only if each entry is nonnegative, i.e., hi ≥ 0 for each i. This
means the portfolio does not include any short positions.
What does it mean if a long-only portfolio is neutral to a sector, say, pharmaceuticals?
Your answer should be in simple English, but you should back up your conclusion with
an argument.

1.15 Cheapest supplier. You must buy n raw materials in quantities given by the n-vector q,
where qi is the amount of raw material i that you must buy. A set of K potential suppliers
oﬀer the raw materials at prices given by the n-vectors p1, . . . , pK . (Note that pk is an
n-vector; (pk)i is the price that supplier k charges per unit of raw material i.) We will
assume that all quantities and prices are positive.
If you must choose just one supplier, how would you do it? Your answer should use vector
notation.
A (highly paid) consultant tells you that you might do better (i.e., get a better total cost)
by splitting your order into two, by choosing two suppliers and ordering (1/2)q (i.e., half
the quantities) from each of the two. He argues that having a diversity of suppliers is
better. Is he right? If so, explain how to ﬁnd the two suppliers you would use to ﬁll half
the order.

1.16 Inner product of nonnegative vectors. A vector is called nonnegative if all its entries are

nonnegative.

(a) Explain why the inner product of two nonnegative vectors is nonnegative.

(b) Suppose the inner product of two nonnegative vectors is zero. What can you say
about them? Your answer should be in terms of their respective sparsity patterns,
i.e., which entries are zero and nonzero.

1.17 Linear combinations of cash ﬂows. We consider cash ﬂow vectors over T time periods,
with a positive entry meaning a payment received, and negative meaning a payment
made. A (unit) single period loan, at time period t, is the T -vector lt that corresponds
to a payment received of $1 in period t and a payment made of $(1 + r) in period t + 1,
with all other payments zero. Here r > 0 is the interest rate (over one period).
Let c be a $1 T − 1 period loan, starting at period 1. This means that $1 is received in
period 1, $(1 + r)T−1 is paid in period T , and all other payments (i.e., c2, . . . , cT−1) are
zero. Express c as a linear combination of single period loans.

1.18 Linear combinations of linear combinations. Suppose that each of the vectors b1, . . . , bk is
a linear combination of the vectors a1, . . . , am, and c is a linear combination of b1, . . . , bk.
Then c is a linear combination of a1, . . . , am. Show this for the case with m = k = 2.
(Showing it in general is not much more diﬃcult, but the notation gets more complicated.)

28

1 Vectors

1.19 Auto-regressive model. Suppose that z1, z2, . . . is a time series, with the number zt giving
the value in period or time t. For example zt could be the gross sales at a particular store
on day t. An auto-regressive (AR) model is used to predict zt+1 from the previous M
values, zt, zt−1, . . . , zt−M +1:

ˆzt+1 = (zt, zt−1, . . . , zt−M +1)T β,

t = M, M + 1, . . . .

Here ˆzt+1 denotes the AR model’s prediction of zt+1, M is the memory length of the
AR model, and the M -vector β is the AR model coeﬃcient vector. For this problem we
will assume that the time period is daily, and M = 10. Thus, the AR model predicts
tomorrow’s value, given the values over the last 10 days.
For each of the following cases, give a short interpretation or description of the AR model
in English, without referring to mathematical concepts like vectors, inner product, and
so on. You can use words like ‘yesterday’ or ‘today’.
(a) β ≈ e1.
(b) β ≈ 2e1 − e2.
(c) β ≈ e6.
(d) β ≈ 0.5e1 + 0.5e2.

1.20 How many bytes does it take to store 100 vectors of length 105? How many ﬂops does
it take to form a linear combination of them (with 100 nonzero coeﬃcients)? About how
long would this take on a computer capable of carrying out 1 Gﬂop/s?

Chapter 2

Linear functions

In this chapter we introduce linear and aﬃne functions, and describe some common
settings where they arise, including regression models.

2.1 Linear functions

Function notation. The notation f : Rn → R means that f is a function that
maps real n-vectors to real numbers, i.e., it is a scalar-valued function of n-vectors.
If x is an n-vector, then f (x), which is a scalar, denotes the value of the function f
at x. (In the notation f (x), x is referred to as the argument of the function.) We
can also interpret f as a function of n scalar arguments, the entries of the vector
argument, in which case we write f (x) as

f (x) = f (x1, x2, . . . , xn).

Here we refer to x1, . . . , xn as the arguments of f . We sometimes say that f is
real-valued, or scalar-valued, to emphasize that f (x) is a real number or scalar.

To describe a function f : Rn → R, we have to specify what its value is for any
possible argument x ∈ Rn. For example, we can deﬁne a function f : R4 → R by

f (x) = x1 + x2 − x2

4

for any 4-vector x.
In words, we might describe f as the sum of the ﬁrst two
elements of its argument, minus the square of the last entry of the argument.
(This particular function does not depend on the third element of its argument.)
Sometimes we introduce a function without formally assigning a symbol for it,
by directly giving a formula for its value in terms of its arguments, or describing
how to ﬁnd its value from its arguments. An example is the sum function, whose
value is x1 + ··· + xn. We can give a name to the value of the function, as in
y = x1 + ··· + xn, and say that y is a function of x, in this case, the sum of its
entries.
Many functions are not given by formulas or equations. As an example, suppose
f : R3 → R is the function that gives the lift (vertical upward force) on a particular

30

2 Linear functions

airplane, as a function of the 3-vector x, where x1 is the angle of attack of the
airplane (i.e., the angle between the airplane body and its direction of motion), x2
is its air speed, and x3 is the air density.

The inner product function. Suppose a is an n-vector. We can deﬁne a scalar-
valued function f of n-vectors, given by

f (x) = aT x = a1x1 + a2x2 + ··· + anxn

(2.1)

for any n-vector x. This function gives the inner product of its n-vector argument x
with some (ﬁxed) n-vector a. We can also think of f as forming a weighted sum of
the elements of x; the elements of a give the weights used in forming the weighted
sum.

Superposition and linearity. The inner product function f deﬁned in (2.1) satisﬁes
the property

f (αx + βy) = aT (αx + βy)

= aT (αx) + aT (βy)
= α(aT x) + β(aT y)
= αf (x) + βf (y)

for all n-vectors x, y, and all scalars α, β. This property is called superposition.
A function that satisﬁes the superposition property is called linear. We have just
shown that the inner product with a ﬁxed vector is a linear function.

The superposition equality

f (αx + βy) = αf (x) + βf (y)

(2.2)

looks deceptively simple; it is easy to read it as just a re-arrangement of the paren-
theses and the order of a few terms. But in fact it says a lot. On the left-hand
side, the term αx + βy involves scalar-vector multiplication and vector addition.
On the right-hand side, αf (x) + βf (y) involves ordinary scalar multiplication and
scalar addition.

If a function f is linear, superposition extends to linear combinations of any

number of vectors, and not just linear combinations of two vectors: We have

f (α1x1 + ··· + αkxk) = α1f (x1) + ··· + αkf (xk),

for any n vectors x1, . . . , xk, and any scalars α1, . . . , αk. (This more general k-term
form of superposition reduces to the two-term form given above when k = 2.) To
see this, we note that

f (α1x1 + ··· + αkxk) = α1f (x1) + f (α2x2 + ··· + αkxk)

= α1f (x1) + α2f (x2) + f (α3x3 + ··· + αkxk)
...
= α1f (x1) + ··· + αkf (xk).

2.1 Linear functions

31

In the ﬁrst line here, we apply (two-term) superposition to the argument

α1x1 + (1)(α2x2 + ··· + αkxk),

and in the other lines we apply this recursively.

The superposition equality (2.2) is sometimes broken down into two proper-
ties, one involving the scalar-vector product and one involving vector addition in
the argument. A function f : Rn → R is linear if it satisﬁes the following two
properties.

• Homogeneity. For any n-vector x and any scalar α, f (αx) = αf (x).
• Additivity. For any n-vectors x and y, f (x + y) = f (x) + f (y).

Homogeneity states that scaling the (vector) argument is the same as scaling the
function value; additivity says that adding (vector) arguments is the same as adding
the function values.

Inner product representation of a linear function. We saw above that a function
deﬁned as the inner product of its argument with some ﬁxed vector is linear. The
converse is also true: If a function is linear, then it can be expressed as the inner
product of its argument with some ﬁxed vector.

Suppose f is a scalar-valued function of n-vectors, and is linear, i.e., (2.2) holds
for all n-vectors x, y, and all scalars α, β. Then there is an n-vector a such that
f (x) = aT x for all x. We call aT x the inner product representation of f .

To see this, we use the identity (1.1) to express an arbitrary n-vector x as

x = x1e1 + ··· + xnen. If f is linear, then by multi-term superposition we have

f (x) = f (x1e1 + ··· + xnen)

= x1f (e1) + ··· + xnf (en)
= aT x,

with a = (f (e1), f (e2), . . . , f (en)). The formula just derived,

f (x) = x1f (e1) + x2f (e2) + ··· + xnf (en)

(2.3)

which holds for any linear scalar-valued function f , has several interesting impli-
cations. Suppose, for example, that the linear function f is given as a subroutine
(or a physical system) that computes (or results in the output) f (x) when we give
the argument (or input) x. Once we have found f (e1), . . . , f (en), by n calls to the
subroutine (or n experiments), we can predict (or simulate) what f (x) will be, for
any vector x, using the formula (2.3).

The representation of a linear function f as f (x) = aT x is unique, which means
that there is only one vector a for which f (x) = aT x holds for all x. To see this,
suppose that we have f (x) = aT x for all x, and also f (x) = bT x for all x. Taking
x = ei, we have f (ei) = aT ei = ai, using the formula f (x) = aT x. Using the
formula f (x) = bT x, we have f (ei) = bT ei = bi. These two numbers must be the
same, so we have ai = bi. Repeating this argument for i = 1, . . . , n, we conclude
that the corresponding elements in a and b are the same, so a = b.

32

2 Linear functions

Examples.

• Average. The mean or average value of an n-vector is deﬁned as

f (x) = (x1 + x2 + ··· + xn)/n,

and is denoted avg(x) (and sometimes x). The average of a vector is a linear
function. It can be expressed as avg(x) = aT x with

a = (1/n, . . . , 1/n) = 1/n.

• Maximum. The maximum element of an n-vector x, f (x) = max{x1, . . . , xn},
is not a linear function (except when n = 1). We can show this by a coun-
terexample for n = 2. Take x = (1,−1), y = (−1, 1), α = 1/2, β = 1/2.
Then

f (αx + βy) = 0 �= αf (x) + βf (y) = 1.

Aﬃne functions. A linear function plus a constant is called an aﬃne function. A
function f : Rn → R is aﬃne if and only if it can be expressed as f (x) = aT x + b
for some n-vector a and scalar b, which is sometimes called the oﬀset. For example,
the function on 3-vectors deﬁned by

f (x) = 2.3 − 2x1 + 1.3x2 − x3,

is aﬃne, with b = 2.3, a = (−2, 1.3,−1).
position property:

Any aﬃne scalar-valued function satisﬁes the following variation on the super-

f (αx + βy) = αf (x) + βf (y),

for all n-vectors x, y, and all scalars α, β that satisfy α+β = 1. For linear functions,
superposition holds for any coeﬃcients α and β; for aﬃne functions, it holds when
the coeﬃcients sum to one (i.e., when the argument is an aﬃne combination).

To see that the restricted superposition property holds for an aﬃne function
f (x) = aT x + b, we note that, for any vectors x, y and scalars α and β that satisfy
α + β = 1,

f (αx + βy) = aT (αx + βy) + b

= αaT x + βaT y + (α + β)b
= α(aT x + b) + β(aT y + b)
= αf (x) + βf (y).

(In the second line we use α + β = 1.)

This restricted superposition property for aﬃne functions is useful in showing
that a function f is not aﬃne: We ﬁnd vectors x, y, and numbers α and β with
α + β = 1, and verify that f (αx + βy) �= αf (x) + βf (y). This shows that f cannot
be aﬃne. As an example, we veriﬁed above that superposition does not hold for
the maximum function (with n > 1); the coeﬃcients in our counterexample are
α = β = 1/2, which sum to one, which allows us to conclude that the maximum
function is not aﬃne.

2.1 Linear functions

33

f (x)

g(x)

x

x

Figure 2.1 Left. The function f is linear. Right. The function g is aﬃne,
but not linear.

The converse is also true: Any scalar-valued function that satisﬁes the restricted

superposition property is aﬃne. An analog of the formula (2.3) is

f (x) = f (0) + x1 (f (e1) − f (0)) + ··· + xn (f (en) − f (0)) ,

(2.4)

which holds when f is aﬃne, and x is any n-vector. (See exercise 2.7.) This formula
shows that for an aﬃne function, once we know the n + 1 numbers f (0), f (e1), . . . ,
f (en), we can predict (or reconstruct or evaluate) f (x) for any n-vector x. It also
shows how the vector a and constant b in the representation f (x) = aT x + b can
be found from the function f : ai = f (ei) − f (0), and b = f (0).
In some contexts aﬃne functions are called linear. For example, when x is a
scalar, the function f deﬁned as f (x) = αx + β is sometimes referred to as a linear
function of x, perhaps because its graph is a line. But when β �= 0, f is not a linear
function of x, in the standard mathematical sense; it is an aﬃne function of x.
In this book we will distinguish between linear and aﬃne functions. Two simple
examples are shown in ﬁgure 2.1.

A civil engineering example. Many scalar-valued functions that arise in science
and engineering are well approximated by linear or aﬃne functions. As a typical
example, consider a steel structure like a bridge, and let w be an n-vector that
gives the weight of the load on the bridge in n speciﬁc locations, in metric tons.
These loads will cause the bridge to deform (move and change shape) slightly.
Let s denote the distance that a speciﬁc point on the bridge sags, in millimeters,
due to the load w. This is shown in ﬁgure 2.2. For weights the bridge is designed
to handle, the sag is very well approximated as a linear function s = f (x). This
function can be expressed as an inner product, s = cT w, for some n-vector c. From
the equation s = c1w1 +··· + cnwn, we see that c1w1 is the amount of the sag that
is due to the weight w1, and similarly for the other weights. The coeﬃcients ci,
which have units of mm/ton, are called compliances, and give the sensitivity of the
sag with respect to loads applied at the n locations.

The vector c can be computed by (numerically) solving a partial diﬀerential
equation, given the detailed design of the bridge and the mechanical properties of

34

2 Linear functions

w1

w2

w3

s

Figure 2.2 A bridge with weights w1, w2, w3 applied in 3 locations. These
weights cause the bridge to sag in the middle, by an amount s. (The sag is
exaggerated in this diagram.)

2.2 Taylor approximation

35

w1 w2 w3 Measured sag Predicted sag

1
0
0

0
1
0

0
0
1

0.5
1.5

1.1
0.8

0.3
1.2

0.12
0.31
0.26

0.481
0.736

—
—
—

0.479
0.740

Table 2.1 Loadings on a bridge (ﬁrst three columns), the associated mea-
sured sag at a certain point (fourth column), and the predicted sag using
the linear model constructed from the ﬁrst three experiments (ﬁfth column).

the steel used to construct it. This is always done during the design of a bridge.
The vector c can also be measured once the bridge is built, using the formula (2.3).
We apply the load w = e1, which means that we place a one ton load at the ﬁrst
load position on the bridge, with no load at the other positions. We can then
measure the sag, which is c1. We repeat this experiment, moving the one ton load
to positions 2, 3, . . . , n, which gives us the coeﬃcients c2, . . . , cn. At this point
we have the vector c, so we can now predict what the sag will be with any other
loading. To check our measurements (and linearity of the sag function) we might
measure the sag under other more complicated loadings, and in each case compare
our prediction (i.e., cT w) with the actual measured sag.

Table 2.1 shows what the results of these experiments might look like, with each
row representing an experiment (i.e., placing the loads and measuring the sag). In
the last two rows we compare the measured sag and the predicted sag, using the
linear function with coeﬃcients found in the ﬁrst three experiments.

2.2 Taylor approximation

In many applications, scalar-valued functions of n variables, or relations between
n variables and a scalar one, can be approximated as linear or aﬃne functions. In
these cases we sometimes refer to the linear or aﬃne function relating the vari-
ables and the scalar variable as a model, to remind us that the relation is only an
approximation, and not exact.

Diﬀerential calculus gives us an organized way to ﬁnd an approximate aﬃne
model. Suppose that f : Rn → R is diﬀerentiable, which means that its par-
tial derivatives exist (see §C.1). Let z be an n-vector. The (ﬁrst-order) Taylor
approximation of f near (or at) the point z is the function ˆf (x) of x deﬁned as

ˆf (x) = f (z) +

∂f
∂x1

(z)(x1 − z1) + ··· +

∂f
∂xn

(z)(xn − zn),

where ∂f
(z) denotes the partial derivative of f with respect to its ith argument,
∂xi
evaluated at the n-vector z. The hat appearing over f on the left-hand side is

36

2 Linear functions

a common notational hint that it is an approximation of the function f .
approximation is named after the mathematician Brook Taylor.)

(The

The ﬁrst-order Taylor approximation ˆf (x) is a very good approximation of f (x)
when all xi are near the associated zi. Sometimes ˆf is written with a second vector
argument, as ˆf (x; z), to show the point z at which the approximation is developed.
The ﬁrst term in the Taylor approximation is a constant; the other terms can be
interpreted as the contributions to the (approximate) change in the function value
(from f (z)) due to the changes in the components of x (from z).

Evidently ˆf is an aﬃne function of x. (It is sometimes called the linear approx-
imation of f near z, even though it is in general aﬃne, and not linear.) It can be
written compactly using inner product notation as

where ∇f (z) is an n-vector, the gradient of f (at the point z),

ˆf (x) = f (z) + ∇f (z)T (x − z),

(2.5)

(2.6)

∂f
∂x1

(z)
...

∇f (z) =

 .

∂f
∂xn

(z)

The ﬁrst term in the Taylor approximation (2.5) is the constant f (z), the value of
the function when x = z. The second term is the inner product of the gradient of
f at z and the deviation or perturbation of x from z, i.e., x − z.
constant,

We can express the ﬁrst-order Taylor approximation as a linear function plus a

but the form (2.5) is perhaps easier to interpret.

ˆf (x) = ∇f (z)T x + (f (z) − ∇f (z)T z),

The ﬁrst-order Taylor approximation gives us an organized way to construct
an aﬃne approximation of a function f : Rn → R, near a given point z, when
there is a formula or equation that describes f , and it is diﬀerentiable. A simple
example, for n = 1, is shown in ﬁgure 2.3. Over the full x-axis scale shown, the
Taylor approximation ˆf does not give a good approximation of the function f . But
for x near z, the Taylor approximation is very good.

Example. Consider the function f : R2 → R given by f (x) = x1 + exp(x2 − x1),
which is not linear or aﬃne. To ﬁnd the Taylor approximation ˆf near the point
z = (1, 2), we take partial derivatives to obtain

∇f (z) =� 1 − exp(z2 − z1)

exp(z2 − z1)

� ,

which evaluates to (−1.7183, 2.7183) at z = (1, 2). The Taylor approximation at
z = (1, 2) is then

ˆf (x) = 3.7183 + (−1.7183, 2.7183)T (x − (1, 2))

= 3.7183 − 1.7183(x1 − 1) + 2.7183(x2 − 2).

Table 2.2 shows f (x) and ˆf (x), and the approximation error | ˆf (x)− f (x)|, for some
values of x relatively near z. We can see that ˆf is indeed a very good approximation
of f , especially when x is near z.

2.2 Taylor approximation

37

f (x)

ˆf (x)

z

Figure 2.3 A function f of one variable, and the ﬁrst-order Taylor approxi-
mation ˆf (x) = f (z) + f�(z)(x − z) at z.

x

(1.00, 2.00)
(0.96, 1.98)
(1.10, 2.11)
(0.85, 2.05)
(1.25, 2.41)

f (x)

3.7183
3.7332
3.8456
4.1701
4.4399

ˆf (x)

3.7183
3.7326
3.8455
4.1119
4.4032

| ˆf (x) − f (x)|

0.0000
0.0005
0.0001
0.0582
0.0367

Table 2.2 Some values of x (ﬁrst column), the function value f (x) (sec-
ond column), the Taylor approximation ˆf (x) (third column), and the error
(fourth column).

38

2 Linear functions

2.3 Regression model

In this section we describe a very commonly used aﬃne function, especially when
the n-vector x represents a feature vector. The aﬃne function of x given by

ˆy = xT β + v,

(2.7)

where β is an n-vector and v is a scalar, is called a regression model. In this context,
the entries of x are called the regressors, and ˆy is called the prediction, since the
regression model is typically an approximation or prediction of some true value y,
which is called the dependent variable, outcome, or label.

The vector β is called the weight vector or coeﬃcient vector, and the scalar
v is called the oﬀset or intercept in the regression model. Together, β and v are
called the parameters in the regression model. (We will see in chapter 13 how the
parameters in a regression model can be estimated or guessed, based on some past
or known observations of the feature vector x and the associated outcome y.) The
symbol ˆy is used in the regression model to emphasize that it is an estimate or
prediction of some outcome y.

The entries in the weight vector have a simple interpretation: βi is the amount
by which ˆy increases (if βi > 0) when feature i increases by one (with all other
features the same). If βi is small, the prediction ˆy doesn’t depend too strongly on
feature i. The oﬀset v is the value of ˆy when all features have the value 0.

The regression model is very interpretable when all of the features are Boolean,
i.e., have values that are either 0 or 1, which occurs when the features represent
which of two outcomes holds. As a simple example consider a regression model
for the lifespan of a person in some group, with x1 = 0 if the person is female
(x1 = 1 if male), x2 = 1 if the person has type II diabetes, and x3 = 1 if the person
smokes cigarettes. In this case, v is the regression model estimate for the lifespan
of a female nondiabetic nonsmoker; β1 is the increase in estimated lifespan if the
person is male, β2 is the increase in estimated lifespan if the person is diabetic,
and β3 is the increase in estimated lifespan if the person smokes cigarettes. (In a
model that ﬁts real data, all three of these coeﬃcients would be negative, meaning
that they decrease the regression model estimate of lifespan.)

Simpliﬁed regression model notation. Vector stacking can be used to lump the
weights and oﬀset in the regression model (2.7) into a single parameter vector,
which simpliﬁes the regression model notation a bit. We create a new regressor
vector ˜x, with n + 1 entries, as ˜x = (1, x). We can think of ˜x as a new feature
vector, consisting of all n original features, and one new feature added (˜x1) at
the beginning, which always has the value one. We deﬁne the parameter vector
˜β = (v, β), so the regression model (2.7) has the simple inner product form

ˆy = xT β + v =� 1

x �T� v

β � = ˜xT ˜β.

(2.8)

Often we omit the tildes, and simply write this as ˆy = xT β, where we assume that
the ﬁrst feature in x is the constant 1. A feature that always has the value 1 is
not particularly informative or interesting, but it does simplify the notation in a
regression model.

2.3 Regression model

39

House

x1 (area)

x2 (beds)

y (price)

ˆy (prediction)

1
2
3
4
5

0.846
1.324
1.150
3.037
3.984

1
2
3
4
5

115.00
234.50
198.00
528.00
572.50

161.37
213.61
168.88
430.67
552.66

Table 2.3 Five houses with associated feature vectors shown in the second
and third columns. The fourth and ﬁfth column give the actual price, and
the price predicted by the regression model.

House price regression model. As a simple example of a regression model, sup-
pose that y is the selling price of a house in some neighborhood, over some time
period, and the 2-vector x contains attributes of the house:

• x1 is the house area (in 1000 square feet),
• x2 is the number of bedrooms.

If y represents the selling price of the house, in thousands of dollars, the regression
model

ˆy = xT β + v = β1x1 + β2x2 + v

predicts the price in terms of the attributes or features. This regression model is
not meant to describe an exact relationship between the house attributes and its
selling price; it is a model or approximation. Indeed, we would expect such a model
to give, at best, only a crude approximation of selling price.

As a speciﬁc numerical example, consider the regression model parameters

β = (148.73,−18.85),

v = 54.40.

(2.9)

These parameter values were found using the methods we will see in chapter 13,
based on records of sales for 774 houses in the Sacramento area. Table 2.3 shows
the feature vectors x for ﬁve houses that sold during the period, the actual sale
price y, and the predicted price ˆy from the regression model above. Figure 2.4
shows the predicted and actual sale prices for 774 houses, including the ﬁve houses
in the table, on a scatter plot, with actual price on the horizontal axis and predicted
price on the vertical axis.

We can see that this particular regression model gives reasonable, but not very
accurate, predictions of the actual sale price. (Regression models for house prices
that are used in practice use many more than two regressors, and are much more
accurate.)

The model parameters in (2.9) are readily interpreted. The parameter β1 =
148.73 is the amount the regression model price prediction increases (in thousands
of dollars) when the house area increases by 1000 square feet (with the same number
of bedrooms). The parameter β2 = −18.85 is the price prediction increase with
the addition of one bedroom, with the total house area held constant, in units of

40

2 Linear functions

800

600

400

200

)
s
r
a
l
l
o
d

d
n
a
s
u
o
h
t
(

ˆy

e
c
i
r
p

d
e
t
c
i
d
e
r
P

House 1

House 2

House 3

House 5

House 4

0

0

200
600
Actual price y (thousand dollars)

400

800

Figure 2.4 Scatter plot of actual and predicted sale prices for 774 houses
sold in Sacramento during a ﬁve-day period.

2.3 Regression model

41

thousands of dollars per bedroom. It might seem strange that β2 is negative, since
one imagines that adding a bedroom to a house would increase its sale price, not
decrease it. To understand why β2 might be negative, we note that it gives the
change in predicted price when we add a bedroom, without adding any additional
area to the house. If we remodel a house by adding a bedroom that also adds more
than around 127 square feet to the house area, the regression model (2.9) does
predict an increase in house sale price. The oﬀset v = 54.40 is the predicted price
for a house with no area and no bedrooms, which we might interpret as the model’s
prediction of the value of the lot. But this regression model is crude enough that
these interpretations are dubious.

42

2 Linear functions

Exercises

2.1 Linear or not? Determine whether each of the following scalar-valued functions of n-
vectors is linear. If it is a linear function, give its inner product representation, i.e., an
n-vector a for which f (x) = aT x for all x. If it is not linear, give speciﬁc x, y, α, and β
for which superposition fails, i.e.,

f (αx + βy) �= αf (x) + βf (y).

(a) The spread of values of the vector, deﬁned as f (x) = maxk xk − mink xk.
(b) The diﬀerence of the last element and the ﬁrst, f (x) = xn − x1.
(c) The median of an n-vector, where we will assume n = 2k + 1 is odd. The median of
the vector x is deﬁned as the (k + 1)st largest number among the entries of x. For
example, the median of (−7.1, 3.2,−1.5) is −1.5.

(d) The average of the entries with odd indices, minus the average of the entries with

even indices. You can assume that n = 2k is even.

(e) Vector extrapolation, deﬁned as xn + (xn − xn−1), for n ≥ 2. (This is a simple
prediction of what xn+1 would be, based on a straight line drawn through xn and
xn−1.)

2.2 Processor powers and temperature. The temperature T of an electronic device containing
three processors is an aﬃne function of the power dissipated by the three processors,
P = (P1, P2, P3). When all three processors are idling, we have P = (10, 10, 10), which
results in a temperature T = 30. When the ﬁrst processor operates at full power and
the other two are idling, we have P = (100, 10, 10), and the temperature rises to T = 60.
When the second processor operates at full power and the other two are idling, we have
P = (10, 100, 10) and T = 70. When the third processor operates at full power and the
other two are idling, we have P = (10, 10, 100) and T = 65. Now suppose that all three
processors are operated at the same power P same. How large can P same be, if we require
that T ≤ 85? Hint. From the given data, ﬁnd the 3-vector a and number b for which
T = aT P + b.

2.3 Motion of a mass in response to applied force. A unit mass moves on a straight line (in
one dimension). The position of the mass at time t (in seconds) is denoted by s(t), and its
derivatives (the velocity and acceleration) by s�(t) and s��(t). The position as a function
of time can be determined from Newton’s second law

s��(t) = F (t),

where F (t) is the force applied at time t, and the initial conditions s(0), s�(0). We assume
F (t) is piecewise-constant, and is kept constant in intervals of one second. The sequence
of forces F (t), for 0 ≤ t < 10, can then be represented by a 10-vector f , with

F (t) = fk,

k − 1 ≤ t < k.

Derive expressions for the ﬁnal velocity s�(10) and ﬁnal position s(10). Show that s(10)
and s�(10) are aﬃne functions of x, and give 10-vectors a, c and constants b, d for which

s�(10) = aT f + b,

s(10) = cT f + d.

This means that the mapping from the applied force sequence to the ﬁnal position and
velocity is aﬃne.
Hint. You can use

s�(t) = s�(0) +� t

0

F (τ ) dτ,

s(t) = s(0) +� t

0

s�(τ ) dτ.

You will ﬁnd that the mass velocity s�(t) is piecewise-linear.

Exercises

43

2.4 Linear function? The function φ : R3 → R satisﬁes
φ(−1, 1, 1) = 1,

φ(1, 1, 0) = −1,

φ(1, −1,−1) = 1.

Choose one of the following, and justify your choice: φ must be linear; φ could be linear;
φ cannot be linear.

2.5 Aﬃne function. Suppose ψ : R2 → R is an aﬃne function, with ψ(1, 0) = 1, ψ(1, −2) = 2.
(a) What can you say about ψ(1,−1)? Either give the value of ψ(1,−1), or state that

it cannot be determined.

(b) What can you say about ψ(2,−2)? Either give the value of ψ(2,−2), or state that

it cannot be determined.

Justify your answers.

2.6 Questionnaire scoring. A questionnaire in a magazine has 30 questions, broken into
two sets of 15 questions. Someone taking the questionnaire answers each question with
‘Rarely’, ‘Sometimes’, or ‘Often’. The answers are recorded as a 30-vector a, with ai =
1, 2, 3 if question i is answered Rarely, Sometimes, or Often, respectively. The total score
on a completed questionnaire is found by adding up 1 point for every question answered
Sometimes and 2 points for every question answered Often on questions 1–15, and by
adding 2 points and 4 points for those responses on questions 16–30. (Nothing is added to
the score for Rarely responses.) Express the total score s in the form of an aﬃne function
s = wT a + v, where w is a 30-vector and v is a scalar (number).

2.7 General formula for aﬃne functions. Verify that formula (2.4) holds for any aﬃne function
f : Rn → R. You can use the fact that f (x) = aT x + b for some n-vector a and scalar b.
2.8 Integral and derivative of polynomial. Suppose the n-vector c gives the coeﬃcients of a
polynomial p(x) = c1 + c2x + ··· + cnxn−1.
(a) Let α and β be numbers with α < β. Find an n-vector a for which

aT c =� β

α

p(x) dx

always holds. This means that the integral of a polynomial over an interval is a
linear function of its coeﬃcients.

(b) Let α be a number. Find an n-vector b for which

bT c = p�(α).

This means that the derivative of the polynomial at a given point is a linear function
of its coeﬃcients.

2.9 Taylor approximation. Consider the function f : R2 → R given by f (x1, x2) = x1x2.
Find the Taylor approximation ˆf at the point z = (1, 1). Compare f (x) and ˆf (x) for the
following values of x:

x = (1, 1),

x = (1.05, 0.95),

x = (0.85, 1.25),

x = (−1, 2).

Make a brief comment about the accuracy of the Taylor approximation in each case.

2.10 Regression model. Consider the regression model ˆy = xT β + v, where ˆy is the predicted
response, x is an 8-vector of features, β is an 8-vector of coeﬃcients, and v is the oﬀset
term. Determine whether each of the following statements is true or false.

(a) If β3 > 0 and x3 > 0, then ˆy ≥ 0.
(b) If β2 = 0 then the prediction ˆy does not depend on the second feature x2.
(c) If β6 = −0.8, then increasing x6 (keeping all other xis the same) will decrease ˆy.

44

2 Linear functions

2.11 Sparse regression weight vector. Suppose that x is an n-vector that gives n features for
some object, and the scalar y is some outcome associated with the object. What does it
mean if a regression model ˆy = xT β + v uses a sparse weight vector β? Give your answer
in English, referring to ˆy as our prediction of the outcome.

2.12 Price change to maximize proﬁt. A business sells n products, and is considering changing
the price of one of the products to increase its total proﬁts. A business analyst develops a
regression model that (reasonably accurately) predicts the total proﬁt when the product
prices are changed, given by ˆP = βT x + P , where the n-vector x denotes the fractional
change in the product prices, xi = (pnew
i − pi)/pi. Here P is the proﬁt with the current
prices, ˆP is the predicted proﬁt with the changed prices, pi is the current (positive) price
of product i, and pnew

is the new price of product i.

i

(a) What does it mean if β3 < 0? (And yes, this can occur.)

(b) Suppose that you are given permission to change the price of one product, by up
to 1%, to increase total proﬁt. Which product would you choose, and would you
increase or decrease the price? By how much?

(c) Repeat part (b) assuming you are allowed to change the price of two products, each

by up to 1%.

Chapter 3

Norm and distance

In this chapter we focus on the norm of a vector, a measure of its magnitude, and
on related concepts like distance, angle, standard deviation, and correlation.

3.1 Norm

The Euclidean norm of an n-vector x (named after the Greek mathematician Eu-
clid), denoted �x�, is the squareroot of the sum of the squares of its elements,

�x� =�x2

1 + x2

2 + ··· + x2
n.

The Euclidean norm can also be expressed as the squareroot of the inner product

of the vector with itself, i.e., �x� = √xT x.

The Euclidean norm is sometimes written with a subscript 2, as �x�2. (The
subscript 2 indicates that the entries of x are raised to the second power.) Other
less widely used terms for the Euclidean norm of a vector are the magnitude, or
length, of a vector. (The term length should be avoided, since it is also often used
to refer to the dimension of the vector.) We use the same notation for the norms
of vectors of diﬀerent dimensions.

As simple examples, we have

= √9 = 3,



������

2
−1

2 ������

�����

0

−1 ����� = 1.

When x is a scalar, i.e., a 1-vector, the Euclidean norm is the same as the
absolute value of x. Indeed, the Euclidean norm can be considered a generalization
or extension of the absolute value or magnitude, that applies to vectors. The double
bar notation is meant to suggest this. Like the absolute value of a number, the
norm of a vector is a (numerical) measure of its magnitude. We say a vector is
small if its norm is a small number, and we say it is large if its norm is a large
number. (The numerical values of the norm that qualify for small or large depend
on the particular application and context.)

46

3 Norm and distance

Properties of norm. Some important properties of the Euclidean norm are given
below. Here x and y are vectors of the same size, and β is a scalar.

• Nonnegative homogeneity. �βx� = |β|�x�. Multiplying a vector by a scalar

multiplies the norm by the absolute value of the scalar.

• Triangle inequality. �x+y� ≤ �x�+�y�. The Euclidean norm of a sum of two
vectors is no more than the sum of their norms. (The name of this property
will be explained later.) Another name for this inequality is subadditivity.

• Nonnegativity. �x� ≥ 0.
• Deﬁniteness. �x� = 0 only if x = 0.

The last two properties together, which state that the norm is always nonnegative,
and zero only when the vector is zero, are called positive deﬁniteness. The ﬁrst,
third, and fourth properties are easy to show directly from the deﬁnition of the
If �x� = 0, then
norm. As an example, let’s verify the deﬁniteness property.
we also have �x�2 = 0, which means that x2
n = 0. This is a sum of n
nonnegative numbers, which is zero. We can conclude that each of the n numbers is
zero, since if any of them were nonzero the sum would be positive. So we conclude
that x2
i = 0 for i = 1, . . . , n, and therefore xi = 0 for i = 1, . . . , n; and thus, x = 0.
Establishing the second property, the triangle inequality, is not as easy; we will
give a derivation on page 57.

1 + ··· + x2

General norms. Any real-valued function of an n-vector that satisﬁes the four
properties listed above is called a (general) norm. But in this book we will only
use the Euclidean norm, so from now on, we refer to the Euclidean norm as the
norm. (See exercise 3.5, which describes some other useful norms.)

Root-mean-square value. The norm is related to the root-mean-square (RMS)
value of an n-vector x, deﬁned as

rms(x) =� x2

n

1 + ··· + x2

n

= �x�√n

.

The argument of the squareroot in the middle expression is called the mean square
value of x, denoted ms(x), and the RMS value is the squareroot of the mean square
value. The RMS value of a vector x is useful when comparing norms of vectors
with diﬀerent dimensions; the RMS value tells us what a ‘typical’ value of |xi| is.
For example, the norm of 1, the n-vector of all ones, is √n, but its RMS value is 1,
independent of n. More generally, if all the entries of a vector are the same, say,
α, then the RMS value of the vector is |α|.

Norm of a sum. A useful formula for the norm of the sum of two vectors x and
y is

�x + y� =��x�2 + 2xT y + �y�2.

(3.1)

3.1 Norm

47

To derive this formula, we start with the square of the norm of x+y and use various
properties of the inner product:

�x + y�2 = (x + y)T (x + y)

= xT x + xT y + yT x + yT y
= �x�2 + 2xT y + �y�2.

Taking the squareroot of both sides yields the formula (3.1) above.
In the ﬁrst
line, we use the deﬁnition of the norm. In the second line, we expand the inner
product. In the fourth line we use the deﬁnition of the norm, and the fact that
xT y = yT x. Some other identities relating norms, sums, and inner products of
vectors are explored in exercise 3.4.

Norm of block vectors. The norm-squared of a stacked vector is the sum of the
norm-squared values of its subvectors. For example, with d = (a, b, c) (where a, b,
and c are vectors), we have

�d�2 = dT d = aT a + bT b + cT c = �a�2 + �b�2 + �c�2.

This idea is often used in reverse, to express the sum of the norm-squared values
of some vectors as the norm-square value of a block vector formed from them.

We can write the equality above in terms of norms as

�(a, b, c)� =��a�2 + �b�2 + �c�2 = �(�a�,�b�,�c�)�.

In words: The norm of a stacked vector is the norm of the vector formed from
the norms of the subvectors. The right-hand side of the equation above should be
carefully read. The outer norm symbols enclose a 3-vector, with (scalar) entries
�a�, �b�, and �c�.
Chebyshev inequality. Suppose that x is an n-vector, and that k of its entries
satisfy |xi| ≥ a, where a > 0. Then k of its entries satisfy x2
i ≥ a2. It follows that

�x�2 = x2

1 + ··· + x2

n ≥ ka2,

since k of the numbers in the sum are at least a2, and the other n− k numbers are
nonnegative. We can conclude that k ≤ �x�2/a2, which is called the Chebyshev
inequality, after the mathematician Pafnuty Chebyshev. When �x�2/a2 ≥ n, the
inequality tells us nothing, since we always have k ≤ n. In other cases it limits
the number of entries in a vector that can be large. For a > �x�, the inequality is
k ≤ �x�2/a2 < 1, so we conclude that k = 0 (since k is an integer). In other words,
no entry of a vector can be larger in magnitude than the norm of the vector.
The Chebyshev inequality is easier to interpret in terms of the RMS value of a

vector. We can write it as

k

n ≤� rms(x)

a

�2

,

(3.2)

where k is, as above, the number of entries of x with absolute value at least a. The
left-hand side is the fraction of entries of the vector that are at least a in absolute

48

3 Norm and distance

b

a

Figure 3.1 The norm of the displacement b − a is the distance between the
points with coordinates a and b.

value. The right-hand side is the inverse square of the ratio of a to rms(x). It says,
for example, that no more than 1/25 = 4% of the entries of a vector can exceed
its RMS value by more than a factor of 5. The Chebyshev inequality partially
justiﬁes the idea that the RMS value of a vector gives an idea of the size of a
typical entry: It states that not too many of the entries of a vector can be much
bigger (in absolute value) than its RMS value. (A converse statement can also be
made: At least one entry of a vector has absolute value as large as the RMS value
of the vector; see exercise 3.8.)

3.2 Distance

Euclidean distance. We can use the norm to deﬁne the Euclidean distance be-
tween two vectors a and b as the norm of their diﬀerence:

dist(a, b) = �a − b�.

For one, two, and three dimensions, this distance is exactly the usual distance
between points with coordinates a and b, as illustrated in ﬁgure 3.1. But the
Euclidean distance is deﬁned for vectors of any dimension; we can refer to the
distance between two vectors of dimension 100. Since we only use the Euclidean
norm in this book, we will refer to the Euclidean distance between vectors as,
simply, the distance between the vectors. If a and b are n-vectors, we refer to the

RMS value of the diﬀerence, �a − b�/√n, as the RMS deviation between the two

vectors.

When the distance between two n-vectors x and y is small, we say they are
‘close’ or ‘nearby’, and when the distance �x − y� is large, we say they are ‘far’.
The particular numerical values of �x−y� that correspond to ‘close’ or ‘far’ depend
on the particular application.

3.2 Distance

49

c

�b − c�

�a − c�

a

�a − b�

b

Figure 3.2 Triangle inequality.

As an example, consider the 4-vectors

u =

1.8
2.0
−3.7
4.7

 ,

v =

0.6
2.1
1.9
−1.4

 ,

w =

2.0
1.9
−4.0
4.6

 .

The distances between pairs of them are

�u − v� = 8.368,

�u − w� = 0.387,

�v − w� = 8.533,

so we can say that u is much nearer (or closer) to w than it is to v. We can also
say that w is much nearer to u than it is to v.

Triangle inequality. We can now explain where the triangle inequality gets its
name. Consider a triangle in two or three dimensions, whose vertices have coordi-
nates a, b, and c. The lengths of the sides are the distances between the vertices,

dist(a, b) = �a − b�,

dist(b, c) = �b − c�,

dist(a, c) = �a − c�.

Geometric intuition tells us that the length of any side of a triangle cannot exceed
the sum of the lengths of the other two sides. For example, we have

�a − c� ≤ �a − b� + �b − c�.

(3.3)

This follows from the triangle inequality, since

�a − c� = �(a − b) + (b − c)� ≤ �a − b� + �b − c�.

This is illustrated in ﬁgure 3.2.

50

3 Norm and distance

z4

x

z6

z3

z2

z5

z1

Figure 3.3 A point x, shown as a square, and six other points z1, . . . , z6.
The point z3 is the nearest neighbor of x among the points z1, . . . , z6.

Examples.

• Feature distance. If x and y represent vectors of n features of two objects,
the quantity �x − y� is called the feature distance, and gives a measure of
how diﬀerent the objects are (in terms of their feature values). Suppose for
example the feature vectors are associated with patients in a hospital, with
entries such as weight, age, presence of chest pain, diﬃculty breathing, and
the results of tests. We can use feature vector distance to say that one patient
case is near another one (at least in terms of their feature vectors).

• RMS prediction error. Suppose that the n-vector y represents a time series
of some quantity, for example, hourly temperature at some location, and ˆy is
another n-vector that represents an estimate or prediction of the time series y,
based on other information. The diﬀerence y− ˆy is called the prediction error,
and its RMS value rms(y− ˆy) is called the RMS prediction error. If this value
is small (say, compared to rms(y)) the prediction is good.

• Nearest neighbor. Suppose z1, . . . , zm is a collection of m n-vectors, and that
x is another n-vector. We say that zj is the nearest neighbor of x (among
z1, . . . , zm) if

�x − zj� ≤ �x − zi�,

i = 1, . . . , m.

In words: zj is the closest vector to x among the vectors z1, . . . , zm. This
is illustrated in ﬁgure 3.3. The idea of nearest neighbor, and generalizations
such as the k-nearest neighbors, are used in many applications.

• Document dissimilarity. Suppose n-vectors x and y represent the histograms
of word occurrences for two documents. Then �x − y� represents a measure
of the dissimilarity of the two documents. We might expect the dissimilarity

3.2 Distance

51

Veterans Memorial Academy Golden Globe Super Bowl

Veterans Day
Memorial Day
Academy A.
Golden Globe A.
Super Bowl

Day

0

0.095
0.130
0.153
0.170

Day

0.095

0

0.122
0.147
0.164

Awards

Awards

0.130
0.122

0

0.108
0.164

0.153
0.147
0.108

0

0.181

0.170
0.164
0.164
0.181

0

Table 3.1 Pairwise word count histogram distances between ﬁve Wikipedia
articles.

to be smaller when the two documents have the same genre, topic, or author;
we would expect it to be larger when they are on diﬀerent topics, or have
diﬀerent authors. As an example we form the word count histograms for the
5 Wikipedia articles with titles ‘Veterans Day’, ‘Memorial Day’, ‘Academy
Awards’, ‘Golden Globe Awards’, and ‘Super Bowl’, using a dictionary of
4423 words. (More detail is given in §4.4.) The pairwise distances between
the word count histograms are shown in table 3.1. We can see that pairs of
related articles have smaller word count histogram distances than less related
pairs of articles.

Units for heterogeneous vector entries. The square of the distance between two
n-vectors x and y is given by

�x − y�2 = (x1 − y1)2 + ··· + (xn − yn)2,

the sum of the squares of the diﬀerences between their respective entries. Roughly
speaking, the entries in the vectors all have equal status in determining the distance
between them. For example, if x2 and y2 diﬀer by one, the contribution to the
square of the distance between them is the same as the contribution when x3 and
y3 diﬀer by one. This makes sense when the entries of the vectors x and y represent
the same type of quantity, using the same units (say, at diﬀerent times or locations),
for example meters or dollars. For example if x and y are word count histograms,
their entries are all word occurrence frequencies, and it makes sense to say they
are close when their distance is small.

When the entries of a vector represent diﬀerent types of quantities, for example
when the vector entries represent diﬀerent types of features associated with an
object, we must be careful about choosing the units used to represent the numerical
values of the entries. If we want the diﬀerent entries to have approximately equal
status in determining distance, their numerical values should be approximately of
the same magnitude. For this reason units for diﬀerent entries in vectors are often
chosen in such a way that their typical numerical values are similar in magnitude,
so that the diﬀerent entries play similar roles in determining distance.

As an example suppose that the 2-vectors x, y, and z are the feature vectors
for three houses that were sold, as in the example described on page 39. The ﬁrst
entry of each vector gives the house area and the second entry gives the number of

52

3 Norm and distance

bedrooms. These are very diﬀerent types of features, since the ﬁrst one is a physical
area, and the second one is a count, i.e., an integer. In the example on page 39, we
chose the unit used to represent the ﬁrst feature, area, to be thousands of square
feet. With this choice of unit used to represent house area, the numerical values of
both of these features range from around 1 to 5; their values have roughly the same
magnitude. When we determine the distance between feature vectors associated
with two houses, the diﬀerence in the area (in thousands of square feet), and the
diﬀerence in the number of bedrooms, play equal roles.

For example, consider three houses with feature vectors

x = (1.6, 2),

y = (1.5, 2),

z = (1.6, 4).

The ﬁrst two are ‘close’ or ‘similar’ since �x − y� = 0.1 is small (compared to the
norms of x and y, which are around 2.5). This matches our intuition that the ﬁrst
two houses are similar, since they both have two bedrooms and are close in area.
The third house would be considered ‘far’ or ‘diﬀerent’ from the ﬁrst two houses,
and rightly so since it has four bedrooms instead of two.

To appreciate the signiﬁcance of our choice of units in this example, suppose
we had chosen instead to represent house area directly in square feet, and not
thousands of square feet. The three houses above would then be represented by
feature vectors

˜x = (1600, 2),

˜y = (1500, 2),

˜z = (1600, 4).

The distance between the ﬁrst and third houses is now 2, which is very small
compared to the norms of the vectors (which are around 1600). The distance
between the ﬁrst and second houses is much larger. It seems strange to consider
a two-bedroom house and a four-bedroom house as ‘very close’, while two houses
with the same number of bedrooms and similar areas are much more dissimilar.
The reason is simple: With our choice of square feet as the unit to measure house
area, distances are very strongly inﬂuenced by diﬀerences in area, with number of
bedrooms playing a much smaller (relative) role.

3.3 Standard deviation

For any vector x, the vector ˜x = x − avg(x)1 is called the associated de-meaned
vector, obtained by subtracting from each entry of x the mean value of the entries.
(This is not standard notation; i.e., ˜x is not generally used to denote the de-meaned
vector.) The mean value of the entries of ˜x is zero, i.e., avg(˜x) = 0. This explains
why ˜x is called the de-meaned version of x; it is x with its mean removed. The
de-meaned vector is useful for understanding how the entries of a vector deviate
from their mean value. It is zero if all the entries in the original vector x are the
same.

The standard deviation of an n-vector x is deﬁned as the RMS value of the

de-meaned vector x − avg(x)1, i.e.,

std(x) =� (x1 − avg(x))2 + ··· + (xn − avg(x))2

n

.

3.3 Standard deviation

53

This is the same as the RMS deviation between a vector x and the vector all of
whose entries are avg(x). It can be written using the inner product and norm as

std(x) = �x − (1T x/n)1�

√n

.

(3.4)

The standard deviation of a vector x tells us the typical amount by which its entries
deviate from their average value. The standard deviation of a vector is zero only
when all its entries are equal. The standard deviation of a vector is small when the
entries of the vector are nearly the same.

As a simple example consider the vector x = (1,−2, 3, 2). Its mean or average
value is avg(x) = 1, so the de-meaned vector is ˜x = (0,−3, 2, 1).
Its standard
deviation is std(x) = 1.872. We interpret this number as a ‘typical’ value by which
the entries diﬀer from the mean of the entries. These numbers are 0, 3, 2, and 1,
so 1.872 is reasonable.
We should warn the reader that another slightly diﬀerent deﬁnition of the stan-
dard deviation of a vector is widely used, in which the denominator √n in (3.4) is
replaced with √n − 1 (for n ≥ 2). In this book we will only use the deﬁnition (3.4).

In some applications the Greek letter σ (sigma) is traditionally used to denote
standard deviation, while the mean is denoted µ (mu). In this notation we have,
for an n-vector x,

µ = 1T x/n,

σ = �x − µ1�/√n.

We will use the symbols avg(x) and std(x), switching to µ and σ only with expla-
nation, when describing an application that traditionally uses these symbols.

Average, RMS value, and standard deviation. The average, RMS value, and
standard deviation of a vector are related by the formula

rms(x)2 = avg(x)2 + std(x)2.

(3.5)

This formula makes sense: rms(x)2 is the mean square value of the entries of x,
which can be expressed as the square of the mean value, plus the mean square
ﬂuctuation of the entries of x around their mean value. We can derive this formula
from our vector notation formula for std(x) given above. We have

std(x)2 = (1/n)�x − (1T x/n)1�2

= (1/n)(xT x − 2xT (1T x/n)1 + ((1T x/n)1)T ((1T x/n)1))
= (1/n)(xT x − (2/n)(1T x)2 + n(1T x/n)2)
= (1/n)xT x − (1T x/n)2
= rms(x)2 − avg(x)2,

which can be re-arranged to obtain the identity (3.5) above. This derivation uses
many of the properties for norms and inner products, and should be read carefully
to understand every step. In the second line, we expand the norm-square of the
sum of two vectors. In the third line, we use the commutative property of scalar-
vector multiplication, moving scalars such as (1T x/n) to the front of each term,
and also the fact that 1T 1 = n.

54

3 Norm and distance

Examples.

• Mean return and risk. Suppose that an n-vector represents a time series of
return on an investment, expressed as a percentage, in n time periods over
some interval of time.
Its average gives the mean return over the whole
interval, often shortened to its return. Its standard deviation is a measure of
how variable the return is, from period to period, over the time interval, i.e.,
how much it typically varies from its mean, and is often called the (per period)
risk of the investment. Multiple investments can be compared by plotting
them on a risk-return plot, which gives the mean and standard deviation of
the returns of each of the investments over some interval. A desirable return
history vector has high mean return and low risk; this means that the returns
in the diﬀerent periods are consistently high. Figure 3.4 shows an example.

• Temperature or rainfall. Suppose that an n-vector is a time series of the
daily average temperature at a particular location, over a one year period.
Its average gives the average temperature at that location (over the year) and
its standard deviation is a measure of how much the temperature varied from
its average value. We would expect the average temperature to be high and
the standard deviation to be low in a tropical location, and the opposite for
a location with high latitude.

Chebyshev inequality for standard deviation. The Chebyshev inequality (3.2)
can be transcribed to an inequality expressed in terms of the mean and standard
deviation: If k is the number of entries of x that satisfy |xi − avg(x)| ≥ a, then
k/n ≤ (std(x)/a)2.
(This inequality is only interesting for a > std(x).) For
example, at most 1/9 = 11.1% of the entries of a vector can deviate from the mean
value avg(x) by 3 standard deviations or more. Another way to state this is: The
fraction of entries of x within α standard deviations of avg(x) is at least 1 − 1/α2
(for α > 1).
As an example, consider a time series of return on an investment, with a mean
return of 8%, and a risk (standard deviation) 3%. By the Chebyshev inequality,
the fraction of periods with a loss (i.e., xi ≤ 0) is no more than (3/8)2 = 14.1%.
(In fact, the fraction of periods when the return is either a loss, xi ≤ 0, or very
good, xi ≥ 16%, is together no more than 14.1%.)

Properties of standard deviation.

• Adding a constant. For any vector x and any number a, we have std(x+a1) =
std(x). Adding a constant to every entry of a vector does not change its
standard deviation.

• Multiplying by a scalar. For any vector x and any number a, we have
std(ax) = |a| std(x). Multiplying a vector by a scalar multiplies the standard
deviation by the absolute value of the scalar.

3.3 Standard deviation

55

10

k
a

5

0

−5

10

k
c

5

0

−5

10

k
b

5

0

−5

10

k
d

5

0

−5

5

k

10

5

k

10

return

5

k

10

5

k

10

3

2

1

0

c

b

a

0

2

4

d

risk

Figure 3.4 The vectors a, b, c, d represent time series of returns on in-
vestments over 10 periods. The bottom plot shows the investments in a
risk-return plane, with return deﬁned as the average value and risk as the
standard deviation of the corresponding vector.

56

3 Norm and distance

4

k
x

0

−4

4

k
˜x

0

−4

4

k
z

0

−4

2

4

6
k

8

10

2

4

6
k

8

10

2

4

6
k

8

10

Figure 3.5 A 10-vector x, the de-meaned vector ˜x = x − avg(x)1, and the
standardized vector z = (1/ std(x))˜x. The horizontal dashed lines indicate
the mean and the standard deviation of each vector. The middle line is
the mean; the distance between the middle line and the other two is the
standard deviation.

Standardization. For any vector x, we refer to ˜x = x− avg(x)1 as the de-meaned
version of x, since it has average or mean value zero. If we then divide by the RMS
value of ˜x (which is the standard deviation of x), we obtain the vector

z =

1

std(x)

(x − avg(x)1).

This vector is called the standardized version of x. It has mean zero, and standard
deviation one.
Its entries are sometimes called the z-scores associated with the
original entries of x. For example, z4 = 1.4 means that x4 is 1.4 standard deviations
above the mean of the entries of x. Figure 3.5 shows an example.

The standardized values for a vector give a simple way to interpret the original
values in the vectors. For example, if an n-vector x gives the values of some
medical test of n patients admitted to a hospital, the standardized values or z-
scores tell us how high or low, compared to the population, that patient’s value is.
A value z6 = −3.2, for example, means that patient 6 has a very low value of the
measurement; whereas z22 = 0.3 says that patient 22’s value is quite close to the
average value.

3.4 Angle

Cauchy–Schwarz inequality. An important inequality that relates norms and in-
ner products is the Cauchy–Schwarz inequality:

for any n-vectors a and b. Written out in terms of the entries, this is

|aT b| ≤ �a��b�

|a1b1 + ··· + anbn| ≤�a2

1 + ··· + a2

n�1/2�b2

n�1/2
1 + ··· + b2

,

3.4 Angle

57

which looks more intimidating. This inequality is attributed to the mathematician
Augustin-Louis Cauchy; Hermann Schwarz gave the derivation given below.

The Cauchy–Schwarz inequality can be shown as follows. The inequality clearly
holds if a = 0 or b = 0 (in this case, both sides of the inequality are zero). So we
suppose now that a �= 0, b �= 0, and deﬁne α = �a�, β = �b�. We observe that

0 ≤ �βa − αb�2

= �βa�2 − 2(βa)T (αb) + �αb�2
= β2�a�2 − 2βα(aT b) + α2�b�2
= �b�2�a�2 − 2�b��a�(aT b) + �a�2�b�2
= 2�a�2�b�2 − 2�a��b�(aT b).

Dividing by 2�a��b� yields aT b ≤ �a��b�. Applying this inequality to −a and
b we obtain −aT b ≤ �a��b�. Putting these two inequalities together we get the
Cauchy–Schwarz inequality, |aT b| ≤ �a��b�.
This argument also reveals the conditions on a and b under which they satisfy
the Cauchy–Schwarz inequality with equality. This occurs only if �βa − αb� = 0,
i.e., βa = αb. This means that each vector is a scalar multiple of the other (in
the case when they are nonzero). This statement remains true when either a or
b is zero. So the Cauchy–Schwarz inequality holds with equality when one of the
vectors is a multiple of the other; in all other cases, it holds with strict inequality.

Veriﬁcation of triangle inequality. We can use the Cauchy–Schwarz inequality to
verify the triangle inequality. Let a and b be any vectors. Then

�a + b�2 = �a�2 + 2aT b + �b�2

≤ �a�2 + 2�a��b� + �b�2
= (�a� + �b�)2 ,

where we used the Cauchy–Schwarz inequality in the second line. Taking the
squareroot we get the triangle inequality, �a + b� ≤ �a� + �b�.

Angle between vectors. The angle between two nonzero vectors a, b is deﬁned
as

θ = arccos� aT b
�a��b��

where arccos denotes the inverse cosine, normalized to lie in the interval [0, π]. In
other words, we deﬁne θ as the unique number between 0 and π that satisﬁes

aT b = �a��b� cos θ.

The angle between a and b is written as � (a, b), and is sometimes expressed in
(The default angle unit is radians; 360◦ is 2π radians.) For example,
degrees.
� (a, b) = 60◦ means � (a, b) = π/3, i.e., aT b = (1/2)�a��b�.
The angle coincides with the usual notion of angle between vectors, when they
have dimension two or three, and they are thought of as displacements from a

58

3 Norm and distance

common point. For example, the angle between the vectors a = (1, 2,−1) and
b = (2, 0,−3) is

arccos�

5

√6√13� = arccos(0.5661) = 0.9690 = 55.52◦

(to 4 digits). But the deﬁnition of angle is more general; we can refer to the angle
between two vectors with dimension 100.

The angle is a symmetric function of a and b: We have � (a, b) = � (b, a). The
angle is not aﬀected by scaling each of the vectors by a positive scalar: We have,
for any vectors a and b, and any positive numbers α and β,

� (αa, βb) = � (a, b).

Acute and obtuse angles. Angles are classiﬁed according to the sign of aT b.
Suppose a and b are nonzero vectors of the same size.

• If the angle is π/2 = 90◦, i.e., aT b = 0, the vectors are said to be orthogonal.
We write a ⊥ b if a and b are orthogonal. (By convention, we also say that a
zero vector is orthogonal to any vector.)

• If the angle is zero, which means aT b = �a��b�, the vectors are aligned. Each

vector is a positive multiple of the other.

• If the angle is π = 180◦, which means aT b = −�a��b�, the vectors are anti-

aligned. Each vector is a negative multiple of the other.

• If � (a, b) < π/2 = 90◦, the vectors are said to make an acute angle. This is

the same as aT b > 0, i.e., the vectors have positive inner product.

• If � (a, b) > π/2 = 90◦, the vectors are said to make an obtuse angle. This is

the same as aT b < 0, i.e., the vectors have negative inner product.

These deﬁnitions are illustrated in ﬁgure 3.6.

Examples.

• Spherical distance. Suppose a and b are 3-vectors that represent two points
that lie on a sphere of radius R (for example, locations on earth). The
spherical distance between them, measured along the sphere, is given by
R� (a, b). This is illustrated in ﬁgure 3.7.

• Document similarity via angles.

If n-vectors x and y represent the word
counts for two documents, their angle � (x, y) can be used as a measure of
document dissimilarity. (When using angle to measure document dissimilar-
ity, either word counts or histograms can be used; they produce the same
result.) As an example, table 3.2 gives the angles in degrees between the
word histograms in the example at the end of §3.2.

3.4 Angle

59

Figure 3.6 Top row. Examples of orthogonal, aligned, and anti-aligned vec-
tors. Bottom row. Vectors that make an obtuse and an acute angle.

a

0

b

Figure 3.7 Two points a and b on a sphere with radius R and center at the
origin. The spherical distance between the points is equal to R� (a, b).

Veterans Memorial Academy Golden Globe Super Bowl

Veterans Day
Memorial Day
Academy A.
Golden Globe A.
Super Bowl

Day

0

60.6
85.7
87.0
87.7

Day

60.6

0

85.6
87.5
87.5

Awards

Awards

85.7
85.6

0

58.7
86.1

87.0
87.5
58.7

0

86.0

87.7
87.5
85.7
86.0

0

Table 3.2 Pairwise angles (in degrees) between word histograms of ﬁve
Wikipedia articles.

60

3 Norm and distance

Norm of sum via angles. For vectors x and y we have

�x + y�2 = �x�2 + 2xT y + �y�2 = �x�2 + 2�x��y� cos θ + �y�2,

(3.6)

where θ = � (x, y). (The ﬁrst equality comes from (3.1).) From this we can make
several observations.

• If x and y are aligned (θ = 0), we have �x + y� = �x� + �y�. Thus, their

norms add.

• If x and y are orthogonal (θ = 90◦), we have �x + y�2 = �x�2 + �y�2. In this
case the norm-squared values add, and we have �x + y� = ��x�2 + �y�2.
This formula is sometimes called the Pythagorean theorem, after the Greek
mathematician Pythagoras of Samos.

Correlation coeﬃcient. Suppose a and b are n-vectors, with associated de-meaned
vectors

˜a = a − avg(a)1,

˜b = b − avg(b)1.

Assuming these de-meaned vectors are not zero (which occurs when the original
vectors have all equal entries), we deﬁne their correlation coeﬃcient as

ρ =

˜aT ˜b
�˜a��˜b�

.

(3.7)

Thus, ρ = cos θ, where θ = � (˜a, ˜b). We can also express the correlation coeﬃcient in
terms of the vectors u and v obtained by standardizing a and b. With u = ˜a/ std(a)
and v = ˜b/ std(b), we have

ρ = uT v/n.

(3.8)

(We use �u� = �v� = √n.)

This is a symmetric function of the vectors: The correlation coeﬃcient between
a and b is the same as the correlation coeﬃcient between b and a. The Cauchy–
Schwarz inequality tells us that the correlation coeﬃcient ranges between −1 and
+1. For this reason, the correlation coeﬃcient is sometimes expressed as a per-
centage. For example, ρ = 30% means ρ = 0.3. When ρ = 0, we say the vectors
are uncorrelated. (By convention, we say that a vector with all entries equal is
uncorrelated with any vector.)

The correlation coeﬃcient tells us how the entries in the two vectors vary to-
gether. High correlation (say, ρ = 0.8) means that entries of a and b are typically
above their mean for many of the same entries. The extreme case ρ = 1 occurs
only if the vectors ˜a and ˜b are aligned, which means that each is a positive mul-
tiple of the other, and the other extreme case ρ = −1 occurs only when ˜a and ˜b
are negative multiples of each other. This idea is illustrated in ﬁgure 3.8, which
shows the entries of two vectors, as well as a scatter plot of them, for cases with
correlation near 1, near −1, and near 0.
The correlation coeﬃcient is often used when the vectors represent time series,
such as the returns on two investments over some time interval, or the rainfall in
two locations over some time interval. If they are highly correlated (say, ρ > 0.8),

3.4 Angle

61

ak

ak

ak

bk

bk

bk

k

k

k

bk

bk

bk

k

k

k

ak

ak

ak

Figure 3.8 Three pairs of vectors a, b of length 10, with correlation coeﬃ-
cients 0.968 (top), −0.988 (middle), and 0.004 (bottom).

62

3 Norm and distance

the two time series are typically above their mean values at the same times. For
example, we would expect the rainfall time series at two nearby locations to be
highly correlated. As another example, we might expect the returns of two similar
companies, in the same business area, to be highly correlated.

Standard deviation of sum. We can derive a formula for the standard deviation
of a sum from (3.6):

std(a + b) =�std(a)2 + 2ρ std(a) std(b) + std(b)2.

(3.9)
To derive this from (3.6) we let ˜a and ˜b denote the de-meaned versions of a and b.
Then ˜a + ˜b is the de-meaned version of a + b, and std(a + b)2 = �˜a + ˜b�2/n. Now
using (3.6) and ρ = cos � (˜a, ˜b), we get
n std(a + b)2 = �˜a + ˜b�2

= �˜a�2 + 2ρ�˜a��˜b� + �˜b�2
= n std(a)2 + 2ρn std(a) std(b) + n std(b)2.

Dividing by n and taking the squareroot yields the formula above.

If ρ = 1, the standard deviation of the sum of vectors is the sum of their

standard deviations, i.e.,

std(a + b) = std(a) + std(b).

As ρ decreases, the standard deviation of the sum decreases. When ρ = 0, i.e., a
and b are uncorrelated, the standard deviation of the sum a + b is

std(a + b) =�std(a)2 + std(b)2,

which is smaller than std(a) + std(b) (unless one of them is zero). When ρ = −1,
the standard deviation of the sum is as small as it can be,
std(a + b) = | std(a) − std(b)|.

Hedging investments. Suppose that vectors a and b are time series of returns
for two assets with the same return (average) µ and risk (standard deviation) σ,
and correlation coeﬃcient ρ. (These are the traditional symbols used.) The vector
c = (a + b)/2 is the time series of returns for an investment with 50% in each of the
assets. This blended investment has the same return as the original assets, since

avg(c) = avg((a + b)/2) = (avg(a) + avg(b))/2 = µ.

The risk (standard deviation) of this blended investment is

std(c) =�2σ2 + 2ρσ2/2 = σ�(1 + ρ)/2,

using (3.9). From this we see that the risk of the blended investment is never
more than the risk of the original assets, and is smaller when the correlation of
the original asset returns is smaller. When the returns are uncorrelated, the risk
is a factor 1/√2 = 0.707 smaller than the risk of the original assets. If the asset
returns are strongly negatively correlated (i.e., ρ is near −1), the risk of the blended
investment is much smaller than the risk of the original assets. Investing in two
assets with uncorrelated, or negatively correlated, returns is called hedging (which
is short for ‘hedging your bets’). Hedging reduces risk.

3.5 Complexity

63

Units for heterogeneous vector entries. When the entries of vectors represent
diﬀerent types of quantities, the choice of units used to represent each entry aﬀects
the angle, standard deviation, and correlation between a pair of vectors. The
discussion on page 51, about how the choice of units can aﬀect distances between
pairs of vectors, therefore applies to these quantities as well. The general rule
of thumb is to choose units for diﬀerent entries so the typical vector entries have
similar sizes or ranges of values.

3.5 Complexity

Computing the norm of an n-vector requires n multiplications (to square each
entry), n − 1 additions (to add the squares), and one squareroot. Even though
computing the squareroot typically takes more time than computing the product
or sum of two numbers, it is counted as just one ﬂop. So computing the norm takes
2n ﬂops. The cost of computing the RMS value of an n-vector is the same, since
we can ignore the two ﬂops involved in division by √n. Computing the distance
between two vectors costs 3n ﬂops, and computing the angle between them costs
6n ﬂops. All of these operations have order n.

De-meaning an n-vector requires 2n ﬂops (n for forming the average and an-
other n ﬂops for subtracting the average from each entry). The standard deviation
is the RMS value of the de-meaned vector, and this calculation takes 4n ﬂops (2n
for computing the de-meaned vector and 2n for computing its RMS value). Equa-
tion (3.5) suggests a slightly more eﬃcient method with a complexity of 3n ﬂops:
ﬁrst compute the average (n ﬂops) and RMS value (2n ﬂops), and then ﬁnd the
standard deviation as std(x) = (rms(x)2−avg(x)2)1/2. Standardizing an n-vector
costs 5n ﬂops. The correlation coeﬃcient between two vectors costs 10n ﬂops to
compute. These operations also have order n.

As a slightly more involved computation, suppose that we wish to determine the
nearest neighbor among a collection of k n-vectors z1, . . . , zk to another n-vector x.
(This will come up in the next chapter.) The simple approach is to compute the
distances �x−zi� for i = 1, . . . , k, and then ﬁnd the minimum of these. (Sometimes
a comparison of two numbers is also counted as a ﬂop.) The cost of this is 3kn ﬂops
to compute the distances, and k − 1 comparisons to ﬁnd the minimum. The latter
term can be ignored, so the ﬂop count is 3kn. The order of ﬁnding the nearest
neighbor in a collection of k n-vectors is kn.

64

3 Norm and distance

Exercises

3.1 Distance between Boolean vectors. Suppose that x and y are Boolean n-vectors, which

means that each of their entries is either 0 or 1. What is their distance �x − y�?

3.2 RMS value and average of block vectors. Let x be a block vector with two vector elements,

x = (a, b), where a and b are vectors of size n and m, respectively.

(a) Express rms(x) in terms of rms(a), rms(b), m, and n.
(b) Express avg(x) in terms of avg(a), avg(b), m, and n.

3.3 Reverse triangle inequality. Suppose a and b are vectors of the same size. The triangle
inequality states that �a + b� ≤ �a� + �b�. Show that we also have �a + b� ≥ �a� − �b�.
Hints. Draw a picture to get the idea. To show the inequality, apply the triangle inequality
to (a + b) + (−b).

3.4 Norm identities. Verify that the following identities hold for any two vectors a and b of

the same size.
(a) (a + b)T (a − b) = �a�2 − �b�2.
(b) �a + b�2 + �a − b�2 = 2(�a�2 + �b�2). This is called the parallelogram law.

3.5 General norms. Any real-valued function f that satisﬁes the four properties given on
page 46 (nonnegative homogeneity, triangle inequality, nonnegativity, and deﬁniteness) is
called a vector norm, and is usually written as f (x) = �x�mn, where the subscript is some
kind of identiﬁer or mnemonic to identify it. The most commonly used norm is the one we
use in this book, the Euclidean norm, which is sometimes written with the subscript 2,
as �x�2. Two other common vector norms for n-vectors are the 1-norm �x�1 and the
∞-norm �x�∞, deﬁned as

�x�1 = |x1| + ··· + |xn|,

�x�∞ = max{|x1|, . . . ,|xn|}.

These norms are the sum and the maximum of the absolute values of the entries in the
vector, respectively. The 1-norm and the ∞-norm arise in some recent and advanced
applications, but we will not encounter them in this book.
Verify that the 1-norm and the ∞-norm satisfy the four norm properties listed on page 46.
3.6 Taylor approximation of norm. Find a general formula for the Taylor approximation of
the function f (x) = �x� near a given nonzero vector z. You can express the approximation
in the form ˆf (x) = aT (x − z) + b.
3.7 Chebyshev inequality. Suppose x is a 100-vector with rms(x) = 1. What is the maximum
number of entries of x that can satisfy |xi| ≥ 3? If your answer is k, explain why no such
vector can have k + 1 entries with absolute values at least 3, and give an example of a
speciﬁc 100-vector that has RMS value 1, with k of its entries larger than 3 in absolute
value.

3.8 Converse Chebyshev inequality. Show that at least one entry of a vector has absolute

value at least as large as the RMS value of the vector.

3.9 Diﬀerence of squared distances. Determine whether the diﬀerence of the squared distances

to two ﬁxed vectors c and d, deﬁned as

f (x) = �x − c�2 − �x − d�2,
If it is linear, give its inner product representation, i.e.,
is linear, aﬃne, or neither.
an n-vector a for which f (x) = aT x for all x.
If it is aﬃne, give a and b for which
f (x) = aT x + b holds for all x. If it is neither linear nor aﬃne, give speciﬁc x, y, α, and
β for which superposition fails, i.e.,

(Provided α + β = 1, this shows the function is neither linear nor aﬃne.)

f (αx + βy) �= αf (x) + βf (y).

Exercises

65

3.10 Nearest neighbor document. Consider the 5 Wikipedia pages in table 3.1 on page 51.
What is the nearest neighbor of (the word count histogram vector of) ‘Veterans Day’
among the others? Does the answer make sense?

3.11 Neighboring electronic health records. Let x1, . . . , xN be n-vectors that contain n features
extracted from a set of N electronic health records (EHRs), for a population of N patients.
(The features might involve patient attributes and current and past symptoms, diagnoses,
test results, hospitalizations, procedures, and medications.) Brieﬂy describe in words a
practical use for identifying the 10 nearest neighbors of a given EHR (as measured by
their associated feature vectors), among the other EHRs.

3.12 Nearest point to a line. Let a and b be diﬀerent n-vectors. The line passing through a
and b is given by the set of vectors of the form (1 − θ)a + θb, where θ is a scalar that
determines the particular point on the line. (See page 18.)
Let x be any n-vector. Find a formula for the point p on the line that is closest to x.
The point p is called the projection of x onto the line. Show that (p − x) ⊥ (a − b), and
draw a simple picture illustrating this in 2-D. Hint. Work with the square of the distance
between a point on the line and x, i.e., �(1 − θ)a + θb − x�2. Expand this, and minimize
over θ.
3.13 Nearest nonnegative vector. Let x be an n-vector and deﬁne y as the nonnegative vector
(i.e., the vector with nonnegative entries) closest to x. Give an expression for the elements
of y. Show that the vector z = y − x is also nonnegative and that zT y = 0.
e1, . . . , en?

3.14 Nearest unit vector. What is the nearest neighbor of the n-vector x among the unit vectors

3.15 Average, RMS value, and standard deviation. Use the formula (3.5) to show that for any

vector x, the following two inequalities hold:
| avg(x)| ≤ rms(x),

std(x) ≤ rms(x).

Is it possible to have equality in these inequalities? If | avg(x)| = rms(x) is possible, give
the conditions on x under which it holds. Repeat for std(x) = rms(x).
3.16 Eﬀect of scaling and oﬀset on average and standard deviation. Suppose x is an n-vector

and α and β are scalars.

(a) Show that avg(αx + β1) = α avg(x) + β.
(b) Show that std(αx + β1) = |α| std(x).

3.17 Average and standard deviation of linear combination. Let x1, . . . , xk be n-vectors, and

3.18 Triangle equality. When does the triangle inequality hold with equality, i.e., what are the

3.19 Norm of sum. Use the formulas (3.1) and (3.6) to show the following:

conditions on a and b to have �a + b� = �a� + �b�?

α1, . . . , αk be numbers, and consider the linear combination z = α1x1 + ··· + αkxk.
(a) Show that avg(z) = α1 avg(x1) + ··· + αk avg(xk).
(b) Now suppose the vectors are uncorrelated, which means that for i �= j, xi and xj

are uncorrelated. Show that std(z) =�α2
(a) a ⊥ b if and only if �a + b� =��a�2 + �b�2.
(b) Nonzero vectors a and b make an acute angle if and only if �a + b� >��a�2 + �b�2.
(c) Nonzero vectors a and b make an obtuse angle if and only if �a+b� <��a�2 + �b�2.

1 std(x1)2 + ··· + α2

k std(xk)2.

Draw a picture illustrating each case in 2-D.

3.20 Regression model sensitivity. Consider the regression model ˆy = xT β + v, where ˆy is the
prediction, x is a feature vector, β is a coeﬃcient vector, and v is the oﬀset term. If x and ˜x
are feature vectors with corresponding predictions ˆy and ˜y, show that |ˆy− ˜y| ≤ �β��x− ˜x�.
This means that when �β� is small, the prediction is not very sensitive to a change in the
feature vector.

66

3 Norm and distance

3.21 Dirichlet energy of a signal. Suppose the T -vector x represents a time series or signal.

The quantity

D(x) = (x1 − x2)2 + ··· + (xT−1 − xT )2,

the sum of the diﬀerences of adjacent values of the signal, is called the Dirichlet energy of
the signal, named after the mathematician Peter Gustav Lejeune Dirichlet. The Dirichlet
energy is a measure of the roughness or wiggliness of the time series.
It is sometimes
divided by T − 1, to give the mean square diﬀerence of adjacent values.
(a) Express D(x) in vector notation. (You can use vector slicing, vector addition or

subtraction, inner product, norm, and angle.)

(b) How small can D(x) be? What signals x have this minimum value of the Dirichlet

energy?

(c) Find a signal x with entries no more than one in absolute value that has the largest

possible value of D(x). Give the value of the Dirichlet energy achieved.

3.22 Distance from Palo Alto to Beijing. The surface of the earth is reasonably approximated
as a sphere with radius R = 6367.5km. A location on the earth’s surface is traditionally
given by its latitude θ and its longitude λ, which correspond to angular distance from the
equator and prime meridian, respectively. The 3-D coordinates of the location are given
by

� R sin λ cos θ

R cos λ cos θ

R sin θ

� .

(In this coordinate system (0, 0, 0) is the center of the earth, R(0, 0, 1) is the North pole,
and R(0, 1, 0) is the point on the equator on the prime meridian, due south of the Royal
Observatory outside London.)
The distance through the earth between two locations (3-vectors) a and b is �a − b�. The
distance along the surface of the earth between points a and b is R� (a, b). Find these two
distances between Palo Alto and Beijing, with latitudes and longitudes given below.

City

Beijing
Palo Alto

Latitude θ
39.914◦
37.429◦

Longitude λ
116.392◦
−122.138◦

3.23 Angle between two nonnegative vectors. Let x and y be two nonzero n-vectors with
nonnegative entries, i.e., each xi ≥ 0 and each yi ≥ 0. Show that the angle between x
and y lies between 0 and 90◦. Draw a picture for the case when n = 2, and give a short
geometric explanation. When are x and y orthogonal?

3.24 Distance versus angle nearest neighbor. Suppose z1, . . . , zm is a collection of n-vectors,
and x is another n-vector. The vector zj is the (distance) nearest neighbor of x (among
the given vectors) if

i.e., x has smallest distance to zj. We say that zj is the angle nearest neighbor of x if

�x − zj� ≤ �x − zi�,

i = 1, . . . , m,

i.e., x has smallest angle to zj.

� (x, zj) ≤ � (x, zi),

i = 1, . . . , m,

(a) Give a simple speciﬁc numerical example where the (distance) nearest neighbor is

not the same as the angle nearest neighbor.

(b) Now suppose that the vectors z1, . . . , zm are normalized, which means that �zi� = 1,
i = 1, . . . , m. Show that in this case the distance nearest neighbor and the angle
nearest neighbor are always the same. Hint. You can use the fact that arccos
is a decreasing function, i.e., for any u and v with −1 ≤ u < v ≤ 1, we have
arccos(u) > arccos(v).

Exercises

67

3.25 Leveraging. Consider an asset with return time series over T periods given by the T -
vector r. This asset has mean return µ and risk σ, which we assume is positive. We also
consider cash as an asset, with return vector µrf 1, where µrf is the cash interest rate per
period. Thus, we model cash as an asset with return µrf and zero risk. (The superscript
in µrf stands for ‘risk-free’.) We will create a simple portfolio consisting of the asset and
cash. If we invest a fraction θ in the asset, and 1 − θ in cash, our portfolio return is given
by the time series

p = θr + (1 − θ)µrf 1.

We interpret θ as the fraction of our portfolio we hold in the asset. We allow the choices
θ > 1, or θ < 0. In the ﬁrst case we are borrowing cash and using the proceeds to buy
more of the asset, which is called leveraging. In the second case we are shorting the asset.
When θ is between 0 and 1 we are blending our investment in the asset and cash, which
is a form of hedging.

(a) Derive a formula for the return and risk of the portfolio, i.e., the mean and standard
deviation of p. These should be expressed in terms of µ, σ, µrf , and θ. Check your
formulas for the special cases θ = 0 and θ = 1.

(b) Explain how to choose θ so the portfolio has a given target risk level σtar (which is
positive). If there are multiple values of θ that give the target risk, choose the one
that results in the highest portfolio return.

(c) Assume we choose the value of θ as in part (b). When do we use leverage? When

do we short the asset? When do we hedge? Your answers should be in English.

3.26 Time series auto-correlation. Suppose the T -vector x is a non-constant time series, with
xt the value at time (or period) t. Let µ = (1T x)/T denote its mean value. The auto-
correlation of x is the function R(τ ), deﬁned for τ = 0, 1, . . . as the correlation coeﬃcient
of the two vectors (x, µ1τ ) and (µ1τ , x). (The subscript τ denotes the length of the ones
vector.) Both of these vectors also have mean µ. Roughly speaking, R(τ ) tells us how
correlated the time series is with a version of itself lagged or shifted by τ periods. (The
argument τ is called the lag.)
(a) Explain why R(0) = 1, and R(τ ) = 0 for τ ≥ T .
(b) Let z denote the standardized or z-scored version of x (see page 56). Show that for

τ = 0, . . . , T − 1,

R(τ ) =

1
T

ztzt+τ .

T−τ�t=1

(c) Find the auto-correlation for the time series x = (+1,−1, +1,−1, . . . , +1,−1). You

can assume that T is even.

(d) Suppose x denotes the number of meals served by a restaurant on day τ .

It is
observed that R(7) is fairly high, and R(14) is also high, but not as high. Give an
English explanation of why this might be.

3.27 Another measure of the spread of the entries of a vector. The standard deviation is
a measure of how much the entries of a vector diﬀer from their mean value. Another
measure of how much the entries of an n-vector x diﬀer from each other, called the mean
square diﬀerence, is deﬁned as

MSD(x) =

1
n2

n�i,j=1

(xi − xj)2.

(The sum means that you should add up the n2 terms, as the indices i and j each range
from 1 to n.) Show that MSD(x) = 2 std(x)2. Hint. First observe that MSD(˜x) =
MSD(x), where ˜x = x − avg(x)1 is the de-meaned vector. Expand the sum and recall

that�n

i=1 ˜xi = 0.

68

3 Norm and distance

3.28 Weighted norm. On page 51 we discuss the importance of choosing the units or scaling for
the individual entries of vectors, when they represent heterogeneous quantities. Another
approach is to use a weighted norm of a vector x, deﬁned as

�x�w =�w1x2

1 + ··· + wnx2
n,

where w1, . . . , wn are given positive weights, used to assign more or less importance to the
diﬀerent elements of the n-vector x. If all the weights are one, the weighted norm reduces
to the usual (‘unweighted’) norm. It can be shown that the weighted norm is a general
norm, i.e., it satisﬁes the four norm properties listed on page 46. Following the discussion
on page 51, one common rule of thumb is to choose the weight wi as the inverse of the
typical value of x2
A version of the Cauchy–Schwarz inequality holds for weighted norms: For any n-vector
x and y, we have

i in the application.

|w1x1y1 + ··· + wnxnyn| ≤ �x�w�y�w.

(The expression inside the absolute value on the left-hand side is sometimes called the
weighted inner product of x and y.) Show that this inequality holds. Hint. Consider the
vectors ˜x = (x1√w1, . . . , xn√wn) and ˜y = (y1√w1, . . . , yn√wn), and use the (standard)
Cauchy–Schwarz inequality.

Chapter 4

Clustering

In this chapter we consider the task of clustering a collection of vectors into groups
or clusters of vectors that are close to each other, as measured by the distance
between pairs of them. We describe a famous clustering method, called the k-
means algorithm, and give some typical applications.

The material in this chapter will not be used in the sequel. But the ideas, and
the k-means algorithm in particular, are widely used in practical applications, and
rely only on the ideas developed in the previous three chapters. So this chapter
can be considered an interlude that covers useful material that builds on the ideas
developed so far.

4.1 Clustering

Suppose we have N n-vectors, x1, . . . , xN . The goal of clustering is to group or
partition the vectors (if possible) into k groups or clusters, with the vectors in each
group close to each other. Clustering is very widely used in many application areas,
typically (but not always) when the vectors represent features of objects.

Normally we have k much smaller than N , i.e., there are many more vectors
than groups. Typical applications use values of k that range from a handful to a
few hundred or more, with values of N that range from hundreds to billions. Part
of the task of clustering a collection of vectors is to determine whether or not the
vectors can be divided into k groups, with vectors in each group near each other.
Of course this depends on k, the number of clusters, and the particular data, i.e.,
the vectors x1, . . . , xN .

Figure 4.1 shows a simple example, with N = 300 2-vectors, shown as small
circles. We can easily see that this collection of vectors can be divided into k = 3
clusters, shown on the right with the colors representing the diﬀerent clusters. We
could partition these data into other numbers of clusters, but we can see that k = 3
is a good value.

This example is not typical in several ways. First, the vectors have dimension
n = 2. Clustering any set of 2-vectors is easy: We simply scatter plot the values

70

4 Clustering

Figure 4.1 300 points in a plane. The points can be clustered in the three
groups shown on the right.

and check visually if the data are clustered, and if so, how many clusters there are.
In almost all applications n is larger than 2 (and typically, much larger than 2),
in which case this simple visual method cannot be used. The second way in which
it is not typical is that the points are very well clustered. In most applications,
the data are not as cleanly clustered as in this simple example; there are several or
even many points that lie in between clusters. Finally, in this example, it is clear
that the best choice of k is k = 3. In real examples, it can be less clear what the
best value of k is. But even when the clustering is not as clean as in this example,
and the best value of k is not clear, clustering can be very useful in practice.

Examples. Before we delve more deeply into the details of clustering and cluster-
ing algorithms, we list some common applications where clustering is used.

• Topic discovery. Suppose xi are word histograms associated with N docu-
ments. A clustering algorithm partitions the documents into k groups, which
typically can be interpreted as groups of documents with the same or similar
topics, genre, or author. Since the clustering algorithm runs automatically
and without any understanding of what the words in the dictionary mean,
this is sometimes called automatic topic discovery.

• Patient clustering. If xi are feature vectors associated with N patients ad-
mitted to a hospital, a clustering algorithm clusters the patients into k groups
of similar patients (at least in terms of their feature vectors).

• Customer market segmentation. Suppose the vector xi gives the quantities (or
dollar values) of n items purchased by customer i over some period of time. A
clustering algorithm will group the customers into k market segments, which
are groups of customers with similar purchasing patterns.

4.1 Clustering

71

• ZIP code clustering. Suppose that xi is a vector giving n quantities or statis-
tics for the residents of ZIP code i, such as numbers of residents in various
age groups, household size, education statistics, and income statistics. (In
this example N is around 40000.) A clustering algorithm might be used to
cluster the 40000 ZIP codes into, say, k = 100 groups of ZIP codes with
similar statistics.

• Student clustering. Suppose the vector xi gives the detailed grading record
of student i in a course, i.e., her grades on each question in the quizzes,
homework assignments, and exams. A clustering algorithm might be used to
cluster the students into k = 10 groups of students who performed similarly.

• Survey response clustering. A group of N people respond to a survey with n
questions. Each question contains a statement, such as ‘The movie was too
long’, followed by some ordered options such as

Strongly Disagree, Disagree, Neutral, Agree,

Strongly Agree.

(This is called a Likert scale, named after the psychologist Rensis Likert.)
Suppose the n-vector xi encodes the selections of respondent i on the n
questions, using the numerical coding −2, −1, 0, +1, +2 for the responses
above. A clustering algorithm can be used to cluster the respondents into k
groups, each with similar responses to the survey.

• Weather zones. For each of N counties we have a 24-vector xi that gives the
average monthly temperature in the ﬁrst 12 entries and the average monthly
rainfall in the last 12 entries. (We can standardize all the temperatures, and
all the rainfall data, so they have a typical range between −1 and +1.) The
vector xi summarizes the annual weather pattern in county i. A clustering
algorithm can be used to cluster the counties into k groups that have similar
weather patterns, called weather zones. This clustering can be shown on a
map, and used to recommend landscape plantings depending on zone.

• Daily energy use patterns. The 24-vectors xi give the average (electric) en-
ergy use for N customers over some period (say, a month) for each hour of
the day. A clustering algorithm partitions customers into groups, each with
similar patterns of daily energy consumption. We might expect a clustering
algorithm to ‘discover’ which customers have a swimming pool, an electric
water heater, or solar panels.

• Financial sectors. For each of N companies we have an n-vector whose com-
ponents are ﬁnancial and business attributes such as total capitalization,
quarterly returns and risks, trading volume, proﬁt and loss, or dividends
paid. (These quantities would typically be scaled so as to have similar ranges
of values.) A clustering algorithm would group companies into sectors, i.e.,
groups of companies with similar attributes.

In each of these examples, it would be quite informative to know that the vectors
can be well clustered into, say, k = 5 or k = 37 groups. This can be used to develop
insight into the data. By examining the clusters we can often understand them,
and assign labels or descriptions to them.

72

4 Clustering

4.2 A clustering objective

In this section we formalize the idea of clustering, and introduce a natural measure
of the quality of a given clustering.

Specifying the cluster assignments. We specify a clustering of the vectors by
saying which cluster or group each vector belongs to. We label the groups 1, . . . , k,
and specify a clustering or assignment of the N given vectors to groups using an
N -vector c, where ci is the group (number) that the vector xi is assigned to. As
a simple example with N = 5 vectors and k = 3 groups, c = (3, 1, 1, 1, 2) means
that x1 is assigned to group 3, x2, x3, and x4 are assigned to group 1, and x5 is
assigned to group 2. We will also describe the clustering by the sets of indices for
each group. We let Gj be the set of indices corresponding to group j. For our
simple example above, we have

G1 = {2, 3, 4},

G2 = {5},

G3 = {1}.

(Here we are using the notation of sets; see appendix A.) Formally, we can express
these index sets in terms of the group assignment vector c as

which means that Gj is the set of all indices i for which ci = j.

Gj = {i | ci = j},

Group representatives. With each of the groups we associate a group represen-
tative n-vector, which we denote z1, . . . , zk. These representatives can be any
n-vectors; they do not need to be one of the given vectors. We want each rep-
resentative to be close to the vectors in its associated group, i.e., we want the
quantities

�xi − zci�

to be small. (Note that xi is in group j = ci, so zci is the representative vector
associated with data vector xi.)

A clustering objective. We can now give a single number that we use to judge a
choice of clustering, along with a choice of the group representatives. We deﬁne

J clust =��x1 − zc1�2 + ··· + �xN − zcN�2� /N,

(4.1)

which is the mean square distance from the vectors to their associated represen-
tatives. Note that J clust depends on the cluster assignments (i.e., c), as well as
the choice of the group representatives z1, . . . , zk. The smaller J clust is, the better
the clustering. An extreme case is J clust = 0, which means that the distance be-
tween every original vector and its assigned representative is zero. This happens
only when the original collection of vectors only takes k diﬀerent values, and each
vector is assigned to the representative it is equal to. (This extreme case would
probably not occur in practice.)

Our choice of clustering objective J clust makes sense, since it encourages all
points to be near their associated representative, but there are other reasonable

4.2 A clustering objective

73

choices. For example, it is possible to use an objective that encourages more
balanced groupings. But we will stick with this basic (and very common) choice of
clustering objective.

Optimal and suboptimal clustering. We seek a clustering, i.e., a choice of group
assignments c1, . . . , cN and a choice of representatives z1, . . . , zk, that minimize the
objective J clust. We call such a clustering optimal. Unfortunately, for all but the
very smallest problems, it is practically impossible to ﬁnd an optimal clustering. (It
can be done in principle, but the amount of computation needed grows extremely
rapidly with N .) The good news is that the k-means algorithm described in the
next section requires far less computation (and indeed, can be run for problems
with N measured in billions), and often ﬁnds a very good, if not the absolute best,
clustering.
(Here, ‘very good’ means a clustering and choice of representatives
that achieves a value of J clust near its smallest possible value.) We say that the
clustering choices found by the k-means algorithm are suboptimal, which means
that they might not give the lowest possible value of J clust.

Even though it is a hard problem to choose the best clustering and the best
representatives, it turns out that we can ﬁnd the best clustering, if the representa-
tives are ﬁxed, and we can ﬁnd the best representatives, if the clustering is ﬁxed.
We address these two topics now.

Partitioning the vectors with the representatives ﬁxed. Suppose that the group
representatives z1, . . . , zk are ﬁxed, and we seek the group assignments c1, . . . , cN
that achieve the smallest possible value of J clust. It turns out that this problem
can be solved exactly.

The objective J clust is a sum of N terms. The choice of ci (i.e., the group
to which we assign the vector xi) only aﬀects the ith term in J clust, which is
(1/N )�xi − zci�2. We can choose ci to minimize just this term, since ci does not
aﬀect the other N − 1 terms in J clust. How do we choose ci to minimize this term?
This is easy: We simply choose ci to be the value of j that minimizes �xi − zj�
over j. In other words, we should assign each data vector xi to its nearest neighbor
among the representatives. This choice of assignment is very natural, and easily
carried out.

So when the group representatives are ﬁxed, we can readily ﬁnd the best group
assignment (i.e., the one that minimizes J clust), by assigning each vector to its
nearest representative. With this choice of group assignment, we have (by the way
the assignment is made)

�xi − zci� = min

j=1,...,k �xi − zj�,

so the value of J clust is given by

� min
j=1,...,k �x1 − zj�2 + ··· + min

j=1,...,k �xN − zj�2� /N.

This has a simple interpretation: It is the mean of the squared distance from the
data vectors to their closest representative.

74

4 Clustering

Optimizing the group representatives with the assignment ﬁxed. Now we turn
to the problem of choosing the group representatives, with the clustering (group
assignments) ﬁxed, in order to minimize our objective J clust. It turns out that this
problem also has a simple and natural solution.

We start by re-arranging the sum of N terms into k sums, each associated with

one group. We write

where

J clust = J1 + ··· + Jk,
Jj = (1/N )�i∈Gj

�xi − zj�2

is the contribution to the objective J clust from the vectors in group j. (The sum
here means that we should add up all terms of the form �xi − zj�2, for any i ∈ Gj,
i.e., for any vector xi in group j; see appendix A.)
The choice of group representative zj only aﬀects the term Jj; it has no eﬀect
on the other terms in J clust. So we can choose each zj to minimize Jj. Thus we
should choose the vector zj so as to minimize the mean square distance to the
vectors in group j. This problem has a very simple solution: We should choose zj
to be the average (or mean or centroid) of the vectors xi in its group:

zj = (1/|Gj|)�i∈Gj

xi,

where |Gj| is standard mathematical notation for the number of elements in the
set Gj, i.e., the size of group j. (See exercise 4.1.)
So if we ﬁx the group assignments, we minimize J clust by choosing each group
representative to be the average or centroid of the vectors assigned to its group.
(This is sometimes called the group centroid or cluster centroid.)

4.3 The k-means algorithm

It might seem that we can now solve the problem of choosing the group assignments
and the group representatives to minimize J clust, since we know how to do this when
one or the other choice is ﬁxed. But the two choices are circular, i.e., each depends
on the other.
Instead we rely on a very old idea in computation: We simply
iterate between the two choices. This means that we repeatedly alternate between
updating the group assignments, and then updating the representatives, using the
methods developed above. In each step the objective J clust gets better (i.e., goes
down) unless the step does not change the choice.
Iterating between choosing
the group representatives and choosing the group assignments is the celebrated
k-means algorithm for clustering a collection of vectors.

The k-means algorithm was ﬁrst proposed in 1957 by Stuart Lloyd, and inde-
pendently by Hugo Steinhaus. It is sometimes called the Lloyd algorithm. The
name ‘k-means’ has been used since the 1960s.

4.3 The k-means algorithm

75

Figure 4.2 One iteration of the k-means algorithm. The 30 2-vectors xi
are shown as ﬁlled circles, and the 3 group representatives zj are shown as
rectangles. In the left-hand ﬁgure the vectors are each assigned to the closest
representative. In the right-hand ﬁgure, the representatives are replaced by
the cluster centroids.

Algorithm 4.1 k-means algorithm

given a list of N vectors x1, . . . , xN , and an initial list of k group representative
vectors z1, . . . , zk

repeat until convergence

1. Partition the vectors into k groups. For each vector i = 1, . . . , N , assign xi to

the group associated with the nearest representative.

2. Update representatives. For each group j = 1, . . . , k, set zj to be the mean of

the vectors in group j.

One iteration of the k-means algorithm is illustrated in ﬁgure 4.2.

Comments and clariﬁcations.

• Ties in step 1 can be broken by assigning xi to the group associated with one

of the closest representatives with the smallest value of j.

• It is possible that in step 1, one or more of the groups can be empty, i.e.,
contain no vectors. In this case we simply drop this group (and its repre-
sentative). When this occurs, we end up with a partition of the vectors into
fewer than k groups.

76

4 Clustering

• If the group assignments found in step 1 are the same in two successive
iterations, the representatives in step 2 will also be the same. It follows that
the group assignments and group representatives will never change in future
iterations, so we should stop the algorithm. This is what we mean by ‘until
convergence’. In practice, one often stops the algorithm earlier, as soon as
the improvement in J clust in successive iterations becomes very small.

• We start the algorithm with a choice of initial group representatives. One
simple method is to pick the representatives randomly from the original vec-
tors; another is to start from a random assignment of the original vectors to k
groups, and use the means of the groups as the initial representatives. (There
are more sophisticated methods for choosing an initial representatives, but
this topic is beyond the scope of this book.)

Convergence. The fact that J clust decreases in each step implies that the k-means
algorithm converges in a ﬁnite number of steps. However, depending on the initial
choice of representatives, the algorithm can, and does, converge to diﬀerent ﬁnal
partitions, with diﬀerent objective values.

The k-means algorithm is a heuristic, which means it cannot guarantee that the
partition it ﬁnds minimizes our objective J clust. For this reason it is common to
run the k-means algorithm several times, with diﬀerent initial representatives, and
choose the one among them with the smallest ﬁnal value of J clust. Despite the fact
that the k-means algorithm is a heuristic, it is very useful in practical applications,
and very widely used.

Figure 4.3 shows a few iterations generated by the k-means algorithm, applied
to the example of ﬁgure 4.1. We take k = 3 and start with randomly chosen group
representatives. The ﬁnal clustering is shown in ﬁgure 4.4. Figure 4.5 shows how
the clustering objective decreases in each step.

Interpreting the representatives. The representatives z1, . . . , zk associated with
a clustering are quite interpretable. Suppose, for example, that voters in some
election can be well clustered into 7 groups, on the basis of a data set that includes
demographic data and questionnaire or poll data.
If the 4th component of our
vectors is the age of the voter, then (z3)4 = 37.8 tells us that the average age
of voters in group 3 is 37.8.
Insight gained from this data can be used to tune
campaign messages, or choose media outlets for campaign advertising.

Another way to interpret the group representatives is to ﬁnd one or a few of the
original data points that are closest to each representive. These can be thought of
as archetypes for the group.

Choosing k.
It is common to run the k-means algorithm for diﬀerent values of k,
and compare the results. How to choose a value of k among these depends on how
the clustering will be used, which we discuss a bit more in §4.5. But some general
statements can be made. For example, if the value of J clust with k = 7 is quite a
bit smaller than the values of J clust for k = 2, . . . , 6, and not much larger than the
values of J clust for k = 8, 9, . . ., we could reasonably choose k = 7, and conclude
that our data (list of vectors) partitions nicely into 7 groups.

4.3 The k-means algorithm

77

Iteration 1

Iteration 2

Iteration 10

Figure 4.3 Three iterations of the k-means algorithm. The group representa-
tives are shown as squares. In each row, the left-hand plot shows the result
of partitioning the vectors in the 3 groups (step 1 of algorithm 4.1). The
right-hand plot shows the updated representatives (step 2 of the algorithm).

78

4 Clustering

Figure 4.4 Final clustering.

e
l
a
c
s

t
s
u
l
c
J

1.5

1

0.5

1

3

5

7
9
Iteration

11

13

15

Figure 4.5 The clustering objective J clust after step 1 of each iteration.

4.4 Examples

79

Complexity.
In step 1 of the k-means algorithm, we ﬁnd the nearest neighbor
to each of N n-vectors, over the list of k centroids. This requires approximately
3N kn ﬂops. In step 2 we average the n-vectors over each of the cluster groups.
For a cluster with p vectors, this requires n(p − 1) ﬂops, which we approximate as
np ﬂops; averaging all clusters requires a total of N n ﬂops. This is less than the
cost of partitioning in step 1. So k-means requires around (3k + 1)N n ﬂops per
iteration. Its order is N kn ﬂops.

Each run of k-means typically takes fewer than a few tens of iterations, and
usually k-means is run some modest number of times, like 10. So a very rough
guess of the number of ﬂops required to run k-means 10 times (in order to choose
the best partition found) is 1000N kn ﬂops.

As an example, suppose we use k-means to partition N = 100000 vectors with
size n = 100 into k = 10 groups. On a 1 Gﬂop/s computer we guess that this will
take around 100 seconds. Given the approximations made here (for example, the
number of iterations that each run of k-means will take), this is obviously a crude
estimate.

4.4 Examples

4.4.1

Image clustering

The MNIST (Mixed National Institute of Standards) database of handwritten dig-
its is a data set containing N = 60000 grayscale images of size 28 × 28, which
we represent as n-vectors with n = 28 × 28 = 784. Figure 4.6 shows a few
examples from the data set.
(The data set is available from Yann LeCun at
yann.lecun.com/exdb/mnist.)

We use the k-means algorithm to partition these images into k = 20 clusters,
starting with a random assignment of the vectors to groups, and repeating the
experiment 20 times. Figure 4.7 shows the clustering objective versus iteration
number for three of the 20 initial assignments, including the two that gave the
lowest and the highest ﬁnal values of the objective.

Figure 4.8 shows the representatives with the lowest ﬁnal value of the clustering
objective. Figure 4.9 shows the set with the highest value. We can see that most
of the representatives are recognizable digits, with some reasonable confusion, for
example between ‘4’ and ‘9’ or ‘3’ and ‘8’. This is impressive when you consider
that the k-means algorithm knows nothing about digits, handwriting, or even that
the 784-vectors represent 28 × 28 images; it uses only the distances between 784-
vectors. One interpretation is that the k-means algorithm has ‘discovered’ the
digits in the data set.

80

4 Clustering

Figure 4.6 25 images of handwritten digits from the MNIST data set. Each
image has size 28 × 28, and can be represented by a 784-vector.

42

40

t
s
u
l
c
J

38

36

1

5

9

13

Iteration

17

21

25

Figure 4.7 Clustering objective J clust after each iteration of the k-means
algorithm, for three initial partitions, on digits of the MNIST set.

4.4 Examples

81

Figure 4.8 Group representatives found by the k-means algorithm applied
to the MNIST set.

Figure 4.9 Group representatives found by the k-means algorithm applied
to the MNIST set, with a diﬀerent starting point than in ﬁgure 4.8.

82

4 Clustering

·10−3

8

t
s
u
l
c
J

7.5

7

1

5

10

Iteration

15

20

Figure 4.10 Clustering objective J clust after each iteration of the k-means
algorithm, for three initial partitions, on Wikipedia word count histograms.

4.4.2 Document topic discovery

We start with a corpus of N = 500 Wikipedia articles, compiled from weekly lists
of the most popular articles between September 6, 2015, and June 11, 2016. We
remove the section titles and reference sections (bibliography, notes, references,
further reading), and convert each document to a list of words. The conversion
removes numbers and stop words, and applies a stemming algorithm to nouns and
verbs. We then form a dictionary of all the words that appear in at least 20
documents. This results in a dictionary of 4423 words. Each document in the
corpus is represented by a word histogram vector of length 4423.

We apply the k-means algorithm with k = 9, and 20 randomly chosen initial
partitions. The k-means algorithm converges to similar but slightly diﬀerent clus-
terings of the documents in each case. Figure 4.10 shows the clustering objective
versus iteration of the k-means algorithm for three of these, including the one that
gave the lowest ﬁnal value of J clust, which we use below.

Table 4.1 summarizes the clustering with the lowest value of J clust. For each of
the nine clusters we show the largest ten coeﬃcients of the word histogram of the
cluster representative. Table 4.2 gives the size of each cluster and the titles of the
ten articles closest to the cluster representative.

Each of the clusters makes sense, and mostly contains documents on similar
topics, or similar themes. The words with largest coeﬃcients in the group rep-
resentatives also make sense. It is interesting to see that k-means clustering has
assigned movies and TV series (mostly) to diﬀerent clusters (9 and 6). One can
also note that clusters 8 and 9 share several top key words but are on separate
topics (actors and movies, respectively).

4.4 Examples

83

Cluster 1

Cluster 2

Cluster 3

Word

Coeﬃcient

Word Coeﬃcient

Word Coeﬃcient

ﬁght
win
event

champion

ﬁghter
bout
title
Ali

championship

bonus

0.038
0.022
0.019
0.015
0.015
0.015
0.013
0.012
0.011
0.010

holiday
celebrate
festival

celebration

calendar
church
united
date
moon
event

0.012
0.009
0.007
0.007
0.006
0.006
0.005
0.005
0.005
0.005

united
family
party

president

government

school

American
university

city

attack

0.004
0.003
0.003
0.003
0.003
0.003
0.003
0.003
0.003
0.003

Cluster 4

Cluster 5

Cluster 6

Word Coeﬃcient Word Coeﬃcient

Word Coeﬃcient

album 0.031
0.016
release
0.015
song
0.014
music
0.011
single
record
0.010
0.009
band
perform 0.007
0.007
0.007

tour
chart

0.023
game
season
0.020
team 0.018
0.017
win
0.014
0.013
0.010
0.010
0.008
0.007

player
play
league
ﬁnal
score
record

series
season
episode
character

ﬁlm

television

cast

announce

release
appear

0.029
0.027
0.013
0.011
0.008
0.007
0.006
0.006
0.005
0.005

Cluster 7

Cluster 8

Cluster 9

Word

Coeﬃcient Word Coeﬃcient Word Coeﬃcient

match

win

championship

team
event
style
raw
title

episode
perform

0.065
0.018
0.016
0.015
0.015
0.014
0.013
0.011
0.010
0.010

ﬁlm
star
role
play
series
appear
award
actor

character

release

0.036
0.014
0.014
0.010
0.009
0.008
0.008
0.007
0.006
0.006

ﬁlm

million
release

star

character

role
movie

weekend

story
gross

0.061
0.019
0.013
0.010
0.006
0.006
0.005
0.005
0.005
0.005

Table 4.1 The 9 cluster representatives. For each representative we show
the largest 10 coeﬃcients of the word histogram.

84

4 Clustering

Cluster

Size

Titles

1

2

3

4

5

6

7

8

9

21

43

Floyd Mayweather, Jr; Kimbo Slice; Ronda Rousey; Jos´e
Aldo; Joe Frazier; Wladimir Klitschko; Saul ´Alvarez;
Gennady Golovkin; Nate Diaz; Conor McGregor.

Halloween; Guy Fawkes Night; Diwali; Hanukkah;
Groundhog Day; Rosh Hashanah; Yom Kippur;
Seventh-day Adventist Church; Remembrance Day;
Mother’s Day.

189 Mahatma Gandhi; Sigmund Freud; Carly Fiorina;

46

49

39

Frederick Douglass; Marco Rubio; Christopher Columbus;
Fidel Castro; Jim Webb; Genie (feral child); Pablo
Escobar.

David Bowie; Kanye West; Celine Dion; Kesha; Ariana
Grande; Adele; Gwen Stefani; Anti (album); Dolly Parton;
Sia Furler.

Kobe Bryant; Lamar Odom; Johan Cruyﬀ; Yogi Berra;
Jos´e Mourinho; Halo 5: Guardians; Tom Brady; Eli
Manning; Stephen Curry; Carolina Panthers.

The X-Files; Game of Thrones; House of Cards (U.S. TV
series); Daredevil (TV series); Supergirl (U.S. TV series);
American Horror Story; The Flash (2014 TV series); The
Man in the High Castle (TV series); Sherlock (TV series);
Scream Queens (2015 TV series).

16 Wrestlemania 32; Payback (2016); Survivor Series (2015);

58

39

Royal Rumble (2016); Night of Champions (2015);
Fastlane (2016); Extreme Rules (2016); Hell in a Cell
(2015); TLC: Tables, Ladders & Chairs (2015); Shane
McMahon.

Ben Aﬄeck; Johnny Depp; Maureen O’Hara; Kate
Beckinsale; Leonardo DiCaprio; Keanu Reeves; Charlie
Sheen; Kate Winslet; Carrie Fisher; Alan Rickman.

Star Wars: The Force Awakens; Star Wars Episode I: The
Phantom Menace; The Martian (ﬁlm); The Revenant
(2015 ﬁlm); The Hateful Eight; Spectre (2015 ﬁlm); The
Jungle Book (2016 ﬁlm); Bajirao Mastani (ﬁlm); Back to
the Future II; Back to the Future.

Table 4.2 Cluster sizes and titles of 10 documents closest to the cluster
representatives.

4.5 Applications

85

The identiﬁcation of these separate topics among the documents is impressive,
when you consider that the k-means algorithm does not understand the meaning of
the words in the documents (and indeed, does not even know the order of the words
in each document). It uses only the simple concept of document dissimilarity, as
measured by the distance between word count histogram vectors.

4.5 Applications

Clustering, and the k-means algorithm in particular, has many uses and applica-
tions. It can be used for exploratory data analysis, to get an idea of what a large
collection of vectors ‘looks like’. When k is small enough, say less than a few tens,
it is common to examine the group representatives, and some of the vectors in the
associated groups, to interpret or label the groups. Clustering can also be used for
more speciﬁc directed tasks, a few of which we describe below.

Classiﬁcation. We cluster a large collection of vectors into k groups, and label
the groups by hand. We can now assign (or classify) new vectors to one of the
k groups by choosing the nearest group representative.
In our example of the
handwritten digits above, this would give us a rudimentary digit classiﬁer, which
would automatically guess what a written digit is from its image.
In the topic
discovery example, we can automatically classify new documents into one of the k
topics. (We will see better classiﬁcation methods in chapter 14.)

Recommendation engine. Clustering can be used to build a recommendation en-
gine, which suggests items that a user or customer might be interested in. Suppose
the vectors give the number of times a user has listened to or streamed each song
from a library of n songs over some period. These vectors are typically sparse, since
each user has listened to only a very small fraction of the music library. Clustering
the vectors reveals groups of users with similar musical taste. The group repre-
sentatives have a nice interpretation: (zj)i is the average number of times users in
group j listened to song i.

This interpretation allows us to create a set of recommendations for each user.
We ﬁrst identify which cluster j her music listening vector xi is in. Then we can
suggest to her songs that she has not listened to, but others in her group (i.e., those
with similar musical taste) have listened to most often. To recommend 5 songs to
her, we ﬁnd the indices l with (xi)l = 0, with the 5 largest values of (zj)l.

Guessing missing entries. Suppose we have a collection of vectors, with some
entries of some of the vectors missing or not given.
(The symbol ‘?’ or ‘*’ is
sometimes used to denote a missing entry in a vector.) For example, suppose
the vectors collect attributes of a collection of people, such as age, sex, years of
education, income, number of children, and so on. A vector containing the symbol
‘?’
in the age entry means that we do not know that particular person’s age.
Guessing missing entries of vectors in a collection of vectors is sometimes called

86

4 Clustering

imputing the missing entries. In our example, we might want to guess the age of
the person whose age we do not know.

We can use clustering, and the k-means algorithm in particular, to guess the
missing entries. We ﬁrst carry out k-means clustering on our data, using only those
vectors that are complete, i.e., all of their entries are known. Now consider a vector
x in our collection that is missing one or more entries. Since some of the entries of
x are unknown, we cannot ﬁnd the distances �x− zj�, and therefore we cannot say
which group representative is closest to x. Instead we will ﬁnd the closest group
representative to x using only the known entries in x, by ﬁnding j that minimizes

�i∈K

(xi − (zj)i)2,

where K is the set of indices for the known entries of the vector x. This gives us
the closest representative to x, calculated using only its known entries. To guess
the missing entries in x, we simply use the corresponding entries in zj, the nearest
representative.

Returning to our example, we would guess the age of the person with a missing
age entry by ﬁnding the closest representative (ignoring age); then we use the age
entry of the representative, which is simply the average age of all the people in that
cluster.

Exercises

Exercises

87

4.1 Minimizing mean square distance to a set of vectors. Let x1, . . . , xL be a collection of
n-vectors. In this exercise you will ﬁll in the missing parts of the argument to show that
the vector z which minimizes the sum-square distance to the vectors,

J(z) = �x1 − z�2 + ··· + �xL − z�2,

is the average or centroid of the vectors, x = (1/L) (x1 + ··· + xL). (This result is used
in one of the steps in the k-means algorithm. But here we have simpliﬁed the notation.)

(a) Explain why, for any z, we have

J(z) =

�xi − x − (z − x)�2 =

L�i=1

i=1(xi − x)T (z − x) = 0. Hint. Write the left-hand side as

L�i=1��xi − x�2 − 2(xi − x)T (z − x)� + L�z − x�2.
(xi − x)�T
� L�i=1

(z − x),

(b) Explain why�L

and argue that the left-hand vector is 0.

(c) Combine the results of (a) and (b) to get J(z) = �L

i=1 �xi − x�2 + L�z − x�2.
Explain why for any z �= x, we have J(z) > J(x). This shows that the choice z = x
minimizes J(z).

4.2 k-means with nonnegative, proportions, or Boolean vectors. Suppose that the vectors

x1, . . . , xN are clustered using k-means, with group representatives z1, . . . , zk.

(a) Suppose the original vectors xi are nonnegative, i.e., their entries are nonnegative.

Explain why the representatives zj are also nonnegative.

(b) Suppose the original vectors xi represent proportions, i.e., their entries are nonneg-
ative and sum to one. (This is the case when xi are word count histograms, for
example.) Explain why the representatives zj also represent proportions, i.e., their
entries are nonnegative and sum to one.

(c) Suppose the original vectors xi are Boolean, i.e., their entries are either 0 or 1. Give

an interpretation of (zj)i, the ith entry of the j group representative.

Hint. Each representative is the average of some of the original vectors.

4.3 Linear separation in 2-way partitioning. Clustering a collection of vectors into k = 2
groups is called 2-way partitioning, since we are partitioning the vectors into 2 groups,
with index sets G1 and G2. Suppose we run k-means, with k = 2, on the n-vectors
x1, . . . , xN . Show that there is a nonzero vector w and a scalar v that satisfy

wT xi + v ≥ 0 for i ∈ G1,

wT xi + v ≤ 0 for i ∈ G2.

In other words, the aﬃne function f (x) = wT x + v is greater than or equal to zero on
the ﬁrst group, and less than or equal to zero on the second group. This is called linear
separation of the two groups (although aﬃne separation would be more accurate).
Hint. Consider the function �x − z1�2 − �x − z2�2, where z1 and z2 are the group
representatives.
4.4 Pre-assigned vectors. Suppose that some of the vectors x1, . . . , xN are assigned to speciﬁc
groups. For example, we might insist that x27 be assigned to group 5. Suggest a simple
modiﬁcation of the k-means algorithm that respects this requirement. Describe a practical
example where this might arise, when each vector represents n features of a medical
patient.

Chapter 5

Linear independence

In this chapter we explore the concept of linear independence, which will play an
important role in the sequel.

5.1 Linear dependence

A collection or list of n-vectors a1, . . . , ak (with k ≥ 1) is called linearly dependent
if

β1a1 + ··· + βkak = 0

holds for some β1, . . . , βk that are not all zero. In other words, we can form the
zero vector as a linear combination of the vectors, with coeﬃcients that are not all
zero. Linear dependence of a list of vectors does not depend on the ordering of the
vectors in the list.

When a collection of vectors is linearly dependent, at least one of the vectors
can be expressed as a linear combination of the other vectors: If βi �= 0 in the
equation above (and by deﬁnition, this must be true for at least one i), we can
move the term βiai to the other side of the equation and divide by βi to get

ai = (−β1/βi)a1 + ··· + (−βi−1/βi)ai−1 + (−βi+1/βi)ai+1 + ··· + (−βk/βi)ak.
The converse is also true: If any vector in a collection of vectors is a linear combi-
nation of the other vectors, then the collection of vectors is linearly dependent.

Following standard mathematical language usage, we will say “The vectors
a1, . . . , ak are linearly dependent” to mean “The list of vectors a1, . . . , ak is linearly
dependent”. But it must be remembered that linear dependence is an attribute of
a collection of vectors, and not individual vectors.

Linearly independent vectors. A collection of n-vectors a1, . . . , ak (with k ≥ 1)
is called linearly independent if it is not linearly dependent, which means that

β1a1 + ··· + βkak = 0

(5.1)

90

5 Linear independence

only holds for β1 = ··· = βk = 0. In other words, the only linear combination of
the vectors that equals the zero vector is the linear combination with all coeﬃcients
zero.

As with linear dependence, we will say “The vectors a1, . . . , ak are linearly
independent” to mean “The list of vectors a1, . . . , ak is linearly independent”. But,
like linear dependence, linear independence is an attribute of a collection of vectors,
and not individual vectors.

It is generally not easy to determine by casual inspection whether or not a list
of vectors is linearly dependent or linearly independent. But we will soon see an
algorithm that does this.

Examples.

• A list consisting of a single vector is linearly dependent only if the vector is

zero. It is linearly independent only if the vector is nonzero.

• Any list of vectors containing the zero vector is linearly dependent.
• A list of two vectors is linearly dependent if and only if one of the vectors
is a multiple of the other one. More generally, a list of vectors is linearly
dependent if any one of the vectors is a multiple of another one.

• The vectors

a1 =

0.2
−7.0

8.6  ,

a2 =

−0.1
2.0

−1.0  ,

a3 =

0.0
−1.0

2.2 

are linearly dependent, since a1 + 2a2 − 3a3 = 0. We can express any of
these vectors as a linear combination of the other two. For example, we have
a2 = (−1/2)a1 + (3/2)a3.

• The vectors

a1 =

1
0

0  ,

a2 =

0
−1

1  ,

a3 =

−1
1

1 

are linearly independent. To see this, suppose β1a1 + β2a2 + β3a3 = 0. This
means that

β1 − β3 = 0,

−β2 + β3 = 0,

β2 + β3 = 0.

Adding the last two equations we ﬁnd that 2β3 = −0, so β3 = 0. Using this,
the ﬁrst equation is then β1 = 0, and the second equation is β2 = 0.

• The standard unit n-vectors e1, . . . , en are linearly independent. To see this,

suppose that (5.1) holds. We have

0 = β1e1 + ··· + βnen =

so we conclude that β1 = ··· = βn = 0.

β1
...
βn

 ,

5.2 Basis

91

Linear combinations of linearly independent vectors. Suppose a vector x is a
linear combination of a1, . . . , ak,

x = β1a1 + ··· + βkak.

When the vectors a1, . . . , ak are linearly independent, the coeﬃcients that form x
are unique: If we also have

x = γ1a1 + ··· + γkak,

then βi = γi for i = 1, . . . , k. This tells us that, in principle at least, we can ﬁnd
the coeﬃcients that form a vector x as a linear combination of linearly independent
vectors.

To see this, we subtract the two equations above to get

0 = (β1 − γ1)a1 + ··· + (βk − γk)ak.

Since a1, . . . , ak are linearly independent, we conclude that βi − γi are all zero.
The converse is also true: If each linear combination of a list of vectors can
only be expressed as a linear combination with one set of coeﬃcients, then the
list of vectors is linearly independent. This gives a nice interpretation of linear
independence: A list of vectors is linearly independent if and only if for any linear
combination of them, we can infer or deduce the associated coeﬃcients. (We will
see later how to do this.)

Supersets and subsets.
If a collection of vectors is linearly dependent, then any
superset of it is linearly dependent. In other words: If we add vectors to a linearly
dependent collection of vectors, the new collection is also linearly dependent. Any
nonempty subset of a linearly independent collection of vectors is linearly inde-
pendent. In other words: Removing vectors from a collection of vectors preserves
linear independence.

5.2 Basis

Independence-dimension inequality.
pendent, then k ≤ n. In words:

If the n-vectors a1, . . . , ak are linearly inde-

A linearly independent collection of n-vectors can have at most n elements.

Put another way:

Any collection of n + 1 or more n-vectors is linearly dependent.

As a very simple example, we can conclude that any three 2-vectors must be linearly
dependent. This is illustrated in ﬁgure 5.1.

We will prove this fundamental fact below; but ﬁrst, we describe the concept

of basis, which relies on the independence-dimension inequality.

92

5 Linear independence

a1

a3

a2

γ1a1

a3

γ2a2

Figure 5.1 Left. Three 2-vectors. Right. The vector a3 is a linear combina-
tion of a1 and a2, which shows that the vectors are linearly dependent.

Basis. A collection of n linearly independent n-vectors (i.e., a collection of linearly
independent vectors of the maximum possible size) is called a basis. If the n-vectors
a1, . . . , an are a basis, then any n-vector b can be written as a linear combination
of them. To see this, consider the collection of n + 1 n-vectors a1, . . . , an, b. By the
independence-dimension inequality, these vectors are linearly dependent, so there
are β1, . . . , βn+1, not all zero, that satisfy

β1a1 + ··· + βnan + βn+1b = 0.

If βn+1 = 0, then we have

β1a1 + ··· + βnan = 0,

which, since a1, . . . , an are linearly independent, implies that β1 = ··· = βn = 0.
But then all the βi are zero, a contradiction. So we conclude that βn+1 �= 0. It
follows that

b = (−β1/βn+1)a1 + ··· + (−βn/βn+1)an,

i.e., b is a linear combination of a1, . . . , an.

Combining this result with the observation above that any linear combination

of linearly independent vectors can be expressed in only one way, we conclude:

Any n-vector b can be written in a unique way as a linear combination
of a basis a1, . . . , an.

Expansion in a basis. When we express an n-vector b as a linear combination of
a basis a1, . . . , an, we refer to

b = α1a1 + ··· + αnan,

as the expansion of b in the a1, . . . , an basis. The numbers α1, . . . , αn are called
the coeﬃcients of the expansion of b in the basis a1, . . . , an. (We will see later how
to ﬁnd the coeﬃcients in the expansion of a vector in a basis.)

5.2 Basis

Examples.

93

• The n standard unit n vectors e1, . . . , en are a basis. Any n-vector b can be

written as the linear combination

b = b1e1 + ··· + bnen.

(This was already observed on page 17.) This expansion is unique, which
means that there is no other linear combination of e1, . . . , en that equals b.

• The vectors

a1 =�

1.2

−2.6 � ,

a2 =� −0.3
−3.7 �

are a basis. The vector b = (1, 1) can be expressed in only one way as a linear
combination of them:

b = 0.6513 a1 − 0.7280 a2.

(The coeﬃcients are given here to 4 signiﬁcant digits. We will see later how
these coeﬃcients can be computed.)

Cash ﬂows and single period loans. As a practical example, we consider cash
ﬂows over n periods, with positive entries meaning income or cash in and negative
entries meaning payments or cash out. We deﬁne the single-period loan cash ﬂow
vectors as

li =

0i−1
1

−(1 + r)
0n−i−1

 ,

i = 1, . . . , n − 1,

where r ≥ 0 is the per-period interest rate. The cash ﬂow li represents a loan of $1
in period i, which is paid back in period i + 1 with interest r. (The subscripts on
the zero vectors above give their dimensions.) Scaling li changes the loan amount;
scaling li by a negative coeﬃcient converts it into a loan to another entity (which
is paid back in period i + 1 with interest).

The vectors e1, l1, . . . , ln−1 are a basis. (The ﬁrst vector e1 represents income of
$1 in period 1.) To see this, we show that they are linearly independent. Suppose
that

β1e1 + β2l1 + ··· + βnln−1 = 0.

We can express this as



β1 + β2

β3 − (1 + r)β2

...

βn − (1 + r)βn−1

−(1 + r)βn



= 0.

The last entry is −(1 + r)βn = 0, which implies that βn = 0 (since 1 + r > 0).
Using βn = 0, the second to last entry becomes −(1 + r)βn−1 = 0, so we conclude
that βn−1 = 0. Continuing this way we ﬁnd that βn−2, . . . , β2 are all zero. The

94

5 Linear independence

ﬁrst entry of the equation above, β1 + β2 = 0, then implies β1 = 0. We conclude
that the vectors e1, l1, . . . , ln−1 are linearly independent, and therefore a basis.

This means that any cash ﬂow n-vector c can be expressed as a linear combi-

nation of (i.e., replicated by) an initial payment and one period loans:

c = α1e1 + α2l1 + ··· + αnln−1.

It is possible to work out what the coeﬃcients are (see exercise 5.3). The most
interesting one is the ﬁrst coeﬃcient

α1 = c1 +

c2

1 + r

+ ··· +

cn

(1 + r)n−1 ,

which is exactly the net present value (NPV) of the cash ﬂow, with interest rate r.
Thus we see that any cash ﬂow can be replicated as an income in period 1 equal
to its net present value, plus a linear combination of one-period loans at interest
rate r.

Proof of independence-dimension inequality. The proof is by induction on the
dimension n. First consider a linearly independent collection a1, . . . , ak of 1-vectors.
We must have a1 �= 0. This means that every element ai of the collection can be
expressed as a multiple ai = (ai/a1)a1 of the ﬁrst element a1. This contradicts
linear independence unless k = 1.

Next suppose n ≥ 2 and the independence-dimension inequality holds for di-
mension n − 1. Let a1, . . . , ak be a linearly independent list of n-vectors. We need
to show that k ≤ n. We partition the vectors as

ai =� bi
αi � ,

i = 1, . . . , k,

where bi is an (n − 1)-vector and αi is a scalar.

independent: �k

i=1 βibi = 0 holds if and only if �k

First suppose that α1 = ··· = αk = 0. Then the vectors b1, . . . , bk are linearly
i=1 βiai = 0, which is only
possible for β1 = ··· = βk = 0 because the vectors ai are linearly independent.
The vectors b1, . . . , bk therefore form a linearly independent collection of (n − 1)-
vectors. By the induction hypothesis we have k ≤ n − 1, so certainly k ≤ n.
Next suppose that the scalars αi are not all zero. Assume αj �= 0. We deﬁne a
collection of k − 1 vectors ci of length n − 1 as follows:

ci = bi −

bj,

i = 1, . . . , j − 1,

ci = bi+1 −

bj,

i = j, . . . , k − 1.

αi+1
αj

αi
αj

i=1 βici = 0 then

βi−1� bi

αi � = 0

(5.2)

with

These k − 1 vectors are linearly independent: If�k−1

βi� bi

j−1�i=1

αj � +
αi � + γ� bj
j−1�i=1

1
αj

γ = −

(

βiαi +

k�i=j+1
k�i=j+1

βi−1αi).

5.3 Orthonormal vectors

95

Figure 5.2 Orthonormal vectors in a plane.

Since the vectors ai = (bi, αi) are linearly independent, the equality (5.2) only holds
when all the coeﬃcients βi and γ are all zero. This in turns implies that the vectors
c1, . . . , ck−1 are linearly independent. By the induction hypothesis k − 1 ≤ n − 1,
so we have established that k ≤ n.

5.3 Orthonormal vectors

A collection of vectors a1, . . . , ak is orthogonal or mutually orthogonal if ai ⊥ aj for
any i, j with i �= j, i, j = 1, . . . , k. A collection of vectors a1, . . . , ak is orthonormal
if it is orthogonal and �ai� = 1 for i = 1, . . . , k. (A vector of norm one is called
normalized ; dividing a vector by its norm is called normalizing it.) Thus, each
vector in an orthonormal collection of vectors is normalized, and two diﬀerent
vectors from the collection are orthogonal. These two conditions can be combined
into one statement about the inner products of pairs of vectors in the collection:
a1, . . . , ak is orthonormal means that

aT

i aj =� 1

0

i = j
i �= j.

Orthonormality, like linear dependence and independence, is an attribute of a col-
lection of vectors, and not an attribute of vectors individually. By convention,
though, we say “The vectors a1, . . . , ak are orthonormal” to mean “The collection
of vectors a1, . . . , ak is orthonormal”.

Examples. The standard unit n-vectors e1, . . . , en are orthonormal. As another
example, the 3-vectors



0
0

−1  ,

1

√2

1
1

0  ,

1

√2

1
−1

0  ,

(5.3)

are orthonormal. Figure 5.2 shows a set of two orthonormal 2-vectors.

Linear independence of orthonormal vectors. Orthonormal vectors are linearly
independent. To see this, suppose a1, . . . , ak are orthonormal, and

β1a1 + ··· + βkak = 0.

96

5 Linear independence

Taking the inner product of this equality with ai yields

0 = aT

= β1(aT
= βi,

i (β1a1 + ··· + βkak)
i a1) + ··· + βk(aT

i ak)

since aT
a1, . . . , ak that is zero is the one with all coeﬃcients zero.

i aj = 0 for j �= i and aT

i ai = 1. Thus, the only linear combination of

Linear combinations of orthonormal vectors. Suppose a vector x is a linear com-
bination of a1, . . . , ak, where a1, . . . , ak are orthonormal,

x = β1a1 + ··· + βkak.

Taking the inner product of the left-hand and right-hand sides of this equation
with ai yields

aT
i x = aT

i (β1a1 + ··· + βkak) = βi,

using the same argument as above. So if a vector x is a linear combination of
orthonormal vectors, we can easily ﬁnd the coeﬃcients of the linear combination
by taking the inner products with the vectors.

For any x that is a linear combination of orthonormal vectors a1, . . . , ak, we

have the identity

x = (aT

1 x)a1 + ··· + (aT

k x)ak.

(5.4)

This identity gives us a simple way to check if an n-vector y is a linear combination
of the orthonormal vectors a1, . . . , ak. If the identity (5.4) holds for y, i.e.,

y = (aT

1 y)a1 + ··· + (aT

k y)ak,

then (evidently) y is a linear combination of a1, . . . , ak; conversely, if y is a linear
combination of a1, . . . , ak, the identity (5.4) holds for y.

Orthonormal basis.
If the n-vectors a1, . . . , an are orthonormal, they are linearly
independent, and therefore also a basis. In this case they are called an orthonormal
basis. The three examples above (on page 95) are orthonormal bases.

If a1, . . . , an is an orthonormal basis, then we have, for any n-vector x, the

identity

x = (aT

1 x)a1 + ··· + (aT

n x)an.

(5.5)

To see this, we note that since a1, . . . , an are a basis, x can be expressed as a
linear combination of them; hence the identity (5.4) above holds. The equation
above is sometimes called the orthonormal expansion formula; the right-hand side
is called the expansion of x in the basis a1, . . . , an. It shows that any n-vector can
be expressed as a linear combination of the basis elements, with the coeﬃcients
given by taking the inner product of x with the elements of the basis.

As an example, we express the 3-vector x = (1, 2, 3) as a linear combination of
the orthonormal basis given in (5.3). The inner products of x with these vectors

5.4 Gram–Schmidt algorithm

97

are

It can be veriﬁed that the expansion of x in this basis is

x =

x = −1
√2

.

,

T

1

3
√2

1
−1

√2
0 
0  .
√2 1
0  + −1
√2

1
−1

1
1



x = −3,

T

0
0

−1 
x = (−3)

T

1

1
1

√2
0 
√2 1
√2
−1  +

0
0

3

5.4 Gram–Schmidt algorithm

In this section we describe an algorithm that can be used to determine if a list of
n-vectors a1, . . . , ak is linearly independent. In later chapters we will see that it
has many other uses as well. The algorithm is named after the mathematicians
Jørgen Pedersen Gram and Erhard Schmidt, although it was already known before
their work.

If the vectors are linearly independent, the Gram–Schmidt algorithm produces
an orthonormal collection of vectors q1, . . . , qk with the following properties: For
each i = 1, . . . , k, ai is a linear combination of q1, . . . , qi, and qi is a linear com-
bination of a1, . . . , ai.
If the vectors a1, . . . , aj−1 are linearly independent, but
a1, . . . , aj are linearly dependent, the algorithm detects this and terminates.
In
other words, the Gram–Schmidt algorithm ﬁnds the ﬁrst vector aj that is a linear
combination of previous vectors a1, . . . , aj−1.

Algorithm 5.1 Gram–Schmidt algorithm

given n-vectors a1, . . . , ak

for i = 1, . . . , k,

1. Orthogonalization. ˜qi = ai − (qT
2. Test for linear dependence. if ˜qi = 0, quit.
3. Normalization. qi = ˜qi/�˜qi�

1 ai)q1 − ··· − (qT

i−1ai)qi−1

The orthogonalization step, with i = 1, reduces to ˜q1 = a1. If the algorithm does
not quit (in step 2), i.e., ˜q1, . . . , ˜qk are all nonzero, we can conclude that the original
collection of vectors is linearly independent; if the algorithm does quit early, say,
with ˜qj = 0, we can conclude that the original collection of vectors is linearly
dependent (and indeed, that aj is a linear combination of a1, . . . , aj−1).

Figure 5.3 illustrates the Gram–Schmidt algorithm for two 2-vectors. The top
row shows the original vectors; the middle and bottom rows show the ﬁrst and
second iterations of the loop in the Gram–Schmidt algorithm, with the left-hand
side showing the orthogonalization step, and the right-hand side showing the nor-
malization step.

98

5 Linear independence

a1

a2

˜q1

q1

a2

a2

˜q2

a2

q1

q1

−(qT

1 a2)q1

q2

Figure 5.3 Gram–Schmidt algorithm applied to two 2-vectors a1, a2. Top.
The original vectors a1 and a2. The gray circle shows the points with norm
one. Middle left. The orthogonalization step in the ﬁrst iteration yields
˜q1 = a1. Middle right. The normalization step in the ﬁrst iteration scales
˜q1 to have norm one, which yields q1. Bottom left. The orthogonalization
step in the second iteration subtracts a multiple of q1 to yield the vector
˜q2, which is orthogonal to q1. Bottom right. The normalization step in the
second iteration scales ˜q2 to have norm one, which yields q2.

5.4 Gram–Schmidt algorithm

99

Analysis of Gram–Schmidt algorithm. Let us show that the following hold, for
i = 1, . . . , k, assuming a1, . . . , ak are linearly independent.

1. ˜qi �= 0, so the linear dependence test in step 2 is not satisﬁed, and we do not

have a divide-by-zero error in step 3.

2. q1, . . . , qi are orthonormal.

3. ai is a linear combination of q1, . . . , qi.

4. qi is a linear combination of a1, . . . , ai.

We show this by induction. For i = 1, we have ˜q1 = a1. Since a1, . . . , ak are
linearly independent, we must have a1 �= 0, and therefore ˜q1 �= 0, so assertion 1
holds. The single vector q1 (considered as a list with one element) is evidently
orthonormal, since �q1� = 1, so assertion 2 holds. We have a1 = �˜q1�q1, and
q1 = (1/�˜q1�)a1, so assertions 3 and 4 hold.
Suppose our assertion holds for some i−1, with i < k; we will show it holds for i.
If ˜qi = 0, then ai is a linear combination of q1, . . . , qi−1 (from the ﬁrst step in the
algorithm); but each of these is (by the induction hypothesis) a linear combination
of a1, . . . , ai−1, so it follows that ai is a linear combination of a1, . . . , ai−1, which
contradicts our assumption that a1, . . . , ak are linearly independent. So assertion 1
holds for i.

Step 3 of the algorithm ensures that q1, . . . , qi are normalized; to show they are
orthogonal we will show that qi ⊥ qj for j = 1, . . . , i− 1. (Our induction hypothesis
tells us that qr ⊥ qs for r, s < i.) For any j = 1, . . . , i − 1, we have (using step 1)

j ˜qi = qT
qT
= qT

j ai − (qT
j ai − qT
j qk = 0 for j �= k and qT

1 ai)(qT
j ai = 0,

j q1) − ··· − (qT

i−1ai)(qT

j qi−1)

using qT
j qj = 1. (This explains why step 1 is called the
orthogonalization step: We subtract from ai a linear combination of q1, . . . , qi−1
that ensures qi ⊥ ˜qj for j < i.) Since qi = (1/�˜qi�)˜qi, we have qT
i qj = 0 for
j = 1, . . . , i − 1. So assertion 2 holds for i.

It is immediate that ai is a linear combination of q1, . . . , qi:

ai = ˜qi + (qT

1 ai)q1 + ··· + (qT

i−1ai)qi−1

= (qT

1 ai)q1 + ··· + (qT

i−1ai)qi−1 + �˜qi�qi.

From step 1 of the algorithm, we see that ˜qi is a linear combination of the vec-
tors a1, q1, . . . , qi−1. By the induction hypothesis, each of q1, . . . , qi−1 is a linear
combination of a1, . . . , ai−1, so ˜qi (and therefore also qi) is a linear combination of
a1, . . . , ai. Thus assertions 3 and 4 hold.

Gram–Schmidt completion implies linear independence. From the properties
1–4 above, we can argue that the original collection of vectors a1, . . . , ak is linearly
independent. To see this, suppose that

β1a1 + ··· + βkak = 0

(5.6)

100

5 Linear independence

holds for some β1, . . . , βk. We will show that β1 = ··· = βk = 0.
We ﬁrst note that any linear combination of q1, . . . , qk−1 is orthogonal to any
multiple of qk, since qT
k−1qk = 0 (by deﬁnition). But each of
a1, . . . , ak−1 is a linear combination of q1, . . . , qk−1, so we have qT
k a1 = ··· =
qT
k ak−1 = 0. Taking the inner product of qk with the left- and right-hand sides
of (5.6) we obtain

1 qk = ··· = qT

0 = qT

k (β1a1 + ··· + βkak)
k a1 + ··· + βk−1qT

= β1qT
= βk�˜qk�,

k ak−1 + βkqT

k ak

where we use qT

k ak = �˜qk� in the last line. We conclude that βk = 0.

From (5.6) and βk = 0 we have

β1a1 + ··· + βk−1ak−1 = 0.

We now repeat the argument above to conclude that βk−1 = 0. Repeating it k
times we conclude that all βi are zero.

Early termination. Suppose that the Gram–Schmidt algorithm terminates pre-
maturely, in iteration j, because ˜qj = 0. The conclusions 1–4 above hold for
i = 1, . . . , j − 1, since in those steps ˜qi is nonzero. Since ˜qj = 0, we have

aj = (qT

1 aj)q1 + ··· + (qT

j−1aj)qj−1,

which shows that aj is a linear combination of q1, . . . , qj−1. But each of these
vectors is in turn a linear combination of a1, . . . , aj−1, by conclusion 3 above. Then
aj is a linear combination of a1, . . . , aj−1, since it is a linear combination of linear
combinations of them (see exercise 1.18). This means that a1, . . . , aj are linearly
dependent, which implies that the larger set a1, . . . , ak is linearly dependent.

In summary, the Gram–Schmidt algorithm gives us an explicit method for de-

termining if a list of vectors is linearly dependent or independent.

Example. We deﬁne three vectors

a1 = (−1, 1,−1, 1),

a2 = (−1, 3,−1, 3),

a3 = (1, 3, 5, 7).

Applying the Gram–Schmidt algorithm gives the following results.

• i = 1. We have �˜q1� = 2, so
1
�˜q1�
which is simply a1 normalized.

q1 =

˜q1 = (−1/2, 1/2,−1/2, 1/2),

• i = 2. We have qT

1 a2 = 4, so

˜q2 = a2 − (qT

1 a2)q1 =

−1
3
−1
3

 − 4

−1/2
1/2
−1/2
1/2

 =

1
1
1
1

 ,

5.4 Gram–Schmidt algorithm

101

which is indeed orthogonal to q1 (and a1). It has norm �˜q2� = 2; normalizing
it gives

˜q2 = (1/2, 1/2, 1/2, 1/2).

• i = 3. We have qT

q2 =

1
�˜q2�
1 a3 = 2 and qT

˜q3 = a3 − (qT

2 a3 = 8, so

1 a3)q1 − (qT
−1/2
1/2
−1/2
1/2

= 
= 

1
3
5
7
−2
−2
2
2

 − 2
 ,

2 a3)q2

 − 8

1/2
1/2
1/2
1/2



which is orthogonal to q1 and q2 (and a1 and a2). We have �˜q3� = 4, so the
normalized vector is

q3 =

1
�˜q3�

˜q3 = (−1/2,−1/2, 1/2, 1/2).

Completion of the Gram–Schmidt algorithm without early termination tells us that
the vectors a1, a2, a3 are linearly independent.

Determining if a vector is a linear combination of linearly independent vectors.
Suppose the vectors a1, . . . , ak are linearly independent, and we wish to determine
if another vector b is a linear combination of them. (We have already noted on
page 91 that if it is a linear combination of them, the coeﬃcients are unique.)
The Gram–Schmidt algorithm provides an explicit way to do this. We apply the
Gram–Schmidt algorithm to the list of k + 1 vectors

a1, . . . , ak, b.

These vectors are linearly dependent if b is a linear combination of a1, . . . , ak;
they are linearly independent if b is not a linear combination of a1, . . . , ak. The
Gram–Schmidt algorithm will determine which of these two cases holds. It cannot
terminate in the ﬁrst k steps, since we assume that a1, . . . , ak are linearly indepen-
dent. It will terminate in the (k+1)st step with ˜qk+1 = 0 if b is a linear combination
of a1, . . . , ak. It will not terminate in the (k + 1)st step (i.e., ˜qk+1 �= 0), otherwise.

Checking if a collection of vectors is a basis. To check if the n-vectors a1, . . . , an
are a basis, we run the Gram–Schmidt algorithm on them.
If Gram–Schmidt
terminates early, they are not a basis; if it runs to completion, we know they are a
basis.

102

5 Linear independence

Complexity of the Gram–Schmidt algorithm. We now derive an operation count
for the Gram–Schmidt algorithm. In the ﬁrst step of iteration i of the algorithm,
i − 1 inner products

qT
1 ai,

. . . , qT

i−1ai

between vectors of length n are computed. This takes (i − 1)(2n − 1) ﬂops. We
then use these inner products as the coeﬃcients in i− 1 scalar multiplications with
the vectors q1, . . . , qi−1. This requires n(i − 1) ﬂops. We then subtract the i − 1
resulting vectors from ai, which requires another n(i−1) ﬂops. The total ﬂop count
for step 1 is

(i − 1)(2n − 1) + n(i − 1) + n(i − 1) = (4n − 1)(i − 1)

ﬂops. In step 3 we compute the norm of ˜qi, which takes approximately 2n ﬂops.
We then divide ˜qi by its norm, which requires n scalar divisions. So the total ﬂop
count for the ith iteration is (4n − 1)(i − 1) + 3n ﬂops.
our counts for i = 1, . . . , k:

The total ﬂop count for all k iterations of the algorithm is obtained by summing

((4n − 1)(i − 1) + 3n) = (4n − 1)

k(k − 1)

2

+ 3nk ≈ 2nk2,

where we use the fact that

k�i=1
k�i=1

(i − 1) = 1 + 2 + ··· + (k − 2) + (k − 1) =

k(k − 1)

2

,

(5.7)

which we justify below. The complexity of the Gram–Schmidt algorithm is 2nk2;
its order is nk2. We can guess that its running time grows linearly with the lengths
of the vectors n, and quadratically with the number of vectors k.

In the special case of k = n, the complexity of the Gram–Schmidt method is
2n3. For example, if the Gram–Schmidt algorithm is used to determine whether a
collection of 1000 1000-vectors is linearly independent (and therefore a basis), the
computational cost is around 2 × 109 ﬂops. On a modern computer, can we can
expect this to take on the order of one second.
A famous anecdote alleges that the formula (5.7) was discovered by the mathe-
matician Carl Friedrich Gauss when he was a child, although it was known before
that time. Here is his argument, for the case when k is odd. Lump the ﬁrst entry
in the sum together with the last entry, the second entry together with the second-
to-last entry, and so on. Each of these pairs of numbers adds up to k; since there
are (k − 1)/2 such pairs, the total is k(k − 1)/2. A similar argument works when
k is even.

Modiﬁed Gram–Schmidt algorithm. When the Gram–Schmidt algorithm is im-
plemented, a variation on it called the modiﬁed Gram–Schmidt algorithm is typ-
ically used. This algorithm produces the same results as the Gram–Schmidt al-
gorithm (5.1), but is less sensitive to the small round-oﬀ errors that occur when
arithmetic calculations are done using ﬂoating point numbers. (We do not consider
round-oﬀ error in ﬂoating-point computations in this book.)

Exercises

Exercises

103

5.1 Linear independence of stacked vectors. Consider the stacked vectors

c1 =� a1
b1 � ,

. . . , ck =� ak
bk � ,

where a1, . . . , ak are n-vectors and b1, . . . , bk are m-vectors.

(a) Suppose a1, . . . , ak are linearly independent. (We make no assumptions about the
vectors b1, . . . , bk.) Can we conclude that the stacked vectors c1, . . . , ck are linearly
independent?

(b) Now suppose that a1, . . . , ak are linearly dependent. (Again, with no assumptions
about b1, . . . , bk.) Can we conclude that the stacked vectors c1, . . . , ck are linearly
dependent?

5.2 A surprising discovery. An intern at a quantitative hedge fund examines the daily returns
of around 400 stocks over one year (which has 250 trading days). She tells her supervisor
that she has discovered that the returns of one of the stocks, Google (GOOG), can be
expressed as a linear combination of the others, which include many stocks that are
unrelated to Google (say, in a diﬀerent type of business or sector).
Her supervisor then says: “It is overwhelmingly unlikely that a linear combination of the
returns of unrelated companies can reproduce the daily return of GOOG. So you’ve made
a mistake in your calculations.”
Is the supervisor right? Did the intern make a mistake? Give a very brief explanation.

5.3 Replicating a cash ﬂow with single-period loans. We continue the example described
on page 93. Let c be any n-vector representing a cash ﬂow over n periods. Find the
coeﬃcients α1, . . . , αn of c in its expansion in the basis e1, l1, . . . , ln−1, i.e.,

c = α1e1 + α2l1 + ··· + αnln−1.

Verify that α1 is the net present value (NPV) of the cash ﬂow c, deﬁned on page 22,
with interest rate r. Hint. Use the same type of argument that was used to show that
e1, l1, . . . , ln−1 are linearly independent. Your method will ﬁnd αn ﬁrst, then αn−1, and
so on.

5.4 Norm of linear combination of orthonormal vectors. Suppose a1, . . . , ak are orthonormal
n-vectors, and x = β1a1 + ··· + βkak, where β1, . . . , βk are scalars. Express �x� in terms
of β = (β1, . . . , βk).

5.5 Orthogonalizing vectors. Suppose that a and b are any n-vectors. Show that we can always
ﬁnd a scalar γ so that (a − γb) ⊥ b, and that γ is unique if b �= 0. (Give a formula for
the scalar γ.) In other words, we can always subtract a multiple of a vector from another
one, so that the result is orthogonal to the original vector. The orthogonalization step in
the Gram–Schmidt algorithm is an application of this.

5.6 Gram–Schmidt algorithm. Consider the list of n n-vectors

a1 =

,

a2 =

,

. . . ,

an =

1
0
0
...
0







1
1
0
...
0



1
1
1
...
1



.



(The vector ai has its ﬁrst i entries equal to one, and the remaining entries zero.) Describe
what happens when you run the Gram–Schmidt algorithm on this list of vectors, i.e., say
what q1, . . . , qn are. Is a1, . . . , an a basis?

104

5 Linear independence

5.7 Running Gram–Schmidt algorithm twice. We run the Gram–Schmidt algorithm once on
a given set of vectors a1, . . . , ak (we assume this is successful), which gives the vectors
q1, . . . , qk. Then we run the Gram–Schmidt algorithm on the vectors q1, . . . , qk, which
produces the vectors z1, . . . , zk. What can you say about z1, . . . , zk?

5.8 Early termination of Gram–Schmidt algorithm. When the Gram–Schmidt algorithm is
run on a particular list of 10 15-vectors, it terminates in iteration 5 (since ˜q5 = 0). Which
of the following must be true?

(a) a2, a3, a4 are linearly independent.

(b) a1, a2, a5 are linearly dependent.

(c) a1, a2, a3, a4, a5 are linearly dependent.

(d) a4 is nonzero.

5.9 A particular computer can carry out the Gram–Schmidt algorithm on a list of k = 1000
n-vectors, with n = 10000, in around 2 seconds. About how long would you expect it to
take to carry out the Gram–Schmidt algorithm with ˜k = 500 ˜n-vectors, with ˜n = 1000?

Part II

Matrices

Chapter 6

Matrices

In this chapter we introduce matrices and some basic operations on them. We give
some applications in which they arise.

6.1 Matrices

A matrix is a rectangular array of numbers written between rectangular brackets,
as in

It is also common to use large parentheses instead of rectangular brackets, as in

An important attribute of a matrix is its size or dimensions, i.e., the numbers of
rows and columns. The matrix above has 3 rows and 4 columns, so its size is 3× 4.
A matrix of size m × n is called an m × n matrix.
The elements (or entries or coeﬃcients) of a matrix are the values in the array.
The i, j element is the value in the ith row and jth column, denoted by double
subscripts: the i, j element of a matrix A is denoted Aij (or Ai,j, when i or j is
more than one digit or character). The positive integers i and j are called the (row
and column) indices. If A is an m × n matrix, then the row index i runs from 1 to
m and the column index j runs from 1 to n. Row indices go from top to bottom,
so row 1 is the top row and row m is the bottom row. Column indices go from left
to right, so column 1 is the left column and column n is the right column.

of the bottom left element (which has value 4.1) is 3; its column index is 1.

If the matrix above is B, then we have B13 = −2.3, B32 = −1. The row index
Two matrices are equal if they have the same size, and the corresponding entries
are all equal. As with vectors, we normally deal with matrices with entries that




0
1.3
4.1 −1

1 −2.3
4 −0.1
0

0
1.3
4.1 −1

1 −2.3
4 −0.1
0

0.1
0

1.7  .
1.7  .

0.1
0

108

6 Matrices

are real numbers, which will be our assumption unless we state otherwise. The set
of real m × n matrices is denoted Rm×n. But matrices with complex entries, for
example, do arise in some applications.

Matrix indexing. As with vectors, standard mathematical notation indexes the
rows and columns of a matrix starting from 1. In computer languages, matrices
are often (but not always) stored as 2-dimensional arrays, which can be indexed in
a variety of ways, depending on the language. Lower level languages typically use
indices starting from 0; higher level languages and packages that support matrix
operations usually use standard mathematical indexing, starting from 1.

Square, tall, and wide matrices. A square matrix has an equal number of rows
and columns. A square matrix of size n × n is said to be of order n. A tall matrix
has more rows than columns (size m × n with m > n). A wide matrix has more
columns than rows (size m × n with n > m).
Column and row vectors. An n-vector can be interpreted as an n× 1 matrix; we
do not distinguish between vectors and matrices with one column. A matrix with
only one row, i.e., with size 1 × n, is called a row vector ; to give its size, we can
refer to it as an n-row-vector. As an example,

� −2.1 −3

0 �

is a 3-row-vector (or 1 × 3 matrix). To distinguish them from row vectors, vectors
are sometimes called column vectors. A 1 × 1 matrix is considered to be the same
as a scalar.

Notational conventions. Many authors (including us) tend to use capital letters
to denote matrices, and lower case letters for (column or row) vectors. But this
convention is not standardized, so you should be prepared to ﬁgure out whether
a symbol represents a matrix, column vector, row vector, or a scalar, from con-
text. (The more considerate authors will tell you what the symbols represent, for
example, by referring to ‘the matrix A’ when introducing it.)

Columns and rows of a matrix. An m× n matrix A has n columns, given by (the
m-vectors)

for j = 1, . . . , n. The same matrix has m rows, given by the (n-row-vectors)

aj =
bi =� Ai1
� 1

4

A1j
...
Amj

 ,
··· Ain � ,
6 �

3

for i = 1, . . . , m.

As a speciﬁc example, the 2 × 3 matrix
2
5

109

6.1 Matrices

has ﬁrst row

(which is a 3-row-vector or a 1 × 3 matrix), and second column

2

3 �
� 1
� 2
5 �

(which is a 2-vector or 2 × 1 matrix), also written compactly as (2, 5).

Block matrices and submatrices.
are themselves matrices, as in

It is useful to consider matrices whose entries

A =� B C
D E � ,

where B, C, D, and E are matrices. Such matrices are called block matrices; the
elements B, C, D, and E are called blocks or submatrices of A. The submatrices
can be referred to by their block row and column indices; for example, C is the 1,2
block of A.

Block matrices must have the right dimensions to ﬁt together. Matrices in the
same (block) row must have the same number of rows (i.e., the same ‘height’);
matrices in the same (block) column must have the same number of columns (i.e.,
the same ‘width’). In the example above, B and C must have the same number of
rows, and C and E must have the same number of columns. Matrix blocks placed
next to each other in the same row are said to be concatenated ; matrix blocks
placed above each other are called stacked.

As an example, consider

B =� 0

2

3 � ,

Then the block matrix A above is given by

C =� −1 � ,
A =

0
2
1

2
2
3

1

D =� 2
4  .

3 −1
1
4
5

2
3

1

5 � ,

E =� 4
4 � .

(6.1)

(Note that we have dropped the left and right brackets that delimit the blocks.
This is similar to the way we drop the brackets in a 1 × 1 matrix to get a scalar.)
We can also divide a larger matrix (or vector) into ‘blocks’. In this context the
blocks are called submatrices of the big matrix. As with vectors, we can use colon
notation to denote submatrices. If A is an m× n matrix, and p, q, r, s are integers
with 1 ≤ p ≤ q ≤ m and 1 ≤ r ≤ s ≤ n, then Ap:q,r:s denotes the submatrix

Ap:q,r:s =

Apr

Ap,r+1
Ap+1,r Ap+1,r+1

...
Aqr

...

Aq,r+1

···

···
Aps
··· Ap+1,s

...
Aqs

.



110

6 Matrices

This submatrix has size (q − p + 1)× (s− r + 1) and is obtained by extracting from
A the elements in rows p through q and columns r through s.

For the speciﬁc matrix A in (6.1), we have

A2:3,3:4 =� 1

5

4

4 � .

Column and row representation of a matrix. Using block matrix notation we
can write an m × n matrix A as a block matrix with one block row and n block
columns,

where aj, which is an m-vector, is the jth column of A. Thus, an m × n matrix
can be viewed as its n columns, concatenated.
Similarly, an m × n matrix A can be written as a block matrix with one block

column and m block rows:

A =� a1 a2
A =

··· an � ,


,

b1
b2
...
bm

where bi, which is a row n-vector, is the ith row of A. In this notation, the matrix
A is interpreted as its m rows, stacked.

Examples

Table interpretation. The most direct interpretation of a matrix is as a table of
numbers that depend on two indices, i and j. (A vector is a list of numbers that
depend on only one index.) In this case the rows and columns of the matrix usually
have some simple interpretation. Some examples are given below.

• Images. A black and white image with M × N pixels is naturally represented
as an M × N matrix. The row index i gives the vertical position of the pixel,
the column index j gives the horizontal position of the pixel, and the i, j
entry gives the pixel value.

• Rainfall data. An m × n matrix A gives the rainfall at m diﬀerent locations
on n consecutive days, so A42 (which is a number) is the rainfall at location
4 on day 2. The jth column of A, which is an m-vector, gives the rainfall at
the m locations on day j. The ith row of A, which is an n-row-vector, is the
time series of rainfall at location i.

• Asset returns. A T × n matrix R gives the returns of a collection of n assets
(called the universe of assets) over T periods, with Rij giving the return of
asset j in period i. So R12,7 = −0.03 means that asset 7 had a 3% loss in
period 12. The 4th column of R is a T -vector that is the return time series

6.1 Matrices

111

Date

AAPL

March 1, 2016
March 2, 2016
March 3, 2016

GOOG
MMM
0.00006 −0.00113

0.00219
0.00202
0.00744 −0.00894 −0.00019 −0.00468
0.01488 −0.00215
0.00433 −0.00407

AMZN

Table 6.1 Daily returns of Apple (AAPL), Google (GOOG), 3M (MMM),
and Amazon (AMZN), on March 1, 2, and 3, 2016 (based on closing prices).

for asset 4. The 3rd row of R is an n-row-vector that gives the returns of all
assets in the universe in period 3.

An example of an asset return matrix, with a universe of n = 4 assets over
T = 3 periods, is shown in table 6.1.

• Prices from multiple suppliers. An m × n matrix P gives the prices of n
diﬀerent goods from m diﬀerent suppliers (or locations): Pij is the price that
supplier i charges for good j. The jth column of P is the m-vector of supplier
prices for good j; the ith row gives the prices for all goods from supplier i.

• Contingency table. Suppose we have a collection of objects with two at-
tributes, the ﬁrst attribute with m possible values and the second with n
possible values. An m × n matrix A can be used to hold the counts of the
numbers of objects with the diﬀerent pairs of attributes: Aij is the number
of objects with ﬁrst attribute i and second attribute j. (This is the analog
of a count n-vector, that records the counts of one attribute in a collection.)
For example, a population of college students can be described by a 4 × 50
matrix, with the i, j entry the number of students in year i of their studies,
from state j (with the states ordered in, say, alphabetical order). The ith row
of A gives the geographic distribution of students in year i of their studies;
the jth column of A is a 4-vector giving the numbers of student from state j
in their ﬁrst through fourth years of study.

• Customer purchase history. An n× N matrix P can be used to store a set of
N customers’ purchase histories of n products, items, or services, over some
period. The entry Pij represents the dollar value of product i that customer j
purchased over the period (or as an alternative, the number or quantity of the
product). The jth column of P is the purchase history vector for customer j;
the ith row gives the sales report for product i across the N customers.

Matrix representation of a collection of vectors. Matrices are very often used
as a compact way to give a set of indexed vectors of the same size. For example,
if x1, . . . , xN are n-vectors that give the n feature values for each of N objects, we
can collect them all into one n × N matrix

X =� x1 x2

··· xN � ,

112

6 Matrices

2

1

3

4

Figure 6.1 The relation (6.2) as a directed graph.

often called a data matrix or feature matrix. Its jth column is the feature n-vector
for the jth object (in this context sometimes called the jth example). The ith row
of the data matrix X is an N -row-vector whose entries are the values of the ith
feature across the examples. We can also directly interpret the entries of the data
matrix: Xij (which is a number) is the value of the ith feature for the jth example.
As another example, a 3 × M matrix can be used to represent a collection of
M locations or positions in 3-D space, with its jth column giving the jth position.

Matrix representation of a relation or graph. Suppose we have n objects labeled
1, . . . , n. A relation R on the set of objects {1, . . . , n} is a subset of ordered pairs
of objects. As an example, R can represent a preference relation among n possible
products or choices, with (i, j) ∈ R meaning that choice i is preferred to choice j.
A relation can also be viewed as a directed graph, with nodes (or vertices) labeled
1, . . . , n, and a directed edge from j to i for each (i, j) ∈ R. This is typically drawn
as a graph, with arrows indicating the direction of the edge, as shown in ﬁgure 6.1,
for the relation on 4 objects

R = {(1, 2), (1, 3), (2, 1), (2, 4), (3, 4), (4, 1)}.

(6.2)

A relation R on {1, . . . , n} is represented by the n × n matrix A with

Aij =� 1

0

(i, j) ∈ R
(i, j) �∈ R.

This matrix is called the adjacency matrix associated with the graph. (Some au-
thors deﬁne the adjacency matrix in the reverse sense, with Aij = 1 meaning there
is an edge from i to j.) The relation (6.2), for example, is represented by the matrix

A =

0
1
0
1

1
0
0
0

1
0
0
0

0
1
1
0

 .

This is the adjacency matrix of the associated graph, shown in ﬁgure 6.1. (We will
encounter another matrix associated with a directed graph in §7.3.)

6.2 Zero and identity matrices

113

6.2 Zero and identity matrices

Zero matrix. A zero matrix is a matrix with all elements equal to zero. The zero
matrix of size m × n is sometimes written as 0m×n, but usually a zero matrix is
denoted just 0, the same symbol used to denote the number 0 or zero vectors. In
this case the size of the zero matrix must be determined from the context.

Identity matrix. An identity matrix is another common matrix.
It is always
square. Its diagonal elements, i.e., those with equal row and column indices, are
all equal to one, and its oﬀ-diagonal elements (those with unequal row and column
indices) are zero.
Identity matrices are denoted by the letter I. Formally, the
identity matrix of size n is deﬁned by

for i, j = 1, . . . , n. For example,

0

Iij =� 1


1 � ,

0

i = j
i �= j,

1
0
0
0

0
1
0
0

0
0
1
0

0
0
0
1



� 1

0

are the 2 × 2 and 4 × 4 identity matrices.
Using block matrix notation, we can write

The column vectors of the n × n identity matrix are the unit vectors of size n.

where ek is the kth unit vector of size n.

I =� e1

e2

···

en � ,

Sometimes a subscript is used to denote the size of an identity matrix, as in
I4 or I2×2. But more often the size is omitted and follows from the context. For
example, if

A =� 1

4

3

6 � ,

then

� I A

I � =

0

2
5

0
1
0
0
0

1
0
0
0
0



1
4
1
0
0

2
5
0
1
0

3
6
0
0
1

.



The dimensions of the two identity matrices follow from the size of A. The identity
matrix in the 1,1 position must be 2× 2, and the identity matrix in the 2,2 position
must be 3 × 3. This also determines the size of the zero matrix in the 2,1 position.

The importance of the identity matrix will become clear later, in §10.1.

114

6 Matrices

Sparse matrices. A matrix A is said to be sparse if many of its entries are zero,
or (put another way) just a few of its entries are nonzero. Its sparsity pattern is
the set of indices (i, j) for which Aij �= 0. The number of nonzeros of a sparse
matrix A is the number of entries in its sparsity pattern, and denoted nnz(A). If
A is m × n we have nnz(A) ≤ mn. Its density is nnz(A)/(mn), which is no more
than one. Densities of sparse matrices that arise in applications are typically small
or very small, as in 10−2 or 10−4. There is no precise deﬁnition of how small the
density must be for a matrix to qualify as sparse. A famous deﬁnition of sparse
matrix due to the mathematician James H. Wilkinson is: A matrix is sparse if it
has enough zero entries that it pays to take advantage of them. Sparse matrices
can be stored and manipulated eﬃciently on a computer.

Many common matrices are sparse. An n × n identity matrix is sparse, since
it has only n nonzeros, so its density is 1/n. The zero matrix is the sparsest
possible matrix, since it has no nonzero entries. Several special sparsity patterns
have names; we describe some important ones below.

Like sparse vectors, sparse matrices arise in many applications. A typical cus-
tomer purchase history matrix (see page 111) is sparse, since each customer has
likely only purchased a small fraction of all the products available.

Diagonal matrices. A square n × n matrix A is diagonal if Aij = 0 for i �= j.
(The entries of a matrix with i = j are called the diagonal entries; those with i �= j
are its oﬀ-diagonal entries.) A diagonal matrix is one for which all oﬀ-diagonal
entries are zero. Examples of diagonal matrices we have already seen are square
zero matrices and identity matrices. Other examples are

� −3

0

0

0 � ,

0
0

1.2  .

0.2
0
0 −3
0
0



(Note that in the ﬁrst example, one of the diagonal elements is also zero.)

The notation diag(a1, . . . , an) is used to compactly describe the n× n diagonal
matrix A with diagonal entries A11 = a1, . . . , Ann = an. This notation is not yet
standard, but is coming into more prevalent use. As examples, the matrices above
would be expressed as

diag(−3, 0),

diag(0.2,−3, 1.2),

respectively. We also allow diag to take one n-vector argument, as in I = diag(1).

Triangular matrices. A square n × n matrix A is upper triangular if Aij = 0 for
i > j, and it is lower triangular if Aij = 0 for i < j. (So a diagonal matrix is
one that is both lower and upper triangular.) If a matrix is either lower or upper
triangular, it is called triangular. For example, the matrices



1 −1
0.7
0 1.2 −1.1
0
0

3.2  ,

are upper and lower triangular, respectively.

� −0.6

−0.3

0

3.5 � ,

6.3 Transpose, addition, and norm

115

A triangular n × n matrix A has up to n(n + 1)/2 nonzero entries, i.e., around
half its entries are zero. Triangular matrices are generally not considered sparse
matrices, since their density is around 50%, but their special sparsity pattern will
be important in the sequel.

6.3 Transpose, addition, and norm

6.3.1 Matrix transpose

If A is an m × n matrix, its transpose, denoted AT (or sometimes A� or A∗), is the
n × m matrix given by (AT )ij = Aji. In words, the rows and columns of A are
transposed in AT . For example,

0
7
3



4
0

1 

T

=� 0

4

7
0

3

1 � .

If we transpose a matrix twice, we get back the original matrix: (AT )T = A. (The
superscript T in the transpose is the same one used to denote the inner product of
two n-vectors; we will soon see how they are related.)

Row and column vectors. Transposition converts row vectors into column vectors
and vice versa. It is sometimes convenient to express a row vector as aT , where
a is a column vector. For example, we might refer to the m rows of an m × n
matrix A as ˜aT
m, where ˜a1, . . . , ˜am are (column) n-vectors. As an example,
the second row of the matrix

i , . . . , ˜aT

� 0

4

7
0

3

1 �

can be written as (the row vector) (4, 0, 1)T .

It is common to extend concepts from (column) vectors to row vectors, by
applying the concept to the transposed row vectors. We say that a collection of
row vectors is linearly dependent (or independent) if their transposes (which are
column vectors) are linearly dependent (or independent). For example, ‘the rows
of a matrix A are linearly independent’ means that the columns of AT are linearly
independent. As another example, ‘the rows of a matrix A are orthonormal’ means
that their transposes, the columns of AT , are orthonormal. ‘Clustering the rows of
a matrix X’ means clustering the columns of X T .

Transpose of block matrix. The transpose of a block matrix has the simple form
(shown here for a 2 × 2 block matrix)

� A B
C D �T

BT DT � ,
=� AT C T

where A, B, C, and D are matrices with compatible sizes. The transpose of a block
matrix is the transposed block matrix, with each element transposed.

116

6 Matrices

Document-term matrix. Consider a corpus (collection) of N documents, with
word count vectors for a dictionary with n words. The document-term matrix
associated with the corpus is the N×n matrix A, with Aij the number of times word
j appears in document i. The rows of the document-term matrix are aT
1 , . . . , aT
N ,
where the n-vectors a1, . . . , aN are the word count vectors for documents 1, . . . , N ,
respectively. The columns of the document-term matrix are also interesting. The
jth column of A, which is an N -vector, gives the number of times word j appears
in the corpus of N documents.

Data matrix. A collection of N n-vectors, for example feature n-vectors associated
with N objects, can be given as an n× N matrix whose N columns are the vectors,
as described on page 111. It is also common to describe this collection of vectors
using the transpose of that matrix. In this case, we give the vectors as an N × n
matrix X. Its ith row is xT
i , the transpose of the ith vector. Its jth column gives
the value of the jth entry (or feature) across the collection of N vectors. When
an author refers to a data matrix or feature matrix, it can usually be determined
from context (for example, its dimensions) whether they mean the data matrix
organized by rows or columns.

Symmetric matrix. A square matrix A is symmetric if A = AT , i.e., Aij = Aji
for all i, j. Symmetric matrices arise in several applications. For example, suppose
that A is the adjacency matrix of a graph or relation (see page 112). The matrix A
is symmetric when the relation is symmetric, i.e., whenever (i, j) ∈ R, we also have
(j, i) ∈ R. An example is the friend relation on a set of n people, where (i, j) ∈ R
means that person i and person j are friends. (In this case the associated graph is
called the ‘social network graph’.)

6.3.2 Matrix addition

Two matrices of the same size can be added together. The result is another matrix
of the same size, obtained by adding the corresponding elements of the two matrices.
For example,

2
3

6
3

5  .

0
7
3



4
0

1
2
0

1
9
3

1  +
4  =
2 � .
3 � − I =� 0
� 1

6

9

6

9

Matrix subtraction is similar. As an example,

(This gives another example where we have to ﬁgure out the size of the identity
matrix. Since we can only add or subtract matrices of the same size, I refers to a
2 × 2 identity matrix.)
Properties of matrix addition. The following important properties of matrix ad-
dition can be veriﬁed directly from the deﬁnition. We assume here that A, B, and
C are matrices of the same size.

6.3 Transpose, addition, and norm

117

• Commutativity. A + B = B + A.
• Associativity.
A + B + C.

(A + B) + C = A + (B + C). We therefore write both as

• Addition with zero matrix. A + 0 = 0 + A = A. Adding the zero matrix to a

matrix has no eﬀect.

• Transpose of sum. (A + B)T = AT + BT . The transpose of a sum of two

matrices is the sum of their transposes.

6.3.3 Scalar-matrix multiplication

Scalar multiplication of matrices is deﬁned in a similar way as for vectors, and is
done by multiplying every element of the matrix by the scalar. For example

(−2)

1
9
6

6
3

0  =

−2 −12
−18 −6
−12

0  .

As with scalar-vector multiplication, the scalar can also appear on the right. Note
that 0 A = 0 (where the left-hand zero is the scalar zero, and the right-hand 0 is
the zero matrix).

Several useful properties of scalar multiplication follow directly from the deﬁni-
tion. For example, (βA)T = β(AT ) for a scalar β and a matrix A. If A is a matrix
and β, γ are scalars, then

(β + γ)A = βA + γA,

(βγ)A = β(γA).

It is useful to identify the symbols appearing in these two equations. The + symbol
on the left of the left-hand equation is addition of scalars, while the + symbol on
the right of the left-hand equation denotes matrix addition. On the left side of
the right-hand equation we see scalar-scalar multiplication (αβ) and scalar-matrix
multiplication; on the right we see two cases of scalar-matrix multiplication.

Finally, we mention that scalar-matrix multiplication has higher precedence
than matrix addition, which means that we should carry out multiplication before
addition (when there are no parentheses to ﬁx the order). So the right-hand side
of the left equation above is to be interpreted as (βA) + (γA).

6.3.4 Matrix norm

The norm of an m × n matrix A, denoted �A�, is the squareroot of the sum of the
squares of its entries,

�A� =����

m�i=1

n�j=1

A2
ij.

(6.3)

118

6 Matrices

This agrees with our deﬁnition for vectors when A is a vector, i.e., n = 1. The
norm of an m × n matrix is the norm of an mn-vector formed from the entries of
the matrix (in any order). Like the vector norm, the matrix norm is a quantitative
In some applications it is more natural
measure of the magnitude of a matrix.
to use the RMS values of the matrix entries, �A�/√mn, as a measure of matrix

size. The RMS value of the matrix entries tells us the typical size of the entries,
independent of the matrix dimensions.

The matrix norm (6.3) satisﬁes the properties of any norm, given on page 46.
For any m×n matrix A, we have �A� ≥ 0 (i.e., the norm is nonnegative), and �A� =
0 only if A = 0 (deﬁniteness). The matrix norm is nonnegative homogeneous: For
any scalar γ and m × n matrix A, we have �γA� = |γ|�A�. Finally, for any two
m × n matrices A and B, we have the triangle inequality,

�A + B� ≤ �A� + �B�.

(The plus symbol on the left-hand side is matrix addition, and the plus symbol on
the right-hand side is addition of numbers.)

The matrix norm allows us to deﬁne the distance between two matrices as
�A − B�. As with vectors, we can say that one matrix is close to, or near, another
one if their distance is small. (What qualiﬁes as small depends on the application.)
In this book we will only use the matrix norm (6.3). Several other norms of
a matrix are commonly used, but are beyond the scope of this book. In contexts
where other norms of a matrix are used, the norm (6.3) is called the Frobenius
norm, after the mathematician Ferdinand Georg Frobenius, and is usually denoted
with a subscript, as �A�F .
matrix is the same as the norm of its transpose. Another one is

One simple property of the matrix norm is �A� = �AT�, i.e., the norm of a

�A�2 = �a1�2 + ··· + �an�2,

where a1, . . . , an are the columns of A. In other words: The squared norm of a
matrix is the sum of the squared norms of its columns.

6.4 Matrix-vector multiplication

If A is an m×n matrix and x is an n-vector, then the matrix-vector product y = Ax
is the m-vector y with elements

yi =

Aikxk = Ai1x1 + ··· + Ainxn,

i = 1, . . . , m.

(6.4)

As a simple example, we have

n�k=1
1 �

�

0
−2

2 −1
1

2
1

−1  =� (0)(2) + (2)(1) + (−1)(−1)

(−2)(2) + (1)(1) + (1)(−1) � =�

3

−4 � .

6.4 Matrix-vector multiplication

119

Row and column interpretations. We can express the matrix-vector product in
terms of the rows or columns of the matrix. From (6.4) we see that yi is the inner
product of x with the ith row of A:
yi = bT

i = 1, . . . , m,

i x,

where bT
i
terms of the columns of A.
written

is the row i of A. The matrix-vector product can also be interpreted in
If ak is the kth column of A, then y = Ax can be

This shows that y = Ax is a linear combination of the columns of A; the coeﬃcients
in the linear combination are the elements of x.

y = x1a1 + x2a2 + ··· + xnan.

General examples.
n-vector.

In the examples below, A is an m × n matrix and x is an

• Zero matrix. When A = 0, we have Ax = 0. In other words, 0x = 0. (The

left-hand 0 is an m × n matrix, and the right-hand zero is an m-vector.)

• Identity. We have Ix = x for any vector x. (The identity matrix here has
dimension n× n.) In other words, multiplying a vector by the identity matrix
gives the same vector.

• Picking out columns and rows. An important identity is Aej = aj, the jth
column of A. Multiplying a unit vector by a matrix ‘picks out’ one of the
columns of the matrix. AT ei, which is an n-vector, is the ith row of A,
transposed. (In other words, (AT ei)T is the ith row of A.)

• Summing or averaging columns or rows. The m-vector A1 is the sum of the
columns of A; its ith entry is the sum of the entries in the ith row of A.
The m-vector A(1/n) is the average of the columns of A; its ith entry is
the average of the entries in the ith row of A. In a similar way, AT 1 is an
n-vector, whose jth entry is the sum of the entries in the jth column of A.

−1
1
0 −1

• Diﬀerence matrix. The (n − 1) × n matrix
···
···
. . .
. . .
. . .
··· −1
···

0
1
. . .

D =

0
0

0
0

0
0



0
0

0
0

1
0 −1

(where entries not shown are zero, and entries with diagonal dots are 1 or
−1, continuing the pattern) is called the diﬀerence matrix. The vector Dx is
the (n − 1)-vector of diﬀerences of consecutive entries of x:

0
0

0
1



(6.5)

Dx =

x2 − x1
x3 − x2

...

xn − xn−1

.



120

6 Matrices

• Running sum matrix. The n × n matrix
0
0
. . .

0
1

1
1

S =



1
1

1
1

1
1

···
···
. . .
. . .
···
···

0
0

. . .
1
1

0
0

0
1



(6.6)

is called the running sum matrix. The ith entry of the n-vector Sx is the
sum of the ﬁrst i entries of x:

Sx =

Application examples.



x1

x1 + x2

x1 + x2 + x3

...

x1 + ··· + xn

.



• Feature matrix and weight vector. Suppose X is a feature matrix, where its
N columns x1, . . . , xN are feature n-vectors for N objects or examples. Let
the n-vector w be a weight vector, and let si = xT
i w be the score associated
with object i using the weight vector w. Then we can write s = X T w, where
s is the N -vector of scores of the objects.

• Portfolio return time series. Suppose that R is a T × n asset return matrix,
that gives the returns of n assets over T periods. A common trading strategy
maintains constant investment weights given by the n-vector w over the T
periods. For example, w4 = 0.15 means that 15% of the total portfolio value
is held in asset 4. (Short positions are denoted by negative entries in w.)
Then Rw, which is a T -vector, is the time series of the portfolio returns over
the periods 1, . . . , T .

As an example, consider a portfolio of the 4 assets in table 6.1, with weights
w = (0.4, 0.3,−0.2, 0.5). The product Rw = (0.00213,−0.00201, 0.00241)
gives the portfolio returns over the three periods in the example.

• Polynomial evaluation at multiple points. Suppose the entries of the n-vector

c are the coeﬃcients of a polynomial p of degree n − 1 or less:

p(t) = c1 + c2t + ··· + cn−1tn−2 + cntn−1.

Let t1, . . . , tm be m numbers, and deﬁne the m-vector y as yi = p(ti). Then
we have y = Ac, where A is the m × n matrix
tn−2
1
tn−2
2
...
tn−2
m

tn−1
1
tn−1
2
...
tn−1
m

1
1
...
1

(6.7)

···
···

t1
t2
...
tm ···

A =

.



6.4 Matrix-vector multiplication

121

So multiplying a vector c by the matrix A is the same as evaluating a poly-
nomial with coeﬃcients c at m points. The matrix A in (6.7) comes up often,
and is called a Vandermonde matrix (of degree n−1, at the points t1, . . . , tm),
named for the mathematician Alexandre-Th´eophile Vandermonde.

• Total price from multiple suppliers. Suppose the m × n matrix P gives the
prices of n goods from m suppliers (or in m diﬀerent locations). If q is an
n-vector of quantities of the n goods (sometimes called a basket of goods),
then c = P q is an N -vector that gives the total cost of the goods, from each
of the N suppliers.

• Document scoring. Suppose A in an N×n document-term matrix, which gives
the word counts of a corpus of N documents using a dictionary of n words,
so the rows of A are the word count vectors for the documents. Suppose that
w in an n-vector that gives a set of weights for the words in the dictionary.
Then s = Aw is an N -vector that gives the scores of the documents, using
the weights and the word counts. A search engine, for example, might choose
w (based on the search query) so that the scores are predictions of relevance
of the documents (to the search).

• Audio mixing. Suppose the k columns of A are vectors representing audio
signals or tracks of length T , and w is a k-vector. Then b = Aw is a T -vector
representing the mix of the audio signals, with track weights given by the
vector w.

Inner product. When a and b are n-vectors, aT b is exactly the inner product of a
and b, obtained from the rules for transposing matrices and forming a matrix-vector
product. We start with the (column) n-vector a, consider it as an n × 1 matrix,
and transpose it to obtain the n-row-vector aT . Now we multiply this 1× n matrix
by the n-vector b, to obtain the 1-vector aT b, which we also consider a scalar.
So the notation aT b for the inner product is just a special case of matrix-vector
multiplication.

Linear dependence of columns. We can express the concepts of linear depen-
dence and independence in a compact form using matrix-vector multiplication.
The columns of a matrix A are linearly dependent if Ax = 0 for some x �= 0. The
columns of a matrix A are linearly independent if Ax = 0 implies x = 0.

Expansion in a basis.
If the columns of A are a basis, which means A is square
with linearly independent columns a1, . . . , an, then for any n-vector b there is a
unique n-vector x that satisﬁes Ax = b. In this case the vector x gives the coeﬃ-
cients in the expansion of b in the basis a1, . . . , an.

Properties of matrix-vector multiplication. Matrix-vector multiplication satisﬁes
several properties that are readily veriﬁed. First, it distributes across the vector
argument: For any m × n matrix A and any n-vectors u and v, we have

A(u + v) = Au + Av.

122

6 Matrices

Matrix-vector multiplication, like ordinary multiplication of numbers, has higher
precedence than addition, which means that when there are no parentheses to force
the order of evaluation, multiplications are to be carried out before additions. This
means that the right-hand side above is to be interpreted as (Au) + (Av). The
equation above looks innocent and natural, but must be read carefully. On the
left-hand side, we ﬁrst add the vectors u and v, which is the addition of n-vectors.
We then multiply the resulting n-vector by the matrix A. On the right-hand
side, we ﬁrst multiply each of n-vectors by the matrix A (this is two matrix-vector
multiplies); and then add the two resulting m-vectors together. The left- and right-
hand sides of the equation above involve very diﬀerent steps and operations, but
the ﬁnal result of each is the same m-vector.

Matrix-vector multiplication also distributes across the matrix argument: For

any m × n matrices A and B, and any n-vector u, we have

(A + B)u = Au + Bu.

On the left-hand side the plus symbol is matrix addition; on the right-hand side it
is vector addition.

Another basic property is, for any m × n matrix A, any n-vector u, and any

scalar α, we have

(αA)u = α(Au)

(and so we can write this as αAu). On the left-hand side, we have scalar-matrix
multiplication, followed by matrix-vector multiplication; on the right-hand side, we
start with matrix-vector multiplication, and then perform scalar-vector multiplica-
tion. (Note that we also have αAu = A(αu).)

Input-output interpretation. We can interpret the relation y = Ax, with A an
m × n matrix, as a mapping from the n-vector x to the m-vector y.
In this
context we might think of x as an input, and y as the corresponding output. From
equation (6.4), we can interpret Aij as the factor by which yi depends on xj. Some
examples of conclusions we can draw are given below.

• If A23 is positive and large, then y2 depends strongly on x3, and increases as

x3 increases.

• If A32 is much larger than the other entries in the third row of A, then y3

depends much more on x2 than the other inputs.

• If A is square and lower triangular, then yi only depends on x1, . . . , xi.

6.5 Complexity

Computer representation of matrices. An m × n matrix is usually represented
on a computer as an m × n array of ﬂoating point numbers, which requires 8mn
bytes.
In some software systems symmetric matrices are represented in a more
eﬃcient way, by only storing the upper triangular elements in the matrix, in some

6.5 Complexity

123

speciﬁc order. This reduces the memory requirement by around a factor of two.
Sparse matrices are represented by various methods that encode for each nonzero
element its row index i (an integer), its column index j (an integer) and its value
Aij (a ﬂoating point number). When the row and column indices are represented
using 4 bytes (which allows m and n to range up to around 4.3 billion) this requires
a total of around 16 nnz(A) bytes.

Complexity of matrix addition, scalar multiplication, and transposition. The
addition of two m × n matrices or a scalar multiplication of an m × n matrix
each take mn ﬂops. When A is sparse, scalar multiplication requires nnz(A)
ﬂops. When at least one of A and B is sparse, computing A + B requires at
most min{nnz(A), nnz(B)} ﬂops. (For any entry i, j for which one of Aij or Bij is
zero, no arithmetic operations are needed to ﬁnd (A + B)ij.) Matrix transposition,
i.e., computing AT , requires zero ﬂops, since we simply copy entries of A to those
of AT . (Copying the entries does take time to carry out, but this is not reﬂected
in the ﬂop count.)

Complexity of matrix-vector multiplication. A matrix-vector multiplication of
an m × n matrix A with an n-vector x requires m(2n − 1) ﬂops, which we simplify
to 2mn ﬂops. This can be seen as follows. The result y = Ax of the product is an
m-vector, so there are m numbers to compute. The ith element of y is the inner
product of the ith row of A and the vector x, which takes 2n − 1 ﬂops.
If A is sparse, computing Ax requires nnz(A) multiplies (of Aij and xj, for
each nonzero entry of A) and a number of additions that is no more than nnz(A).
Thus, the complexity is between nnz(A) and 2 nnz(A) ﬂops. As a special example,
suppose A is n× n and diagonal. Then Ax can be computed with n multiplies (Aii
times xi) and no additions, a total of n = nnz(A) ﬂops.

124

Exercises

6 Matrices

6.1 Matrix and vector notation. Suppose a1, . . . , an are m-vectors. Determine whether each
If the expression does make

expression below makes sense (i.e., uses valid notation).
sense, give its dimensions.

a1
...
an




···
···

(a) 
(b) 
(c) � a1
(d) � aT

1

aT
1
...
aT
n

an �
n �

aT

6.2 Matrix notation. Suppose the block matrix

� A I
I C �

makes sense, where A is a p × q matrix. What are the dimensions of C?

6.3 Block matrix. Assuming the matrix

K =� I AT
0 �

A

makes sense, which of the following statements must be true? (‘Must be true’ means that
it follows with no additional assumptions.)

(a) K is square.

(b) A is square or wide.
(c) K is symmetric, i.e., K T = K.
(d) The identity and zero submatrices in K have the same dimensions.

(e) The zero submatrix is square.

6.4 Adjacency matrix row and column sums. Suppose A is the adjacency matrix of a directed
graph (see page 112). What are the entries of the vector A1? What are the entries of the
vector AT 1?

6.5 Adjacency matrix of reversed graph. Suppose A is the adjacency matrix of a directed
graph (see page 112). The reversed graph is obtained by reversing the directions of all
the edges of the original graph. What is the adjacency matrix of the reversed graph?
(Express your answer in terms of A.)

6.6 Matrix-vector multiplication. For each of the following matrices, describe in words how x

and y = Ax are related. In each case x and y are n-vectors, with n = 3k.

Ik
0

0 �.

0
Ik
0

0
Ik

(a) A =� 0
(b) A =� E 0

0 E �, where E is the k × k matrix with all entries 1/k.

0
0 E 0
0

Exercises

125

6.7 Currency exchange matrix. We consider a set of n currencies, labeled 1, . . . , n. (These
might correspond to USD, RMB, EUR, and so on.) At a particular time the exchange or
conversion rates among the n currencies are given by an n × n (exchange rate) matrix R,
where Rij is the amount of currency i that you can buy for one unit of currency j. (All
entries of R are positive.) The exchange rates include commission charges, so we have
RjiRij < 1 for all i �= j. You can assume that Rii = 1.
Suppose y = Rx, where x is a vector (with nonnegative entries) that represents the
amounts of the currencies that we hold. What is yi? Your answer should be in English.

6.8 Cash ﬂow to bank account balance. The T -vector c represents the cash ﬂow for an interest
bearing bank account over T time periods. Positive values of c indicate a deposit, and
negative values indicate a withdrawal. The T -vector b denotes the bank account balance
in the T periods. We have b1 = c1 (the initial deposit or withdrawal) and

bt = (1 + r)bt−1 + ct,

t = 2, . . . , T,

where r > 0 is the (per-period) interest rate. (The ﬁrst term is the previous balance plus
the interest, and the second term is the deposit or withdrawal.)
Find the T × T matrix A for which b = Ac. That is, the matrix A maps a cash ﬂow
sequence into a bank account balance sequence. Your description must make clear what
all entries of A are.

6.9 Multiple channel marketing campaign. Potential customers are divided into m market
segments, which are groups of customers with similar demographics, e.g., college educated
women aged 25–29. A company markets its products by purchasing advertising in a set of
n channels, i.e., speciﬁc TV or radio shows, magazines, web sites, blogs, direct mail, and
so on. The ability of each channel to deliver impressions or views by potential customers
is characterized by the reach matrix, the m × n matrix R, where Rij is the number of
views of customers in segment i for each dollar spent on channel j. (We assume that the
total number of views in each market segment is the sum of the views from each channel,
and that the views from each channel scale linearly with spending.) The n-vector c will
denote the company’s purchases of advertising, in dollars, in the n channels. The m-vector
v gives the total number of impressions in the m market segments due to the advertising
in all channels. Finally, we introduce the m-vector a, where ai gives the proﬁt in dollars
per impression in market segment i. The entries of R, c, v, and a are all nonnegative.

(a) Express the total amount of money the company spends on advertising using vec-

tor/matrix notation.

(b) Express v using vector/matrix notation, in terms of the other vectors and matrices.

(c) Express the total proﬁt from all market segments using vector/matrix notation.

(d) How would you ﬁnd the single channel most eﬀective at reaching market segment 3,

in terms of impressions per dollar spent?

(e) What does it mean if R35 is very small (compared to other entries of R)?

6.10 Resource requirements. We consider an application with n diﬀerent job (types), each of
which consumes m diﬀerent resources. We deﬁne the m × n resource matrix R, with
entry Rij giving the amount of resource i that is needed to run one unit of job j, for
i = 1, . . . , m and j = 1, . . . , n. (These numbers are typically positive.) The number (or
amount) of each of the diﬀerent jobs to be processed or run is given by the entries of the
n-vector x. (These entries are typically nonnegative integers, but they can be fractional
if the jobs are divisible.) The entries of the m-vector p give the price per unit of each of
the resources.

(a) Let y be the m-vector whose entries give the total of each of the m resources needed
to process the jobs given by x. Express y in terms of R and x using matrix and
vector notation.

126

6 Matrices

(b) Let c be an n-vector whose entries gives the cost per unit for each job type. (This
is the total cost of the resources required to run one unit of the job type.) Express
c in terms of R and p using matrix and vector notation.

Remark. One example is a data center, which runs many instances of each of n types of
application programs. The resources include number of cores, amount of memory, disk,
and network bandwidth.

6.11 Let A and B be two m × n matrices. Under each of the assumptions below, determine

whether A = B must always hold, or whether A = B holds only sometimes.

(a) Suppose Ax = Bx holds for all n-vectors x.

(b) Suppose Ax = Bx for some nonzero n-vector x.

6.12 Skew-symmetric matrices. An n × n matrix A is called skew-symmetric if AT = −A, i.e.,

its transpose is its negative. (A symmetric matrix satisﬁes AT = A.)
(a) Find all 2 × 2 skew-symmetric matrices.
(b) Explain why the diagonal entries of a skew-symmetric matrix must be zero.
(c) Show that for a skew-symmetric matrix A, and any n-vector x, (Ax) ⊥ x. This
means that Ax and x are orthogonal. Hint. First show that for any n × n matrix A

and n-vector x, xT (Ax) =�n

i,j=1 Aijxixj.

(d) Now suppose A is any matrix for which (Ax) ⊥ x for any n-vector x. Show that A

must be skew-symmetric. Hint. You might ﬁnd the formula

(ei + ej)T (A(ei + ej)) = Aii + Ajj + Aij + Aji,

valid for any n × n matrix A, useful. For i = j, this reduces to eT

i (Aei) = Aii.

6.13 Polynomial diﬀerentiation. Suppose p is a polynomial of degree n − 1 or less, given by
p(t) = c1 + c2t + ··· + cntn−1. Its derivative (with respect to t) p�(t) is a polynomial of
degree n− 2 or less, given by p�(t) = d1 + d2t +··· + dn−1tn−2. Find a matrix D for which
d = Dc. (Give the entries of D, and be sure to specify its dimensions.)
6.14 Norm of matrix-vector product. Suppose A is an m × n matrix and x is an n-vector. A

famous inequality relates �x�, �A�, and �Ax�:

�Ax� ≤ �A��x�.

The left-hand side is the (vector) norm of the matrix-vector product; the right-hand side
is the (scalar) product of the matrix and vector norms. Show this inequality. Hints. Let
aT
i x)2 ≤ �ai�2�x�2.
i be the ith row of A. Use the Cauchy–Schwarz inequality to get (aT
Then add the resulting m inequalities.
6.15 Distance between adjacency matrices. Let A and B be the n × n adjacency matrices of
two directed graphs with n vertices (see page 112). The squared distance �A − B�2 can
be used to express how diﬀerent the two graphs are. Show that �A − B�2 is the total
number of directed edges that are in one of the two graphs but not in the other.
6.16 Columns of diﬀerence matrix. Are the columns of the diﬀerence matrix D, deﬁned in (6.5),

linearly independent?

6.17 Stacked matrix. Let A be an m × n matrix, and consider the stacked matrix S deﬁned by

S =� A
I � .

When does S have linearly independent columns? When does S have linearly independent
rows? Your answer can depend on m, n, or whether or not A has linearly independent
columns or rows.

Exercises

127

6.18 Vandermonde matrices. A Vandermonde matrix is an m × n matrix of the form

V =

1
1
...
1

t2
···
t1
1
t2
···
t2
2
...
...
. . .
tm t2
m ···

tn−1
1
tn−1
2
...
tn−1
m



where t1, . . . , tm are numbers. Multiplying an n-vector c by the Vandermonde matrix V
is the same as evaluating the polynomial of degree less than n, with coeﬃcients c1, . . . , cn,
at the points t1, . . . , tm; see page 120. Show that the columns of a Vandermonde matrix
are linearly independent if the numbers t1, . . . , tm are distinct, i.e., diﬀerent from each
other. Hint. Use the following fact from algebra: If a polynomial p with degree less than
n has n or more roots (points t for which p(t) = 0) then all its coeﬃcients are zero.

6.19 Back-test timing. The T × n asset returns matrix R gives the returns of n assets over
T periods. (See page 120.) When the n-vector w gives a set of portfolio weights, the
T -vector Rw gives the time series of portfolio return over the T time periods. Evaluating
portfolio return with past returns data is called back-testing.
Consider a speciﬁc case with n = 5000 assets, and T = 2500 returns. (This is 10 years
of daily returns, since there are around 250 trading days in each year.) About how long
would it take to carry out this back-test, on a 1 Gﬂop/s computer?

6.20 Complexity of matrix-vector multiplication. On page 123 we worked out the complexity of
computing the m-vector Ax, where A is an m× n matrix and x is an n-vector, when each
entry of Ax is computed as an inner product of a row of A and the vector x. Suppose
instead that we compute Ax as a linear combination of the columns of A, with coeﬃcients
x1, . . . , xn. How many ﬂops does this method require? How does it compare to the method
described on page 123?

6.21 Complexity of matrix-sparse-vector multiplication. On page 123 we consider the complex-
ity of computing Ax, where A is a sparse m × n matrix and x is an n-vector x (not
assumed to be sparse). Now consider the complexity of computing Ax when the m × n
matrix A is not sparse, but the n-vector x is sparse, with nnz(x) nonzero entries. Give
the total number of ﬂops in terms of m, n, and nnz(x), and simplify it by dropping terms
that are dominated by others when the dimensions are large. Hint. The vector Ax is a
linear combination of nnz(x) columns of A.

6.22 Distribute or not? Suppose you need to compute z = (A + B)(x + y), where A and B are

m × n matrices and x and y are n-vectors.
(a) What is the approximate ﬂop count if you evaluate z as expressed, i.e., by adding
A and B, adding x and y, and then carrying out the matrix-vector multiplication?

(b) What is the approximate ﬂop count if you evaluate z as z = Ax + Ay + Bx + By,

i.e., with four matrix-vector multiplies and three vector additions?

(c) Which method requires fewer ﬂops? Your answer can depend on m and n. Remark.
When comparing two computation methods, we usually do not consider a factor of
2 or 3 in ﬂop counts to be signiﬁcant, but in this exercise you can.

Chapter 7

Matrix examples

In this chapter we describe some special matrices that occur often in applications.

7.1 Geometric transformations

Suppose the 2-vector (or 3-vector) x represents a position in 2-D (or 3-D) space.
Several important geometric transformations or mappings from points to points
can be expressed as matrix-vector products y = Ax, with A a 2 × 2 (or 3 × 3)
matrix. In the examples below, we consider the mapping from x to y, and focus
on the 2-D case (for which some of the matrices are simpler to describe).

Scaling. Scaling is the mapping y = ax, where a is a scalar. This can be expressed
as y = Ax with A = aI. This mapping stretches a vector by the factor |a| (or
shrinks it when |a| < 1), and it ﬂips the vector (reverses its direction) if a < 0.
Dilation. Dilation is the mapping y = Dx, where D is a diagonal matrix, D =
diag(d1, d2). This mapping stretches the vector x by diﬀerent factors along the
two diﬀerent axes. (Or shrinks, if |di| < 1, and ﬂips, if di < 0.)
Rotation. Suppose that y is the vector obtained by rotating x by θ radians coun-
terclockwise. Then we have

(7.1)

y =� cos θ − sin θ

cos θ � x.

sin θ

This matrix is called (for obvious reasons) a rotation matrix.

Reﬂection. Suppose that y is the vector obtained by reﬂecting x through the line
that passes through the origin, inclined θ radians with respect to horizontal. Then
we have

y =� cos(2θ)

sin(2θ) − cos(2θ) � x.

sin(2θ)

130

7 Matrix examples

x

Ax

Ax

x

Ax

x

Figure 7.1 From left to right: A dilation with A = diag(2, 2/3), a coun-
terclockwise rotation by π/6 radians, and a reﬂection through a line that
makes an angle of π/4 radians with the horizontal line.

Projection onto a line. The projection of the point x onto a set is the point in
the set that is closest to x. Suppose y is the projection of x onto the line that
passes through the origin, inclined θ radians with respect to horizontal. Then we
have

y =� (1/2)(1 + cos(2θ))

(1/2) sin(2θ)

(1/2) sin(2θ)

(1/2)(1 − cos(2θ)) � x.

Some of these geometric transformations are illustrated in ﬁgure 7.1.

Finding the matrix. When a geometric transformation is represented by matrix-
vector multiplication (as in the examples above), a simple method to ﬁnd the
matrix is to ﬁnd its columns. The ith column is the vector obtained by applying
the transformation to ei. As a simple example consider clockwise rotation by 90◦
in 2-D. Rotating the vector e1 = (1, 0) by 90◦ gives (0,−1); rotating e2 = (0, 1) by
90◦ gives (1, 0). So rotation by 90◦ is given by

y =�

0
−1

1

0 � x.

Change of coordinates.
In many applications multiple coordinate systems are
used to describe locations or positions in 2-D or 3-D. For example in aerospace
engineering we can describe a position using earth-ﬁxed coordinates or body-ﬁxed
coordinates, where the body refers to an aircraft. Earth-ﬁxed coordinates are with
respect to a speciﬁc origin, with the three axes pointing East, North, and straight
up, respectively. The origin of the body-ﬁxed coordinates is a speciﬁc location on
the aircraft (typically the center of gravity), and the three axes point forward (along
the aircraft body), left (with respect to the aircraft body), and up (with respect
to the aircraft body). Suppose the 3-vector xbody describes a location using the
body coordinates, and xearth describes the same location in earth-ﬁxed coordinates.
These are related by

xearth = p + Qxbody,

where p is the location of the airplane center (in earth-ﬁxed coordinates) and Q is
a 3 × 3 matrix. The ith column of Q gives the earth-ﬁxed coordinates for the ith

7.2 Selectors

131

axis of the airplane. For an airplane in level ﬂight, heading due South, we have

Q =

0
−1
0

1
0
0

0
0

1  .

7.2 Selectors

An m× n selector matrix A is one in which each row is a unit vector (transposed):

A =

eT
k1
...
eT
km

 ,

where k1, . . . , km are integers in the range 1, . . . , n. When it multiplies a vector, it
simply copies the kith entry of x into the ith entry of y = Ax:

In words, each entry of Ax is a selection of an entry of x.

y = (xk1 , xk2 , . . . , xkm).

The identity matrix, and the reverser matrix
···
···
. . .
···
···

A =

 =

eT
n
...
eT
1

0
0
...
0
1

0
0
...
1
0



0
1
...
0
0

1
0
...
0
0



are special cases of selector matrices. (The reverser matrix reverses the order of
the entries of a vector: Ax = (xn, xn−1, . . . , x2, x1).) Another one is the r : s slicing
matrix, which can be described as the block matrix

A =� 0m×(r−1)

Im×m 0m×(n−s) � ,

where m = s − r + 1. (We show the dimensions of the blocks for clarity.) We have
Ax = xr:s, i.e., multiplying by A gives the r : s slice of a vector.

Down-sampling. Another example is the (n/2) × n matrix (with n even)

A =



1
0
0
...
0
0

0
0
0
...
0
0

0
1
0
...
0
0

0
0
0
...
0
0

0
0
1
...
0
0

0
0
0
...
0
0

···
···
···

···
···

0
0
0
...
1
0

0
0
0
...
0
0

0
0
0
...
0
1

0
0
0
...
0
0

.



If y = Ax, we have y = (x1, x3, x5, . . . , xn−3, xn−1). When x is a time series, y is
called the 2× down-sampled version of x. If x is a quantity sampled every hour,
then y is the same quantity, sampled every 2 hours.

132

7 Matrix examples

1

2

1

3

2

4

5

3

4

Figure 7.2 Directed graph with four vertices and ﬁve edges.

Image cropping. As a more interesting example, suppose that x is an image with
M × N pixels, with M and N even. (That is, x is an M N -vector, with its entries
giving the pixel values in some speciﬁc order.) Let y be the (M/2) × (N/2) image
that is the upper left corner of the image x, i.e., a cropped version. Then we have
y = Ax, where A is an (M N/4) × (M N ) selector matrix. The ith row of A is eT
,
where ki is the index of the pixel in x that corresponds to the ith pixel in y.

ki

Permutation matrices. An n×n permutation matrix is one in which each column
is a unit vector, and each row is the transpose of a unit vector. (In other words, A
and AT are both selector matrices.) Thus, exactly one entry of each row is one, and
exactly one entry of each column is one. This means that y = Ax can be expressed
as yi = xπi , where π is a permutation of 1, 2, . . . , n, i.e., each integer from 1 to n
appears exactly once in π1, . . . , πn.

As a simple example consider the permutation π = (3, 1, 2). The associated

permutation matrix is

A =

0
1
0

0
0
1

1
0

0  .

Multiplying a 3-vector by A re-orders its entries: Ax = (x3, x1, x2).

7.3

Incidence matrix

Directed graph. A directed graph consists of a set of vertices (or nodes), labeled
1, . . . , n, and a set of directed edges (or branches), labeled 1, . . . , m. Each edge is
connected from one of the nodes and into another one, in which case we say the
two nodes are connected or adjacent. Directed graphs are often drawn with the
vertices as circles or dots, and the edges as arrows, as in ﬁgure 7.2. A directed

7.3

Incidence matrix

133

graph can be described by its n × m incidence matrix, deﬁned as

1
−1
0

edge j points to node i
edge j points from node i
otherwise.

The incidence matrix is evidently sparse, since it has only two nonzero entries
in each column (one with value 1 and other with value −1). The jth column is
associated with the jth edge; the indices of its two nonzero entries give the nodes
that the edge connects. The ith row of A corresponds to node i; its nonzero entries
tell us which edges connect to the node, and whether they point into or away from
the node. The incidence matrix for the graph shown in ﬁgure 7.2 is

Aij =

A =

−1 −1
0
0
0 −1
1
0
1 −1 −1
0
0
1
0
1
0

1
0

0

 .

A directed graph can also be described by its adjacency matrix, described on
page 112. The adjacency and incidence matrices for a directed graph are closely
related, but not the same. The adjacency matrix does not explicitly label the
edges j = 1, . . . , m. There are also some small diﬀerences in the graphs that can
be represented using incidence and adjacency matrices. For example, self edges
(that connect from and to the same vertex) cannot be represented in an incidence
matrix.

7.3.1 Networks

In many applications a graph is used to represent a network, through which some
commodity or quantity such as electricity, water, heat, or vehicular traﬃc ﬂows.
The edges of the graph represent the paths or links over which the quantity can
move or ﬂow, in either direction. If x is an m-vector representing a ﬂow in the
network, we interpret xj as the ﬂow (rate) along the edge j, with a positive value
meaning the ﬂow is in the direction of edge j, and negative meaning the ﬂow is
in the opposite direction of edge j. In a network, the direction of the edge or link
does not specify the direction of ﬂow; it only speciﬁes which direction of ﬂow we
consider to be positive.

Flow conservation. When x represents a ﬂow in a network, the matrix-vector
product y = Ax can be given a very simple interpretation. The n-vector y = Ax
can be interpreted as the vector of net ﬂows, from the edges, into the nodes: yi is
equal to the total of the ﬂows that come in to node i, minus the total of the ﬂows
that go out from node i. The quantity yi is sometimes called the ﬂow surplus at
node i.

If Ax = 0, we say that ﬂow conservation occurs, since at each node, the total in-
ﬂow matches the total out-ﬂow. In this case the ﬂow vector x is called a circulation.
This could be used as a model of traﬃc ﬂow (in a closed system), with the nodes

134

7 Matrix examples

s2

s1

x1

2

1

x3

x4

x5

x2

3

4

s3

s4

Figure 7.3 Network with four nodes and ﬁve edges, with source ﬂows shown.

representing intersections and the edges representing road segments (one for each
direction).

For a network described by the directed graph example above, the vector

x = (1,−1, 1, 0, 1)

is a circulation, since Ax = 0. This ﬂow corresponds to a unit clockwise ﬂow on
the outer edges (1, 3, 5, and 2) and no ﬂow on the diagonal edge (4). (Visualizing
this explains why such vectors are called circulations.)

Sources.
In many applications it is useful to include additional ﬂows called source
ﬂows or exogenous ﬂows, that enter or leave the network at the nodes, but not along
the edges, as shown in ﬁgure 7.3. We denote these ﬂows with an n-vector s. We can
think of si as a ﬂow that enters the network at node i from outside, i.e., not from
any edge. When si > 0 the exogenous ﬂow is called a source, since it is injecting
the quantity into the network at the node. When si < 0 the exogenous ﬂow is
called a sink, since it is removing the quantity from the network at the node.

Flow conservation with sources. The equation Ax + s = 0 means that the ﬂow
is conserved at each node, counting the source ﬂow: The total of all incoming ﬂow,
from the incoming edges and exogenous source, minus the total outgoing ﬂow from
outgoing edges and exogenous sinks, is zero.

As an example, ﬂow conservation with sources can be used as an approximate
model of a power grid (ignoring losses), with x being the vector of power ﬂows
along the transmission lines, si > 0 representing a generator injecting power into
the grid at node i, si < 0 representing a load that consumes power at node i, and
si = 0 representing a substation where power is exchanged among transmission
lines, with no generation or load attached.

For the example above, consider the source vector s = (1, 0,−1, 0), which cor-
responds to an injection of one unit of ﬂow into node 1, and the removal of one
unit of ﬂow at node 3. In other words, node 1 is a source, node 3 is a sink, and

7.3

Incidence matrix

135

ﬂow is conserved at nodes 2 and 4. For this source, the ﬂow vector

x = (0.6, 0.3, 0.6, −0.1, −0.3)

satisﬁes ﬂow conservation, i.e., Ax + s = 0. This ﬂow can be explained in words:
The unit external ﬂow entering node 1 splits three ways, with 0.6 ﬂowing up,
0.3 ﬂowing right, and 0.1 ﬂowing diagonally up (on edge 4). The upward ﬂow on
edge 1 passes through node 2, where ﬂow is conserved, and proceeds right on edge 3
towards node 3. The rightward ﬂow on edge 2 passes through node 4, where ﬂow
is conserved, and proceeds up on edge 5 to node 3. The one unit of excess ﬂow
arriving at node 3 is removed as external ﬂow.

Node potentials. A graph is also useful when we focus on the values of some
quantity at each graph vertex or node. Let v be an n-vector, often interpreted as a
potential, with vi the potential value at node i. We can give a simple interpretation
to the matrix-vector product u = AT v. The m-vector u = AT v gives the potential
diﬀerences across the edges: uj = vl − vk, where edge j goes from node k to node l.

Dirichlet energy. When the m-vector AT v is small, it means that the potential
diﬀerences across the edges are small. Another way to say this is that the potentials
of connected vertices are near each other. A quantitative measure of this is the
function of v given by

D(v) = �AT v�2.

This function arises in many applications, and is called the Dirichlet energy (or
Laplacian quadratic form) associated with the graph. It can be expressed as

D(v) = �edges (k,l)

(vl − vk)2,

which is the sum of the squares of the potential diﬀerences of v across all edges in
the graph. The Dirichlet energy is small when the potential diﬀerences across the
edges of the graph are small, i.e., nodes that are connected by edges have similar
potential values.

The Dirichlet energy is used as a measure the non-smoothness (roughness) of
a set of node potentials on a graph. A set of node potentials with small Dirichlet
energy can be thought of as smoothly varying across the graph. Conversely, a set
of potentials with large Dirichlet energy can be thought of as non-smooth or rough.
The Dirichlet energy will arise as a measure of roughness in several applications
we will encounter later.

As a simple example, consider the potential vector v = (1,−1, 2,−1) for the
graph shown in ﬁgure 7.2. For this set of potentials, the potential diﬀerences
across the edges are relatively large, with AT v = (−2,−2, 3,−1,−3), and the
associated Dirichlet energy is �AT v�2 = 27. Now consider the potential vector v =
(1, 2, 2, 1). The associated edge potential diﬀerences are AT v = (1, 0, 0,−1,−1),
and the Dirichlet energy has the much smaller value �AT v�2 = 3.

136

7 Matrix examples

1

1

2

2

3

3

n − 1

n

Figure 7.4 Chain graph.

k
a

3

2

1

0

k
b

3

2

1

0

0

20

40

60

80

100

0

20

40

60

80

100

k

k

Figure 7.5 Two vectors of length 100, with Dirichlet energy D(a) = 1.14 and
D(b) = 8.99.

Chain graph. The incidence matrix and the Dirichlet energy function have a
particularly simple form for the chain graph shown in ﬁgure 7.4, with n vertices
and n− 1 edges. The n× (n− 1) incidence matrix is the transpose of the diﬀerence
matrix D described on page 119, in (6.5). The Dirichlet energy is then

D(v) = �Dv�2 = (v2 − v1)2 + ··· + (vn − vn−1)2,

the sum of squares of the diﬀerences between consecutive entries of the n-vector v.
This is used as a measure of the non-smoothness of the vector v, considered as a
time series. Figure 7.5 shows an example.

7.4 Convolution

The convolution of an n-vector a and an m-vector b is the (n + m − 1)-vector
denoted c = a ∗ b, with entries

ck = �i+j=k+1

aibj,

k = 1, . . . , n + m − 1,

(7.2)

where the subscript in the sum means that we should sum over all values of i and
j in their index ranges 1, . . . , n and 1, . . . , m, for which the sum i + j is k + 1. For

7.4 Convolution

137

example, with n = 4, m = 3, we have

c1 = a1b1
c2 = a1b2 + a2b1
c3 = a1b3 + a2b2 + a3b1
c4 = a2b3 + a3b2 + a4b1
c5 = a3b3 + a4b2
c6 = a4b3.

Convolution reduces to ordinary multiplication of numbers when n = m = 1, and
to scalar-vector multiplication when either n = 1 or m = 1. Convolution arises in
many applications and contexts.

As a speciﬁc numerical example, we have (1, 0,−1)∗(2, 1,−1) = (2, 1,−3,−1, 1),

where the entries of the convolution result are found from

2 = (1)(2)

1 = (1)(1) + (0)(2)
−3 = (1)(−1) + (0)(1) + (−1)(2)
−1 = (0)(−1) + (−1)(1)
1 = (−1)(−1).

Polynomial multiplication.

If a and b represent the coeﬃcients of two polynomials

p(x) = a1 + a2x + ··· + anxn−1,

q(x) = b1 + b2x + ··· + bmxm−1,

then the coeﬃcients of the product polynomial p(x)q(x) are represented by c = a∗b:

p(x)q(x) = c1 + c2x + ··· + cn+m−1xn+m−2.

To see this we will show that ck is the coeﬃcient of xk−1 in p(x)q(x). We expand the
product polynomial into mn terms, and collect those terms associated with xk−1.
These terms have the form aibjxi+j−2, for i and j that satisfy i + j − 2 = k − 1,
It follows that ck = �i+j=k+1 aibj, which agrees with the
i.e., i + j = k − 1.
convolution formula (7.2).

Properties of convolution. Convolution is symmetric: We have a ∗ b = b ∗ a. It
is also associative: We have (a ∗ b) ∗ c = a ∗ (b ∗ c), so we can write both as a ∗ b∗ c.
Another property is that a ∗ b = 0 implies that either a = 0 or b = 0. These
properties follow from the polynomial coeﬃcient property above, and can also be
directly shown. As an example, let us show that a ∗ b = b ∗ a. Suppose p is the
polynomial with coeﬃcients a, and q is the polynomial with coeﬃcients b. The two
polynomials p(x)q(x) and q(x)p(x) are the same (since multiplication of numbers
is commutative), so they have the same coeﬃcients. The coeﬃcients of p(x)q(x)
are a ∗ b and the coeﬃcients of q(x)p(x) are b ∗ a. These must be the same.
A basic property is that for ﬁxed a, the convolution a ∗ b is a linear function
of b; and for ﬁxed b, it is a linear function of a. This means we can express a ∗ b as
a matrix-vector product:

a ∗ b = T (b)a = T (a)b,

T (b) =

,

T (a) =

b1
b2
b3
0
0
0

0
b1
b2
b3
0
0

0
0
b1
b2
b3
0

0
0
0
b1
b2
b3





(7.3)

.

a1
0
0
a2 a1
0
a3 a2 a1
a4 a3 a2
a4 a3
0
0
0
a4





138

7 Matrix examples

where T (b) is the (n + m − 1) × n matrix with entries

T (b)ij =� bi−j+1

0

1 ≤ i − j + 1 ≤ m
otherwise

and similarly for T (a). For example, with n = 4 and m = 3, we have

The matrices T (b) and T (a) are called Toeplitz matrices (named after the math-
ematician Otto Toeplitz), which means the entries on any diagonal (i.e., indices
with i − j constant) are the same. The columns of the Toeplitz matrix T (a) are
simply shifted versions of the vector a, padded with zero entries.

Variations. Several slightly diﬀerent deﬁnitions of convolution are used in diﬀerent
applications. In one variation, a and b are inﬁnite two-sided sequences (and not
vectors) with indices ranging from −∞ to ∞. In another variation, the rows of
T (a) at the top and bottom that do not contain all the coeﬃcients of a are dropped.
(In this version, the rows of T (a) are shifted versions of the vector a, reversed.)
For consistency, we will use the one deﬁnition (7.2).

Examples.

• Time series smoothing. Suppose the n-vector x is a time series, and a =
(1/3, 1/3, 1/3). Then the (n + 2)-vector y = a ∗ x can be interpreted as a
smoothed version of the original time series: for i = 3, . . . , n, yi is the average
of xi, xi−1, xi−2. The time series y is called the (3-period) moving average
of the time series x. Figure 7.6 shows an example.

• First order diﬀerences. If the n-vector x is a time series and a = (1,−1), the

time series y = a ∗ x gives the ﬁrst order diﬀerences in the series x:

y = (x1, x2 − x1, x3 − x2, . . . , xn − xn−1, −xn).

(The ﬁrst and last entries here would be the ﬁrst order diﬀerence if we take
x0 = xn+1 = 0.)

• Audio ﬁltering. If the n-vector x is an audio signal, and a is a vector (typically
with length less than around 0.1 second of real time) the vector y = a ∗ x
is called the ﬁltered audio signal, with ﬁlter coeﬃcients a. Depending on
the coeﬃcients a, y will be perceived as enhancing or suppressing diﬀerent
frequencies, like the familiar audio tone controls.

• Communication channel. In a modern data communication system, a time
series u is transmitted or sent over some channel (e.g., electrical, optical, or
radio) to a receiver, which receives the time series y. A very common model
is that y and u are related via convolution: y = c ∗ u, where the vector c is
the channel impulse response.

7.4 Convolution

139

3

2.5

2

k
x

1.5

1

0.5

0

3

2.5

2

k
)
x
∗
a
(

1.5

1

0.5

0

0

20

40

60

80

100

k

0

20

40

60

80

100

k

Figure 7.6 Top. A time series represented by a vector x of length 100.
Bottom. The 3-period moving average of the time series as a vector of
length 102. This vector is the convolution of x with a = (1/3, 1/3, 1/3).

140

7 Matrix examples

Input-output convolution system. Many physical systems with an input (time
series) m-vector u and output (time series) y are well modeled as y = h∗u, where the
n-vector h is called the system impulse response. For example, ut might represent
the power level of a heater at time period t, and yt might represent the resulting
temperature rise (above the surrounding temperature). The lengths of u and y, n
and m + n − 1, are typically large, and not relevant in these applications. We can
express the ith entry of the output y as

yi =

n�j=1

ui−j+1hj,

where we interpret uk as zero for k < 0 or k > n. This formula states that y at
time i is a linear combination of ui, ui−1, . . . , ui−n+1, i.e., a linear combination
of the current input value ui, and the past n − 1 input values ui−1, . . . , ui−n+1.
The coeﬃcients are precisely h1, . . . , hn. Thus, h3 can be interpreted as the factor
by which the current output depends on what the input was, 2 time steps before.
Alternatively, we can say that h3 is the factor by which the input at any time will
aﬀect the output 2 steps in the future.

Complexity of convolution. The na¨ıve method to compute the convolution c =
a ∗ b of an n-vector a and an m-vector b, using the basic formula (7.2) to calculate
each ck, requires around 2mn ﬂops. The same number of ﬂops is required to
compute the matrix-vector products T (a)b or T (b)a, taking into account the zeros
at the top right and bottom left in the Toeplitz matrices T (b) and T (a). Forming
these matrices requires us to store mn numbers, even though the original data
contains only m + n numbers.

It turns out that the convolution of two vectors can be computed far faster, using
a so-called fast convolution algorithm. By exploiting the special structure of the
convolution equations, this algorithm can compute the convolution of an n-vector
and an m-vector in around 5(m + n) log2(m + n) ﬂops, and with no additional
memory requirement beyond the original m + n numbers. The fast convolution
algorithm is based on the fast Fourier transform (FFT), which is beyond the scope
of this book. (The Fourier transform is named for the mathematician Jean-Baptiste
Fourier.)

7.4.1

2-D convolution

Convolution has a natural extension to multiple dimensions. Suppose that A is an
m×n matrix and B is a p×q matrix. Their convolution is the (m+p−1)×(n+q−1)
matrix

Crs =

�i+k=r+1, j+l=s+1

AijBkl,

r = 1, . . . , m + p − 1,

s = 1, . . . , n + q − 1,

where the indices are restricted to their ranges (or alternatively, we assume that
Aij and Bkl are zero, when the indices are out of range). This is not denoted

7.4 Convolution

141

C = A ∗ B, however, in standard mathematical notation. So we will use the
notation C = A � B.
The same properties that we observed for 1-D convolution hold for 2-D convo-
lution: We have A � B = B � A, (A � B) � C = A � (B � C), and for ﬁxed B, A � B
is a linear function of A.

Image blurring.
If the m×n matrix X represents an image, Y = X �B represents
the eﬀect of blurring the image by the point spread function (PSF) given by the
entries of the matrix B. If we represent X and Y as vectors, we have y = T (B)x,
for some (m + p − 1)(n + q − 1) × mn-matrix T (B).

As an example, with

B =� 1/4

1/4

1/4

1/4 � ,

(7.4)

Y = X � B is an image where each pixel value is the average of a 2 × 2 block of 4
adjacent pixels in X. The image Y would be perceived as the image X, with some
blurring of the ﬁne details. This is illustrated in ﬁgure 7.7 for the 8 × 9 matrix

X =



1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1

1
1
0
1
1
1
1
1

1
1
0
0
0
0
1
1

1
1
0
1
1
1
1
1

1
1
0
1
1
1
1
1

1
1
0
0
0
0
1
1

1
1
1
1
1
1
1
1

1
1
1
1
1
1
1
1



(7.5)

1/4
1/2
1/2
1/2
1/2
1/2
1/2
1/2
1/4

.



and its convolution with B,

X � B =



1/4
1/2
1/2
1/2
1/2
1/2
1/2
1/2
1/4

1/2
1
1
1
1
1
1
1
1/2

1/2
1

3/4
3/4

1
1
1
1
1/2

1/2
1

1/2
1/4
1/2
1/2
3/4

1

1/2
1

1/2
1/4
1/2
1/2
3/4

1

1/2
1

1/2
1/2

1
1
1
1

1/2

1/2

1/2

1/2
1
1/2
1/4
1/2
1/2
3/4
1
1/2

1/2
1
3/4
1/2
1/2
1/2
3/4
1
1/2

1/2
1
1
1
1
1
1
1
1/2

With the point spread function

the pixel values in the image Y = X � Dhor are the horizontal ﬁrst order diﬀerences
of those in X:

Dhor =� 1 −1 � ,

Yij = Xij − Xi,j−1,

i = 1, . . . , m,

j = 2, . . . , n

(and Yi1 = Xi1, Xi,n+1 = −Xin for i = 1, . . . , m). With the point spread function

Dver =�

1

−1 � ,

142

7 Matrix examples

Figure 7.7 An 8 × 9 image and its convolution with the point spread func-
tion (7.4).

the pixel values in the image Y = X � Dver are the vertical ﬁrst order diﬀerences
of those in X:

Yij = Xij − Xi−1,j,

i = 2, . . . , m,

j = 1, . . . , n

(and Y1j = X1j, Xm+1,j = −Xmj for j = 1, . . . , n). As an example, the convolu-
tions of the matrix (7.5) with Dhor and Dver are

1
1
1
1
1
1
1
1



0
0
0
0
0 −1
0
0
0
0
0

0
0
0
0 −1
0 −1
0 −1
0
0
0
0

0
0
0
1
1
1
0
0

0
0
0
0
0
0
0 −1
0 −1
0 −1
0
0
0
0

0
0
1
1
1
1
0
0

0 −1
0 −1
0 −1
0 −1
0 −1
0 −1
0 −1
0 −1



1
0

1
0
0
0
0

1
1
0
0
0 −1 −1 −1 −1 −1
0
0
0
0
0
0
1
0
0
0

1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
−1 −1 −1 −1 −1 −1 −1 −1 −1

1
0
0
0
0
0
0
0

1
0

0
0
0
1
0

1
0

1
0
0
0
0

1
0

1
0
0
0
0

.



Figure 7.8 shows the eﬀect of convolution on a larger image. The ﬁgure shows
an image of size 512×512 and its convolution with the 8×8 matrix B with constant
entries Bij = 1/64.

X � Dhor =

and

X � Dver =



7.4 Convolution

143

Figure 7.8 512 × 512 image and the 519 × 519 image that results from the
convolution of the ﬁrst image with an 8 × 8 matrix with constant entries
1/64. Image credit: NASA.

144

7 Matrix examples

Exercises

7.1 Projection on a line. Let P (x) denote the projection of the 2-D point (2-vector) x onto
the line that passes through (0, 0) and (1, 3). (This means that P (x) is the point on the
line that is closest to x; see exercise 3.12.) Show that P is a linear function, and give the
matrix A for which P (x) = Ax for any x.

7.2 3-D rotation. Let x and y be 3-vectors representing positions in 3-D. Suppose that the
vector y is obtained by rotating the vector x about the vertical axis (i.e., e3) by 45◦
(counterclockwise, i.e., from e1 toward e2). Find the 3 × 3 matrix A for which y = Ax.
Hint. Determine the three columns of A by ﬁnding the result of the transformation on
the unit vectors e1, e2, e3.

7.3 Trimming a vector. Find a matrix A for which Ax = (x2, . . . , xn−1), where x is an

n-vector. (Be sure to specify the size of A, and describe all its entries.)

7.4 Down-sampling and up-conversion. We consider n-vectors x that represent signals, with
xk the value of the signal at time k for k = 1, . . . , n. Below we describe two functions of
x that produce new signals f (x). For each function, give a matrix A such that f (x) = Ax
for all x.

(a) 2× downsampling. We assume n is even and deﬁne f (x) as the n/2-vector y with

elements yk = x2k. To simplify your notation you can assume that n = 8, i.e.,

f (x) = (x2, x4, x6, x8).

(On page 131 we describe a diﬀerent type of down-sampling, that uses the average
of pairs of original values.)

(b) 2× up-conversion with linear interpolation. We deﬁne f (x) as the (2n − 1)-vector y
with elements yk = x(k+1)/2 if k is odd and yk = (xk/2 + xk/2+1)/2 if k is even. To
simplify your notation you can assume that n = 5, i.e.,

f (x) =�x1,

x1 + x2

2

, x2,

x2 + x3

2

, x3,

x3 + x4

2

, x4,

x4 + x5

2

, x5� .

7.5 Transpose of selector matrix. Suppose the m × n matrix A is a selector matrix. Describe

the relation between the m-vector u and the n-vector v = AT u.

7.6 Rows of incidence matrix. Show that the rows of the incidence matrix of a graph are

always linearly dependent. Hint. Consider the sum of the rows.

7.7 Incidence matrix of reversed graph. (See exercise 6.5.) Suppose A is the incidence matrix
of a graph. The reversed graph is obtained by reversing the directions of all the edges of
the original graph. What is the incidence matrix of the reversed graph? (Express your
answer in terms of A.)

7.8 Flow conservation with sources. Suppose that A is the incidence matrix of a graph, x is
the vector of edge ﬂows, and s is the external source vector, as described in §7.3. Assuming
that ﬂow is conserved, i.e., Ax + s = 0, show that 1T s = 0. This means that the total
amount injected into the network by the sources (si > 0) must exactly balance the total
amount removed from the network at the sink nodes (si < 0). For example if the network
is a (lossless) electrical power grid, the total amount of electrical power generated (and
injected into the grid) must exactly balance the total electrical power consumed (from
the grid).

7.9 Social network graph. Consider a group of n people or users, and some symmetric social
relation among them. This means that some pairs of users are connected, or friends (say).
We can create a directed graph by associating a node with each user, and an edge between
each pair of friends, arbitrarily choosing the direction of the edge. Now consider an n-
vector v, where vi is some quantity for user i, for example, age or education level (say,
given in years). Let D(v) denote the Dirichlet energy associated with the graph and v,
thought of as a potential on the nodes.

Exercises

145

Figure 7.9 Tree with six vertices.

(a) Explain why the number D(v) does not depend on the choice of directions for the

edges of the graph.

(b) Would you guess that D(v) is small or large? This is an open-ended, vague question;
there is no right answer. Just make a guess as to what you might expect, and give
a short English justiﬁcation of your guess.

7.10 Circle graph. A circle graph (also called a cycle graph) has n vertices, with edges pointing
from vertex 1 to vertex 2, from vertex 2 to vertex 3, . . . , from vertex n − 1 to vertex n,
and ﬁnally, from vertex n to vertex 1. (This last edge completes the circle.)

(a) Draw a diagram of a circle graph, and give its incidence matrix A.

(b) Suppose that x is a circulation for a circle graph. What can you say about x?

(c) Suppose the n-vector v is a potential on a circle graph. What is the Dirichlet energy

D(v) = �AT v�2?

Remark. The circle graph arises when an n-vector v represents a periodic time series. For
example, v1 could be the value of some quantity on Monday, v2 its value on Tuesday, and
v7 its value on Sunday. The Dirichlet energy is a measure of the roughness of such an
n-vector v.

7.11 Tree. An undirected graph is called a tree if it is connected (there is a path from every
vertex to every other vertex) and contains no cycles, i.e., there is no path that begins and
ends at the same vertex. Figure 7.9 shows a tree with six vertices. For the tree in the
ﬁgure, ﬁnd a numbering of the vertices and edges, and an orientation of the edges, so that
the incidence matrix A of the resulting directed graph satisﬁes Aii = 1 for i = 1, . . . , 5
and Aij = 0 for i < j. In other words, the ﬁrst 5 rows of A form a lower triangular matrix
with ones on the diagonal.

7.12 Some properties of convolution. Suppose that a is an n-vector.

(a) Convolution with 1. What is 1 ∗ a? (Here we interpret 1 as a 1-vector.)
(b) Convolution with a unit vector. What is ek ∗ a, where ek is the kth unit vector of
dimension q? Describe this vector mathematically (i.e., give its entries), and via a
brief English description. You might ﬁnd vector slice notation useful.

7.13 Sum property of convolution. Show that for any vectors a and b, we have 1T (a ∗ b) =
(1T a)(1T b). In words: The sum of the coeﬃcients of the convolution of two vectors is the
product of the sums of the coeﬃcients of the vectors. Hint. If the vector a represents the
coeﬃcients of a polynomial p, 1T a = p(1).

146

7 Matrix examples

7.14 Rainfall and river height. The T -vector r gives the daily rainfall in some region over a
period of T days. The vector h gives the daily height of a river in the region (above its
normal height). By careful modeling of water ﬂow, or by ﬁtting a model to past data, it
is found that these vectors are (approximately) related by convolution: h = g ∗ r, where

g = (0.1, 0.4, 0.5, 0.2).

Give a short story in English (with no mathematical terms) to approximately describe
this relation. For example, you might mention how many days after a one day heavy
rainfall the river height is most aﬀected. Or how many days it takes for the river height
to return to the normal height once the rain stops.

7.15 Channel equalization. We suppose that u1, . . . , um is a signal (time series) that is trans-
mitted (for example by radio). A receiver receives the signal y = c∗ u, where the n-vector
c is called the channel impulse response. (See page 138.) In most applications n is small,
e.g., under 10, and m is much larger. An equalizer is a k-vector h that satisﬁes h∗ c ≈ e1,
the ﬁrst unit vector of length n + k − 1. The receiver equalizes the received signal y by
convolving it with the equalizer to obtain z = h ∗ y.
(a) How are z (the equalized received signal) and u (the original transmitted signal)

related? Hint. Recall that h ∗ (c ∗ u) = (h ∗ c) ∗ u.

(b) Numerical example. Generate a signal u of length m = 50, with each entry a random
value that is either −1 or +1. Plot u and y = c ∗ u, with c = (1, 0.7, −0.3). Also
plot the equalized signal z = h ∗ y, with

h = (0.9, −0.5, 0.5, −0.4, 0.3, −0.3, 0.2, −0.1).

Chapter 8

Linear equations

In this chapter we consider vector-valued linear and aﬃne functions, and systems
of linear equations.

8.1 Linear and aﬃne functions

Vector-valued functions of vectors. The notation f : Rn → Rm means that f
is a function that maps real n-vectors to real m-vectors. The value of the function
f , evaluated at an n-vector x, is an m-vector f (x) = (f1(x), f2(x), . . . , fm(x)).
Each of the components fi of f is itself a scalar-valued function of x. As with
scalar-valued functions, we sometimes write f (x) = f (x1, x2, . . . , xn) to emphasize
that f is a function of n scalar arguments. We use the same notation for each of
the components of f , writing fi(x) = fi(x1, x2, . . . , xn) to emphasize that fi is a
function mapping the scalar arguments x1, . . . , xn into a scalar.

The matrix-vector product function. Suppose A is an m × n matrix. We can
deﬁne a function f : Rn → Rm by f (x) = Ax. The inner product function
f : Rn → R, deﬁned as f (x) = aT x, discussed in §2.1, is the special case with
m = 1.

Superposition and linearity. The function f : Rn → Rm, deﬁned by f (x) = Ax,
is linear, i.e., it satisﬁes the superposition property:

f (αx + βy) = αf (x) + βf (y)

(8.1)

holds for all n-vectors x and y and all scalars α and β. It is a good exercise to
parse this simple looking equation, since it involves overloading of notation. On
the left-hand side, the scalar-vector multiplications αx and βy involve n-vectors,
and the sum αx+βy is the sum of two n-vectors. The function f maps n-vectors to
m-vectors, so f (αx + βy) is an m-vector. On the right-hand side, the scalar-vector
multiplications and the sum are those for m-vectors. Finally, the equality sign is
equality between two m-vectors.

148

8 Linear equations

We can verify that superposition holds for f using properties of matrix-vector

and scalar-vector multiplication:

f (αx + βy) = A(αx + βy)

= A(αx) + A(βy)

= α(Ax) + β(Ay)
= αf (x) + βf (y)

Thus we can associate with every matrix A a linear function f (x) = Ax.

The converse is also true. Suppose f is a function that maps n-vectors to m-
vectors, and is linear, i.e., (8.1) holds for all n-vectors x and y and all scalars α
and β. Then there exists an m × n matrix A such that f (x) = Ax for all x. This
can be shown in the same way as for scalar-valued functions in §2.1, by showing
that if f is linear, then

f (x) = x1f (e1) + x2f (e2) + ··· + xnf (en),

(8.2)

where ek is the kth unit vector of size n. The right-hand side can also be written
as a matrix-vector product Ax, with

A =� f (e1)

f (e2)

···

f (en) � .

The expression (8.2) is the same as (2.3), but here f (x) and f (ek) are vectors. The
implications are exactly the same: A linear vector-valued function f is completely
characterized by evaluating f at the n unit vectors e1, . . . , en.

As in §2.1 it is easily shown that the matrix-vector representation of a linear
function is unique. If f : Rn → Rm is a linear function, then there exists exactly
one matrix A such that f (x) = Ax for all x.

Examples of linear functions.
In the examples below we deﬁne functions f that
map n-vectors x to n-vectors f (x). Each function is described in words, in terms of
its eﬀect on an arbitrary x. In each case we give the associated matrix multiplication
representation.

• Negation. f changes the sign of x: f (x) = −x.

Negation can be expressed as f (x) = Ax with A = −I.

• Reversal. f reverses the order of the elements of x: f (x) = (xn, xn−1, . . . , x1).

The reversal function can be expressed as f (x) = Ax with

A =

0
0
...
1

···
···
. . .
···

0
1
...
0

1
0
...
0

.



(This is the n × n identity matrix with the order of its columns reversed. It
is the reverser matrix introduced in §7.2.)

8.1 Linear and aﬃne functions

149

• Running sum. f forms the running sum of the elements in x:

f (x) = (x1, x1 + x2, x1 + x2 + x3, . . . , x1 + x2 + ··· + xn).

The running sum function can be expressed as f (x) = Ax with

A =



1
1
...
1
1

0
1
...
1
1

···
···
. . .
···
···

0
0
...
1
1

0
0
...
0
1

,



i.e., Aij = 1 if i ≥ j and Aij = 0 otherwise. This is the running sum matrix
deﬁned in (6.6).

• De-meaning. f subtracts the mean from each entry of a vector x: f (x) =

x − avg(x)1.
The de-meaning function can be expressed as f (x) = Ax with

A =

1 − 1/n −1/n
−1/n

...

−1/n

··· −1/n
1 − 1/n ··· −1/n
. . .
···

−1/n

1 − 1/n

...

...

.



Examples of functions that are not linear. Here we list some examples of func-
tions f that map n-vectors x to n-vectors f (x) that are not linear. In each case
we show a superposition counterexample.

• Absolute value. f replaces each element of x with its absolute value: f (x) =

(|x1|,|x2|, . . . ,|xn|).
The absolute value function is not linear. For example, with n = 1, x = 1,
y = 0, α = −1, β = 0, we have

f (αx + βy) = 1 �= αf (x) + βf (y) = −1,

so superposition does not hold.

• Sort. f sorts the elements of x in decreasing order.

The sort function is not linear (except when n = 1, in which case f (x) = x).
For example, if n = 2, x = (1, 0), y = (0, 1), α = β = 1, then

f (αx + βy) = (1, 1) �= αf (x) + βf (y) = (2, 0).

Aﬃne functions. A vector-valued function f : Rn → Rm is called aﬃne if it can
be expressed as f (x) = Ax + b, where A is an m × n matrix and b is an m-vector.
It can be shown that a function f : Rn → Rm is aﬃne if and only if

f (αx + βy) = αf (x) + βf (y)

150

8 Linear equations

holds for all n-vectors x, y, and all scalars α, β that satisfy α + β = 1. In other
words, superposition holds for aﬃne combinations of vectors. (For linear functions,
superposition holds for any linear combinations of vectors.)

The matrix A and the vector b in the representation of an aﬃne function as
f (x) = Ax + b are unique. These parameters can be obtained by evaluating f at
the vectors 0, e1, . . . , en, where ek is the kth unit vector in Rn. We have

A =� f (e1) − f (0)

f (e2) − f (0)

···

f (en) − f (0) � ,

b = f (0).

Just like aﬃne scalar-valued functions, aﬃne vector-valued functions are often

called linear, even though they are linear only when the vector b is zero.

8.2 Linear function models

Many functions or relations between variables that arise in natural science, engi-
neering, and social sciences can be approximated as linear or aﬃne functions. In
these cases we refer to the linear function relating the two sets of variables as a
model or an approximation, to remind us that the relation is only an approximation,
and not exact. We give a few examples here.

• Price elasticity of demand. Consider n goods or services with prices given by
the n-vector p, and demands for the goods given by the n-vector d. A change
in prices will induce a change in demands. We let δprice be the n-vector
that gives the fractional change in the prices, i.e., δprice
i − pi)/pi,
where pnew is the n-vector of new (changed) prices. We let δdem be the n-
vector that gives the fractional change in the product demands, i.e., δdem
i =
(dnew
i − di)/di, where dnew is the n-vector of new demands. A linear demand
elasticity model relates these vectors as δdem = Edδprice, where Ed is the n×n
demand elasticity matrix. For example, suppose Ed
21 = 0.2.
This means that a 1% increase in the price of the ﬁrst good, with other
prices kept the same, will cause demand for the ﬁrst good to drop by 0.4%,
and demand for the second good to increase by 0.2%. (In this example, the
second good is acting as a partial substitute for the ﬁrst good.)

11 = −0.4 and Ed

= (pnew

i

• Elastic deformation. Consider a steel structure like a bridge or the structural
frame of a building. Let f be an n-vector that gives the forces applied to
the structure at n speciﬁc places (and in n speciﬁc directions), sometimes
called a loading. The structure will deform slightly due to the loading. Let
d be an m-vector that gives the displacements (in speciﬁc directions) of m
points in the structure, due to the load, e.g., the amount of sag at a speciﬁc
point on a bridge. For small displacements, the relation between displacement
and loading is well approximated as linear: d = Cf , where C is the m × n
compliance matrix. The units of the entries of C are m/N.

8.2 Linear function models

151

8.2.1 Taylor approximation

Suppose f : Rn → Rm is diﬀerentiable, i.e., has partial derivatives, and z is an
n-vector. The ﬁrst-order Taylor approximation of f near z is given by

ˆf (x)i = fi(z) +

∂fi
∂x1

(z)(x1 − z1) + ··· +

∂fi
∂xn

(z)(xn − zn)

= fi(z) + ∇fi(z)T (x − z),

for i = 1, . . . , m. (This is just the ﬁrst-order Taylor approximation of each of the
scalar-valued functions fi, described in §2.2.) For x near z, ˆf (x) is a very good
approximation of f (x). We can express this approximation in compact notation,
using matrix-vector multiplication, as

ˆf (x) = f (z) + Df (z)(x − z),

(8.3)

where the m × n matrix Df (z) is the derivative or Jacobian matrix of f at z
(see §C.1). Its components are the partial derivatives of f ,

Df (z)ij =

∂fi
∂xj

(z),

i = 1, . . . , m,

j = 1, . . . , n,

evaluated at the point z. The rows of the Jacobian are ∇fi(z)T , for i = 1, . . . , m.
The Jacobian matrix is named for the mathematician Carl Gustav Jacob Jacobi.
As in the scalar-valued case, Taylor approximation is sometimes written with a
second argument as ˆf (x; z) to show the point z around which the approximation is
made. Evidently the Taylor series approximation ˆf is an aﬃne function of x. (It is
often called a linear approximation of f , even though it is not, in general, a linear
function.)

8.2.2 Regression model

Recall the regression model (2.7)

ˆy = xT β + v,

(8.4)

where the n-vector x is a feature vector for some object, β is an n-vector of weights,
v is a constant (the oﬀset), and ˆy is the (scalar) value of the regression model
prediction.

Now suppose we have a set of N objects (also called samples or examples), with
feature vectors x(1), . . . , x(N ). The regression model predictions associated with the
examples are given by

ˆy(i) = (x(i))T β + v,

i = 1, . . . , N.

These numbers usually correspond to predictions of the value of the outputs or
responses. If in addition to the example feature vectors x(i) we are also given the

152

8 Linear equations

actual value of the associated response variables, y(1), . . . , y(N ), then our prediction
errors or residuals are

r(i) = y(i) − ˆy(i),

i = 1, . . . , N.

(Some authors deﬁne the prediction errors as ˆy(i) − y(i).)
We can express this using compact matrix-vector notation. We form the n× N
feature matrix X with columns x(1), . . . , x(N ). We let yd denote the N -vector whose
entries are the actual values of the response for the N examples. (The superscript
‘d’ stands for ‘data’.) We let ˆyd denote the N -vector of regression model predictions
for the N examples, and we let rd denote the N -vector of residuals or prediction
errors. We can then express the regression model predictions for this data set in
matrix-vector form as

ˆyd = X T β + v1.

The vector of N prediction errors for the examples is given by

rd = yd − ˆyd = yd − X T β − v1.

We can include the oﬀset v in the regression model by including an additional

feature equal to one as the ﬁrst entry of each feature vector:

ˆyd =� 1T

X �T� v

β � = ˜X T ˜β,

where ˜X is the new feature matrix, with a new ﬁrst row of ones, and ˜β = (v, β) is
the vector of regression model parameters. This is often written without the tildes,
as ˆyd = X T β, by simply including the feature one as the ﬁrst feature.

The equation above shows that the N -vector of predictions for the N examples
is a linear function of the model parameters (v, β). The N -vector of prediction
errors is an aﬃne function of the model parameters.

8.3 Systems of linear equations

Consider a set (also called a system) of m linear equations in n variables or un-
knowns x1, . . . , xn:

A11x1 + A12x2 + ··· + A1nxn = b1
A21x1 + A22x2 + ··· + A2nxn = b2

...

Am1x1 + Am2x2 + ··· + Amnxn = bm.

The numbers Aij are called the coeﬃcients in the linear equations, and the numbers
bi are called the right-hand sides (since by tradition, they appear on the right-hand

8.3 Systems of linear equations

153

side of the equation). These equations can be written succinctly in matrix notation
as

Ax = b.

(8.5)

In this context, the m × n matrix A is called the coeﬃcient matrix, and the m-
vector b is called the right-hand side. An n-vector x is called a solution of the
linear equations if Ax = b holds. A set of linear equations can have no solutions,
one solution, or multiple solutions.

Examples.

• The set of linear equations

x1 + x2 = 1,

x1 = −1,

x1 − x2 = 0

is written as Ax = b with

A =

It has no solutions.

• The set of linear equations

1
1

1
0

1 −1  ,

b =

1
−1

0  .

x1 + x2 = 1,

x2 + x3 = 2

is written as Ax = b with

A =� 1

0

1
1

0

1 � ,

b =� 1
2 � .

It has multiple solutions, including x = (1, 0, 2) and x = (0, 1, 1).

Over-determined and under-determined systems of linear equations. The set
of linear equations is called over-determined if m > n, under-determined if m < n,
and square if m = n; these correspond to the coeﬃcient matrix being tall, wide,
and square, respectively. When the system of linear equations is over-determined,
there are more equations than variables or unknowns. When the system of linear
equations is under-determined, there are more unknowns than equations. When
the system of linear equations is square, the numbers of unknowns and equations
is the same. A set of equations with zero right-hand side, Ax = 0, is called a
homogeneous set of equations. Any homogeneous set of equations has x = 0 as a
solution.

In chapter 11 we will address the question of how to determine if a system of
linear equations has a solution, and how to ﬁnd one when it does. For now, we
give a few interesting examples.

154

8.3.1 Examples

8 Linear equations

Coeﬃcients of linear combinations. Let a1, . . . , an denote the columns of A. The
system of linear equations Ax = b can be expressed as

x1a1 + ··· + xnan = b,

i.e., b is a linear combination of a1, . . . , an with coeﬃcients x1, . . . , xn. So solving
Ax = b is the same as ﬁnding coeﬃcients that express b as a linear combination of
the vectors a1, . . . , an.

Polynomial interpolation. We seek a polynomial p of degree at most n − 1 that
interpolates a set of m given points (ti, yi), i = 1, . . . , m. (This means that p(ti) =
yi.) We can express this as a set of m linear equations in the n unknowns c, where
c is the n-vector of coeﬃcients: Ac = y. Here the matrix A is the Vandermonde
matrix (6.7), and the vector c is the vector of polynomial coeﬃcients, as described
in the example on page 120.

Balancing chemical reactions. A chemical reaction involves p reactants (molecules)
and q products, and can be written as

a1R1 + ··· + apRp −→ b1P1 + ··· + bqPq.

Here R1, . . . , Rp are the reactants, P1, . . . , Pq are the products, and the numbers
a1, . . . , ap and b1, . . . , bq are positive numbers that tell us how many of each of
these molecules is involved in the reaction. They are typically integers, but can be
scaled arbitrarily; we could double all of these numbers, for example, and we still
have the same reaction. As a simple example, we have the electrolysis of water,

2H2O −→ 2H2 + O2,

which has one reactant, water (H2O), and two products, molecular hydrogen (H2)
and molecular oxygen (O2). The coeﬃcients tell us that 2 water molecules create
2 hydrogen molecules and 1 oxygen molecule. The coeﬃcients in a reaction can
be multiplied by any nonzero numbers; for example, we could write the reaction
above as 3H2O −→ 3H2 + (3/2)O2. By convention reactions are written with all
coeﬃcients integers, with least common divisor one.
In a chemical reaction the numbers of constituent atoms must balance. This
means that for each atom appearing in any of the reactants or products, the total
amount on the left-hand side must equal the total amount on the right-hand side.
(If any of the reactants or products is charged, i.e., an ion, then the total charge
must also balance.) In the simple water electrolysis reaction above, for example,
we have 4 hydrogen atoms on the left (2 water molecules, each with 2 hydrogen
atoms), and 4 on the right (2 hydrogen molecules, each with 2 hydrogen atoms).
The oxygen atoms also balance, so this reaction is balanced.

Balancing a chemical reaction with speciﬁed reactants and products, i.e., ﬁnd-
ing the numbers a1, . . . , ap and b1, . . . , bq, can be expressed as a system of linear
equations. We can express the requirement that the reaction balances as a set of

8.3 Systems of linear equations

155

m equations, where m is the number of diﬀerent atoms appearing in the chemical
reaction. We deﬁne the m × p matrix R by

Rij = number of atoms of type i in Rj,

i = 1, . . . , m,

j = 1, . . . , p.

(The entries of R are nonnegative integers.) The matrix R is interesting; for ex-
ample, its jth column gives the chemical formula for reactant Rj. We let a denote
the p-vector with entries a1, . . . , ap. Then, the m-vector Ra gives the total number
of atoms of each type appearing in the reactants. We deﬁne an m × q matrix P
in a similar way, so the m-vector P b gives the total number of atoms of each type
that appears in the products.

We write the balance condition using vectors and matrices as Ra = P b. We

can express this as

� R −P �� a

b � = 0,

which is a set of m homogeneous linear equations.

A simple solution of these equations is a = 0, b = 0. But we seek a nonzero
solution. We can set one of the coeﬃcients, say a1, to be one. (This might cause
the other quantities to be fractional-valued.) We can add the condition that a1 = 1
to our system of linear equations as

� R −P

0 �� a

b � = em+1.

eT
1

Finally, we have a set of m + 1 equations in p + q variables that expresses the
requirement that the chemical reaction balances. Finding a solution of this set of
equations is called balancing the chemical reaction.

For the example of electrolysis of water described above, we have p = 1 reac-
tant (water) and q = 2 products (molecular hydrogen and oxygen). The reaction
involves m = 2 atoms, hydrogen and oxygen. The reactant and product matrices
are

The balancing equations are then

0

0

R =� 2
1 � ,
P =� 2
2 � .
0 
 =
1  .

2 −2
0
0 −2
1
0
1

a1
b1
b2

0
0



These equations are easily solved, and have the solution (1, 1, 1/2). (Multiplying
these coeﬃcients by 2 gives the reaction given above.)

Diﬀusion systems. A diﬀusion system is a common model that arises in many
areas of physics to describe ﬂows and potentials. We start with a directed graph
with n nodes and m edges. (See §6.1.) Some quantity (like electricity, heat, energy,
or mass) can ﬂow across the edges, from one node to another.
With edge j we associate a ﬂow (rate) fj, which is a scalar; the vector of all
m ﬂows is the ﬂow m-vector f . The ﬂows fj can be positive or negative: Positive

156

8 Linear equations

s1

1

1

2

3

Figure 8.1 A node in a diﬀusion system with label 1, exogenous ﬂow s1 and
three incident edges.

fj means the quantity ﬂows in the direction of edge j, and negative fj means
the quantity ﬂows in the opposite direction of edge j. The ﬂows can represent, for
example, heat ﬂow (in units of Watts) in a thermal model, electrical current (Amps)
in an electrical circuit, or movement (diﬀusion) of mass (such as, for example, a
pollutant). We also have a source (or exogenous) ﬂow si at each node, with si > 0
meaning that an exogenous ﬂow is injected into node i, and si < 0 means that
an exogenous ﬂow is removed from node i. (In some contexts, a node where ﬂow
is removed is called a sink.) In a thermal system, the sources represent thermal
(heat) sources; in an electrical circuit, they represent electrical current sources; in
a system with diﬀusion, they represent external injection or removal of the mass.
In a diﬀusion system, the ﬂows must satisfy (ﬂow) conservation, which means
that at each node, the total ﬂow entering each node from adjacent edges and the
exogenous source, must be zero. This is illustrated in ﬁgure 8.1, which shows three
edges adjacent to node 1, two entering node 1 (ﬂows 1 and 2), and one (ﬂow 3)
leaving node 1, and an exogenous ﬂow. Flow conservation at this node is expressed
as

f1 + f2 − f3 + s1 = 0.

Flow conservation at every node can be expressed by the simple matrix-vector

equation

Af + s = 0,

(8.6)

where A is the incidence matrix described in §7.3.
(This is called Kirchhoﬀ ’s
current law in an electrical circuit, after the physicist Gustav Kirchhoﬀ; when the
ﬂows represent movement of mass, it is called conservation of mass.)

With node i we associate a potential ei; the n-vector e gives the potential
at all nodes.
(Note that here, e represents the n-vector of potentials; ei is the
scalar potential at node i, and not the standard ith unit vector.) The potential
might represent the node temperature in a thermal model, the electrical potential
(voltage) in an electrical circuit, and the concentration in a system that involves
mass diﬀusion.

8.3 Systems of linear equations

157

2

8

3

Figure 8.2 The ﬂow through edge 8 is equal to f8 = (e2 − e3)/r8.

In a diﬀusion system the ﬂow on an edge is proportional to the potential diﬀer-
ence across its adjacent nodes. This is typically written as rjfj = ek − el, where
edge j goes from node k to node l, and rj (which is typically positive) is called the
resistance of edge j. In a thermal model, rj is called the thermal resistance of the
edge; in an electrical circuit, it is called the electrical resistance. This is illustrated
in ﬁgure 8.2, which shows edge 8, connecting node 2 and node 3, corresponding to
an edge ﬂow equation

r8f8 = e2 − e3.

We can write the edge ﬂow equations in a compact way as

Rf = −AT e,

(8.7)

where R = diag(r) is called the resistance matrix.

The diﬀusion model can be expressed as one set of block linear equations in the

variables f , s, and e:

0

R 0 AT �
� A I

f
s

e  = 0.

This is a set of n + m homogeneous equations in m + 2n variables. To these under-
determined equations we can add others, for example, by specifying some of the
entries of f , s, and e.

Leontief input-ouput model. We consider an economy with n diﬀerent industrial
sectors. We let xi be the economic activity level, or total production output, of
sector i, for i = 1, . . . , n, measured in a common unit, such as (billions of) dollars.
The output of each sector ﬂows to other sectors, to support their production, and
also to consumers. We denote the total consumer demand for sector i as di, for
i = 1, . . . , n.

Supporting the output level xj for sector j requires Aijxj output for sector i.
We refer to Aijxj as the sector i input that ﬂows to sector j. (We can have Aii �= 0;
for example, it requires some energy to support the production of energy.) Thus,
Ai1x1 + ··· + Ainxn is the total sector i output required by, or ﬂowing into, the n
industrial sectors. The matrix A is called the input-output matrix of the economy,
since it describes the ﬂows of sector outputs to the inputs of itself and other sectors.
The vector Ax gives the sector outputs required to support the production levels
given by x. (This sounds circular, but isn’t.)

158

8 Linear equations

Finally, we require that for each sector, the total production level matches the
demand plus the total amount required to support production. This leads to the
balance equations,

x = Ax + d.

Suppose the demand vector d is given, and we wish to ﬁnd the sector output levels
that will support it. We can write this as a set of n equations in n unknowns,

(I − A)x = d.

This model of the sector inputs and outputs of an economy was developed by
Wassily Leontief in the late 1940s, and is now known as Leontief input-output
analysis. He was awarded the Nobel Prize in economics for this work in 1973.

Exercises

Exercises

159

8.1 Sum of linear functions. Suppose f : Rn → Rm and g : Rn → Rm are linear functions.
Their sum is the function h : Rn → Rm, deﬁned as h(x) = f (x) + g(x) for any n-vector x.
The sum function is often denoted as h = f +g. (This is another case of overloading the +
symbol, in this case to the sum of functions.) If f has matrix representation f (x) = F x,
and g has matrix representation f (x) = Gx, where F and G are m × n matrices, what
is the matrix representation of the sum function h = f + g? Be sure to identify any +
symbols appearing in your justiﬁcation.

8.2 Averages and aﬃne functions. Suppose that G : Rn → Rm is an aﬃne function. Let

x1, . . . , xk be n-vectors, and deﬁne the m-vectors y1 = G(x1), . . . , yk = G(xk). Let

x = (x1 + ··· + xk)/k,

y = (y1 + ··· + yk)/k

be the averages of these two lists of vectors. (Here x is an n-vector and y is an m-vector.)
Show that we always have y = G(x). In words: The average of an aﬃne function applied
to a list of vectors is the same as the aﬃne function applied to the average of the list of
vectors.

8.3 Cross-product. The cross product of two 3-vectors a = (a1, a2, a3) and x = (x1, x2, x3) is

deﬁned as the vector

a1x2 − a2x1 � .
a × x =� a2x3 − a3x2

a3x1 − a1x3

The cross product comes up in physics, for example in electricity and magnetism, and in
dynamics of mechanical systems like robots or satellites. (You do not need to know this
for this exercise.)
Assume a is ﬁxed. Show that the function f (x) = a× x is a linear function of x, by giving
a matrix A that satisﬁes f (x) = Ax for all x.

8.4 Linear functions of images.

In this problem we consider several linear functions of a
monochrome image with N × N pixels. To keep the matrices small enough to work out
by hand, we will consider the case with N = 3 (which would hardly qualify as an image).
We represent a 3 × 3 image as a 9-vector using the ordering of pixels shown below.

1

2

3

4

5

6

7

8

9

(This ordering is called column-major.) Each of the operations or transformations below
deﬁnes a function y = f (x), where the 9-vector x represents the original image, and the
9-vector y represents the resulting or transformed image. For each of these operations,
give the 9 × 9 matrix A for which y = Ax.
(a) Turn the original image x upside-down.
(b) Rotate the original image x clockwise 90◦.
(c) Translate the image up by 1 pixel and to the right by 1 pixel. In the translated

image, assign the value yi = 0 to the pixels in the ﬁrst column and the last row.

(d) Set each pixel value yi to be the average of the neighbors of pixel i in the original
image. By neighbors, we mean the pixels immediately above and below, and imme-
diately to the left and right. The center pixel has 4 neighbors; corner pixels have 2
neighbors, and the remaining pixels have 3 neighbors.

160

8 Linear equations

8.5 Symmetric and anti-symmetric part. An n-vector x is symmetric if xk = xn−k+1 for

k = 1, . . . , n. It is anti-symmetric if xk = −xn−k+1 for k = 1, . . . , n.
(a) Show that every vector x can be decomposed in a unique way as a sum x = xs + xa

of a symmetric vector xs and an anti-symmetric vector xa.

(b) Show that the symmetric and anti-symmetric parts xs and xa are linear functions

of x. Give matrices As and Aa such that xs = Asx and xa = Aax for all x.

8.6 Linear functions. For each description of y below, express it as y = Ax for some A. (You

should specify A.)
(a) yi is the diﬀerence between xi and the average of x1, . . . , xi−1. (We take y1 = x1.)
(b) yi is the diﬀerence between xi and the average value of all other xjs, i.e., the average

of x1, . . . , xi−1, xi+1, . . . , xn.

8.7 Interpolation of polynomial values and derivatives. The 5-vector c represents the coeﬃ-
cients of a quartic polynomial p(x) = c1 + c2x + c3x2 + c4x3 + c5x4. Express the conditions

p(0) = 0,

p�(0) = 0,

p(1) = 1,

p�(1) = 0,

as a set of linear equations of the form Ac = b.
determined, over-determined, or square?

Is the system of equations under-

8.8 Interpolation of rational functions. A rational function of degree two has the form

f (t) =

c1 + c2t + c3t2
1 + d1t + d2t2 ,

where c1, c2, c3, d1, d2 are coeﬃcients. (‘Rational’ refers to the fact that f is a ratio of
polynomials. Another name for f is bi-quadratic.) Consider the interpolation conditions

f (ti) = yi,

i = 1, . . . , K,

where ti and yi are given numbers. Express the interpolation conditions as a set of linear
equations in the vector of coeﬃcients θ = (c1, c2, c3, d1, d2), as Aθ = b. Give A and b, and
their dimensions.

8.9 Required nutrients. We consider a set of n basic foods (such as rice, beans, apples) and
a set of m nutrients or components (such as protein, fat, sugar, vitamin C). Food j has
a cost given by cj (say, in dollars per gram), and contains an amount Nij of nutrient i
(per gram). (The nutrients are given in some appropriate units, which can depend on
the particular nutrient.) A daily diet is represented by an n-vector d, with di the daily
intake (in grams) of food i. Express the condition that a diet d contains the total nutrient
amounts given by the m-vector ndes, and has a total cost B (the budget) as a set of
linear equations in the variables d1, . . . , dn. (The entries of d must be nonnegative, but
we ignore this issue here.)

8.10 Blending crude oil. A set of K diﬀerent types of crude oil are blended (mixed) together in
proportions θ1, . . . , θK . These numbers sum to one; they must also be nonnegative, but we
will ignore that requirement here. Associated with crude oil type k is an n-vector ck that
gives its concentration of n diﬀerent constituents, such as speciﬁc hydrocarbons. Find a
set of linear equations on the blending coeﬃcients, Aθ = b, that expresses the requirement
that the blended crude oil achieves a target set of constituent concentrations, given by
the n-vector ctar. (Include the condition that θ1 + ··· + θK = 1 in your equations.)
8.11 Location from range measurements. The 3-vector x represents a location in 3-D. We
measure the distances (also called the range) of x to four points at known locations a1,
a2, a3, a4:

ρ1 = �x − a1�,

ρ2 = �x − a2�,

ρ3 = �x − a3�,

ρ4 = �x − a4�.

Express these distance conditions as a set of three linear equations in the vector x. Hint.
Square the distance equations, and subtract one from the others.

Exercises

161

8.12 Quadrature. Consider a function f : R → R. We are interested in estimating the deﬁnite
f (x) dx based on the value of f at some points t1, . . . , tn. (We typically
have −1 ≤ t1 < t2 < ··· < tn ≤ 1, but this is not needed here.) The standard method for
estimating α is to form a weighted sum of the values f (ti):

integral α =� 1

−1

ˆα = w1f (t1) + ··· + wnf (tn),

where ˆα is our estimate of α, and w1, . . . , wn are the weights. This method of estimating
the value of an integral of a function from its values at some points is a classical method in
applied mathematics called quadrature. There are many quadrature methods (i.e., choices
of the points ti and weights wi). The most famous one is due to the mathematician Carl
Friedrich Gauss, and bears his name.

(a) A typical requirement in quadrature is that the approximation should be exact (i.e.,
ˆα = α) when f is any polynomial up to degree d, where d is given. In this case
we say that the quadrature method has order d. Express this condition as a set of
linear equations on the weights, Aw = b, assuming the points t1, . . . , tn are given.
Hint. If ˆα = α holds for the speciﬁc cases f (x) = 1, f (x) = x, . . . , f (x) = xd, then
it holds for any polynomial of degree up to d.

(b) Show that the following quadrature methods have order 1, 2, and 3 respectively.

• Trapezoid rule: n = 2, t1 = −1, t2 = 1, and

w1 = 1/2,

w2 = 1/2.

• Simpson’s rule: n = 3, t1 = −1, t2 = 0, t3 = 1, and

w1 = 1/3,

w2 = 4/3,

w3 = 1/3.

(Named after the mathematician Thomas Simpson.)

• Simpson’s 3/8 rule: n = 4, t1 = −1, t2 = −1/3, t3 = 1/3, t4 = 1,
w4 = 1/4.

w1 = 1/4,

w3 = 3/4,

w2 = 3/4,

8.13 Portfolio sector exposures.

(See exercise 1.14.) The n-vector h denotes a portfolio of
investments in n assets, with hi the dollar value invested in asset i. We consider a set
of m industry sectors, such as pharmaceuticals or consumer electronics. Each asset is
assigned to one of these sectors. (More complex models allow for an asset to be assigned
to more than one sector.) The exposure of the portfolio to sector i is deﬁned as the sum
of investments in the assets in that sector. We denote the sector exposures using the
m-vector s, where si is the portfolio exposure to sector i. (When si = 0, the portfolio
is said to be neutral to sector i.) An investment advisor speciﬁes a set of desired sector
exposures, given as the m-vector sdes. Express the requirement s = sdes as a set of linear
equations of the form Ah = b. (You must describe the matrix A and the vector b.)
Remark. A typical practical case involves n = 1000 assets and m = 50 sectors. An advisor
might specify sdes
i = 0 if she does not have an opinion as how companies in that sector
will do in the future; she might specify a positive value for sdes
if she thinks the companies
in that sector will do well (i.e., generate positive returns) in the future, and a negative
value if she thinks they will do poorly.

i

8.14 Aﬃne combinations of solutions of linear equations. Consider the set of m linear equations
in n variables Ax = b, where A is an m× n matrix, b is an m-vector, and x is the n-vector
of variables. Suppose that the n-vectors z1, . . . , zk are solutions of this set of equations,
i.e., satisfy Azi = b. Show that if the coeﬃcients α1, . . . , αk satisfy α1 + ··· + αk = 1,
then the aﬃne combination

is a solution of the linear equations, i.e., satisﬁes Aw = b. In words: Any aﬃne combina-
tion of solutions of a set of linear equations is also a solution of the equations.

w = α1z1 + ··· + αkzk

162

8 Linear equations

8.15 Stoichiometry and equilibrium reaction rates. We consider a system (such as a single cell)
containing m metabolites (chemical species), with n reactions among the metabolites
occurring at rates given by the n-vector r. (A negative reaction rate means the reaction
runs in reverse.) Each reaction consumes some metabolites and produces others, in known
rates proportional to the reaction rate. This is speciﬁed in the m× n stoichiometry matrix
S, where Sij is the rate of metabolite i production by reaction j, running at rate one.
(When Sij is negative, it means that when reaction j runs at rate one, metabolite i is
consumed.) The system is said to be in equilibrium if the total production rate of each
metabolite, due to all the reactions, is zero. This means that for each metabolite, the
total production rate balances the total consumption rate, so the total quantities of the
metabolites in the system do not change. Express the condition that the system is in
equilibrium as a set of linear equations in the reaction rates.

8.16 Bi-linear interpolation. We are given a scalar value at each of the four corners of a square
in 2-D, (x1, y1), (x1, y2), (x2, y1), and (x2, y2), where x1 < x2 and y1 < y2. We refer to
these four values as F11, F12, F21, and F22, respectively. A bi-linear interpolation is a
function of the form

f (u, v) = θ1 + θ2u + θ3v + θ4uv,

where θ1, . . . , θ4 are coeﬃcients, that satisﬁes

f (x1, y1) = F11,

f (x1, y2) = F12,

f (x2, y1) = F21,

f (x2, y2) = F22,

i.e., it agrees with (or interpolates) the given values on the four corners of the square.
(The function f is usually evaluated only for points (u, v) inside the square. It is called
bi-linear since it is aﬃne in u when v is ﬁxed, and aﬃne in v when u is ﬁxed.)
Express the interpolation conditions as a set of linear equations of the form Aθ = b, where
A is a 4 × 4 matrix and b is a 4-vector. Give the entries of A and b in terms of x1, x2, y1,
y2, F11, F12, F21, and F22.
Remark. Bi-linear interpolation is used in many applications to guess or approximate the
values of a function at an arbitrary point in 2-D, given the function values on a grid of
points. To approximate the value at a point (x, y), we ﬁrst ﬁnd the square of grid points
that the point lies in. Then we use bi-linear interpolation to get the approximate value
at (x, y).

Chapter 9

Linear dynamical systems

In this chapter we consider a useful application of matrix-vector multiplication,
which is used to describe many systems or phenomena that change or evolve over
time.

9.1 Linear dynamical systems

Suppose x1, x2, . . . is a sequence of n-vectors. The index (subscript) denotes time
or period, and is written as t; xt, the value of the sequence at time (or period)
t, is called the state at time t. We can think of xt as a vector that changes over
time, i.e., one that changes dynamically. In this context, the sequence x1, x2, . . . is
sometimes called a trajectory or state trajectory. We sometimes refer to xt as the
current state of the system (implicitly assuming the current time is t), and xt+1 as
the next state, xt−1 as the previous state, and so on.

The state xt can represent a portfolio that changes daily, or the positions and
velocities of the parts of a mechanical system, or the quarterly activity of an econ-
omy. If xt represents a portfolio that changes daily, (x5)3 is the amount of asset 3
held in the portfolio on (trading) day 5.

A linear dynamical system is a simple model for the sequence, in which each

xt+1 is a linear function of xt:

xt+1 = Atxt,

t = 1, 2, . . . .

(9.1)

Here the n × n matrices At are called the dynamics matrices. The equation above
is called the dynamics or update equation, since it gives us the next value of x, i.e.,
xt+1, as a function of the current value xt. Often the dynamics matrix does not
depend on t, in which case the linear dynamical system is called time-invariant.

If we know xt (and At, At+1, . . .) we can determine xt+1, xt+2, . . . simply by
iterating the dynamics equation (9.1).
In other words: If we know the current
value of x, we can ﬁnd all future values. In particular, we do not need to know
the past states. This is why xt is called the state of the system. It contains all the
information needed at time t to determine the future evolution of the system.

164

9 Linear dynamical systems

Linear dynamical system with input. There are many variations on and exten-
sions of the basic linear dynamical system model (9.1), some of which we will
encounter later. As an example, we can add additional terms to the update equa-
tion:

xt+1 = Atxt + Btut + ct,

(9.2)
Here ut is an m-vector called the input, Bt is the n × m input matrix, and the
n-vector ct is called the oﬀset, all at time t. The input and oﬀset are used to model
other factors that aﬀect the time evolution of the state. Another name for the
input ut is exogenous variable, since, roughly speaking, it comes from outside the
system.

t = 1, 2, . . . .

Markov model. The linear dynamical system (9.1) is sometimes called a Markov
model (after the mathematician Andrey Markov). Markov studied systems in which
the next state value depends on the current one, and not on the previous state
values xt−1, xt−2, . . .. The linear dynamical system (9.1) is the special case of a
Markov system where the next state is a linear function of the current state.

In a variation on the Markov model, called a (linear) K-Markov model, the
next state xt+1 depends on the current state and K − 1 previous states. Such a
system has the form

xt+1 = A1xt + ··· + AKxt−K+1,

t = K, K + 1, . . . .

(9.3)

Models of this form are used in time series analysis and econometrics, where they
are called (vector) auto-regressive models. When K = 1, the Markov model (9.3) is
the same as a linear dynamical system (9.1). When K > 1, the Markov model (9.3)
can be reduced to a standard linear dynamical system (9.1), with an appropriately
chosen state; see exercise 9.4.

Simulation.
If we know the dynamics (and input) matrices, and the state at
time t, we can ﬁnd the future state trajectory xt+1, xt+2, . . . by iterating the equa-
tion (9.1) (or (9.2), provided we also know the input sequence ut, ut+1, . . .). This is
called simulating the linear dynamical system. Simulation makes predictions about
the future state of a system. (To the extent that (9.1) is only an approximation or
model of some real system, we must be careful when interpreting the results.) We
can carry out what-if simulations, to see what would happen if the system changes
in some way, or if a particular set of inputs occurs.

9.2 Population dynamics

Linear dynamical systems can be used to describe the evolution of the age dis-
tribution in some population over time. Suppose xt is a 100-vector, with (xt)i
denoting the number of people in some population (say, a country) with age i − 1
(say, on January 1) in year t, where t is measured starting from some base year, for
i = 1, . . . , 100. While (xt)i is an integer, it is large enough that we simply consider

9.2 Population dynamics

165

5

4

3

2

1

0

)
s
n
o
i
l
l
i

m

(

n
o
i
t
a
l
u
p
o
P

0

10 20 30 40 50 60 70 80 90 100

Age

Figure 9.1 Age distribution in the US in 2010. (United States Census Bu-
reau, census.gov).

it a real number. In any case, our model certainly is not accurate at the level of
individual people. Also, note that the model does not track people 100 and older.
The distribution of ages in the US in 2010 is shown in ﬁgure 9.1.

The birth rate is given by a 100-vector b, where bi is the average number of
births per person with age i − 1, i = 1, . . . , 100. (This is half the average number
of births per woman with age i − 1, assuming equal numbers of men and women
in the population.) Of course bi is approximately zero for i < 13 and i > 50. The
approximate birth rates for the US in 2010 are shown in ﬁgure 9.2. The death rate
is given by a 100-vector d, where di is the portion of those aged i − 1 who will die
this year. The death rates for the US in 2010 are shown in ﬁgure 9.3.

To derive the dynamics equation (9.1), we ﬁnd xt+1 in terms of xt, taking into
account only births and deaths, and not immigration. The number of 0-year olds
next year is the total number of births this year:

(xt+1)1 = bT xt.

The number of i-year olds next year is the number of (i − 1)-year-olds this year,
minus those who die:

(xt+1)i+1 = (1 − di)(xt)i,

i = 1, . . . , 99.

We can assemble these equations into the time-invariant linear dynamical system

xt+1 = Axt,

t = 1, 2, . . . ,

(9.4)

166

9 Linear dynamical systems

)

%

(

e
t
a
r

h
t
r
i
B

4

2

0

0

10 20 30 40 50 60 70 80 90 100

Age

Figure 9.2 Approximate birth rate versus age in the US in 2010. The ﬁgure is
based on statistics for age groups of ﬁve years (hence, the piecewise-constant
shape) and assumes an equal number of men and women in each age group.
(Martin J.A., Hamilton B.E., Ventura S.J. et al., Births: Final data for 2010.
National Vital Statistics Reports; vol. 61, no. 1. National Center for Health
Statistics, 2012.)

)

%

(

e
t
a
r

h
t
a
e
D

30

20

10

0

0

10 20 30 40 50 60 70 80 90 100

Age

Figure 9.3 Death rate versus age, for ages 0–99, in the US in 2010. (Centers
for Disease Control and Prevention, National Center for Health Statistics,
wonder.cdc.gov.)

9.2 Population dynamics

167

)
s
n
o
i
l
l
i

m

(

n
o
i
t
a
l
u
p
o
P

4

3

2

1

0

0

10 20 30 40 50 60 70 80 90 100

Age

Figure 9.4 Predicted age distribution in the US in 2020.

where A is given by

A =



b1

1 − d1

0
...
0
0

b2
0

1 − d2

...
0
0

b3
0
0
...
0
0

···
···
···

···
···

b98
0
0
...

1 − d98

0

b99
0
0
...
0

1 − d99

b100
0
0
...
0
0

.



We can use this model to predict the total population in 10 years (not including
immigration), or to predict the number of school age children, or retirement age
adults. Figure 9.4 shows the predicted age distribution in 2020, computed by
iterating the model xt+1 = Axt for t = 1, . . . , 10, with initial value x1 given by
the 2010 age distribution of ﬁgure 9.1. Note that the distribution is based on an
approximate model, since we neglect the eﬀect of immigration, and assume that the
death and birth rates remain constant and equal to the values shown in ﬁgures 9.2
and 9.3.

Population dynamics models are used to carry out projections of the future age
distribution, which in turn is used to predict how many retirees there will be in
some future year. They are also used to carry out various ‘what if’ analyses, to
predict the eﬀect of changes in birth or death rates on the future age distribution.
It is easy to include the eﬀects of immigration and emigration in the population

dynamics model (9.4), by simply adding a 100-vector ut:

xt+1 = Axt + ut,

which is a time-invariant linear dynamical system of the form (9.2), with input ut
and B = I. The vector ut gives the net immigration in year t over all ages; (ut)i
is the number of immigrants in year t of age i − 1. (Negative entries mean net
emigration.)

168

9 Linear dynamical systems

9.3 Epidemic dynamics

The dynamics of infection and the spread of an epidemic can be modeled using a
linear dynamical system. (More sophisticated nonlinear epidemic dynamic models
are also used.) In this section we describe a simple example.

A disease is introduced into a population. In each period (say, days) we count

the fraction of the population that is in four diﬀerent infection states:

• Susceptible. These individuals can acquire the disease the next day.
• Infected. These individuals have the disease.
• Recovered (and immune). These individuals had the disease and survived,

and now have immunity.

• Deceased. These individuals had the disease, and unfortunately died from it.
We denote the fractions of each of these as a 4-vector xt, so, for example, xt =
(0.75, 0.10, 0.10, 0.05) means that in day t, 75% of the population is susceptible,
10% is infected, 10% is recovered and immune, and 5% has died from the disease.
There are many mathematical models that predict how the disease state frac-
tions xt evolve over time. One simple model can be expressed as a linear dynamical
system. The model assumes the following happens over each day.

• 5% of the susceptible population will acquire the disease. (The other 95%

will remain susceptible.)

• 1% of the infected population will die from the disease, 10% will recover
and acquire immunity, and 4% will recover and not acquire immunity (and
therefore, become susceptible). The remaining 85% will remain infected.

(Those who have have recovered with immunity and those who have died remain
in those states.)

We ﬁrst determine (xt+1)1, the fraction of susceptible individuals in the next
day. These include the susceptible individuals from today, who did not become
infected, which is 0.95(xt)1, plus the infected individuals today who recovered
without immunity, which is 0.04(xt)2. All together we have (xt+1)1 = 0.95(xt)1 +
0.04(xt)2. We have (xt+1)2 = 0.85(xt)2 + 0.05(xt)1; the ﬁrst term counts those
who are infected and remain infected, and the second term counts those who are
susceptible and acquire the disease. Similar arguments give (xt+1)3 = (xt)3 +
0.10(xt)2, and (xt+1)4 = (xt)4 + 0.01(xt)2. We put these together to get

0.95
0.05

0
0

0.04
0.85
0.10
0.01

0
0
1
0

0
0
0
1

 xt,

xt+1 =

which is a time-invariant linear dynamical system of the form (9.1).

Figure 9.5 shows the evolution of the four groups from the initial condition x0 =
(1, 0, 0, 0). The simulation shows that after around 100 days, the state converges
to one with a little under 10% of the population deceased, and the remaining
population immune.

9.4 Motion of a mass

169

Susceptible

Recovered

1

0.8

0.6

0.4

0.2

t
x

Infected

0

0

50

100

Time t

Deceased

150

200

Figure 9.5 Simulation of epidemic dynamics.

f (τ )

0

m

p(τ )

Figure 9.6 Mass moving along a line.

9.4 Motion of a mass

Linear dynamical systems can be used to (approximately) describe the motion of
many mechanical systems, for example, an airplane (that is not undergoing extreme
maneuvers), or the (hopefully not too large) movement of a building during an
earthquake. Here we describe the simplest example: A single mass moving in 1-D
(i.e., a straight line), with an external force and a drag force acting on it. This
is illustrated in ﬁgure 9.6. The (scalar) position of the mass at time τ is given by
p(τ ). (Here τ is continuous, i.e., a real number.) The position satisﬁes Newton’s
law of motion, the diﬀerential equation

m

d2p
dτ 2 (τ ) = −η

dp
dτ

(τ ) + f (τ ),

where m > 0 is the mass, f (τ ) is the external force acting on the mass at time τ ,
and η > 0 is the drag coeﬃcient. The right-hand side is the total force acting on
the mass; the ﬁrst term is the drag force, which is proportional to the velocity and
in the opposite direction.

170

9 Linear dynamical systems

Introducing the velocity of the mass, v(τ ) = dp(τ )/dτ , we can write the equation

above as two coupled diﬀerential equations,

dp
dτ

(τ ) = v(τ ),

m

dv
dτ

(τ ) = −ηv(τ ) + f (τ ).

The ﬁrst equation relates the position and velocity; the second is from the law of
motion.

Discretization. To develop an (approximate) linear dynamical system model from
the diﬀerential equations above, we ﬁrst discretize time. We let h > 0 be a time
interval (called the ‘sampling interval’) that is small enough that the velocity and
forces do not change very much over h seconds. We deﬁne

pk = p(kh),

vk = v(kh),

fk = f (kh),

which are the continuous quantities ‘sampled’ at multiples of h seconds. We now
use the approximations

dp
dτ

(kh) ≈

pk+1 − pk

h

,

dv
dτ

(kh) ≈

vk+1 − vk

h

,

(9.5)

which are justiﬁed since h is small. This leads to the (approximate) equations
(replacing ≈ with =)

pk+1 − pk

h

= vk,

m

vk+1 − vk

h

= fk − ηvk.

Finally, using state xk = (pk, vk), we write this as

xk+1 =� 1

0

h

1 − hη/m � xk +�

0

h/m � fk,

k = 1, 2, . . . ,

which is a linear dynamical system of the form (9.2), with input fk and dynamics
and input matrices

A =� 1

0

h

1 − hη/m � ,

B =�

0

h/m � .

This linear dynamical system gives an approximation of the true motion, due to
our approximation (9.5) of the derivatives. But for h small enough, it is accurate.
This linear dynamical system can be used to simulate the motion of the mass, if
we know the external force applied to it, i.e., ut for t = 1, 2, . . ..

The approximation (9.5), which turns a set of diﬀerential equations into a re-
cursion that approximates it, is called the Euler method, named after the math-
ematician Leonhard Euler.
(There are other, more sophisticated, methods for
approximating diﬀerential equations as recursions.)

9.5 Supply chain dynamics

171

Example. As a simple example, we consider the case with m = 1 (kilogram),
η = 1 (Newtons per meter per second), and sampling period h = 0.01 (seconds).
The external force is

f (τ ) =

0.0
1.0
−1.3
0.0

0.0 ≤ τ < 0.5
0.5 ≤ τ < 1.0
1.0 ≤ τ < 1.4
1.4 ≤ τ.

We simulate this system for a period of 2.5 seconds, starting from initial state x1 =
(0, 0), which corresponds to the mass starting at rest (zero velocity) at position 0.
The simulation involves iterating the dynamics equation from k = 1 to k = 250.
Figure 9.7 shows the force, position, and velocity of the mass, with the axes labeled
using continuous time τ .

9.5 Supply chain dynamics

The dynamics of a supply chain can often be modeled using a linear dynamical
system.
(This simple model does not include some important aspects of a real
supply chain, for example limits on storage at the warehouses, or the fact that
demand ﬂuctuates.) We give a simple example here.

We consider a supply chain for a single divisible commodity (say, oil or gravel, or
discrete quantities so small that their quantities can be considered real numbers).
The commodity is stored at n warehouses or storage locations. Each of these
locations has a target (desired) level or amount of the commodity, and we let the
n-vector xt denote the deviations of the levels of the commodities from their target
levels. For example, (x5)3 is the actual commodity level at location 3, in period 5,
minus the target level for location 3. If this is positive it means we have more than
the target level at the location; if it is negative, we have less than the target level
at the location.

The commodity is moved or transported in each period over a set of m trans-
portation links between the storage locations, and also enters and exits the nodes
through purchases (from suppliers) and sales (to end-users). The purchases and
sales are given by the n-vectors pt and st, respectively. We expect these to be posi-
tive; but they can be negative if we include returns. The net eﬀect of the purchases
and sales is that we add (pt − st)i of the commodity at location i. (This number is
negative if we sell more than we purchase at the location.)
We describe the links by the n×m incidence matrix Asc (see §7.3). The direction
of each link does not indicate the direction of commodity ﬂow; it only sets the
reference direction for the ﬂow: Commodity ﬂow in the direction of the link is
considered positive and commodity ﬂow in the opposite direction is considered
negative. We describe the commodity ﬂow in period t by the m-vector ft. For
example, (f6)2 = −1.4 means that in time period 6, 1.4 units of the commodity
are moved along link 2 in the direction opposite the link direction (since the ﬂow
is negative). The n-vector Ascft gives the net ﬂow of the commodity into the n
locations, due to the transport across the links.

172

9 Linear dynamical systems

)
τ
(
f

1

0.5

0

−0.5

−1

−1.5

0

1

2

3

4

5

6

0.15

0.1

0.05

0

0.4

0.2

0

)
τ
(
p

)
τ
(
v

0

1

2

3

4

5

6

−0.2

0

1

2

3
τ

4

5

6

Figure 9.7 Simulation of mass moving along a line. Applied force (top),
position (middle), and velocity (bottom).

9.5 Supply chain dynamics

173

s1

p1

1

f1

f2

2

3

f3

s2

p2

s3

p3

Figure 9.8 A simple supply chain with n = 3 storage locations and m = 3
transportation links.

Taking into account the movement of the commodity across the network, and

the purchase and sale of the commodity, we get the dynamics

xt+1 = xt + Ascft + pt − st,

t = 1, 2, . . . .

In applications where we control or run a supply chain, st is beyond our control,
but we can manipulate ft (the ﬂow of goods between storage locations) and pt
(purchases at locations). This suggests treating st as the oﬀset, and ut = (ft, pt)
as the input in a linear dynamical system with input (9.2). We can write the
dynamics equations above in this form, with dynamics and input matrices

A = I,

B =� Asc

I � .

(Note that Asc refers to the supply chain graph incidence matrix, while A is the
dynamics matrix in (9.2).) This gives

xt+1 = Axt + B(ft, pt) − st,

t = 1, 2, . . . , .

A simple example is shown in ﬁgure 9.8. The supply chain dynamics equation

is

xt+1 = xt +

−1 −1
0
0 −1
1
1
1
0

1
0
0

0
1
0

0
0

1 � ft

pt � − st,

It is a good exercise to check that the matrix-vector product (the middle term of
the right-hand side) gives the amount of commodity added at each location, as a
result of shipment and purchasing.

t = 1, 2, . . . .

174

9 Linear dynamical systems

Exercises

9.1 Compartmental system. A compartmental system is a model used to describe the move-
ment of some material over time among a set of n compartments of a system, and the
outside world. It is widely used in pharmaco-kinetics, the study of how the concentration
of a drug varies over time in the body. In this application, the material is a drug, and
the compartments are the bloodstream, lungs, heart, liver, kidneys, and so on. Compart-
mental systems are special cases of linear dynamical systems.
In this problem we will consider a very simple compartmental system with 3 compart-
ments. We let (xt)i denote the amount of the material (say, a drug) in compartment i at
time period t. Between period t and period t + 1, the material moves as follows.

• 10% of the material in compartment 1 moves to compartment 2. (This decreases

the amount in compartment 1 and increases the amount in compartment 2.)

• 5% of the material in compartment 2 moves to compartment 3.
• 5% of the material in compartment 3 moves to compartment 1.
• 5% of the material in compartment 3 is eliminated.

Express this compartmental system as a linear dynamical system, xt+1 = Axt. (Give the
matrix A.) Be sure to account for all the material entering and leaving each compartment.

9.2 Dynamics of an economy. An economy (of a country or region) is described by an n-
vector at, where (at)i is the economic output in sector i in year t (measured in billions of
dollars, say). The total output of the economy in year t is 1T at. A very simple model of
how the economic output changes over time is at+1 = Bat, where B is an n × n matrix.
(This is closely related to the Leontief input-output model described on page 157 of the
book. But the Leontief model is static, i.e., doesn’t consider how an economy changes
over time.) The entries of at and B are positive in general.
In this problem we will consider the speciﬁc model with n = 4 sectors and

B =

0.10
0.48
0.00
0.04

0.06
0.44
0.55
0.01

0.05
0.10
0.52
0.42

0.70
0.04
0.04
0.51

 .

(a) Brieﬂy interpret B23, in English.

(b) Simulation. Suppose a1 = (0.6, 0.9, 1.3, 0.5). Plot the four sector outputs (i.e., (at)i
for i = 1, . . . , 4) and the total economic output (i.e., 1T at) versus t, for t = 1, . . . , 20.

9.3 Equilibrium point for linear dynamical system. Consider a time-invariant linear dynamical
system with oﬀset, xt+1 = Axt +c, where xt is the state n-vector. We say that a vector z is
an equilibrium point of the linear dynamical system if x1 = z implies x2 = z, x3 = z, . . . .
(In words: If the system starts in state z, it stays in state z.)
Find a matrix F and vector g for which the set of linear equations F z = g characterizes
equilibrium points. (This means: If z is an equilibrium point, then F z = g; conversely if
F z = g, then z is an equilibrium point.) Express F and g in terms of A, c, any standard
matrices or vectors (e.g., I, 1, or 0), and matrix and vector operations.
Remark. Equilibrium points often have interesting interpretations. For example, if the
linear dynamical system describes the population dynamics of a country, with the vector c
denoting immigration (emigration when entries of c are negative), an equilibrium point is
a population distribution that does not change, year to year. In other words, immigration
exactly cancels the changes in population distribution caused by aging, births, and deaths.

Exercises

175

9.4 Reducing a Markov model to a linear dynamical system. Consider the 2-Markov model

xt+1 = A1xt + A2xt−1,

t = 2, 3, . . . ,

where xt is an n-vector. Deﬁne zt = (xt, xt−1). Show that zt satisﬁes the linear dynamical
system equation zt+1 = Bzt, for t = 2, 3, . . ., where B is a (2n) × (2n) matrix. This idea
can be used to express any K-Markov model as a linear dynamical system, with state
(xt, . . . , xt−K+1).

9.5 Fibonacci sequence. The Fibonacci sequence y0, y1, y2, . . . starts with y0 = 0, y1 = 1, and
for t = 2, 3, . . ., yt is the sum of the previous two entries, i.e., yt−1 + yt−2. (Fibonacci
is the name used by the 13th century mathematician Leonardo of Pisa.) Express this
as a time-invariant linear dynamical system with state xt = (yt, yt−1) and output yt,
for t = 1, 2, . . .. Use your linear dynamical system to simulate (compute) the Fibonacci
sequence up to t = 20. Also simulate a modiﬁed Fibonacci sequence z0, z1, z2, . . ., which
starts with the same values z0 = 0 and z1 = 1, but for t = 2, 3, . . ., zt is the diﬀerence of
the two previous values, i.e., zt−1 − zt−2.

9.6 Recursive averaging. Suppose that u1, u2, . . . is a sequence of n-vectors. Let x1 = 0, and
for t = 2, 3, . . ., let xt be the average of u1, . . . , ut−1, i.e., xt = (u1 + ··· + ut−1)/(t − 1).
Express this as a linear dynamical system with input, i.e., xt+1 = Atxt +Btut, t = 1, 2, . . .
(with initial state x1 = 0). Remark. This can be used to compute the average of an
extremely large collection of vectors, by accessing them one-by-one.

9.7 Complexity of linear dynamical system simulation. Consider the time-invariant linear
dynamical system with n-vector state xt and m-vector input ut, and dynamics xt+1 =
Axt + But, t = 1, 2, . . .. You are given the matrices A and B, the initial state x1, and the
inputs u1, . . . , uT−1. What is the complexity of carrying out a simulation, i.e., computing
x2, x3, . . . , xT ? About how long would it take to carry out a simulation with n = 15,
m = 5, and T = 105, using a 1 Gﬂop/s computer?

Chapter 10

Matrix multiplication

In this chapter we introduce matrix multiplication, a generalization of matrix-vector
multiplication, and describe several interpretations and applications.

10.1 Matrix-matrix multiplication

It is possible to multiply two matrices using matrix multiplication. You can multiply
two matrices A and B provided their dimensions are compatible, which means the
number of columns of A equals the number of rows of B. Suppose A and B are
compatible, e.g., A has size m × p and B has size p × n. Then the product matrix
C = AB is the m × n matrix with elements

Cij =

p�k=1

AikBkj = Ai1B1j+···+AipBpj,

i = 1, . . . , m,

j = 1, . . . , n. (10.1)

There are several ways to remember this rule. To ﬁnd the i, j element of the
product C = AB, you need to know the ith row of A and the jth column of B.
The summation above can be interpreted as ‘moving left to right along the ith row
of A’ while moving ‘top to bottom’ down the jth column of B. As you go, you
keep a running sum of the product of elements, one from A and one from B.

As a speciﬁc example, we have

� −1.5

3
1 −1

2

0 �

−1 −1
0 −2
1

0  =� 3.5 −4.5
1 � .

−1

To ﬁnd the 1, 2 entry of the right-hand matrix, we move along the ﬁrst row of
the left-hand matrix, and down the second column of the middle matrix, to get
(−1.5)(−1) + (3)(−2) + (2)(0) = −4.5.
multiplication (or product) we have encountered so far.

Matrix-matrix multiplication includes as special cases several other types of

178

10 Matrix multiplication

Scalar-vector product.
If x is an n-vector and a is a number, we can interpret the
scalar-vector product xa, with the scalar appearing on the right, as matrix-matrix
multiplication. We consider the n-vector x to be an n × 1 matrix, and the scalar
a to be a 1 × 1 matrix. The matrix product xa then makes sense, and is an n × 1
matrix, which we consider the same as an n-vector. It coincides with the scalar-
vector product xa, which we usually write (by convention) as ax. But note that ax
cannot be interpreted as matrix-matrix multiplication (except when n = 1), since
the number of columns of a (which is one) is not equal to the number of rows of x
(which is n).

Inner product. An important special case of matrix-matrix multiplication is the
multiplication of a row vector with a column vector. If a and b are n-vectors, then
the inner product

aT b = a1b1 + a2b2 + ··· + anbn

can be interpreted as the matrix-matrix product of the 1 × n matrix aT and the
n× 1 matrix b. The result is a 1× 1 matrix, which we consider to be a scalar. (This
explains the notation aT b for the inner product of vectors a and b, deﬁned in §1.4.)
Matrix-vector multiplication. The matrix-vector product y = Ax deﬁned in (6.4)
can be interpreted as a matrix-matrix product of A with the n × 1 matrix x.
Vector outer product. The outer product of an m-vector a and an n-vector b is
given by abT , which is an m × n matrix

abT =

a1b1
a2b1
...

a1b2
a2b2
...
amb1 amb2

···
···

a1bn
a2bn
...

··· ambn

,



whose entries are all products of the entries of a and the entries of b. Note that
the outer product does not satisfy abT = baT , i.e., it is not symmetric (like the
inner product). Indeed, the equation abT = baT does not even make sense, unless
m = n; even then, it is not true in general.

Multiplication by identity.
If A is any m × n matrix, then AI = A and IA = A,
i.e., when you multiply a matrix by an identity matrix, it has no eﬀect. (Note the
diﬀerent sizes of the identity matrices in the formulas AI = A and IA = A.)

Matrix multiplication order matters. Matrix multiplication is (in general) not
commutative: We do not (in general) have AB = BA. In fact, BA may not even
make sense, or, if it makes sense, may be a diﬀerent size than AB. For example, if
A is 2 × 3 and B is 3 × 4, then AB makes sense (the dimensions are compatible)
but BA does not even make sense (the dimensions are incompatible). Even when
AB and BA both make sense and are the same size, i.e., when A and B are square,
we do not (in general) have AB = BA. As a simple example, take the matrices

A =� 1

9

6

3 � ,

B =�

0 −1
−1

2 � .

10.1 Matrix-matrix multiplication

179

We have

AB =� −6

−3 −3 � ,

11

BA =� −9 −3
0 � .

17

Two matrices A and B that satisfy AB = BA are said to commute. (Note that for
AB = BA to make sense, A and B must both be square.)

Properties of matrix multiplication. The following properties hold and are easy
to verify from the deﬁnition of matrix multiplication. We assume that A, B, and
C are matrices for which all the operations below are valid, and that γ is a scalar.

• Associativity: (AB)C = A(BC). Therefore we can write the product simply

as ABC.

• Associativity with scalar multiplication: γ(AB) = (γA)B, where γ is a scalar,
and A and B are matrices (that can be multiplied). This is also equal to
A(γB).
(Note that the products γA and γB are deﬁned as scalar-matrix
products, but in general, unless A and B have one row, not as matrix-matrix
products.)

• Distributivity with addition. Matrix multiplication distributes across matrix
addition: A(B+C) = AB+AC and (A+B)C = AC +BC. On the right-hand
sides of these equations we use the higher precedence of matrix multiplication
over addition, so, for example, AC + BC is interpreted as (AC) + (BC).

• Transpose of product. The transpose of a product is the product of the

transposes, but in the opposite order: (AB)T = BT AT .

From these properties we can derive others. For example, if A, B, C, and D are
square matrices of the same size, we have the identity

(A + B)(C + D) = AC + AD + BC + BD.

This is the same as the usual formula for expanding a product of sums of scalars;
but with matrices, we must be careful to preserve the order of the products.

Inner product and matrix-vector products. As an exercise on matrix-vector prod-
ucts and inner products, one can verify that if A is m × n, x is an n-vector, and y
is an m-vector, then

yT (Ax) = (yT A)x = (AT y)T x,

i.e., the inner product of y and Ax is equal to the inner product of x and AT y. (Note
that when m �= n, these inner products involve vectors with diﬀerent dimensions.)

Products of block matrices. Suppose A is a block matrix with m×p block entries
Aij, and B is a block matrix with p× n block entries Bij, and for each k = 1, . . . , p,
the matrix product AikBkj makes sense, i.e., the number of columns of Aik equals
the number of rows of Bkj. (In this case we say that the block matrices conform or

180

10 Matrix multiplication

are compatible.) Then C = AB can be expressed as the m × n block matrix with
entries Cij, given by the formula (10.1). For example, we have

� A B
C D �� E F

G H � =� AE + BG AF + BH
CE + DG CF + DH � ,

for any matrices A, B, . . . , H for which the matrix products above make sense. This
formula is the same as the formula for multiplying two 2 × 2 matrices (i.e., with
scalar entries); but when the entries of the matrix are themselves matrices (as in
the block matrix above), we must be careful to preserve the multiplication order.

Column interpretation of matrix-matrix product. We can derive some additional
insight into matrix multiplication by interpreting the operation in terms of the
columns of the second matrix. Consider the matrix product of an m × p matrix
A and a p × n matrix B, and denote the columns of B by bk. Using block-matrix
notation, we can write the product AB as

AB = A� b1

b2

···

bn � =� Ab1 Ab2

··· Abn � .

Thus, the columns of AB are the matrix-vector products of A and the columns
of B. The product AB can be interpreted as the matrix obtained by ‘applying’ A
to each of the columns of B.

Multiple sets of linear equations. We can use the column interpretation of matrix
multiplication to express a set of k linear equations with the same m× n coeﬃcient
matrix A,

in the compact form

Axi = bi,

i = 1, . . . , k,

AX = B,

where X = [x1 ··· xk] and B = [b1 ··· bk]. The matrix equation AX = B is
sometimes called a linear equation with matrix right-hand side, since it looks like
Ax = b, but X (the variable) and B (the right-hand side) are now n × k matrices,
instead of n-vectors (which are n × 1 matrices).

Row interpretation of matrix-matrix product. We can give an analogous row
interpretation of the product AB, by partitioning A and AB as block matrices
with row vector blocks. Let aT

m be the rows of A. Then we have

1 , . . . , aT

AB =

aT
1
aT
2
...
aT
m



B =

aT
1 B
aT
2 B
...
aT
mB



=

This shows that the rows of AB are obtained by applying BT to the transposed
row vectors ak of A, and transposing the result.

(BT a1)T
(BT a2)T

...

(BT am)T

.



10.1 Matrix-matrix multiplication

181

Inner product representation. From the deﬁnition of the i, j element of AB
in (10.1), we also see that the elements of AB are the inner products of the rows
of A with the columns of B:

AB =

aT
aT
1 b1
1 b2
aT
aT
2 b1
2 b2
...
...
aT
mb1 aT
mb2

aT
···
1 bn
aT
···
2 bn
...
. . .
··· aT
mbn

,



where aT
the matrix-matrix product as the mn inner products aT
matrix.

i are the rows of A and bj are the columns of B. Thus we can interpret
i bj arranged in an m × n

Gram matrix. For an m×n matrix A, with columns a1, . . . , an, the matrix product
G = AT A is called the Gram matrix associated with the set of m-vectors a1, . . . , an.
(It is named after the mathematician Jørgen Pedersen Gram.) From the inner
product interpretation above, the Gram matrix can be expressed as

G = AT A =

aT
1 a1 aT
1 a2
aT
2 a1 aT
2 a2
...
...
aT
n a1 aT
n a2

··· aT
1 an
··· aT
2 an
...
. . .
··· aT
n an

.



The entries of the Gram matrix G give all inner products of pairs of columns of A.
Note that a Gram matrix is symmetric, since aT
j ai. This can also be seen
using the transpose-of-product rule:

i aj = aT

GT = (AT A)T = (AT )(AT )T = AT A = G.

The Gram matrix will play an important role later in this book.

As an example, suppose the m × n matrix A gives the membership of m items

in n groups, with entries

Aij =� 1

0

item i is in group j
item i is not in group j.

(So the jth column of A gives the membership in the jth group, and the ith row
gives the groups that item i is in.) In this case the Gram matrix G has a nice
interpretation: Gij is the number of items that are in both groups i and j, and Gii
is the number of items in group i.

Outer product representation.
columns a1, . . . , ap and the p × n matrix B in terms of its rows bT

If we express the m × p matrix A in terms of its

1 , . . . , bT
p ,

A =� a1

··· ap � ,

B =

bT
1
...
bT
p

 ,

AB = a1bT

1 + ··· + apbT
p .

then we can express the product matrix AB as a sum of outer products:

182

10 Matrix multiplication

Complexity of matrix multiplication. The total number of ﬂops required for a
matrix-matrix product C = AB with A of size m × p and B of size p × n can be
found several ways. The product matrix C has size m×n, so there are mn elements
to compute. The i, j element of C is the inner product of row i of A with column
j of B. This is an inner product of vectors of length p and requires 2p − 1 ﬂops.
Therefore the total is mn(2p − 1) ﬂops, which we approximate as 2mnp ﬂops. The
order of computing the matrix-matrix product is mnp, the product of the three
dimensions involved.

In some special cases the complexity is less than 2mnp ﬂops. As an example,
when we compute the n × n Gram matrix G = BT B we only need to compute the
entries in the upper (or lower) half of G, since G is symmetric. This saves around
half the ﬂops, so the complexity is around pn2 ﬂops. But the order is the same.

Complexity of sparse matrix multiplication. Multiplying sparse matrices can be
done eﬃciently, since we don’t need to carry out any multiplications in which one
or the other entry is zero. We start by analyzing the complexity of multiplying
a sparse matrix with a non-sparse matrix. Suppose that A is m × p and sparse,
and B is p × n, but not necessarily sparse. The inner product of the ith row aT
i
of A with the jth column of B requires no more than 2 nnz(aT
i ) ﬂops. Summing
over i = 1, . . . , m and j = 1, . . . , n we get 2 nnz(A)n ﬂops.
If B is sparse, the
total number of ﬂops is no more that 2 nnz(B)m ﬂops. (Note that these formulas
agree with the one given above, 2mnp, when the sparse matrices have all entries
nonzero.)

There is no simple formula for the complexity of multiplying two sparse matri-

ces, but it is certainly no more than 2 min{nnz(A)n, nnz(B)m} ﬂops.

Complexity of matrix triple product. Consider the product of three matrices,

D = ABC

with A of size m × n, B of size n × p, and C of size p × q. The matrix D can be
computed in two ways, as (AB)C and as A(BC). In the ﬁrst method we start with
AB (2mnp ﬂops) and then form D = (AB)C (2mpq ﬂops), for a total of 2mp(n+q)
ﬂops. In the second method we compute the product BC (2npq ﬂops) and then
form D = A(BC) (2mnq ﬂops), for a total of 2nq(m + p) ﬂops.

You might guess that the total number of ﬂops required is the same with the
two methods, but it turns out it is not. The ﬁrst method is less expensive when
2mp(n + q) < 2nq(m + p), i.e., when

1
n

+

1
q

<

1
m

+

1
p

.

For example, if m = p and n = q, the ﬁrst method has a complexity proportional
to m2n, while the second method has complexity mn2, and one would prefer the
ﬁrst method when m � n.
As a more speciﬁc example, consider the product abT c, where a, b, c are n-
vectors. If we ﬁrst evaluate the outer product abT , the cost is n2 ﬂops, and we
need to store n2 values. We then multiply the vector c by this n× n matrix, which

10.2 Composition of linear functions

183

costs 2n2 ﬂops. The total cost is 3n2 ﬂops. On the other hand if we ﬁrst evaluate
the inner product bT c, the cost is 2n ﬂops, and we only need to store one number
(the result). Multiplying the vector a by this number costs n ﬂops, so the total cost
is 3n ﬂops. For n large, there is a dramatic diﬀerence between 3n and 3n2 ﬂops.
(The storage requirements are also dramatically diﬀerent for the two methods of
evaluating abT c: one number versus n2 numbers.)

10.2 Composition of linear functions

Matrix-matrix products and composition. Suppose A is an m × p matrix and B
is p × n. We can associate with these matrices two linear functions f : Rp → Rm
and g : Rn → Rp, deﬁned as f (x) = Ax and g(x) = Bx. The composition of the
two functions is the function h : Rn → Rm with

h(x) = f (g(x)) = A(Bx) = (AB)x.

In words: To ﬁnd h(x), we ﬁrst apply the function g, to obtain the partial result
g(x) (which is a p-vector); then we apply the function f to this result, to obtain
h(x) (which is an m-vector). In the formula h(x) = f (g(x)), f appears to the left
of g; but when we evaluate h(x), we apply g ﬁrst. The composition h is evidently
a linear function, that can be written as h(x) = Cx with C = AB.

Using this interpretation of matrix multiplication as composition of linear func-
tions, it is easy to understand why in general AB �= BA, even when the dimen-
sions are compatible. Evaluating the function h(x) = ABx means we ﬁrst evaluate
y = Bx, and then z = Ay. Evaluating the function BAx means we ﬁrst evaluate
y = Ax, and then z = By. In general, the order matters. As an example, take the
2 × 2 matrices

0

A =� −1
0 1 � ,
AB =� 0 −1
0 � ,

1

1

B =� 0
BA =�

1

0 � ,
0 � .

1

0
−1

for which

The mapping f (x) = Ax = (−x1, x2) changes the sign of the ﬁrst element of the
vector x. The mapping g(x) = Bx = (x2, x1) reverses the order of two elements
of x. If we evaluate f (g(x)) = ABx = (−x2, x1), we ﬁrst reverse the order, and
then change the sign of the ﬁrst element. This result is obviously diﬀerent from
g(f (x)) = BAx = (x2,−x1), obtained by changing the sign of the ﬁrst element,
and then reversing the order of the elements.

Second diﬀerence matrix. As a more interesting example of composition of linear
functions, consider the (n − 1) × n diﬀerence matrix Dn deﬁned in (6.5). (We use
the subscript n here to denote size of D.) Let Dn−1 denote the (n − 2) × (n − 1)
diﬀerence matrix. Their product Dn−1Dn is called the second diﬀerence matrix,
and sometimes denoted Δ.

184

10 Matrix multiplication

We can interpret Δ in terms of composition of linear functions. Multiplying an
n-vector x by Dn yields the (n − 1)-vector of consecutive diﬀerences of the entries:

Dnx = (x2 − x1, . . . , xn − xn−1).

Multiplying this vector by Dn−1 gives the (n − 2)-vector of consecutive diﬀerences
of consecutive diﬀerences (or second diﬀerences) of x:

Dn−1Dnx = (x1 − 2x2 + x3, x2 − 2x3 + x4, . . . , xn−2 − 2xn−1 + xn).

The (n − 2) × n product matrix Δ = Dn−1Dn is the matrix associated with the
second diﬀerence function.

For the case n = 5, Δ = Dn−1Dn has the form



1 −2
1
1 −2
0
0
0

0
1
1 −2

0
0

1  =

−1
1
0 −1
0

0
1
0 −1

0
0

1 



−1
1
0
0 −1
1
0 −1
0
0
0

0
0
1
0 −1

0
0
0
1

 .

The left-hand matrix Δ is associated with the second diﬀerence linear function
that maps 5-vectors into 3-vectors. The middle matrix D4 is associated with the
diﬀerence function that maps 4-vectors into 3-vectors. The right-hand matrix D5
is associated with the diﬀerence function that maps 5-vectors into 4-vectors.

Composition of aﬃne functions. The composition of aﬃne functions is an aﬃne
function. Suppose f : Rp → Rm is the aﬃne function given by f (x) = Ax + b, and
g : Rn → Rp is the aﬃne function given by g(x) = Cx + d. The composition h is
given by

h(x) = f (g(x)) = A(Cx + d) + b = (AC)x + (Ad + b) = ˜Ax + ˜b,

where ˜A = AC, ˜b = Ad + b.

Chain rule of diﬀerentiation. Let f : Rp → Rm and g : Rn → Rp be dif-
ferentiable functions. The composition of f and g is deﬁned as the function
h : Rn → Rm with

h(x) = f (g(x)) = f (g1(x), . . . , gp(x)).

The function h is diﬀerentiable and its partial derivatives follow from those of f
and g via the chain rule:

∂hi
∂xj

(z) =

∂fi
∂y1

(g(z))

∂g1
∂xj

(z) + ··· +

∂fi
∂yp

(g(z))

∂gp
∂xj

(z)

for i = 1, . . . , m and j = 1, . . . , n. This relation can be expressed concisely as a
matrix-matrix product: The derivative matrix of h at z is the product

Dh(z) = Df (g(z))Dg(z)

10.2 Composition of linear functions

185

of the derivative matrix of f at g(z) and the derivative matrix of g at z. This
compact matrix formula generalizes the chain rule for scalar-valued functions of a
single variable, i.e., h�(z) = f�(g(z))g�(z).

The ﬁrst order Taylor approximation of h at z can therefore be written as

ˆh(x) = h(z) + Dh(z)(x − z)

= f (g(z)) + Df (g(z))Dg(z)(x − z).

The same result can be interpreted as a composition of two aﬃne functions, the
ﬁrst order Taylor approximation of f at g(z),

ˆf (y) = f (g(z)) + Df (g(z))(y − g(z))

and the ﬁrst order Taylor approximation of g at z,

ˆg(x) = g(z) + Dg(z)(x − z).

The composition of these two aﬃne functions is
ˆf (ˆg(x)) = ˆf (g(z) + Dg(z)(x − z))

= f (g(z)) + Df (g(z))(g(z) + Dg(z)(x − z) − g(z))
= f (g(z)) + Df (g(z))Dg(z)(x − z)

which is equal to ˆh(x).

When f is a scalar-valued function (m = 1), the derivative matrices Dh(z) and

Df (g(z)) are the transposes of the gradients, and we write the chain rule as

∇h(z) = Dg(z)T∇f (g(z)).

In particular, if g(x) = Ax + b is aﬃne, then the gradient of h(x) = f (g(x)) =
f (Ax + b) is given by ∇h(z) = AT∇f (Ax + b).
Linear dynamical system with state feedback. We consider a time-invariant lin-
ear dynamical system with n-vector state xt and m-vector input ut, with dynamics

xt+1 = Axt + But,

t = 1, 2, . . . .

Here we think of the input ut as something we can manipulate, e.g., the control
surface deﬂections for an airplane or the amount of material we order or move in a
supply chain. In state feedback control the state xt is measured, and the input ut
is a linear function of the state, expressed as

ut = Kxt,

where K is the m × n state-feedback gain matrix. The term feedback refers to
the idea that the state is measured, and then (after multiplying by K) fed back
into the system, via the input. This leads to a loop, where the state aﬀects the
input, and the input aﬀects the (next) state. State feedback is very widely used
in many applications. (In §17.2.3 we will see methods for choosing or designing an
appropriate state feedback matrix.)

186

10 Matrix multiplication

With state feedback, we have

xt+1 = Axt + But = Axt + B(Kxt) = (A + BK)xt,

t = 1, 2, . . . .

This recursion is called the closed-loop system. The matrix A + BK is called the
closed-loop dynamics matrix. (In this context, the recursion xt+1 = Axt is called
the open-loop system. It gives the dynamics when ut = 0.)

10.3 Matrix power

It makes sense to multiply a square matrix A by itself to form AA. We refer to
this matrix as A2. Similarly, if k is a positive integer, then k copies of A multiplied
together is denoted Ak.
If k and l are positive integers, and A is square, then
AkAl = Ak+l and (Ak)l = Akl. By convention we take A0 = I, which makes the
formulas above hold for all nonnegative integer values of k and l.

We should mention one ambiguity in matrix power notation that occasionally
arises. When A is a square matrix and T is a nonnegative integer, AT can mean
either the transpose of the matrix A or its T th power. Usually which is meant is
clear from the context, or the author explicitly states which meaning is intended.
To avoid this ambiguity, some authors use a diﬀerent symbol for the transpose,
such as AT (with the superscript in roman font) or A�, or avoid referring to the
T th power of a matrix. When A is not square there is no ambiguity, since AT can
only be the transpose in this case.

Other matrix powers. Matrix powers Ak with k a negative integer will be dis-
cussed in §11.2. Non-integer powers, such as A1/2 (the matrix squareroot), need
not make sense, or can be ambiguous, unless certain conditions on A hold. This is
an advanced topic in linear algebra that we will not pursue in this book.

Paths in a directed graph. Suppose A is the n× n adjacency matrix of a directed
graph with n vertices:

Aij =� 1

0

there is a edge from vertex j to vertex i
otherwise

(see page 112). A path of length � is a sequence of � + 1 vertices, with an edge
from the ﬁrst to the second vertex, an edge from the second to third vertex, and
so on. We say the path goes from the ﬁrst vertex to the last one. An edge can be
considered a path of length one. By convention, every vertex has a path of length
zero (from the vertex to itself).

The elements of the matrix powers A� have a simple meaning in terms of paths
in the graph. First examine the expression for the i, j element of the square of A:

(A2)ij =

AikAkj.

n�k=1

10.3 Matrix power

187

2

1

5

3

4

Figure 10.1 Directed graph.

Each term in the sum is 0 or 1, and equal to one only if there is an edge from vertex
j to vertex k and an edge from vertex k to vertex i, i.e., a path of length exactly
two from vertex j to vertex i via vertex k. By summing over all k, we obtain the
total number of paths of length two from j to i.

The adjacency matrix A for the graph in ﬁgure 10.1, for example, and its square

are given by

A =



0
1
0
1
0

1
0
0
0
0

0
1
1
0
0

0
0
1
0
1

1
0
1
0
0



,

A2 =



1
0
1
0
1

0
1
0
1
0

1
1
1
0
0

1
1
2
0
0

0
2
1
1
0

.



We can verify there is exactly one path of length two from vertex 1 to itself, i.e.,
the path (1, 2, 1)), and one path of length two from vertex 3 to vertex 1, i.e., the
path (3, 2, 1). There are two paths of length two from vertex 4 to vertex 3, (4, 3, 3)
and (4, 5, 3), so (A2)34 = 2.

The property extends to higher powers of A.

If � is a positive integer, then
the i, j element of A� is the number of paths of length � from vertex j to vertex i.
This can be proved by induction on �. We have already shown the result for � = 2.
Assume that it is true that the elements of A� give the paths of length � between
the diﬀerent vertices. Consider the expression for the i, j element of A�+1:

(A�+1)ij =

Aik(A�)kj.

n�k=1

The kth term in the sum is equal to the number of paths of length � from j to k if
there is an edge from k to i, and is equal to zero otherwise. Therefore it is equal
to the number of paths of length � + 1 from j to i that end with the edge (k, i),
i.e., of the form (j, . . . , k, i). By summing over all k we obtain the total number of
paths of length � + 1 from vertex j to i.

188

10 Matrix multiplication

r
o
t
c
a
F

1.5

1

0.5

0

0

10 20 30 40 50 60 70 80 90 100

Age

Figure 10.2 Contribution factor per age in 2010 to the total population in
2020. The value for age i − 1 is the ith component of the row vector 1T A10.

This can be veriﬁed in the example. The third power of A is

A3 =



1
2
2
1
0

1
0
1
0
1

1
2
1
1
0

1
3
2
1
0

2
1
2
0
1

.



The (A3)24 = 3 paths of length three from vertex 4 to vertex 2 are (4, 3, 3, 2),
(4, 5, 3, 2), (4, 5, 1, 2).

Linear dynamical system. Consider a time-invariant linear dynamical system,
described by xt+1 = Axt. We have xt+2 = Axt+1 = A(Axt) = A2xt. Continuing
this argument, we have

xt+� = A�xt,

for � = 1, 2, . . .. In a linear dynamical system, we can interpret A� as the matrix
that propagates the state forward � time steps.

For example, in a population dynamics model, A� is the matrix that maps the
current population distribution into the population distribution � periods in the
future, taking into account births, deaths, and the births and deaths of children,
and so on. The total population � periods in the future is given by 1T (A�xt), which
we can write as (1T A�)xt. The row vector 1T A� has an interesting interpretation:
Its ith entry is the contribution to the total population in � periods due to each
person with current age i − 1. It is plotted in ﬁgure 10.2 for the US data given
in §9.2.

10.4 QR factorization

189

Matrix powers also come up in the analysis of a time-invariant linear dynamical

system with an input. We have

xt+2 = Axt+1 + But+1 = A(Axt + But) = A2xt + ABut + But+1.

Iterating this over � periods we obtain

xt+� = A�xt + A�−1But + A�−2But+1 + ··· + But+�−1.

(10.2)

(The ﬁrst term agrees with the formula for xt+� with no input.) The other terms
are readily interpreted. The term AjBut+�−j is the contribution to the state xt+�
due to the input at time t + � − j.

10.4 QR factorization

Matrices with orthonormal columns. As an application of Gram matrices, we
can express the condition that the n-vectors a1, . . . , ak are orthonormal in a simple
way using matrix notation:

AT A = I,

where A is the n× k matrix with columns a1, . . . , ak. There is no standard term for
a matrix whose columns are orthonormal: We refer to a matrix whose columns are
orthonormal as ‘a matrix whose columns are orthonormal’. But a square matrix
that satisﬁes AT A = I is called orthogonal ; its columns are an orthonormal basis.
Orthogonal matrices have many uses, and arise in many applications.

We have already encountered some orthogonal matrices, including identity ma-

trices, 2-D reﬂections and rotations (page 129), and permutation matrices (page 132).

Norm, inner product, and angle properties. Suppose the columns of the m × n
matrix A are orthonormal, and x and y are any n-vectors. We let f : Rn → Rm
be the function that maps z to Az. Then we have the following:

• �Ax� = �x�. That is, f is norm preserving.
• (Ax)T (Ay) = xT y. f preserves the inner product between vectors.
• � (Ax, Ay) = � (x, y). f also preserves angles between vectors.

Note that in each of the three equations above, the vectors appearing in the left-
and right-hand sides have diﬀerent dimensions, m on the left and n on the right.

We can verify these properties using simple matrix properties. We start with
the second statement, that multiplication by A preserves the inner product. We
have

(Ax)T (Ay) = (xT AT )(Ay)

= xT (AT A)y
= xT Iy
= xT y.

190

10 Matrix multiplication

In the ﬁrst line, we use the transpose-of-product rule; in the second, we re-associate
a product of 4 matrices (considering the row vector xT and column vector x as
matrices); in the third line, we use AT A = I; and in the fourth line, we use Iy = y.
From the second property we can derive the ﬁrst one: By taking y = x we get
(Ax)T (Ax) = xT x; taking the squareroot of each side gives �Ax� = �x�. The third
property, angle preservation, follows from the ﬁrst two, since

� (Ax, Ay) = arccos� (Ax)T (Ay)

�Ax��Ay�� = arccos� xT y

�x��y�� = � (x, y).

QR factorization. We can express the result of the Gram–Schmidt algorithm de-
scribed in §5.4 in a compact form using matrices. Let A be an n × k matrix
with linearly independent columns a1, . . . , ak. By the independence-dimension in-
equality, A is tall or square. Let Q be the n × k matrix with columns q1, . . . , qk,
the orthonormal vectors produced by the Gram–Schmidt algorithm applied to the
n-vectors a1, . . . , ak. Orthonormality of q1, . . . , qk is expressed in matrix form as
QT Q = I. We express the equation relating ai and qi,

ai = (qT

1 ai)q1 + ··· + (qT

i−1ai)qi−1 + �˜qi�qi,

where ˜qi is the vector obtained in the ﬁrst step of the Gram–Schmidt algorithm, as

ai = R1iq1 + ··· + Riiqi,

where Rij = qT
write the equations above in compact matrix form as

i aj for i < j and Rii = �˜qi�. Deﬁning Rij = 0 for i > j, we can

A = QR.

This is called the QR factorization of A, since it expresses the matrix A as a
product of two matrices, Q and R. The n × k matrix Q has orthonormal columns,
and the k × k matrix R is upper triangular, with positive diagonal elements. If
A is square, with linearly independent columns, then Q is orthogonal and the QR
factorization expresses A as a product of two square matrices.

The attributes of the matrices Q and R in the QR factorization come directly
from the Gram–Schmidt algorithm. The equation QT Q = I follows from the
orthonormality of the vectors q1, . . . , qk. The matrix R is upper triangular because
each vector ai is a linear combination of q1, . . . , qi.

The Gram–Schmidt algorithm is not the only algorithm for QR factorization.
Several other QR factorization algorithms exist, that are more reliable in the pres-
ence of round-oﬀ errors. (These QR factorization methods may also change the
order in which the columns of A are processed.)

Sparse QR factorization. There are algorithms for QR factorization that eﬃ-
ciently handle the case when the matrix A is sparse. In this case the matrix Q is
stored in a special format that requires much less memory than if it were stored
as a generic n × k matrix, i.e., nk numbers. The ﬂop count for these sparse QR
factorizations is also much smaller than 2nk2.

Exercises

Exercises

191

10.1 Scalar-row-vector multiplication. Suppose a is a number and x = [x1 ··· xn] is an n-
row-vector. The scalar-row-vector product ax is the n-row-vector [ax1 ··· axn]. Is this a
special case of matrix-matrix multiplication? That is, can you interpret scalar-row-vector
multiplication as matrix multiplication? (Recall that scalar-vector multiplication, with
the scalar on the left, is not a special case of matrix-matrix multiplication; see page 177.)
10.2 Ones matrix. There is no special notation for an m × n matrix all of whose entries are
one. Give a simple expression for this matrix in terms of matrix multiplication, transpose,
and the ones vectors 1m, 1n (where the subscripts denote the dimension).

10.3 Matrix sizes. Suppose A, B, and C are matrices that satisfy A + BBT = C. Determine
which of the following statements are necessarily true. (There may be more than one true
statement.)

(a) A is square.

(b) A and B have the same dimensions.

(c) A, B, and C have the same number of rows.

(d) B is a tall matrix.

10.4 Block matrix notation. Consider the block matrix

A =

0
0

0 BBT  ,

B
0

I
BT
0

where B is 10 × 5. What are the dimensions of the four zero matrices and the identity
matrix in the deﬁnition of A? What are the dimensions of A?

10.5 When is the outer product symmetric? Let a and b be n-vectors. The inner product is
symmetric, i.e., we have aT b = bT a. The outer product of the two vectors is generally
not symmetric; that is, we generally have abT �= baT . What are the conditions on a and b
under which ab = baT ? You can assume that all the entries of a and b are nonzero. (The
conclusion you come to will hold even when some entries of a or b are zero.) Hint. Show
that abT = baT implies that ai/bi is a constant (i.e., independent of i).

10.6 Product of rotation matrices. Let A be the 2 × 2 matrix that corresponds to rotation by
θ radians, deﬁned in (7.1), and let B be the 2 × 2 matrix that corresponds to rotation by
ω radians. Show that AB is also a rotation matrix, and give the angle by which it rotates
vectors. Verify that AB = BA in this case, and give a simple English explanation.

10.7 Two rotations. Two 3-vectors x and y are related as follows. First, the vector x is rotated
40◦ around the e3 axis, counterclockwise (from e1 toward e2), to obtain the 3-vector z.
Then, z is rotated 20◦ around the e1 axis, counterclockwise (from e2 toward e3), to form y.
Find the 3 × 3 matrix A for which y = Ax. Verify that A is an orthogonal matrix. Hint.
Express A as a product of two matrices, which carry out the two rotations described
above.

10.8 Entries of matrix triple product. (See page 182.) Suppose A has dimensions m× n, B has

dimensions n × p, C has dimensions p × q, and let D = ABC. Show that

Dij =

n�k=1

p�l=1

AikBklClj.

This is the formula analogous to (10.1) for the product of two matrices.

10.9 Multiplication by a diagonal matrix. Suppose that A is an m × n matrix, D is a diagonal
matrix, and B = DA. Describe B in terms of A and the entries of D. You can refer to
the rows or columns or entries of A.

192

10 Matrix multiplication

10.10 Converting from purchase quantity matrix to purchase dollar matrix. An n × N matrix
Q gives the purchase history of a set of n products by N customers, over some period,
with Qij being the quantity of product i bought by customer j. The n-vector p gives
the product prices. A data analyst needs the n × N matrix D, where Dij is the total
dollar value that customer j spent on product i. Express D in terms of Q and p, using
compact matrix/vector notation. You can use any notation or ideas we have encountered,
e.g., stacking, slicing, block matrices, transpose, matrix-vector product, matrix-matrix
product, inner product, norm, correlation, diag(), and so on.

10.11 Trace of matrix-matrix product. The sum of the diagonal entries of a square matrix is

called the trace of the matrix, denoted tr(A).

(a) Suppose A and B are m × n matrices. Show that

tr(AT B) =

m�i=1

n�j=1

AijBij.

What is the complexity of calculating tr(AT B)?

(b) The number tr(AT B) is sometimes referred to as the inner product of the matrices
A and B. (This allows us to extend concepts like angle to matrices.) Show that
tr(AT B) = tr(BT A).

the trace of its Gram matrix.

(c) Show that tr(AT A) = �A�2. In other words, the square of the norm of a matrix is
(d) Show that tr(AT B) = tr(BAT ), even though in general AT B and BAT can have
diﬀerent dimensions, and even when they have the same dimensions, they need not
be equal.

10.12 Norm of matrix product. Suppose A is an m × p matrix and B is a p × n matrix. Show
that �AB� ≤ �A��B�, i.e., the (matrix) norm of the matrix product is no more than
the product of the norms of the matrices. Hint. Let aT
m be the rows of A, and
b1, . . . , bn be the columns of B. Then

1 , . . . , aT

�AB�2 =

m�i=1

n�j=1

(aT

i bj)2.

Now use the Cauchy–Schwarz inequality.

10.13 Laplacian matrix of a graph. Let A be the incidence matrix of a directed graph with n
nodes and m edges (see §7.3). The Laplacian matrix associated with the graph is deﬁned
as L = AAT , which is the Gram matrix of AT .
It is named after the mathematician
Pierre-Simon Laplace.
(a) Show that D(v) = vT Lv, where D(v) is the Dirichlet energy deﬁned on page 135.
(b) Describe the entries of L. Hint. The following two quantities might be useful: The
degree of a node, which is the number of edges that connect to the node (in either
direction), and the number of edges that connect a pair of distinct nodes (in either
direction).

10.14 Gram matrix. Let a1, . . . , an be the columns of the m × n matrix A. Suppose that the
columns all have norm one, and for i �= j, � (ai, aj) = 60◦. What can you say about the
Gram matrix G = AT A? Be as speciﬁc as you can be.

10.15 Pairwise distances from Gram matrix. Let A be an m×n matrix with columns a1, . . . , an,
and associated Gram matrix G = AT A. Express �ai − aj� in terms of G, speciﬁcally Gii,
Gij, and Gjj.

Exercises

193

10.16 Covariance matrix. Consider a list of k n-vectors a1, . . . , ak, and deﬁne the n × k matrix

A = [a1 ··· ak].
(a) Let the k-vector µ give the means of the columns, i.e., µi = avg(ai), i = 1, . . . , k.
(The symbol µ is a traditional one to denote an average value.) Give an expression
for µ in terms of the matrix A.

(b) Let ˜a1, . . . , ˜ak be the de-meaned versions of a1, . . . , ak, and deﬁne ˜A as the n × k

matrix ˜A = [˜a1 ··· ˜ak]. Give a matrix expression for ˜A in terms of A and µ.

(c) The covariance matrix of the vectors a1, . . . , ak is the k × k matrix Σ = (1/N ) ˜AT ˜A,

the Gram matrix of ˜A multiplied with 1/N . Show that

Σij =� std(ai)2

std(ai) std(aj)ρij

i = j
i �= j

where ρij is the correlation coeﬃcient of ai and aj. (The expression for i �= j assumes
that ρij is deﬁned, i.e., std(ai) and std(aj) are nonzero. If not, we interpret the
formula as Σij = 0.) Thus the covariance matrix encodes the standard deviations
of the vectors, as well as correlations between all pairs. The correlation matrix is
widely used in probability and statistics.

(d) Let z1, . . . , zk be the standardized versions of a1, . . . , ak. (We assume the de-meaned
vectors are nonzero.) Derive a matrix expression for Z = [z1 ··· zk], the ma-
trix of standardized vectors. Your expression should use A, µ, and the numbers
std(a1), . . . , std(ak).

10.17 Patients and symptoms. Each of a set of N patients can exhibit any number of a set of n

symptoms. We express this as an N × n matrix S, with

Sij =� 1 patient i exhibits symptom j

0 patient i does not exhibit symptom j.

Give simple English descriptions of the following expressions.
and describe the entries.

Include the dimensions,

(a) S1.
(b) ST 1.
(c) ST S.
(d) SST .

10.18 Students, classes, and majors. We consider m students, n classes, and p majors. Each
student can be in any number of the classes (although we’d expect the number to range
from 3 to 6), and can have any number of the majors (although the common values would
be 0, 1, or 2). The data about the students’ classes and majors are given by an m × n
matrix C and an m × p matrix M , where

Cij =� 1
Mij =� 1

0

0

student i is in class j
student i is not in class j,

student i is in major j
student i is not in major j.

and

(a) Let E be the n-vector with Ei being the enrollment in class i. Express E using

matrix notation, in terms of the matrices C and M .

(b) Deﬁne the n × p matrix S where Sij is the total number of students in class i with

major j. Express S using matrix notation, in terms of the matrices C and M .

194

10 Matrix multiplication

10.19 Student group membership. Let G ∈ Rm×n represent a contingency matrix of m students

who are members of n groups:

Gij =� 1

0

student i is in group j
student i is not in group j.

(A student can be in any number of the groups.)

(a) What is the meaning of the 3rd column of G?

(b) What is the meaning of the 15th row of G?

(c) Give a simple formula (using matrices, vectors, etc.) for the n-vector M , where Mi

is the total membership (i.e., number of students) in group i.

(d) Interpret (GGT )ij in simple English.
(e) Interpret (GT G)ij in simple English.

10.20 Products, materials, and locations. P diﬀerent products each require some amounts of M
diﬀerent materials, and are manufactured in L diﬀerent locations, which have diﬀerent
material costs. We let Clm denote the cost of material m in location l, for l = 1, . . . , L and
m = 1, . . . , M . We let Qmp denote the amount of material m required to manufacture
one unit of product p, for m = 1, . . . , M and p = 1, . . . , P . Let Tpl denote the total cost to
manufacture product p in location l, for p = 1, . . . , P and l = 1, . . . , L. Give an expression
for the matrix T .

10.21 Integral of product of polynomials. Let p and q be two quadratic polynomials, given by

p(x) = c1 + c2x + c3x2,

q(x) = d1 + d2x + d3x2.

Express the integral J =� 1

Give the entries of G (as numbers).

p(x)q(x) dx in the form J = cT Gd, where G is a 3× 3 matrix.

0

10.22 Composition of linear dynamical systems. We consider two time-invariant linear dynami-

cal systems with outputs. The ﬁrst one is given by

xt+1 = Axt + But,

yt = Cxt,

t = 1, 2, . . . ,

with state xt, input ut, and output yt. The second is given by

˜xt+1 = ˜A˜xt + ˜Bwt,

vt = ˜C ˜xt,

t = 1, 2, . . . ,

with state ˜xt, input wt, and output vt. We now connect the output of the ﬁrst linear
dynamical system to the input of the second one, which means we take wt = yt. (This
is called the composition of the two systems.) Show that this composition can also be
expressed as a linear dynamical system with state zt = (xt, ˜xt), input ut, and output vt.
(Give the state transition matrix, input matrix, and output matrix.)

10.23 Suppose A is an n × n matrix that satisﬁes A2 = 0. Does this imply that A = 0? (This
is the case when n = 1.) If this is (always) true, explain why. If it is not, give a speciﬁc
counterexample, i.e., a matrix A that is nonzero but satisﬁes A2 = 0.
10.24 Matrix power identity. A student says that for any square matrix A,

(A + I)3 = A3 + 3A2 + 3A + I.

Is she right? If she is, explain why; if she is wrong, give a speciﬁc counterexample, i.e., a
square matrix A for which it does not hold.

10.25 Squareroots of the identity. The number 1 has two squareroots (i.e., numbers who square

is 1), 1 and −1. The n × n identity matrix In has many more squareroots.
(a) Find all diagonal squareroots of In. How many are there? (For n = 1, you should

get 2.)

Exercises

195

(b) Find a nondiagonal 2× 2 matrix A that satisﬁes A2 = I. This means that in general

there are even more squareroots of In than you found in part (a).

10.26 Circular shift matrices. Let A be the 5 × 5 matrix
0
0
0
0
1

0
1
0
0
0

0
0
0
1
0

0
0
1
0
0

A =

1
0
0
0
0

.



(a) How is Ax related to x? Your answer should be in English. Hint. See exercise title.
(b) What is A5? Hint. The answer should make sense, given your answer to part (a).

10.27 Dynamics of an economy. Let x1, x2, . . . be n-vectors that give the level of economic
activity of a country in years 1, 2, . . ., in n diﬀerent sectors (like energy, defense, manu-
facturing). Speciﬁcally, (xt)i is the level of economic activity in economic sector i (say,
in billions of dollars) in year t. A common model that connects these economic activity
vectors is xt+1 = Bxt, where B is an n × n matrix. (See exercise 9.2.)
Give a matrix expression for the total economic activity across all sectors in year t = 6,
in terms of the matrix B and the vector of initial activity levels x1. Suppose you can
increase economic activity in year t = 1 by some ﬁxed amount (say, one billion dollars)
in one sector, by government spending. How should you choose which sector to stimulate
so as to maximize the total economic output in year t = 6?

10.28 Controllability matrix. Consider the time-invariant linear dynamical system xt+1 = Axt +
But, with n-vector state xt and m-vector input ut. Let U = (u1, u2, . . . , uT−1) denote
the sequence of inputs, stacked in one vector. Find the matrix CT for which

xT = AT−1x1 + CT U

holds. The ﬁrst term is what xT would be if u1 = ··· = uT−1 = 0; the second term
shows how the sequence of inputs u1, . . . , uT−1 aﬀect xT . The matrix CT is called the
controllability matrix of the linear dynamical system.

10.29 Linear dynamical system with 2× down-sampling. We consider a linear dynamical system

with n-vector state xt, m-vector input ut, and dynamics given by

xt+1 = Axt + But,

t = 1, 2, . . . ,

where A is n × n matrix A and B is n × m. Deﬁne zt = x2t−1 for t = 1, 2, . . ., i.e.,

z1 = x1,

z2 = x3,

z3 = x5, . . . .

(The sequence zt is the original state sequence xt ‘down-sampled’ by 2×.) Deﬁne the
(2m)-vectors wt as wt = (u2t−1, u2t) for t = 1, 2, . . ., i.e.,

w1 = (u1, u2), w2 = (u3, u4), w3 = (u5, u6), . . . .

(Each entry of the sequence wt is a stack of two consecutive original inputs.) Show that
zt, wt satisfy the linear dynamics equation zt+1 = F zt + Gwt, for t = 1, 2, . . .. Give the
matrices F and G in terms of A and B.

10.30 Cycles in a graph. A cycle of length � in a directed graph is a path of length � that starts
and ends at the same vertex. Determine the total number of cycles of length � = 10 for
the directed graph given in the example on page 187. Break this number down into the
number of cycles that begin (and end) at vertex 1, vertex 2, . . . , vertex 5. (These should
add up to the total.) Hint. Do not count the cycles by hand.

196

10 Matrix multiplication

10.31 Diameter of a graph. A directed graph with n vertices is described by its n× n adjacency

matrix A (see §10.3).
(a) Derive an expression Pij for the total number of paths, with length no more than k,
from vertex j to vertex i. (We include in this total the number of paths of length
zero, which go from each vertex j to itself.) Hint. You can derive an expression for
the matrix P , in terms of the matrix A.

(b) The diameter D of a graph is the smallest number for which there is a path of length
≤ D from node j to node i, for every pair of vertices j and i. Using part (a), explain
how to compute the diameter of a graph using matrix operations (such as addition,
multiplication).

Remark. Suppose the vertices represent all people on earth, and the graph edges represent
acquaintance, i.e., Aij = 1 if person j and person i are acquainted.
(This graph is
symmetric.) Even though n is measured in billions, the diameter of this acquaintance
graph is thought to be quite small, perhaps 6 or 7.
In other words, any two people
on earth can be connected though a set of 6 or 7 (or fewer) acquaintances. This idea,
originally conjectured in the 1920s, is sometimes called six degrees of separation.

10.32 Matrix exponential. You may know that for any real number a, the sequence (1 + a/k)k
converges as k → ∞ to the exponential of a, denoted exp a or ea. The matrix exponential
of a square matrix A is deﬁned as the limit of the matrix sequence (I + A/k)k as k → ∞.
(It can shown that this sequence always converges.) The matrix exponential arises in
many applications, and is covered in more advanced courses on linear algebra.

(a) Find exp 0 (the zero matrix) and exp I.

(b) Find exp A, for A =� 0

0

1

0 �.

10.33 Matrix equations. Consider two m × n matrices A and B. Suppose that for j = 1, . . . , n,
the jth column of A is a linear combination of the ﬁrst j columns of B. How do we
express this as a matrix equation? Choose one of the matrix equations below and justify
your choice.

(a) A = GB for some upper triangular matrix G.

(b) A = BH for some upper triangular matrix H.

(c) A = F B for some lower triangular matrix F .

(d) A = BJ for some lower triangular matrix J.

10.34 Choose one of the responses always, never, or sometimes for each of the statements
below. ‘Always’ means the statement is always true, ‘never’ means it is never true, and
‘Sometimes’ means it can be true or false, depending on the particular values of the matrix
or matrices. Give a brief justiﬁcation of each answer.

(a) An upper triangular matrix has linearly independent columns.

(b) The rows of a tall matrix are linearly dependent.

(c) The columns of A are linearly independent, and AB = 0 for some nonzero matrix B.

10.35 Orthogonal matrices. Let U and V be two orthogonal n × n matrices. Show that the

matrix U V and the (2n) × (2n) matrix
1

√2� U

V −V �

U

are orthogonal.

Exercises

197

10.36 Quadratic form. Suppose A is an n × n matrix and x is an n-vector. The triple product
xT Ax, a 1× 1 matrix which we consider to be a scalar (i.e., number), is called a quadratic
form of the vector x, with coeﬃcient matrix A. A quadratic form is the vector analog
of a quadratic function αu2, where α and u are both numbers. Quadratic forms arise in
many ﬁelds and applications.

(a) Show that xT Ax =�n

i,j=1 Aijxixj.

(b) Show that xT (AT )x = xT Ax. In other words, the quadratic form with the trans-
posed coeﬃcient matrix has the same value for any x. Hint. Take the transpose of
the triple product xT Ax.

(c) Show that xT ((A + AT )/2)x = xT Ax.

In other words, the quadratic form with
coeﬃcient matrix equal to the symmetric part of a matrix (i.e., (A + AT )/2) has
the same value as the original quadratic form.

(d) Express 2x2

2 as a quadratic form, with symmetric coeﬃcient matrix A.
10.37 Orthogonal 2 × 2 matrices. In this problem, you will show that every 2 × 2 orthogonal

1 − 3x1x2 − x2

matrix is either a rotation or a reﬂection (see §7.1).
(a) Let

Q =� a

c

b

d �

be an orthogonal 2 × 2 matrix. Show that the following equations hold:

a2 + c2 = 1,

b2 + d2 = 1,

ab + cd = 0.

(b) Deﬁne s = ad − bc. Combine the three equalities in part (a) to show that

|s| = 1,

b = −sc,

d = sa.

(c) Suppose a = cos θ. Show that there are two possible matrices Q: A rotation (coun-
terclockwise over θ radians), and a reﬂection (through the line that passes through
the origin at an angle of θ/2 radians with respect to horizontal).

10.38 Orthogonal matrix with nonnegative entries. Suppose the n × n matrix A is orthogonal,
and all of its entries are nonnegative, i.e., Aij ≥ 0 for i, j = 1, . . . , n. Show that A must
be a permutation matrix, i.e., each entry is either 0 or 1, each row has exactly one entry
with value one, and each column has exactly one entry with value one. (See page 132.)

10.39 Gram matrix and QR factorization. Suppose the matrix A has linearly independent
columns and QR factorization A = QR. What is the relationship between the Gram
matrix of A and the Gram matrix of R? What can you say about the angles between the
columns of A and the angles between the columns of R?

10.40 QR factorization of ﬁrst i columns of A. Suppose the n×k matrix A has QR factorization

A = QR. We deﬁne the n × i matrices

Ai =� a1

···

ai � ,

Qi =� q1

···

qi � ,

for i = 1, . . . , k. Deﬁne the i × i matrix Ri as the submatrix of R containing its ﬁrst i
rows and columns, for i = 1, . . . , k. Using index range notation, we have

Ai = A1:n,1:i, Qi = A1:n,1:i, Ri = R1:i,1:i.

Show that Ai = QiRi is the QR factorization of Ai. This means that when you compute
the QR factorization of A, you are also computing the QR factorization of all submatrices
A1, . . . , Ak.

198

10 Matrix multiplication

10.41 Clustering via k-means as an approximate matrix factorization. Suppose we run the
k-means algorithm on the N n-vectors x1, . . . , xN , to obtain the group representatives
z1, . . . , zk. Deﬁne the matrices

X = [ x1

··· xN ],

Z = [ z1

···

zN ].

X has size n × N and Z has size n × k. We encode the assignment of vectors to groups
by the k × N clustering matrix C, with Cij = 1 if xj is assigned to group i, and Cij = 0
otherwise. Each column of C is a unit vector; its transpose is a selector matrix.
(a) Give an interpretation of the columns of the matrix X − ZC, and the squared norm

(matrix) norm �X − ZC�2.
(b) Justify the following statement: The goal of the k-means algorithm is to ﬁnd an
n × k matrix Z, and a k × N matrix C, which is the transpose of a selector matrix,
so that �X − ZC� is small, i.e., X ≈ ZC.

10.42 A matrix-vector multiplication Ax of an n× n matrix A and an n-vector x takes 2n2 ﬂops
in general. Formulate a faster method, with complexity linear in n, for matrix-vector
multiplication with the matrix A = I + abT , where a and b are given n-vectors.

10.43 A particular computer takes about 0.2 seconds to multiply two 1500 × 1500 matrices.
About how long would you guess the computer takes to multiply two 3000×3000 matrices?
Give your prediction (i.e., the time in seconds), and your (very brief) reasoning.

10.44 Complexity of matrix quadruple product. (See page 182.) We wish to compute the product

E = ABCD, where A is m × n, B is n × p, C is p × q, and D is q × r.
(a) Find all methods for computing E using three matrix-matrix multiplications. For
example, you can compute AB, CD, and then the product (AB)(CD). Give the
total number of ﬂops required for each of these methods. Hint. There are four other
methods.

(b) Which method requires the fewest ﬂops, with dimensions m = 10, n = 1000, p = 10,

q = 1000, r = 100?

Chapter 11

Matrix inverses

In this chapter we introduce the concept of matrix inverse. We show how matrix
inverses can be used to solve linear equations, and how they can be computed using
the QR factorization.

11.1 Left and right inverses

Recall that for a number a, its (multiplicative) inverse is the number x for which
xa = 1, which we usually denote as x = 1/a or (less frequently) x = a−1. The
inverse x exists provided a is nonzero. For matrices the concept of inverse is more
complicated than for scalars; in the general case, we need to distinguish between
left and right inverses. We start with the left inverse.

Left inverse. A matrix X that satisﬁes

XA = I

is called a left inverse of A. The matrix A is said to be left-invertible if a left
inverse exists. Note that if A has size m × n, a left inverse X will have size n × m,
the same dimensions as AT .

Examples.

• If A is a number (i.e., a 1 × 1 matrix), then a left inverse X is the same as
In this case, A is left-invertible whenever A is

the inverse of the number.
nonzero, and it has only one left inverse.

• Any nonzero n-vector a, considered as an n × 1 matrix, is left-invertible. For

any index i with ai �= 0, the row n-vector x = (1/ai)eT

i satisﬁes xa = 1.

• The matrix

A =

−3 −4
4
6
1

1 

200

11 Matrix inverses

has two diﬀerent left inverses:

B =

1

9� −11 −10

8 −11 � ,

16

7

C =

1

2� 0 −1

1 −4 � .

6

0

This can be veriﬁed by checking that BA = CA = I. The example illustrates
that a left-invertible matrix can have more than one left inverse. (In fact, if it
has more than one left inverse, then it has inﬁnitely many; see exercise 11.1.)

• A matrix A with orthonormal columns satisﬁes AT A = I, so it is left-

invertible; its transpose AT is a left inverse.

Left-invertibility and column independence.
If A has a left inverse C then the
columns of A are linearly independent. To see this, suppose that Ax = 0. Multi-
plying on the left by a left inverse C, we get

0 = C(Ax) = (CA)x = Ix = x,

which shows that the only linear combination of the columns of A that is 0 is the
one with all coeﬃcients zero.

We will see below that the converse is also true; a matrix has a left inverse if
and only if its columns are linearly independent. So the generalization of ‘a number
has an inverse if and only if it is nonzero’ is ‘a matrix has a left inverse if and only
if its columns are linearly independent’.

Dimensions of left inverses. Suppose the m × n matrix A is wide, i.e., m < n.
By the independence-dimension inequality, its columns are linearly dependent, and
therefore it is not left-invertible. Only square or tall matrices can be left-invertible.

Solving linear equations with a left inverse. Suppose that Ax = b, where A is
an m × n matrix and x is an n-vector. If C is a left inverse of A, we have

Cb = C(Ax) = (CA)x = Ix = x,

which means that x = Cb is a solution of the set of linear equations. The columns
of A are linearly independent (since it has a left inverse), so there is only one
solution of the linear equations Ax = b; in other words, x = Cb is the solution of
Ax = b.

Now suppose there is no x that satisﬁes the linear equations Ax = b, and
let C be a left inverse of A. Then x = Cb does not satisfy Ax = b, since no
vector satisﬁes this equation by assumption. This gives a way to check if the linear
equations Ax = b have a solution, and to ﬁnd one when there is one, provided we
have a left inverse of A. We simply test whether A(Cb) = b. If this holds, then we
have found a solution of the linear equations; if it does not, then we can conclude
that there is no solution of Ax = b.

In summary, a left inverse can be used to determine whether or not a solution of
an over-determined set of linear equations exists, and when it does, ﬁnd the unique
solution.

11.1 Left and right inverses

201

Right inverse. Now we turn to the closely related concept of right inverse. A
matrix X that satisﬁes

AX = I

is called a right inverse of A. The matrix A is right-invertible if a right inverse
exists. Any right inverse has the same dimensions as AT .

Left and right inverse of matrix transpose.
If A has a right inverse B, then
BT is a left inverse of AT , since BT AT = (AB)T = I. If A has a left inverse C,
then C T is a right inverse of AT , since AT C T = (CA)T = I. This observation
allows us to map all the results for left-invertibility given above to similar results
for right-invertibility. Some examples are given below.

• A matrix is right-invertible if and only if its rows are linearly independent.
• A tall matrix cannot have a right inverse. Only square or wide matrices can

be right-invertible.

Solving linear equations with a right inverse. Consider the set of m linear equa-
tions in n variables Ax = b. Suppose A is right-invertible, with right inverse B.
This implies that A is square or wide, so the linear equations Ax = b are square or
under-determined.

Then for any m-vector b, the n-vector x = Bb satisﬁes the equation Ax = b.

To see this, we note that

Ax = A(Bb) = (AB)b = Ib = b.

We can conclude that if A is right-invertible, then the linear equations Ax = b can
be solved for any vector b.
Indeed, x = Bb is a solution. (There can be other
solutions of Ax = b; the solution x = Bb is simply one of them.)

In summary, a right inverse can be used to ﬁnd a solution of a square or under-

determined set of linear equations, for any vector b.

Examples. Consider the matrix appearing in the example above on page 199,

−3 −4
4
6
1

1 

A =
8 −11 � ,

16

C =

1

2� 0 −1

1 −4 � .

6

0

and the two left inverses

B =

1

9� −11 −10

7

• The over-determined linear equations Ax = (1,−2, 0) have the unique solu-

tion x = (1,−1), which can be obtained from either left inverse:

x = B(1,−2, 0) = C(1,−2, 0).

• The over-determined linear equations Ax = (1,−1, 0) do not have a solution,

since x = C(1,−1, 0) = (1/2,−1/2) does not satisfy Ax = (1,−1, 0).

202

11 Matrix inverses

• The under-determined linear equations AT y = (1, 2) has (diﬀerent) solutions

BT (1, 2) = (1/3, 2/3, 38/9),

C T (1, 2) = (0, 1/2,−1).

(Recall that BT and C T are both right inverses of AT .) We can ﬁnd a solution
of AT y = b for any vector b.

Left and right inverse of matrix product. Suppose A and D are compatible for
the matrix product AD (i.e., the number of columns in A is equal to the number
of rows in D.) If A has a right inverse B and D has a right inverse E, then EB is
a right inverse of AD. This follows from

(AD)(EB) = A(DE)B = A(IB) = AB = I.

If A has a left inverse C and D has a left inverse F , then F C is a left inverse

of AD. This follows from

(F C)(AD) = F (CA)D = F D = I.

11.2

Inverse

If a matrix is left- and right-invertible, then the left and right inverses are unique
and equal. To see this, suppose that AX = I and Y A = I, i.e., X is any right
inverse and Y is any left inverse of A. Then we have

X = (Y A)X = Y (AX) = Y,

i.e., any left inverse of A is equal to any right inverse of A. This implies that the
left inverse is unique: If we have A ˜X = I, then the argument above tells us that
˜X = Y , so we have ˜X = X, i.e., there is only one right inverse of A. A similar
argument shows that Y (which is the same as X) is the only left inverse of A.

When a matrix A has both a left inverse Y and a right inverse X, we call the
matrix X = Y simply the inverse of A, and denote it as A−1. We say that A is
invertible or nonsingular. A square matrix that is not invertible is called singular.

Dimensions of invertible matrices.
Invertible matrices must be square, since tall
matrices are not right-invertible, while wide matrices are not left-invertible. A
matrix A and its inverse (if it exists) satisfy

AA−1 = A−1A = I.

If A has inverse A−1, then the inverse of A−1 is A;
in other words, we have
(A−1)−1 = A. For this reason we say that A and A−1 are inverses (of each other).

11.2

Inverse

203

Solving linear equations with the inverse. Consider the square system of n linear
equations with n variables, Ax = b. If A is invertible, then for any n-vector b,

x = A−1b

(11.1)

is a solution of the equations. (This follows since A−1 is a right inverse of A.)
Moreover, it is the only solution of Ax = b. (This follows since A−1 is a left inverse
of A.) We summarize this very important result as

The square system of linear equations Ax = b, with A invertible, has
the unique solution x = A−1b, for any n-vector b.

One immediate conclusion we can draw from the formula (11.1) is that the
solution of a square set of linear equations is a linear function of the right-hand
side vector b.

Invertibility conditions. For square matrices, left-invertibility, right-invertibility,
and invertibility are equivalent: If a matrix is square and left-invertible, then it is
also right-invertible (and therefore invertible) and vice-versa.

To see this, suppose A is an n × n matrix and left-invertible. This implies that
the n columns of A are linearly independent. Therefore they form a basis and so
any n-vector can be expressed as a linear combination of the columns of A.
In
particular, each of the n unit vectors ei can be expressed as ei = Abi for some

n-vector bi. The matrix B =� b1
AB =� Ab1 Ab2

So B is a right inverse of A.

b2

···

··· Abn � =� e1

bn � satisﬁes

e2

···

en � = I.

We have just shown that for a square matrix A,

left-invertibility =⇒ column independence =⇒ right-invertibility.

(The symbol =⇒ means that the left-hand condition implies the right-hand condi-
tion.) Applying the same result to the transpose of A allows us to also conclude
that

right-invertibility =⇒ row independence =⇒ left-invertibility.

So all six of these conditions are equivalent; if any one of them holds, so do the
other ﬁve.

In summary, for a square matrix A, the following are equivalent.

• A is invertible.
• The columns of A are linearly independent.
• The rows of A are linearly independent.
• A has a left inverse.
• A has a right inverse.

204

11 Matrix inverses

Examples.

• The identity matrix I is invertible, with inverse I−1 = I, since II = I.
• A diagonal matrix A is invertible if and only if its diagonal entries are nonzero.
The inverse of an n × n diagonal matrix A with nonzero diagonal entries is

since

A−1 =
AA−1 =

0
...
0

1/A11

0

0
...
0

1/A22

...
0

0

A22/A22

...
0

A11/A11

···
···
. . .
···

0
0
...

1/Ann



0
0
...

···
···
. . .
··· Ann/Ann

,



= I.

In compact notation, we have

diag(A11, . . . , Ann)−1 = diag(A−1

11 , . . . , A−1

nn).

Note that the inverse on the left-hand side of this equation is the matrix
inverse, while the inverses appearing on the right-hand side are scalar inverses.

• As a non-obvious example, the matrix

is invertible, with inverse

3
2

1 −2
2
0

−3 −4 −4 
A =
30
2  .

0 −20 −10
−6
5 −2
10
6

A−1 =

1

This can be veriﬁed by checking that AA−1 = I (or that A−1A = I, since
either of these implies the other).

• 2× 2 matrices. A 2× 2 matrix A is invertible if and only if A11A22 �= A12A21,

with inverse

A−1 =� A11 A12

A21 A22 �−1

=

1

A11A22 − A12A21� A22 −A12
−A21 A11 � .

(There are similar formulas for the inverse of a matrix of any size, but they
grow very quickly in complexity and so are not very useful in most applica-
tions.)

• Orthogonal matrix. If A is square with orthonormal columns, we have AT A =

I, so A is invertible with inverse A−1 = AT .

11.2

Inverse

205

Inverse of matrix transpose.
and its inverse is (A−1)T :

If A is invertible, its transpose AT is also invertible

(AT )−1 = (A−1)T .

Since the order of the transpose and inverse operations does not matter, this matrix
is sometimes written as A−T .

Inverse of matrix product.
same size, then AB is invertible, and

If A and B are invertible (hence, square) and of the

(AB)−1 = B−1A−1.

(11.2)

The inverse of a product is the product of the inverses, in reverse order.

Dual basis. Suppose that A is invertible with inverse B = A−1. Let a1, . . . , an be
the columns of A, and bT

n denote the rows of B, i.e., the columns of BT :

1 , . . . , bT

A =� a1

··· an � ,

B =

bT
1
...
bT
n

 .

We know that a1, . . . , an form a basis, since the columns of A are linearly inde-
pendent. The vectors b1, . . . , bn also form a basis, since the rows of B are linearly
independent. They are called the dual basis of a1, . . . , an.
(The dual basis of
b1, . . . , bn is a1, . . . , an, so they called dual bases.)

Now suppose that x is any n-vector. It can be expressed as a linear combination

of the basis vectors a1, . . . , an:

The dual basis gives us a simple way to ﬁnd the coeﬃcients β1, . . . , βn.

x = β1a1 + ··· + βnan.

We start with AB = I, and multiply by x to get

x = ABx =� a1

··· an �

bT
1
...
bT
n

 x = (bT

1 x)a1 + ··· + (bT

n x)an.

This means (since the vectors a1, . . . , an are linearly independent) that βi = bT
i x.
In words: The coeﬃcients in the expansion of a vector in a basis are given by the
inner products with the dual basis vectors. Using matrix notation, we can say that
β = BT x = (A−1)T x is the vector of coeﬃcients of x in the basis given by the
columns of A.

As a simple numerical example, consider the basis
a2 = (1,−1).

a1 = (1, 1),

The dual basis consists of the rows of [ a1 a2 ]−1, which are

To express the vector x = (−5, 1) as a linear combination of a1 and a2, we have

bT

1 =� 1/2

1/2 � ,

x = (bT

1 x)a1 + (bT

bT

2 =� 1/2 −1/2 � .

2 x)a2 = (−2)a1 + (−3)a2,

which can be directly veriﬁed.

206

11 Matrix inverses

Negative matrix powers. We can now give a meaning to matrix powers with
negative integer exponents. Suppose A is a square invertible matrix and k is a
positive integer. Then by repeatedly applying property (11.2), we get

(Ak)−1 = (A−1)k.

We denote this matrix as A−k. For example, if A is square and invertible, then
A−2 = A−1A−1 = (AA)−1. With A0 deﬁned as A0 = I, the identity Ak+l = AkAl
holds for all integers k and l.

Triangular matrix. A triangular matrix with nonzero diagonal elements is invert-
ible. We ﬁrst discuss this for a lower triangular matrix. Let L be n × n and lower
triangular with nonzero diagonal elements. We show that the columns are linearly
independent, i.e., Lx = 0 is only possible if x = 0. Expanding the matrix-vector
product, we can write Lx = 0 as

L11x1 = 0
L21x1 + L22x2 = 0
L31x1 + L32x2 + L33x3 = 0

...

Ln1x1 + Ln2x2 + ··· + Ln,n−1xn−1 + Lnnxn = 0.

Since L11 �= 0, the ﬁrst equation implies x1 = 0. Using x1 = 0, the second equation
reduces to L22x2 = 0. Since L22 �= 0, we conclude that x2 = 0. Using x1 = x2 = 0,
the third equation now reduces to L33x3 = 0, and since L33 is assumed to be
nonzero, we have x3 = 0. Continuing this argument, we ﬁnd that all entries of x
are zero, and this shows that the columns of L are linearly independent. It follows
that L is invertible.

A similar argument can be followed to show that an upper triangular matrix
with nonzero diagonal elements is invertible. One can also simply note that if R
is upper triangular, then L = RT is lower triangular with the same diagonal, and
use the formula (LT )−1 = (L−1)T for the inverse of the transpose.

Inverse via QR factorization. The QR factorization gives a simple expression
for the inverse of an invertible matrix. If A is square and invertible, its columns
are linearly independent, so it has a QR factorization A = QR. The matrix Q is
orthogonal and R is upper triangular with positive diagonal entries. Hence Q and
R are invertible, and the formula for the inverse product gives

A−1 = (QR)−1 = R−1Q−1 = R−1QT .

(11.3)

In the following section we give an algorithm for computing R−1, or more
directly, the product R−1QT . This gives us a method to compute the matrix
inverse.

11.3 Solving linear equations

207

11.3 Solving linear equations

Back substitution. We start with an algorithm for solving a set of linear equa-
tions, Rx = b, where the n× n matrix R is upper triangular with nonzero diagonal
entries (hence, invertible). We write out the equations as

R11x1 + R12x2 + ··· + R1,n−1xn−1 + R1nxn = b1

...

Rn−2,n−2xn−2 + Rn−2,n−1xn−1 + Rn−2,nxn = bn−2
Rn−1,n−1xn−1 + Rn−1,nxn = bn−1
Rnnxn = bn.

From the last equation, we ﬁnd that xn = bn/Rnn. Now that we know xn, we
substitute it into the second to last equation, which gives us

xn−1 = (bn−1 − Rn−1,nxn)/Rn−1,n−1.

We can continue this way to ﬁnd xn−2, xn−3, . . . , x1. This algorithm is known as
back substitution, since the variables are found one at a time, starting from xn, and
we substitute the ones that are known into the remaining equations.

Algorithm 11.1 Back substitution
given an n × n upper triangular matrix R with nonzero diagonal entries, and an
n-vector b.

For i = n, . . . , 1,

xi = (bi − Ri,i+1xi+1 − ··· − Ri,nxn)/Rii.

(In the ﬁrst step, with i = n, we have xn = bn/Rnn.) The back substitution
algorithm computes the solution of Rx = b, i.e., x = R−1b. It cannot fail since the
divisions in each step are by the diagonal entries of R, which are assumed to be
nonzero.

Lower triangular matrices with nonzero diagonal elements are also invertible;
we can solve equations with lower triangular invertible matrices using forward sub-
stitution, the obvious analog of the algorithm given above. In forward substitution,
we ﬁnd x1 ﬁrst, then x2, and so on.

Complexity of back substitution. The ﬁrst step requires 1 ﬂop (division by Rnn).
The next step requires one multiply, one subtraction, and one division, for a total
of 3 ﬂops. The kth step requires k − 1 multiplies, k − 1 subtractions, and one
division, for a total of 2k − 1 ﬂops. The total number of ﬂops for back substitution
is then

1 + 3 + 5 + ··· + (2n − 1) = n2

ﬂops.

This formula can be obtained from the formula (5.7), or directly derived using
a similar argument. Here is the argument for the case when n is even; a similar

208

11 Matrix inverses

argument works when n is odd. Lump the ﬁrst entry in the sum together with
the last entry, the second entry together with the second-to-last entry, and so on.
Each of these pairs add up to 2n; since there are n/2 such pairs, the total is
(n/2)(2n) = n2.

Solving linear equations using the QR factorization. The formula (11.3) for the
inverse of a matrix in terms of its QR factorization suggests a method for solving
a square system of linear equations Ax = b with A invertible. The solution

x = A−1b = R−1QT b

(11.4)

can be found by ﬁrst computing the matrix-vector product y = QT b, and then
solving the triangular equation Rx = y by back substitution.

Algorithm 11.2 Solving linear equations via QR factorization
given an n × n invertible matrix A and an n-vector b.

1. QR factorization. Compute the QR factorization A = QR.
2. Compute QT b.
3. Back substitution. Solve the triangular equation Rx = QT b using back substi-

tution.

The ﬁrst step requires 2n3 ﬂops (see §5.4), the second step requires 2n2 ﬂops,

and the third step requires n2 ﬂops. The total number of ﬂops is then

so the order is n3, cubic in the number of variables, which is the same as the number
of equations.

2n3 + 3n2 ≈ 2n3,

In the complexity analysis above, we found that the ﬁrst step, the QR factor-
ization, dominates the other two; that is, the cost of the other two is negligible
in comparison to the cost of the ﬁrst step. This has some interesting practical
implications, which we discuss below.

Factor-solve methods. Algorithm 11.2 is similar to many methods for solving a
set of linear equations and is sometimes referred to as a factor-solve scheme. A
factor-solve scheme consists of two steps. In the ﬁrst (factor) step the coeﬃcient
matrix is factored as a product of matrices with special properties. In the second
(solve) step one or more linear equations that involve the factors in the factorization
are solved.
(In algorithm 11.2, the solve step consists of steps 2 and 3.) The
complexity of the solve step is smaller than the complexity of the factor step, and
in many cases, it is negligible by comparison. This is the case in algorithm 11.2,
where the factor step has order n3 and the solve step has order n2.

Factor-solve methods with multiple right-hand sides. Now suppose that we must
solve several sets of linear equations,

Ax1 = b1,

. . . , Axk = bk,

11.3 Solving linear equations

209

all with the same coeﬃcient matrix A, but diﬀerent right-hand sides. We can
express this as the matrix equation AX = B, where X is the n × k matrix with
columns x1, . . . , xk, and B is the n×k matrix with columns b1, . . . , bk (see page 180).
Assuming A is invertible, the solution of AX = B is X = A−1B.
A na¨ıve way to solve the k problems Axi = bi (or in matrix notation, compute
X = A−1B) is to apply algorithm 11.2 k times, which costs 2kn3 ﬂops. A more
eﬃcient method exploits the fact that A is the same matrix in each problem, so
we can re-use the matrix factorization in step 1 and only need to repeat steps 2
and 3 to compute ˆxk = R−1QT bk for l = 1, . . . , k. (This is sometimes referred to
as factorization caching, since we save or cache the factorization after carrying it
out, for later use.) The cost of this method is 2n3 + 3kn2 ﬂops, or approximately
2n3 ﬂops if k � n. The (surprising) conclusion is that we can solve multiple sets of
linear equations, with the same coeﬃcient matrix A, at essentially the same cost
as solving one set of linear equations.

Backslash notation.
In several software packages for manipulating matrices, A\b
is taken to mean the solution of Ax = b, i.e., A−1b, when A is invertible. This
backslash notation is extended to matrix right-hand sides: A\B, with B an n × k
matrix, denotes A−1B, the solution of the matrix equation AX = B. (The compu-
tation is implemented as described above, by factoring A just once, and carrying
out k back substitutions.) This backslash notation is not standard mathematical
notation, however, so we will not use it in this book.

Computing the matrix inverse. We can now describe a method to compute the
inverse B = A−1 of an (invertible) n × n matrix A. We ﬁrst compute the QR
factorization of A, so A−1 = R−1QT . We can write this as RB = QT , which,
written out by columns is

Rbi = ˜qi,

i = 1, . . . , n,

where bi is the ith column of B and ˜qi is the ith column of QT . We can solve these
equations using back substitution, to get the columns of the inverse B.

Algorithm 11.3 Computing the inverse via QR factorization
given an n × n invertible matrix A.

1. QR factorization. Compute the QR factorization A = QR.

2. For i = 1, . . . , n,

Solve the triangular equation Rbi = ˜qi using back substitution.

The complexity of this method is 2n3 ﬂops (for the QR factorization) and n3 for
n back substitutions, each of which costs n2 ﬂops. So we can compute the matrix
inverse in around 3n3 ﬂops.

This gives an alternative method for solving the square set of linear equations
Ax = b: We ﬁrst compute the inverse matrix A−1, and then the matrix-vector
product x = (A−1)b. This method has a higher ﬂop count than directly solving

210

11 Matrix inverses

the equations using algorithm 11.2 (3n3 versus 2n3), so algorithm 11.2 is the usual
method of choice. While the matrix inverse appears in many formulas (such as the
solution of a set of linear equations), it is computed far less often.

Sparse linear equations. Systems of linear equations with sparse coeﬃcient ma-
trix arise in many applications. By exploiting the sparsity of the coeﬃcient matrix,
these linear equations can be solved far more eﬃciently than by using the generic
algorithm 11.2. One method is to use the same basic algorithm 11.2, replacing
the QR factorization with a variant that handles sparse matrices (see page 190).
The memory usage and complexity of these methods depends in a complicated
way on the sparsity pattern of the coeﬃcient matrix. In order, the memory usage
is typically a modest multiple of nnz(A) + n, the number of scalars required to
specify the problem data A and b, which is typically much smaller than n2 + n, the
number of scalars required to store A and b if they are not sparse. The ﬂop count
for solving sparse linear equations is also typically closer in order to nnz(A) than
n3, the order when the matrix A is not sparse.

11.4 Examples

Polynomial interpolation. The 4-vector c gives the coeﬃcients of a cubic polyno-
mial,

p(x) = c1 + c2x + c3x2 + c4x3

(see pages 154 and 120). We seek the coeﬃcients that satisfy

p(−1.1) = b1,

p(−0.4) = b2,

p(0.2) = b3,

p(0.8) = b4.

We can express this as the system of 4 equations in 4 variables Ac = b, where

1 −1.1
1 −0.4
0.2
1
1
0.8

(−1.1)2
(−0.4)2
(0.2)2
(0.8)2

(−1.1)3
(−0.4)3
(0.2)3
(0.8)3

 ,

A =
A−1 =

which is a speciﬁc Vandermonde matrix (see (6.7)). The unique solution is c =
A−1b, where

1.9841 −2.1368
−0.5784
0.7310
0.1984 −1.4957
0.3470
0.9503
0.1388 −1.8651
1.6239
0.1023
0.7521 −0.0643
−0.0370
0.3492



(to 4 decimal places). This is illustrated in ﬁgure 11.1, which shows the two cu-
bic polynomials that interpolate the two sets of points shown as ﬁlled circles and
squares, respectively.

The columns of A−1 are interesting: They give the coeﬃcients of a polynomial
that evaluates to 0 at three of the points, and 1 at the other point. For example, the

11.4 Examples

p(x)

211

−1.5

−1

−0.5

0

0.5

x

1

Figure 11.1 Cubic interpolants through two sets of points, shown as circles
and squares.

ﬁrst column of A−1, which is A−1e1, gives the coeﬃcients of the polynomial that
has value 1 at −1.1, and value 0 at −0.4, 0.2, and 0.8. The four polynomials with
coeﬃcients given by the columns of A−1 are called the Lagrange polynomials asso-
ciated with the points −1.1, −0.4, 0.2, 0.8. These are plotted in ﬁgure 11.2. (The
Lagrange polynomials are named after the mathematician Joseph-Louis Lagrange,
whose name will re-appear in several other contexts.)

The rows of A−1 are also interesting: The ith row shows how the values b1,
. . . , b4, the polynomial values at the points −1.1, −0.4, 0.2, 0.8, map into the ith
coeﬃcient of the polynomial, ci. For example, we see that the coeﬃcient c4 is not
very sensitive to the value of b1 (since (A−1)41 is small). We can also see that for
each increase of one in b4, the coeﬃcient c2 increases by around 0.95.

Balancing chemical reactions.
problem of balancing the chemical reaction

(See page 154 for background.) We consider the

a1Cr2O2−7 + a2Fe2+ + a3H+ −→ b1Cr3+ + b2Fe3+ + b3H2O,

where the superscript gives the charge of each reactant and product. There are 4
atoms (Cr, O, Fe, H) and charge to balance. The reactant and product matrices
are (using the order just listed)

R =



2
7
0
0
−2

0
0
1
0
2

0
0
0
1
1



,

P =

1
0
0
0
3



0
0
1
0
3

0
1
0
2
0

.



212

11 Matrix inverses

p(x)

1

0

p(x)

1

0

−1

0

x

1

p(x)

1

0

p(x)

1

0

−1

0

x

1

−1

0

x

1

−1

0

x

1

Figure 11.2 Lagrange polynomials associated with the points −1.1, −0.4,
0.2, 0.8.

11.4 Examples

213

Imposing the condition that a1 = 1 we obtain a square set of 6 linear equations,

2
7
0
0
−2
1

0
0
1
0
2
0

0 −1
0
0
0 −1
0
0
0 −1
0
0
0 −2
0
1
1 −3 −3
0
0
0
0

0







a1
a2
a3
b1
b2
b3



=

.



0
0
0
0
0
1



Solving these equations we obtain

a1 = 1,

a2 = 6,

a3 = 14,

b1 = 2,

b2 = 6,

b3 = 7.

(Setting a1 = 1 could have yielded fractional values for the other coeﬃcients, but
in this case, it did not.) The balanced reaction is

Cr2O2−7 + 6Fe2+ + 14H+ −→ 2Cr3+ + 6Fe3+ + 7H2O.

Heat diﬀusion. We consider a diﬀusion system as described on page 155. Some of
the nodes have ﬁxed potential, i.e., ei is given; for the other nodes, the associated
external source si is zero. This would model a thermal system in which some
nodes are in contact with the outside world or a heat source, which maintains
their temperatures (via external heat ﬂows) at constant values; the other nodes are
internal, and have no heat sources. This gives us a set of n additional equations:

ei = eﬁx
i

,

i ∈ P,

si = 0,

i �∈ P,

where P is the set of indices of nodes with ﬁxed potential. We can write these n
equations in matrix-vector form as

Bs + Ce = d,

where B and C are the n × n diagonal matrices, and d is the n-vector given by

Bii =� 0

1

i ∈ P
i �∈ P,

We assemble the ﬂow conservation, edge ﬂow, and the boundary conditions into
one set of m + 2n equations in m + 2n variables (f, s, e):

i ∈ P
i �∈ P.

0

f
s

i
0

i ∈ P
i �∈ P,

A I
0
R 0 AT

Cii =� 1
0 B C 
e  =

di =� eﬁx
e  =
d  .
−1
d  .
0 B C 

A I
0
R 0 AT

0
0

0
0




f
s

(The matrix A is the incidence matrix of the graph, and R is the resistance matrix;
see page 155.) Assuming the coeﬃcient matrix is invertible, we have

This is illustrated with an example in ﬁgure 11.3. The graph is a 100× 100 grid,
with 10000 nodes, and edges connecting each node to its horizontal and vertical
neighbors. The resistance on each edge is the same. The nodes at the top and
bottom are held at zero temperature, and the three sets of nodes with rectilinear
shapes are held at temperature one. All other nodes have zero source value.

214

11 Matrix inverses

1

0.8

0.6

0.4

0.2

0

Figure 11.3 Temperature distribution on a 100 × 100 grid of nodes. Nodes
in the top and bottom rows are held at zero temperature. The three sets of
nodes with rectilinear shapes are held at temperature one.

11.5 Pseudo-inverse

Linearly independent columns and Gram invertibility. We ﬁrst show that an
m × n matrix A has linearly independent columns if and only if its n × n Gram
matrix AT A is invertible.
First suppose that the columns of A are linearly independent. Let x be an

n-vector which satisﬁes (AT A)x = 0. Multiplying on the left by xT we get

0 = xT 0 = xT (AT Ax) = xT AT Ax = �Ax�2,

which implies that Ax = 0. Since the columns of A are linearly independent, we
conclude that x = 0. Since the only solution of (AT A)x = 0 is x = 0, we conclude
that AT A is invertible.

Now let’s show the converse. Suppose the columns of A are linearly dependent,
which means there is a nonzero n-vector x which satisﬁes Ax = 0. Multiply on the
left by AT to get (AT A)x = 0. This shows that the Gram matrix AT A is singular.

Pseudo-inverse of square or tall matrix. We show here that if A has linearly
independent columns (and therefore, is square or tall) then it has a left inverse.
(We already have observed the converse, that a matrix with a left inverse has
linearly independent columns.) Assuming A has linearly independent columns, we
know that AT A is invertible. We now observe that the matrix (AT A)−1AT is a
left inverse of A:

�(AT A)−1AT� A = (AT A)−1(AT A) = I.

This particular left-inverse of A will come up in the sequel, and has a name,

11.5 Pseudo-inverse

215

the pseudo-inverse of A. It is denoted A† (or A+):

A† = (AT A)−1AT .

(11.5)

The pseudo-inverse is also called the Moore–Penrose inverse, after the mathemati-
cians Eliakim Moore and Roger Penrose.

When A is square, the pseudo-inverse A† reduces to the ordinary inverse:

A† = (AT A)−1AT = A−1A−T AT = A−1I = A−1.

Note that this equation does not make sense (and certainly is not correct) when A
is not square.

Pseudo-inverse of a square or wide matrix. Transposing all the equations, we
can show that a (square or wide) matrix A has a right inverse if and only if its
rows are linearly independent. Indeed, one right inverse is given by

AT (AAT )−1.

(11.6)

(The matrix AAT is invertible if and only if the rows of A are linearly independent.)
The matrix in (11.6) is also referred to as the pseudo-inverse of A, and denoted
A†. The only possible confusion in deﬁning the pseudo-inverse using the two dif-
ferent formulas (11.5) and (11.6) occurs when the matrix A is square. In this case,
however, they both reduce to the ordinary inverse:

AT (AAT )−1 = AT A−T A−1 = A−1.

Pseudo-inverse in other cases. The pseudo-inverse A† is deﬁned for any matrix,
including the case when A is tall but its columns are linearly dependent, the case
when A is wide but its rows are linearly dependent, and the case when A is square
but not invertible. In these cases, however, it is not a left inverse, right inverse, or
inverse, respectively. We mention it here since the reader may encounter it. (We
will see what A† means in these cases in exercise 15.11.)

Pseudo-inverse via QR factorization. The QR factorization gives a simple for-
mula for the pseudo-inverse. If A is left-invertible, its columns are linearly inde-
pendent and the QR factorization A = QR exists. We have

AT A = (QR)T (QR) = RT QT QR = RT R,

so

A† = (AT A)−1AT = (RT R)−1(QR)T = R−1R−T RT QT = R−1QT .

We can compute the pseudo-inverse using the QR factorization, followed by back
substitution on the columns of QT . (This is exactly the same as algorithm 11.3
when A is square and invertible.) The complexity of this method is 2n2m ﬂops (for
the QR factorization), and mn2 ﬂops for the m back substitutions. So the total is
3mn2 ﬂops.

216

11 Matrix inverses

Similarly, if A is right-invertible, the QR factorization AT = QR of its transpose

exists. We have AAT = (QR)T (QR) = RT QT QR = RT R and

A† = AT (AAT )−1 = QR(RT R)−1 = QRR−1R−T = QR−T .

We can compute it using the method described above, using the formula

(AT )† = (A†)T .

Solving over- and under-determined systems of linear equations. The pseudo-
inverse gives us a method for solving over-determined and under-determined sys-
tems of linear equations, provided the columns of the coeﬃcient matrix are linearly
independent (in the over-determined case), or the rows are linearly independent
(in the under-determined case). If the columns of A are linearly independent, and
the over-determined equations Ax = b have a solution, then x = A†b is it. If the
rows of A are linearly independent, the under-determined equations Ax = b have
a solution for any vector b, and x = A†b is a solution.

Numerical example. We illustrate these ideas with a simple numerical example,
using the 3 × 2 matrix A used in earlier examples on pages 199 and 201,

This matrix has linearly independent columns, and QR factorization with (to 4
digits)

1  .

−3 −4
4
6
1

A =
Q =
0.1961 −0.7191  ,
A† = R−1QT =� −1.2222 −1.1111

−0.5883
0.7845

0.4576
0.5230

It has pseudo-inverse (to 4 digits)

R =� 5.0990

0

7.2563

0.5883 � .

0.7778

0.8889 −1.2222 � .

1.7778

We can use the pseudo-inverse to check if the over-determined systems of equations
Ax = b, with b = (1,−2, 0), has a solution, and to ﬁnd a solution if it does. We
compute x = A†(1,−2, 0) = (1,−1) and check whether Ax = b holds. It does, so
we have found the unique solution of Ax = b.

Exercises

Exercises

217

11.1 Aﬃne combinations of left inverses. Let Z be a tall m×n matrix with linearly independent
columns, and let X and Y be left inverses of Z. Show that for any scalars α and β satisfying
α + β = 1, αX + βY is also a left inverse of Z. It follows that if a matrix has two diﬀerent
left inverses, it has an inﬁnite number of diﬀerent left inverses.

11.2 Left and right inverses of a vector. Suppose that x is a nonzero n-vector with n > 1.

(a) Does x have a left inverse?

(b) Does x have a right inverse?

In each case, if the answer is yes, give a left or right inverse; if the answer is no, give a
speciﬁc nonzero vector and show that it is not left- or right-invertible.

11.3 Matrix cancellation. Suppose the scalars a, x, and y satisfy ax = ay. When a �= 0 we
can conclude that x = y; that is, we can cancel the a on the left of the equation. In this
exercise we explore the matrix analog of cancellation, speciﬁcally, what properties of A
are needed to conclude X = Y from AX = AY , for matrices A, X, and Y ?
(a) Give an example showing that A �= 0 is not enough to conclude that X = Y .
(b) Show that if A is left-invertible, we can conclude from AX = AY that X = Y .
(c) Show that if A is not left-invertible, there are matrices X and Y with X �= Y , and

AX = AY .

Remark. Parts (b) and (c) show that you can cancel a matrix on the left when, and only
when, the matrix is left-invertible.

11.4 Transpose of orthogonal matrix. Let U be an orthogonal n × n matrix. Show that its

transpose U T is also orthogonal.

11.5 Inverse of a block matrix. Consider the (n + 1) × (n + 1) matrix

A =� I

aT

a

0 � ,

where a is an n-vector.

(a) When is A invertible? Give your answer in terms of a. Justify your answer.

(b) Assuming the condition you found in part (a) holds, give an expression for the inverse

matrix A−1.

11.6 Inverse of a block upper triangular matrix. Let B and D be invertible matrices of sizes

m × m and n × n, respectively, and let C be any m × n matrix. Find the inverse of

A =� B C
0 D �

in terms of B−1, C, and D−1. (The matrix A is called block upper triangular.)
Hints. First get an idea of what the solution should look like by considering the case
when B, C, and D are scalars. For the matrix case, your goal is to ﬁnd matrices W , X,
Y , Z (in terms of B−1, C, and D−1) that satisfy

A� W X

Z � = I.

Y

Use block matrix multiplication to express this as a set of four matrix equations that you
can then solve. The method you will ﬁnd is sometimes called block back substitution.

218

11 Matrix inverses

11.7 Inverse of an upper triangular matrix. Suppose the n × n matrix R is upper triangular
and invertible, i.e., its diagonal entries are all nonzero. Show that R−1 is also upper
triangular. Hint. Use back substitution to solve Rsk = ek, for k = 1, . . . , n, and argue
that (sk)i = 0 for i > k.

11.8 If a matrix is small, its inverse is large. If a number a is small, its inverse 1/a (assuming
a �= 0) is large. In this exercise you will explore a matrix analog of this idea. Suppose the
n × n matrix A is invertible. Show that �A−1� ≥ √n/�A�. This implies that if a matrix
is small, its inverse is large. Hint. You can use the inequality �AB� ≤ �A��B�, which
holds for any matrices for which the product makes sense. (See exercise 10.12.)
11.9 Push-through identity. Suppose A is m × n, B is n × m, and the m × m matrix I + AB

is invertible.
(a) Show that the n × n matrix I + BA is invertible. Hint. Show that (I + BA)x = 0
(b) Establish the identity

implies (I + AB)y = 0, where y = Ax.

B(I + AB)−1 = (I + BA)−1B.

This is sometimes called the push-through identity since the matrix B appearing on
the left ‘moves’ into the inverse, and ‘pushes’ the B in the inverse out to the right
side. Hint. Start with the identity

and multiply on the right by (I + AB)−1, and on the left by (I + BA)−1.

11.10 Reverse-time linear dynamical system. A linear dynamical system has the form

B(I + AB) = (I + BA)B,

xt+1 = Axt,

where xt in the (n-vector) state in period t, and A is the n × n dynamics matrix. This
formula gives the state in the next period as a function of the current state.
We want to derive a recursion of the form

xt−1 = Arevxt,

which gives the previous state as a function of the current state. We call this the reverse
time linear dynamical system.

(a) When is this possible? When it is possible, what is Arev?
(b) For the speciﬁc linear dynamical system with dynamics matrix

A =�

3
−1

2

4 � ,

ﬁnd Arev, or explain why the reverse time linear dynamical system doesn’t exist.

11.11 Interpolation of rational functions. (Continuation of exercise 8.8.) Find a rational function

f (t) =

c1 + c2t + c3t2
1 + d1t + d2t2

that satisﬁes the following interpolation conditions:

f (1) = 2,

f (2) = 5,

f (3) = 9,

f (4) = −1,

f (5) = −4.

In exercise 8.8 these conditions were expressed as a set of linear equations in the coeﬃcients
c1, c2, c3, d1 and d2; here we are asking you to form and (numerically) solve the system
of equations. Plot the rational function you ﬁnd over the range x = 0 to x = 6. Your
plot should include markers at the interpolation points (1, 2), . . . , (5,−4). (Your rational
function graph should pass through these points.)

Exercises

219

11.12 Combinations of invertible matrices. Suppose the n × n matrices A and B are both
invertible. Determine whether each of the matrices given below is invertible, without any
further assumptions about A and B.

(a) A + B.

(b) � A 0
0 B �.
(c) � A A + B

B

0

(d) ABA.

�.

11.13 Another left inverse. Suppose the m × n matrix A is tall and has linearly independent
In this problem we explore

columns. One left inverse of A is the pseudo-inverse A†.
another one. Write A as the block matrix

A =� A1
A2 � ,

where A1 is n × n. We assume that A1 is invertible (which need not happen in general).
Show that the following matrix is a left inverse of A:

˜A =� A−1

1

0n×(m−n) � .

11.14 Middle inverse. Suppose A is an n × p matrix and B is a q × n matrix. If a p × q matrix
X exists that satisﬁes AXB = I, we call it a middle inverse of the pair A, B. (This is
not a standard concept.) Note that when A or B is an identity matrix, the middle inverse
reduces to the right or left inverse, respectively.

(a) Describe the conditions on A and B under which a middle inverse X exists. Give
your answer using only the following four concepts: Linear independence of the rows
or columns of A, and linear independence of the rows or columns of B. You must
justify your answer.

(b) Give an expression for a middle inverse, assuming the conditions in part (a) hold.

11.15 Invertibility of population dynamics matrix. Consider the population dynamics matrix

A =



b1

1 − d1

0
...
0

b2
0

1 − d2

...
0

···
···
···
. . .
···

b99
0
0
...

1 − d99

b100

0
0
...
0

,



where bi ≥ 0 are the birth rates and 0 ≤ di ≤ 1 are death rates. What are the conditions
on bi and di under which A is invertible? (If the matrix is never invertible or always
invertible, say so.) Justify your answer.

11.16 Inverse of running sum matrix. Find the inverse of the n × n running sum matrix,

S =

Does your answer make sense?



1
1
...
1
1

0
1
...
1
1

···
···
. . .
···
···

0
0
...
1
1

0
0
...
0
1

.



220

11 Matrix inverses

11.17 A matrix identity. Suppose A is a square matrix that satisﬁes Ak = 0 for some integer k.
(Such a matrix is called nilpotent.) A student guesses that (I − A)−1 = I + A +··· + Ak−1,
based on the inﬁnite series 1/(1 − a) = 1 + a + a2 + ···, which holds for numbers a that
satisfy |a| < 1.
Is the student right or wrong? If right, show that her assertion holds with no further
assumptions about A.
If she is wrong, give a counterexample, i.e., a matrix A that
satisﬁes Ak = 0, but I + A + ··· + Ak−1 is not the inverse of I − A.
11.18 Tall-wide product. Suppose A is an n × p matrix and B is a p × n matrix, so C = AB
makes sense. Explain why C cannot be invertible if A is tall and B is wide, i.e., if p < n.
Hint. First argue that the columns of B must be linearly dependent.

11.19 Control restricted to one time period. A linear dynamical system has the form xt+1 =
Axt + ut, where the n-vector xt is the state and ut is the input at time t. Our goal is to
choose the input sequence u1, . . . , uN−1 so as to achieve xN = xdes, where xdes is a given
n-vector, and N is given. The input sequence must satisfy ut = 0 unless t = K, where
K < N is given. In other words, the input can only act at time t = K. Give a formula
for uK that achieves this goal. Your formula can involve A, N , K, x1, and xdes. You
can assume that A is invertible. Hint. First derive an expression for xK , then use the
dynamics equation to ﬁnd xK+1. From xK+1 you can ﬁnd xN .

11.20 Immigration. The population dynamics of a country is given by xt+1 = Axt + u, t =
1, . . . , T − 1, where the 100-vector xt gives the population age distribution in year t,
and u gives the immigration age distribution (with negative entries meaning emigration),
which we assume is constant (i.e., does not vary with t). You are given A, x1, and xdes, a
100-vector that represents a desired population distribution in year T . We seek a constant
level of immigration u that achieves xT = xdes.
Give a matrix formula for u. If your formula only makes sense when some conditions hold
(for example invertibility of one or more matrices), say so.

11.21 Quadrature weights. Consider a quadrature problem (see exercise 8.12) with n = 4, with
points t = (−0.6,−0.2, 0.2, 0.6). We require that the quadrature rule be exact for all
polynomials of degree up to d = 3.
Set this up as a square system of linear equations in the weight vector. Numerically solve
this system to get the weights. Compute the true value and the quadrature estimate,

α =� 1

−1

f (x) dx,

ˆα = w1f (−0.6) + w2f (−0.2) + w3f (0.2) + w4f (0.6),

for the speciﬁc function f (x) = ex.

11.22 Properties of pseudo-inverses. For an m × n matrix A and its pseudo-inverse A†, show

that A = AA†A and A† = A†AA† in each of the following cases.

(a) A is tall with linearly independent columns.

(b) A is wide with linearly independent rows.

(c) A is square and invertible.

11.23 Product of pseudo-inverses. Suppose A and D are right-invertible matrices and the prod-
uct AD exists. We have seen that if B is a right inverse of A and E is a right inverse of
D, then EB is a right inverse of AD. Now suppose B is the pseudo-inverse of A and E is
the pseudo-inverse of D. Is EB the pseudo-inverse of AD? Prove that this is always true
or give an example for which it is false.

11.24 Simultaneous left inverse. The two matrices

A =

1
3
2
2

2
1
1
2

 ,

B =

3
1
2
1

2
0
1
3



Exercises

221

and both left-invertible, and have multiple left inverses. Do they have a common left
inverse? Explain how to ﬁnd a 2 × 4 matrix C that satisﬁes CA = CB = I, or determine
that no such matrix exists. (You can use numerical computing to ﬁnd C.) Hint. Set up
a set of linear equations for the entries of C. Remark. There is nothing special about the
particular entries of the two matrices A and B.

11.25 Checking the computed solution of linear equations. One of your colleagues says that
whenever you compute the solution x of a square set of n equations Ax = b (say, using
QR factorization), you should compute the number �Ax−b� and check that it is small. (It
is not exactly zero due to the small rounding errors made in ﬂoating point computations.)
Another colleague says that this would be nice to do, but the additional cost of computing
�Ax − b� is too high. Brieﬂy comment on your colleagues’ advice. Who is right?

11.26 Sensitivity of solution of linear equations. Let A be an invertible n× n matrix, and b and
x be n-vectors satisfying Ax = b. Suppose we perturb the jth entry of b by � �= 0 (which
is a traditional symbol for a small quantity), so b becomes ˜b = b + �ej. Let ˜x be the n-
vector that satisﬁes A˜x = ˜b, i.e., the solution of the linear equations using the perturbed
right-hand side. We are interested in �x − ˜x�, which is how much the solution changes
due to the change in the right-hand side. The ratio �x− ˜x�/|�| gives the sensitivity of the
solution to changes (perturbations) of the jth entry of b.
(a) Show that �x − ˜x� does not depend on b; it only depends on the matrix A, �, and j.
(b) How would you ﬁnd the index j that maximizes the value of �x − ˜x�? By part (a),

your answer should be in terms of A (or quantities derived from A) and � only.

Remark. If a small change in the right-hand side vector b can lead to a large change in the
solution, we say that the linear equations Ax = b are poorly conditioned or ill-conditioned.
As a practical matter it means that unless you are very conﬁdent in what the entries of
b are, the solution A−1b may not be useful in practice.

11.27 Timing test. Generate a random n× n matrix A and an n-vector b, for n = 500, n = 1000,
and n = 2000. For each of these, compute the solution x = A−1b (for example using the
backslash operator, if the software you are using supports it), and verify that Ax − b is
(very) small. Report the time it takes to solve each of these three sets of linear equations,
and for each one work out the implied speed of your processor in Gﬂop/s, based on the
2n3 complexity of solving equations using the QR factorization.

11.28 Solving multiple linear equations eﬃciently. Suppose the n× n matrix A is invertible. We
can solve the system of linear equations Ax = b in around 2n3 ﬂops using algorithm 11.2.
Once we have done that (speciﬁcally, computed the QR factorization of A), we can solve an
additional set of linear equations with same matrix but diﬀerent right-hand side, Ay = c,
in around 3n2 additional ﬂops. Assuming we have solved both of these sets of equations,
suppose we want to solve Az = d, where d = αb + βc is a linear combination of b and c.
(We are given the coeﬃcients α and β.) Suggest a method for doing this that is even
faster than re-using the QR factorization of A. Your method should have a complexity
that is linear in n. Give rough estimates for the time needed to solve Ax = b, Ay = c,
and Az = d (using your method) for n = 3000 on a computer capable of carrying out 1
Gﬂop/s.

Part III

Least squares

Chapter 12

Least squares

In this chapter we look at the powerful idea of ﬁnding approximate solutions of
over-determined systems of linear equations by minimizing the sum of the squares
of the errors in the equations. The method, and some extensions we describe in
later chapters, are widely used in many application areas. It was discovered inde-
pendently by the mathematicians Carl Friedrich Gauss and Adrien-Marie Legendre
around the beginning of the 19th century.

12.1 Least squares problem

Suppose that the m × n matrix A is tall, so the system of linear equations Ax = b,
where b is an m-vector, is over-determined, i.e., there are more equations (m)
than variables to choose (n). These equations have a solution only if b is a linear
combination of the columns of A.

For most choices of b, however, there is no n-vector x for which Ax = b. As a
compromise, we seek an x for which r = Ax− b, which we call the residual (for the
equations Ax = b), is as small as possible. This suggests that we should choose x
so as to minimize the norm of the residual, �Ax− b�. If we ﬁnd an x for which the
residual vector is small, we have Ax ≈ b, i.e., x almost satisﬁes the linear equations
Ax = b. (Some authors deﬁne the residual as b− Ax, which will not aﬀect us since
�Ax − b� = �b − Ax�.)
Minimizing the norm of the residual and its square are the same, so we can just
as well minimize

�Ax − b�2 = �r�2 = r2

1 + ··· + r2
m,

the sum of squares of the residuals. The problem of ﬁnding an n-vector ˆx that
minimizes �Ax − b�2, over all possible choices of x, is called the least squares
problem. It is denoted using the notation

where we should specify that the variable is x (meaning that we should choose x).
The matrix A and the vector b are called the data for the problem (12.1), which

minimize

�Ax − b�2,

(12.1)

226

12 Least squares

means that they are given to us when we are asked to choose x. The quantity to
be minimized, �Ax − b�2, is called the objective function (or just objective) of the
least squares problem (12.1).
The problem (12.1) is sometimes called linear least squares to emphasize that
the residual r (whose norm squared we are to minimize) is an aﬃne function of x,
and to distinguish it from the nonlinear least squares problem, in which we allow
the residual r to be an arbitrary function of x. We will study the nonlinear least
squares problem in chapter 18.

Any vector ˆx that satisﬁes �Aˆx−b�2 ≤ �Ax−b�2 for all x is a solution of the least
squares problem (12.1). Such a vector is called a least squares approximate solution
of Ax = b. It is very important to understand that a least squares approximate
solution ˆx of Ax = b need not satisfy the equations Aˆx = b; it simply makes the
norm of the residual as small as it can be. Some authors use the confusing phrase
‘ˆx solves Ax = b in the least squares sense’, but we emphasize that a least squares
approximate solution ˆx does not, in general, solve the equation Ax = b.

If �Aˆx− b� (which we call the optimal residual norm) is small, then we can say
that ˆx approximately solves Ax = b. On the other hand, if there is an n-vector x
that satisﬁes Ax = b, then it is a solution of the least squares problem, since its
associated residual norm is zero.

Another name for the least squares problem (12.1), typically used in data ﬁtting
applications (the topic of the next chapter), is regression. We say that ˆx, a solution
of the least squares problem, is the result of regressing the vector b onto the columns
of A.

Column interpretation.
If the columns of A are the m-vectors a1, . . . , an, then
the least squares problem (12.1) is the problem of ﬁnding a linear combination of
the columns that is closest to the m-vector b; the vector x gives the coeﬃcients:

�Ax − b�2 = �(x1a1 + ··· + xnan) − b�2.
If ˆx is a solution of the least squares problem, then the vector

Aˆx = ˆx1a1 + ··· + ˆxnan

is closest to the vector b, among all linear combinations of the vectors a1, . . . , an.

Row interpretation. Suppose the rows of A are the n-row-vectors ˜aT
the residual components are given by

1 , . . . , ˜aT

m, so

ri = ˜aT

i x − bi,

i = 1, . . . , m.

The least squares objective is then

�Ax − b�2 = (˜aT

1 x − b1)2 + ··· + (˜aT

mx − bm)2,

the sum of the squares of the residuals in m scalar linear equations. Minimizing
this sum of squares of the residuals is a reasonable compromise if our goal is to
choose x so that all of them are small.

12.2 Solution

227

Example. We consider the least squares problem with data

A =

2 0
−1
1

0 2  ,

b =

1
0

−1  .

The over-determined set of three equations in two variables Ax = b,

2x1 = 1,

−x1 + x2 = 0,

2x2 = −1,

has no solution. (From the ﬁrst equation we have x1 = 1/2, and from the last
equation we have x2 = −1/2; but then the second equation does not hold.) The
corresponding least squares problem is

minimize

(2x1 − 1)2 + (−x1 + x2)2 + (2x2 + 1)2.

This least squares problem can be solved using the methods described in the next
Its unique solution is ˆx = (1/3,−1/3). The least
section (or simple calculus).
squares approximate solution ˆx does not satisfy the equations Ax = b; the corre-
sponding residuals are

ˆr = Aˆx − b = (−1/3,−2/3, 1/3),

with sum of squares value �Aˆx − b�2 = 2/3. Let us compare this to another choice
of x, ˜x = (1/2,−1/2), which corresponds to (exactly) solving the ﬁrst and last of
the three equations in Ax = b. It gives the residual

˜r = A˜x − b = (0,−1, 0),

with sum of squares value �A˜x − b�2 = 1.
The column interpretation tells us that

(1/3)

2
−1

0  + (−1/3)

0
1

2  =

2/3
−2/3

−2/3 

is the linear combination of the columns of A that is closest to b.

Figure 12.1 shows the values of the least squares objective �Ax − b�2 versus
x = (x1, x2), with the least squares solution ˆx shown as the dark point, with
objective value �Aˆx− b�2 = 2/3. The curves show the points x that have objective
value �Aˆx − b�2 + 1, �Aˆx − b�2 + 2, and so on.

12.2 Solution

In this section we derive several expressions for the solution of the least squares
problem (12.1), under one assumption on the data matrix A:

The columns of A are linearly independent.

(12.2)

228

12 Least squares

1

0.5

0

2
x

−0.5

−1

f (ˆx) + 2
f (ˆx) + 1

ˆx

−1.5

−1

−0.5

0

0.5

1

1.5

x1

Figure 12.1 Level curves of the function �Ax − b�2 = (2x1 − 1)2 + (−x1 +
x2)2 + (2x2 + 1)2. The point ˆx minimizes the function.

Solution via calculus.
In this section we ﬁnd the solution of the least squares
problem using some basic results from calculus, reviewed in §C.2. (We will also
give an independent veriﬁcation of the result, that does not rely on calculus, below.)
We know that any minimizer ˆx of the function f (x) = �Ax − b�2 must satisfy

∂f
∂xi

(ˆx) = 0,

i = 1, . . . , n,

which we can express as the vector equation

∇f (ˆx) = 0,

where ∇f (ˆx) is the gradient of f evaluated at ˆx. The gradient can be expressed in
matrix form as

∇f (x) = 2AT (Ax − b).

(12.3)

This formula can be derived from the chain rule given on page 184, and the gradient
of the sum of squares function, given in §C.1. For completeness, we will derive the
formula (12.3) from scratch here. Writing the least squares objective out as a sum,
we get

f (x) = �Ax − b�2 =

m�i=1



n�j=1

2

.

Aijxj − bi

12.2 Solution

229

To ﬁnd ∇f (x)k we take the partial derivative of f with respect to xk. Diﬀerenti-
ating the sum term by term, we get

∇f (x)k =

∂f
∂xk

(x)

2

Aijxj − bi (Aik)

=

m�i=1
n�j=1
m�i=1
2(AT )ki(Ax − b)i
= �2AT (Ax − b)�k .

=

This is our formula (12.3), written out in terms of its components.

Now we continue the derivation of the solution of the least squares problem.

Any minimizer ˆx of �Ax − b�2 must satisfy

∇f (ˆx) = 2AT (Aˆx − b) = 0,

which can be written as

(12.4)
These equations are called the normal equations. The coeﬃcient matrix AT A is
the Gram matrix associated with A; its entries are inner products of columns of A.
Our assumption (12.2) that the columns of A are linearly independent implies

AT Aˆx = AT b.

that the Gram matrix AT A is invertible (§11.5, page 214). This implies that

ˆx = (AT A)−1AT b

(12.5)

is the only solution of the normal equations (12.4). So this must be the unique
solution of the least squares problem (12.1).

We have already encountered the matrix (AT A)−1AT that appears in (12.5): It
is the pseudo-inverse of the matrix A, given in (11.5). So we can write the solution
of the least squares problem in the simple form

ˆx = A†b.

(12.6)
We observed in §11.5 that A† is a left inverse of A, which means that ˆx = A†b
solves Ax = b if this set of over-determined equations has a solution. But now
we see that ˆx = A†b is the least squares approximate solution, i.e., it minimizes
�Ax − b�2. (And if there is a solution of Ax = b, then ˆx = A†b is it.)
The equation (12.6) looks very much like the formula for solution of the linear
equations Ax = b, when A is square and invertible, i.e., x = A−1b.
It is very
important to understand the diﬀerence between the formula (12.6) for the least
squares approximate solution, and the formula for the solution of a square set
of linear equations, x = A−1b.
In the case of linear equations and the inverse,
x = A−1b actually satisﬁes Ax = b. In the case of the least squares approximate
solution, ˆx = A†b generally does not satisfy Aˆx = b.

The formula (12.6) shows us that the solution ˆx of the least squares problem
is a linear function of b. This generalizes the fact that the solution of a square
invertible set of linear equations is a linear function of its right-hand side.

230

12 Least squares

Direct veriﬁcation of least squares solution.
In this section we directly show
that ˆx = (AT A)−1AT b is the solution of the least squares problem (12.1), without
relying on calculus. We will show that for any x �= ˆx, we have

�Aˆx − b�2 < �Ax − b�2,

establishing that ˆx is the unique vector that minimizes �Ax − b�2.

We start by writing

�Ax − b�2 = �(Ax − Aˆx) + (Aˆx − b)�2

= �Ax − Aˆx�2 + �Aˆx − b�2 + 2(Ax − Aˆx)T (Aˆx − b),

(12.7)

where we use the identity

�u + v�2 = (u + v)T (u + v) = �u�2 + �v�2 + 2uT v.

The third term in (12.7) is zero:

(Ax − Aˆx)T (Aˆx − b) = (x − ˆx)T AT (Aˆx − b)

= (x − ˆx)T (AT Aˆx − AT b)
= (x − ˆx)T 0
= 0,

where we use (AT A)ˆx = AT b (the normal equations) in the third line. With this
simpliﬁcation, (12.7) reduces to

�Ax − b�2 = �A(x − ˆx)�2 + �Aˆx − b�2.

The ﬁrst term on the right-hand side is nonnegative and therefore

�Ax − b�2 ≥ �Aˆx − b�2.

This shows that ˆx minimizes �Ax−b�2; we now show that it is the unique minimizer.
Suppose equality holds above, that is, �Ax − b�2 = �Aˆx − b�2. Then we have
�A(x − ˆx)�2 = 0, which implies A(x − ˆx) = 0. Since A has linearly independent
columns, we conclude that x − ˆx = 0, i.e., x = ˆx. So the only x with �Ax − b�2 =
�Aˆx − b�2 is x = ˆx; for all x �= ˆx, we have �Ax − b�2 > �Aˆx − b�2.

Row form. The formula for the least squares approximate solution can be ex-
pressed in a useful form in terms of the rows ˜aT

i of the matrix A.

ˆx = (AT A)−1AT b =� m�i=1

˜ai˜aT

i�−1� m�i=1

bi˜ai� .

(12.8)

In this formula we express the n × n Gram matrix AT A as a sum of m outer
products, and the n-vector AT b as a sum of m n-vectors.

12.3 Solving least squares problems

231

b

ˆr

Aˆx

a1

a2

Figure 12.2 Illustration of orthogonality principle for a least squares problem
of size m = 3, n = 2. The optimal residual ˆr is orthogonal to any linear
combination of a1 and a2, the two columns of A.

Orthogonality principle. The point Aˆx is the linear combination of the columns
of A that is closest to b. The optimal residual is ˆr = Aˆx − b. The optimal
residual satisﬁes a property that is sometimes called the orthogonality principle:
It is orthogonal to the columns of A, and therefore, it is orthogonal to any linear
combination of the columns of A. In other words, for any n-vector z, we have

(Az) ⊥ ˆr.

(12.9)

We can derive the orthogonality principle from the normal equations, which can
be expressed as AT (Aˆx − b) = 0. For any n-vector z, we have

(Az)T ˆr = (Az)T (Aˆx − b) = zT AT (Aˆx − b) = 0.

The orthogonality principle is illustrated in ﬁgure 12.2, for a least squares prob-
lem with m = 3 and n = 2. The shaded plane is the set of all linear combinations
z1a1 + z2a2 of a1 and a2, the two columns of A. The point Aˆx is the closest point
in the plane to b. The optimal residual ˆr is shown as the vector from b to Aˆx. This
vector is orthogonal to any point in the shaded plane.

12.3 Solving least squares problems

We can use the QR factorization to compute the least squares approximate so-
lution (12.5). Let A = QR be the QR factorization of A (which exists by our
assumption (12.2) that its columns are linearly independent). We have already
seen that the pseudo-inverse A† can be expressed as A† = R−1QT , so we have

ˆx = R−1QT b.

(12.10)

To compute ˆx we ﬁrst multiply b by QT ; then we compute R−1(QT b) using back
substitution. This is summarized in the following algorithm, which computes the
least squares approximate solution ˆx, given A and b.

232

12 Least squares

Algorithm 12.1 Least squares via QR factorization
given an m × n matrix A with linearly independent columns and an m-vector b.

1. QR factorization. Compute the QR factorization A = QR.
2. Compute QT b.
3. Back substitution. Solve the triangular equation Rˆx = QT b.

Comparison to solving a square system of linear equations. Recall that the
solution of the square invertible system of linear equations Ax = b is x = A−1b.
We can express x using the QR factorization of A as x = R−1QT b (see (11.4)). This
equation is formally identical to (12.10). The only diﬀerence is that in (12.10), A
and Q need not be square, and R−1QT b is the least squares approximate solution,
which is not (in general) a solution of Ax = b.

Indeed, algorithm 12.1 is formally the same as algorithm 11.2, the QR fac-
(The only diﬀerence is that in

torization method for solving linear equations.
algorithm 12.1, A and Q can be tall.)

When A is square, solving the linear equations Ax = b and the least squares
problem of minimizing �Ax − b�2 are the same, and algorithm 11.2 and algo-
rithm 12.1 are the same. So we can think of algorithm 12.1 as a generalization of
algorithm 11.2, which solves the equation Ax = b when A is square, and computes
the least squares approximate solution when A is tall.

Backslash notation. Several software packages for manipulating matrices extend
the backslash operator (see page 209) to mean the least squares approximate solu-
tion of an over-determined set of linear equations. In these packages A\b is taken
to mean the solution A−1b of Ax = b when A is square and invertible, and the
least squares approximate solution A†b when A is tall and has linearly indepen-
dent columns. (We remind the reader that this backslash notation is not standard
mathematical notation.)

Complexity. The complexity of the ﬁrst step of algorithm 12.1 is 2mn2 ﬂops. The
second step involves a matrix-vector multiplication, which takes 2mn ﬂops. The
third step requires n2 ﬂops. The total number of ﬂops is

2mn2 + 2mn + n2 ≈ 2mn2,

neglecting the second and third terms, which are smaller than the ﬁrst by factors
of n and 2m, respectively. The order of the algorithm is mn2. The complexity is
linear in the row dimension of A and quadratic in the number of variables.

Sparse least squares. Least squares problems with sparse A arise in several appli-
cations and can be solved more eﬃciently, for example by using a QR factorization
tailored for sparse matrices (see page 190) in the generic algorithm 12.1.

12.3 Solving least squares problems

233

Another simple approach for exploiting sparsity of A is to solve the normal

equations AT Aˆx = AT b by solving a larger (but sparse) system of equations,

� 0 AT
A I �� ˆx

ˆy � =� 0
b � .

(12.11)

This is a square set of m+n linear equations. Its coeﬃcient matrix is sparse when A
is sparse. If (ˆx, ˆy) satisﬁes these equations, it is easy to see that ˆx satisﬁes (12.11);
conversely, if ˆx satisﬁes the normal equations, (ˆx, ˆy) satisﬁes (12.11) with ˆy =
b− Aˆx. Any method for solving a sparse system of linear equations can be used to
solve (12.11).

Matrix least squares. A simple extension of the least squares problem is to choose
the n× k matrix X so as to minimize �AX − B�2. Here A is an m× n matrix and
B is an m × k matrix, and the norm is the matrix norm. This is sometimes called
the matrix least squares problem. When k = 1, x and b are vectors, and the matrix
least squares problem reduces to the usual least squares problem.

The matrix least squares problem is in fact nothing but a set of k ordinary least

squares problems. To see this, we note that

�AX − B�2 = �Ax1 − b1�2 + ··· + �Axk − bk�2,

where xj is the jth column of X and bj is the jth column of B. (Here we use the
property that the square of the matrix norm is the sum of the squared norms of
the columns of the matrix.) So the objective is a sum of k terms, with each term
depending on only one column of X. It follows that we can choose the columns xj
independently, each one by minimizing its associated term �Axj − bj�2. Assuming
that A has linearly independent columns, the solution is ˆxj = A†bj. The solution
of the matrix least squares problem is therefore

···

ˆX = � ˆx1
= � A†b1
= A†� b1

= A†B.

ˆxk �
··· A†bk �
bk �

···

(12.12)

The very simple solution ˆX = A†B of the matrix least squares problem agrees with
the solution of the ordinary least squares problem when k = 1 (as it must). Many
software packages for linear algebra use the backslash operator A\B to denote A†B,
but this is not standard mathematical notation.
The matrix least squares problem can be solved eﬃciently by exploiting the fact
that algorithm 12.1 is another example of a factor-solve algorithm. To compute
ˆX = A†B we carry out the QR factorization of A once; we carry out steps 2 and 3 of
algorithm 12.1 for each of the k columns of B. The total cost is 2mn2 +k(2mn+n2)
ﬂops. When k is small compared to n this is roughly 2mn2 ﬂops, the same cost as
solving a single least squares problem (i.e., one with a vector right-hand side).

234

12.4 Examples

12 Least squares

Advertising purchases. We have m demographic groups or audiences that we
want to advertise to, with a target number of impressions or views for each group,
which we give as a vector vdes. (The entries are positive.) To reach these audiences,
we purchase advertising in n diﬀerent channels (say, diﬀerent web publishers, radio,
print, . . . ), in amounts that we give as an n-vector s. (The entries of s are non-
negative, which we ignore.) The m × n matrix R gives the number of impressions
in each group per dollar spending in the channels: Rij is the number of impres-
sions in group i per dollar spent on advertising in channel j. (These entries are
estimated, and are nonnegative.) The jth column of R gives the eﬀectiveness or
reach (in impressions per dollar) for channel j. The ith row of R shows which
media demographic group i is exposed to. The total number of impressions in each
demographic group is the m-vector v, which is given by v = Rs. The goal is to
ﬁnd s so that v = Rs ≈ vdes. We can do this using least squares, by choosing
s to minimize �Rs − vdes�2. (We are not guaranteed that the resulting channel
spend vector will be nonnegative.) This least squares formulation does not take
into account the total cost of the advertising; we will see in chapter 16 how this
can be done.

We consider a simple numerical example, with n = 3 channels and m = 10

demographic groups, and matrix

0.97
1.23
0.80
1.29
1.10
0.67
0.87
1.10
1.92
1.29

1.86
2.18
1.24
0.98
1.23
0.34
0.26
0.16
0.22
0.12

0.41
0.53
0.62
0.51
0.69
0.54
0.62
0.48
0.71
0.62



,



R =

with units of 1000 views per dollar. The entries of R range over an 18:1 range, so
the 3 channels are quite diﬀerent in terms of their audience reach; see ﬁgure 12.3.
We take vdes = (103)1, i.e., our goal is to reach one million customers in each of
the 10 demographic groups. Least squares gives the advertising budget allocation

ˆs = (62, 100, 1443),

which achieves a views vector with RMS error 132, or 13.2% of the target values.
The views vector is shown in ﬁgure 12.4.

Illumination. A set of n lamps illuminates an area that we divide into m regions
or pixels. We let li denote the lighting level in region i, so the m-vector l gives the
illumination levels across all regions. We let pi denote the power at which lamp
i operates, so the n-vector p gives the set of lamp powers. (The lamp powers are
nonnegative and also must not exceed a maximum allowed power, but we ignore
these issues here.)

12.4 Examples

235

Channel 1
Channel 2
Channel 3

2

1

s
n
o
i
s
s
e
r
p
m

I

1

2

3

4

5
6
Group

7

8

9

10

Figure 12.3 Number of impressions in ten demographic groups, per dollar
spent on advertising in three channels. The units are 1000 views per dollar.

s
n
o
i
s
s
e
r
p
m

I

1,000

500

0

1

2

3

4

6
5
Group

7

8

9

10

Figure 12.4 Views vector that best approximates the target of one million
impressions in each group.

236

12 Least squares

The vector of illumination levels is a linear function of the lamp powers, so we
have l = Ap for some m× n matrix A. The jth column of A gives the illumination
pattern for lamp j, i.e., the illumination when lamp j has power 1 and all other
lamps are oﬀ. We will assume that A has linearly independent columns (and
therefore is tall). The ith row of A gives the sensitivity of pixel i to the n lamp
powers.

The goal is to ﬁnd lamp powers that result in a desired illumination pattern
ldes, such as ldes = α1, which is uniform illumination with value α across the area.
In other words, we seek p so that Ap ≈ ldes. We can use least squares to ﬁnd ˆp that
minimizes the sum square deviation from the desired illumination, �Ap − ldes�2.
This gives the lamp power levels

ˆp = A†ldes = (AT A)−1AT ldes.

(We are not guaranteed that these powers are nonnegative, or less than the maxi-
mum allowed power level.)

An example is shown in ﬁgure 12.5. The area is a 25 × 25 grid with m = 625
pixels, each (say) 1m square. The lamps are at various heights ranging from 3m
to 6m, and at the positions shown in the ﬁgure. The illumination decays with an
inverse square law, so Aij is proportional to d−2
ij , where dij is the (3-D) distance
between the center of the pixel and the lamp position. The matrix A is scaled so
that when all lamps have power one, the average illumination level is one. The
desired illumination pattern is 1, i.e., uniform with value 1.

With p = 1, the resulting illumination pattern is shown in the top part of
ﬁgure 12.5. The RMS illumination error is 0.24. We can see that the corners are
quite a bit darker than the center, and there are pronounced bright spots directly
beneath each lamp. Using least squares we ﬁnd the lamp powers

ˆp = (1.46, 0.79, 2.97, 0.74, 0.08, 0.21, 0.21, 2.05, 0.91, 1.47).

The resulting illumination pattern has an RMS error of 0.14, about half of the
RMS error with all lamp powers set to one. The illumination pattern is shown in
the bottom plot of ﬁgure 12.5; we can see that the illumination is more uniform
than when all lamps have power 1. Most illumination values are near the target
level 1, with the corners a bit darker and the illumination a bit brighter directly
below each lamp, but less so than when all lamps have power one. This is clear
from ﬁgure 12.6, which shows the histogram of patch illumination values for all
lamp powers one, and for lamp powers ˆp.

12.4 Examples

237

25m

0

0

25m

1 (4.0m)

2 (3.5m)

3 (6.0m)

4 (4.0m)

6 (6.0m)

5 (4.0m)

7 (5.5m)

8 (5.0m)

9 (5.0m)

10 (4.5m)

1 (4.0m)

2 (3.5m)

25m

3 (6.0m)

4 (4.0m)

6 (6.0m)

5 (4.0m)

7 (5.5m)

8 (5.0m)

9 (5.0m)

10 (4.5m)

0

0

25m

1.4

1.2

1

0.8

0.6

1.4

1.2

1

0.8

0.6

Figure 12.5 A square area divided in a 25 × 25 grid. The circles show the
positions of 10 lamps, the number in parentheses next to each circle is the
height of the lamp. The top plot shows the illumination pattern with lamps
set to power one. The bottom plot shows the illumination pattern for the
lamp powers that minimize the sum square deviation with a desired uniform
illumination of one.

238

12 Least squares

s
l
e
x
i
p

f
o

r
e
b
m
u
N

s
l
e
x
i
p

f
o

r
e
b
m
u
N

120

100

80

60

40

20

0
0.2

120

100

80

60

40

20

0
0.2

0.4

0.6

0.8

1

Intensity

1.2

1.4

1.6

1.8

0.4

0.6

0.8

1

Intensity

1.2

1.4

1.6

1.8

Figure 12.6 Histograms of pixel illumination values using p = 1 (top) and ˆp
(bottom). The target intensity value is one.

Exercises

Exercises

239

12.1 Approximating a vector as a multiple of another one. In the special case n = 1, the general
least squares problem (12.1) reduces to ﬁnding a scalar x that minimizes �ax− b�2, where
a and b are m-vectors. (We write the matrix A here in lower case, since it is an m-vector.)
Assuming a and b are nonzero, show that �aˆx − b�2 = �b�2(sin θ)2, where θ = � (a, b).
This shows that the optimal relative error in approximating one vector by a multiple of
another one depends on their angle.

12.2 Least squares with orthonormal columns. Suppose the m × n matrix Q has orthonormal
columns and b is an m-vector. Show that ˆx = QT b is the vector that minimizes �Qx− b�2.
What is the complexity of computing ˆx, given Q and b, and how does it compare to the
complexity of a general least squares problem with an m × n coeﬃcient matrix?
12.3 Least angle property of least squares. Suppose the m×n matrix A has linearly independent
columns, and b is an m-vector. Let ˆx = A†b denote the least squares approximate solution
of Ax = b.

(a) Show that for any n-vector x, (Ax)T b = (Ax)T (Aˆx), i.e., the inner product of Ax
and b is the same as the inner product of Ax and Aˆx. Hint. Use (Ax)T b = xT (AT b)
and (AT A)ˆx = AT b.

(b) Show that when Aˆx and b are both nonzero, we have

(Aˆx)T b
�Aˆx��b�

= �Aˆx�
�b�

.

The left-hand side is the cosine of the angle between Aˆx and b. Hint. Apply part (a)
with x = ˆx.

(c) Least angle property of least squares. The choice x = ˆx minimizes the distance
between Ax and b. Show that x = ˆx also minimizes the angle between Ax and b.
(You can assume that Ax and b are nonzero.) Remark. For any positive scalar α,
x = αˆx also minimizes the angle between Ax and b.

12.4 Weighted least squares. In least squares, the objective (to be minimized) is

�Ax − b�2 =

m�i=1

(˜aT
i x − bi)2,

where ˜aT
problem, we minimize the objective

i are the rows of A, and the n-vector x is to chosen. In the weighted least squares

m�i=1

wi(˜aT

i x − bi)2,

where wi are given positive weights. The weights allow us to assign diﬀerent weights to the
diﬀerent components of the residual vector. (The objective of the weighted least squares
problem is the square of the weighted norm, �Ax − b�2
(a) Show that the weighted least squares objective can be expressed as �D(Ax − b)�2
for an appropriate diagonal matrix D. This allows us to solve the weighted least
squares problem as a standard least squares problem, by minimizing �Bx − d�2,
where B = DA and d = Db.

w, as deﬁned in exercise 3.28.)

(b) Show that when A has linearly independent columns, so does the matrix B.
(c) The least squares approximate solution is given by ˆx = (AT A)−1AT b. Give a similar
formula for the solution of the weighted least squares problem. You might want to
use the matrix W = diag(w) in your formula.

240

12 Least squares

12.5 Approximate right inverse. Suppose the tall m × n matrix A has linearly independent
columns. It does not have a right inverse, i.e., there is no n × m matrix X for which
AX = I. So instead we seek the n×m matrix X for which the residual matrix R = AX−I
has the smallest possible matrix norm. We call this matrix the least squares approximate
right inverse of A. Show that the least squares right inverse of A is given by X = A†.
Hint. This is a matrix least squares problem; see page 233.

12.6 Least squares equalizer design.

(See exercise 7.15.) You are given a channel impulse
response, the n-vector c. Your job is to ﬁnd an equalizer impulse response, the n-vector
h, that minimizes �h ∗ c − e1�2. You can assume that c1 �= 0. Remark. h is called an
equalizer since it approximately inverts, or undoes, convolution by c.
Explain how to ﬁnd h. Apply your method to ﬁnd the equalizer h for the channel c =
(1.0, 0.7, −0.3, −0.1, 0.05). Plot c, h, and h ∗ c.

12.7 Network tomography. A network consists of n links, labeled 1, . . . , n. A path through the
network is a subset of the links. (The order of the links on a path does not matter here.)
Each link has a (positive) delay, which is the time it takes to traverse it. We let d denote
the n-vector that gives the link delays. The total travel time of a path is the sum of the
delays of the links on the path. Our goal is to estimate the link delays (i.e., the vector d),
from a large number of (noisy) measurements of the travel times along diﬀerent paths.
This data is given to you as an N × n matrix P , where

Pij =� 1

0

link j is on path i
otherwise,

and an N -vector t whose entries are the (noisy) travel times along the N paths. You can
assume that N > n. You will choose your estimate ˆd by minimizing the RMS deviation
between the measured travel times (t) and the travel times predicted by the sum of the
link delays. Explain how to do this, and give a matrix expression for ˆd. If your expression
requires assumptions about the data P or t, state them explicitly.
Remark. This problem arises in several contexts. The network could be a computer
network, and a path gives the sequence of communication links data packets traverse.
The network could be a transportation system, with the links representing road segments.
12.8 Least squares and QR factorization. Suppose A is an m × n matrix with linearly inde-
pendent columns and QR factorization A = QR, and b is an m-vector. The vector Aˆx is
the linear combination of the columns of A that is closest to the vector b, i.e., it is the
projection of b onto the set of linear combinations of the columns of A.

(a) Show that Aˆx = QQT b. (The matrix QQT is called the projection matrix.)
(b) Show that �Aˆx− b�2 = �b�2 −�QT b�2. (This is the square of the distance between b

and the closest linear combination of the columns of A.)

12.9 Invertibility of matrix in sparse least squares formulation. Show that the (m+n)×(m+n)
coeﬃcient matrix appearing in equation (12.11) is invertible if and only if the columns of
A are linearly independent.

12.10 Numerical check of the least squares approximate solution. Generate a random 30 × 10
matrix A and a random 30-vector b. Compute the least squares approximate solution
ˆx = A†b and the associated residual norm squared �Aˆx − b�2. (There may be several
ways to do this, depending on the software package you use.) Generate three diﬀerent
random 10-vectors d1, d2, d3, and verify that �A(ˆx + di) − b�2 > �Aˆx − b�2 holds. (This
shows that x = ˆx has a smaller associated residual than the choices x = ˆx + di, i = 1, 2, 3.)

12.11 Complexity of matrix least squares problem. Explain how to compute the matrix least
squares approximate solution of AX = B, given by ˆX = A†B (see (12.12)), in no more
than 2mn2 + 3mnk ﬂops. (In contrast, solving k vector least squares problems to obtain
the columns of ˆX, in a na¨ıve way, requires 2mn2k ﬂops.)

Exercises

241

12.12 Least squares placement. The 2-vectors p1, . . . , pN represent the locations or positions of
N objects, for example, factories, warehouses, and stores. The last K of these locations
are ﬁxed and given; the goal in a placement problem to choose the locations of the ﬁrst
N − K objects. Our choice of the locations is guided by an undirected graph; an edge
between two objects means we would like them to be close to each other. In least squares
placement, we choose the locations p1, . . . , pN−K so as to minimize the sum of the squares
of the distances between objects connected by an edge,

where the L edges of the graph are given by (i1, j1), . . . , (iL, jL).

�pi1 − pj1�2 + ··· + �piL − pjL�2,

(a) Let D be the Dirichlet energy of the graph, as deﬁned on page 135. Show that the
sum of the squared distances between the N objects can be expressed as D(u)+D(v),
where u = ((p1)1, . . . , (pN )1) and v = ((p1)2, . . . , (pN )2) are N -vectors containing
the ﬁrst and second coordinates of the objects, respectively.

(b) Express the least squares placement problem as a least squares problem, with vari-
able x = (u1:(N−K), v1:(N−K)). In other words, express the objective above (the sum
of squares of the distances across edges) as �Ax−b�2, for an appropriate m×n matrix
A and m-vector b. You will ﬁnd that m = 2L. Hint. Recall that D(y) = �BT y�2,
where B is the incidence matrix of the graph.

(c) Solve the least squares placement problem for the speciﬁc problem with N = 10,

K = 4, L = 13, ﬁxed locations

p7 = (0, 0),

p8 = (0, 1),

p8 = (1, 1),

p10 = (1, 0),

and edges

(1, 3),

(1, 4),

(1, 7),

(2, 3),

(2, 5),

(2, 8),

(2, 9),

(3, 4),

(3, 5),

(4, 6),

(5, 6),

(6, 9),

(6, 10).

Plot the locations, showing the graph edges as lines connecting the locations.

12.13 Iterative method for least squares problem. Suppose that A has linearly independent
columns, so ˆx = A†b minimizes �Ax − b�2.
In this exercise we explore an iterative
method, due to the mathematician Lewis Richardson, that can be used to compute ˆx. We
deﬁne x(1) = 0 and for k = 1, 2, . . .,

x(k+1) = x(k) − µAT (Ax(k) − b),

where µ is a positive parameter, and the superscripts denote the iteration number. This
deﬁnes a sequence of vectors that converge to ˆx provided µ is not too large; the choice
µ = 1/�A�2, for example, always works. The iteration is terminated when AT (Ax(k) − b)
is small enough, which means the least squares optimality conditions are almost satisﬁed.
To implement the method we only need to multiply vectors by A and by AT . If we have
eﬃcient methods for carrying out these two matrix-vector multiplications, this iterative
method can be faster than algorithm 12.1 (although it does not give the exact solution).
Iterative methods are often used for very large scale least squares problems.

(a) Show that if x(k+1) = x(k), we have x(k) = ˆx.
(b) Express the vector sequence x(k) as a linear dynamical system with constant dy-

namics matrix and oﬀset, i.e., in the form x(k+1) = F x(k) + g.

(c) Generate a random 20 × 10 matrix A and 20-vector b, and compute ˆx = A†b. Run
the Richardson algorithm with µ = 1/�A�2 for 500 iterations, and plot �x(k) − ˆx�
to verify that x(k) appears to be converging to ˆx.

242

12 Least squares

12.14 Recursive least squares. In some applications of least squares the rows of the coeﬃcient
matrix A become available (or are added) sequentially, and we wish to solve the resulting
family of growing least squares problems. Deﬁne the k × n matrices and k-vectors

A(k) =

aT
1
...
aT
k

 = A1:k,1:n,

b(k) =

b1
...
bk

 = b1:k,

for k = 1, . . . , m. We wish to compute ˆx(k) = A(k)†b(k), for k = n, n + 1, . . . , m. We will
assume that the columns of A(n) are linearly independent, which implies that the columns
of A(k) are linearly independent for k = n, . . . , m. We will also assume that m is much
larger than n. The na¨ıve method for computing x(k) requires 2kn2 ﬂops, so the total cost
for k = n, . . . , m is

2kn2 =� m�k=n

m�k=n

k� (2n2) =� m2 − n2 + m + n

2

� (2n2) ≈ m2n2 ﬂops.

A simple trick allows us to compute x(k) for k = n . . . , m much more eﬃciently, with a
cost that grows linearly with m. The trick also requires memory storage order n2, which
does not depend on m. for k = 1, . . . , m, deﬁne

G(k) = (A(k))T A(k),

h(k) = (A(k))T b(k).

(a) Show that ˆx(k) = (G(k))−1h(k) for k = n, . . . , m. Hint. See (12.8).
(b) Show that G(k+1) = G(k) + akaT
(c) Recursive least squares is the following algorithm. For k = n, . . . , m, compute G(k+1)
and h(k+1) using (b); then compute ˆx(k) using (a). Work out the total ﬂop count for
this method, keeping only dominant terms. (You can include the cost of computing
G(n) and h(n), which should be negligible in the total.) Compare to the ﬂop count
for the na¨ıve method.

k and h(k+1) = h(k) + bkak, for k = 1, . . . , m − 1.

Remark. A further trick called the matrix inversion lemma (which is beyond the scope of
this book) can be used to reduce the complexity of recursive least squares to order mn2.
12.15 Minimizing a squared norm plus an aﬃne function. A generalization of the least squares

problem (12.1) adds an aﬃne function to the least squares objective,

minimize

�Ax − b�2 + cT x + d,

where the n-vector x is the variable to be chosen, and the (given) data are the m×n matrix
A, the m-vector b, the n-vector c, and the number d. We will use the same assumption
we use in least squares: The columns of A are linearly independent. This generalized
problem can be solved by reducing it to a standard least squares problem, using a trick
called completing the square.
Show that the objective of the problem above can be expressed in the form

�Ax − b�2 + cT x + d = �Ax − b + f�2 + g,

for some m-vector f and some constant g. It follows that we can solve the generalized
least squares problem by minimizing �Ax − (b − f )�, an ordinary least squares problem
with solution ˆx = A†(b − f ).
Hints. Express the norm squared term on the right-hand side as �(Ax − b) + f�2 and
expand it. Then argue that the equality above holds provided 2AT f = c. One possible
choice is f = (1/2)(A†)T c. (You must justify these statements.)

Exercises

243

12.16 Gram method for computing least squares approximate solution. Algorithm 12.1 uses the
QR factorization to compute the least squares approximate solution ˆx = A†b, where the
m × n matrix A has linearly independent columns. It has a complexity of 2mn2 ﬂops. In
this exercise we consider an alternative method: First, form the Gram matrix G = AT A
and the vector h = AT b; and then compute ˆx = G−1h (using algorithm 11.2). What is
the complexity of this method? Compare it to algorithm 12.1. Remark. You might ﬁnd
that the Gram algorithm appears to be a bit faster than the QR method, but the factor is
not large enough to have any practical signiﬁcance. The idea is useful in situations where
G is partially available and can be computed more eﬃciently than by multiplying A and
its transpose. An example is exercise 13.21.

Chapter 13

Least squares data ﬁtting

In this chapter we introduce one of the most important applications of least squares
methods, to the problem of data ﬁtting. The goal is to ﬁnd a mathematical model,
or an approximate model, of some relation, given some observed data.

13.1 Least squares data ﬁtting

Least squares is widely used as a method to construct a mathematical model from
some data, experiments, or observations. Suppose we have an n-vector x, and a
scalar y, and we believe that they are related, perhaps approximately, by some
function f : Rn → R:

y ≈ f (x).

The vector x might represent a set of n feature values, and is called the feature
vector or the vector of independent variables, depending on the context. The scalar
y represents some outcome (also called response variable) that we are interested
in. Or x might represent the previous n values of a time series, and y represents
the next value.

Data. We don’t know f , although we might have some idea about its general
form. But we do have some data, given by

x(1), . . . , x(N ),

y(1), . . . , y(N ),

where the n-vector x(i) is the feature vector and the scalar y(i) is the associated
value of the outcome for data sample i. We sometimes refer to the pair x(i), y(i)
as the ith data pair. These data are also called observations, examples, samples,
or measurements, depending on the context. Here we use the superscript (i) to
denote the ith data point: x(i) is an n-vector, the ith independent variable; the
number x(i)
j

is the value of jth feature for example i.

246

13 Least squares data ﬁtting

Model. We will form a model of the relationship between x and y, given by

y ≈ ˆf (x),

where ˆf : Rn → R. We write ˆy = ˆf (x), where ˆy is the (scalar) prediction (of the
outcome y), given the independent variable (vector) x. The hat appearing over f
is traditional notation that suggests that the function ˆf is an approximation of the
function f . The function ˆf is called the model, prediction function, or predictor.
For a speciﬁc value of the feature vector x, ˆy = ˆf (x) is the prediction of the
outcome.

Linear in the parameters model. We will focus on a speciﬁc form for the model,
which has the form

ˆf (x) = θ1f1(x) + ··· + θpfp(x),

where fi : Rn → R are basis functions or feature mappings that we choose, and
θi are the model parameters that we choose. This form of model is called linear in
the parameters, since for each x, ˆf (x) is a linear function of the model parameter
p-vector θ. The basis functions are usually chosen based on our idea of what f
looks like. (We will see many examples of this below.) Once the basis functions
have been chosen, there is the question of how to choose the model parameters,
given our set of data.

Prediction error. Our goal is to choose the model ˆf so that it is consistent with
the data, i.e., we have y(i) ≈ ˆf (x(i)), for i = 1, . . . , N . (There is another goal in
choosing ˆf , that we will discuss in §13.2.) For data sample i, our model predicts
the value ˆy(i) = ˆf (x(i)), so the prediction error or residual for this data point is

(Some authors deﬁne the prediction error in the opposite way, as ˆy(i) − y(i). We
will see that this does not aﬀect the methods developed in this chapter.)

r(i) = y(i) − ˆy(i).

Vector notation for outcomes, predictions, and residuals. For our data set and
model, we have the observed response y(i), the prediction ˆy(i), and the residual
or prediction error r(i), for each example i = 1, . . . , N . We will now use vector
notation to express these as N -vectors,

yd = (y(1), . . . , y(N )),

ˆyd = (ˆy(1), . . . , ˆy(N )),

rd = (r(1), . . . , r(N )),

respectively.
(In the notation used above to describe the approximate relation
between the feature vector and the outcome, y ≈ f (x), and the prediction function
ˆy = ˆf (x), the symbols y and ˆy refer to generic scalar values. With the superscript d
(for ‘data’), yd, ˆyd, and rd refer to the N -vectors of observed data values, predicted
values, and associated residuals.)

Using this vector notation we can express the (vector of) residuals as rd =
yd − ˆyd. A natural measure of how well the model predicts the observed data, or
how consistent it is with the observed data, is the RMS prediction error rms(rd).
The ratio rms(rd)/ rms(yd) gives a relative prediction error. For example, if the
relative prediction error is 0.1, we might say that the model predicts the outcomes,
or ﬁts the data, within 10%.

13.1 Least squares data ﬁtting

247

Least squares model ﬁtting. A very common method for choosing the model
parameters θ1, . . . , θp is to minimize the RMS prediction error on the given data
set, which is the same as minimizing the sum of squares of the prediction errors,
�rd�2. We now show that this is a least squares problem.

Expressing ˆy(i) = ˆf (x(i)) in terms of the model parameters, we have

ˆy(i) = Ai1θ1 + ··· + Aipθp,

i = 1, . . . , N,

where we deﬁne the N × p matrix A as

Aij = ˆfj(x(i)),

i = 1, . . . , N,

j = 1, . . . , p,

(13.1)

and the p-vector θ as θ = (θ1, . . . , θp). The jth column of A is the jth basis
function, evaluated at each of the data points x(1), . . . , x(N ). Its ith row gives the
values of the p basis functions on the ith data point x(i). In matrix-vector notation
we have

ˆyd = Aθ.

This simple equation shows how our choice of model parameters maps into the
vector of predicted values of the outcomes in the N diﬀerent experiments. We
know the matrix A from the given data points, and choice of basis functions; our
goal is to choose the p-vector of model coeﬃcients θ.

The sum of squares of the residuals is then

�rd�2 = �yd − ˆyd�2 = �yd − Aθ�2 = �Aθ − yd�2.

(In the last step we use the fact that the norm of a vector is the same as the norm
of its negative.) Choosing θ to minimize this is evidently a least squares problem,
of the same form as (12.1). Provided the columns of A are linearly independent,
we can solve this least squares problem to ﬁnd ˆθ, the model parameter values that
minimize the norm of the prediction error on our data set, as

(13.2)
We say that the model parameter values ˆθ are obtained by least squares ﬁtting on
the data set.

ˆθ = (AT A)−1AT yd = A†yd.

We can interpret each term in �yd − Aθ�2. The term ˆyd = Aθ is the N -vector
of measurements or outcomes that is predicted by our model, with the parameter
vector θ. The term yd is the N -vector of actual observed or measured outcomes.
The diﬀerence yd − Aθ is the N -vector of prediction errors. Finally, �yd − Aθ�2 is
the sum of squares of the prediction errors, also called the residual sum of squares
(RSS). This is minimized by the least squares ﬁt θ = ˆθ.

The number �yd − Aˆθ�2 is called the minimum sum square error (for the given
model basis and data set). The number �yd − Aˆθ�2/N is called the minimum
mean square error (MMSE) (of our model, on the data set).
Its squareroot is
the minimum RMS ﬁtting error. The model performance on the data set can be
visualized by plotting ˆy(i) versus y(i) on a scatter plot, with a dashed line showing
ˆy = y for reference.

Since �yd − Aθ�2 = �Aθ − yd�2, the same least squares model parameter is
obtained when the residual or prediction error is deﬁned as ˆyd − yd instead of (our
deﬁnition) yd − ˆyd. The residual sum of squares, minimum mean square error, and
RMS ﬁtting error also agree using this alternate deﬁnition of prediction error.

248

13 Least squares data ﬁtting

Some notation diﬀerences from chapter 12. Before proceeding we note some
diﬀerences in the meanings of symbols used in chapter 12 (on least squares) and in
this chapter on data ﬁtting, that the reader will need to keep in mind. In chapter 12,
the symbol x denotes a generic variable, the vector that we would like to ﬁnd, and
b refers to the so-called right-hand side, the vector we seek to approximate. In this
chapter, in the context of ﬁtting a model to data, the symbol x generically refers
to a feature vector; we want to ﬁnd θ, the vector of coeﬃcients in our model, and
the vector we seek to approximate is yd, a vector of (observed) data outcomes.
When we use least squares in this chapter, we will need to transcribe the results
or formulas from chapter 12 to the current context, as in the formula (13.2).

Least squares ﬁt with a constant. We start with the simplest possible ﬁt: We
take p = 1, with f1(x) = 1 for all x. In this case the model ˆf is a constant function,
with ˆf (x) = θ1 for all x. Least squares ﬁtting in this case is the same as choosing
the best constant value θ1 to approximate the data y(1), . . . , y(N ).

In this simple case, the matrix A in (13.1) is the N × 1 matrix 1, which always
has linearly independent columns (since it has one column, which is nonzero). The
formula (13.2) is then

ˆθ1 = (AT A)−1AT yd = N−11T yd = avg(yd),

where we use 1T 1 = N . So the best constant ﬁt to the data is simply its mean,

ˆf (x) = avg(yd).

The RMS ﬁt to the data (i.e., the RMS value of the optimal residual) is

rms(yd − avg(yd)1) = std(yd),

the standard deviation of the data. This gives a nice interpretation of the average
value and the standard deviation of the outcomes, as the best constant ﬁt and the
associated RMS error, respectively. It is common to compare the RMS ﬁtting error
for a more sophisticated model with the standard deviation of the outcomes, which
is the optimal RMS ﬁtting error for a constant model.

A simple example of a constant ﬁt is shown in ﬁgure 13.1. In this example we
have n = 1, so the data points x(i) are scalars. The green circles in the left-hand
plot show the data points; the blue line shows the prediction function ˆf (x) (which
has constant value). The right-hand plot shows a scatter plot of the data outcomes
y(i) versus the predicted values ˆy(i) (all of which are the same), with a dashed line
showing y = ˆy.

Independent column assumption. To use least squares ﬁtting we assume that
the columns of the matrix A in (13.1) are linearly independent. We can give
an interesting interpretation of what it means when this assumption fails. If the
columns of A are linearly dependent, it means that one of the columns can be
expressed as a linear combination of the others. Suppose, for example, that the
last column can be expressed as a linear combination of the ﬁrst p − 1 columns.
Using Aij = fj(x(i)), this means

fp(x(i)) = β1f1(x(i)) + ··· + βp−1fp−1(x(i)),

i = 1, . . . , N.

13.1 Least squares data ﬁtting

249

1

0.8

0.6

0.4

0.2

)
x
(
ˆf

0

0

0.2

0.4

0.8

1

0.6
x

)
d
y
(
g
v
a
=

)
i
(
ˆy

1

0.8

0.6

0.4

0.2

0

0

0.2

0.4

0.6

0.8

1

y(i)

Figure 13.1 The constant ﬁt ˆf (x) = avg(yd) to N = 20 data points and a
scatter plot of ˆy(i) versus y(i).

This says that the value of the pth basis function can be expressed as a linear
combination of the values of the ﬁrst p − 1 basis functions on the given data set.
Evidently, then, the pth basis function is redundant (on the given data set).

13.1.1 Fitting univariate functions

Suppose that n = 1, so the feature vector x is a scalar (as is the outcome y). The
relationship y ≈ f (x) says that y is approximately a (univariate) function f of x.
We can plot the data (x(i), y(i)) as points in the (x, y) plane, and we can plot the
model ˆf as a curve in the (x, y)-plane. This allows us to visualize the ﬁt of our
model to the data.

Straight-line ﬁt. We take basis functions f1(x) = 1 and f2(x) = x. Our model
has the form

ˆf (x) = θ1 + θ2x,

which is a straight line when plotted. (This is perhaps why ˆf is sometimes called a
linear model, even though it is in general an aﬃne, and not linear, function of x.)
Figure 13.2 shows an example. The matrix A in (13.1) is given by

A =

x(1)
1
x(2)
1
...
...
1 x(N )



=� 1 xd � ,

where in the right-hand side we use xd to denote the N -vector of values xd =
(x(1), . . . , x(N )). Provided that there are at least two diﬀerent values appearing in

250

13 Least squares data ﬁtting

ˆf (x)

Figure 13.2 Straight-line ﬁt to 50 points (x(i), y(i)) in a plane.

x

x(1), . . . , x(N ), this matrix has linearly independent columns. The parameters in
the optimal straight-line ﬁt to the data are given by

� θ1
θ2 � = (AT A)−1AT yd.

This expression is simple enough for us to work it out explicitly, although there is
no computational advantage to doing so. The Gram matrix is

The 2-vector AT yd is

1T xd

1T xd

AT A =� N
AT y =� 1T yd

(xd)T xd � .
(xd)T yd � ,

so we have (using the formula for the inverse of a 2 × 2 matrix)

�� 1T yd
(xd)T yd � .

ˆθ2 � =
� ˆθ1

N (xd)T xd − (1T xd)2� (xd)T xd −1T xd

−1T xd

N

1

Multiplying the scalar term by N 2, and dividing the matrix and vector terms by
N , we can express this as

� ˆθ1
ˆθ2 � =

rms(xd)2 − avg(xd)2� rms(xd)2 − avg(xd)

− avg(xd)

1

1

��

avg(yd)

(xd)T yd/N � .

The optimal slope ˆθ2 of the straight line ﬁt can be expressed more simply in terms of
the correlation coeﬃcient ρ between the data vectors xd and yd, and their standard

13.1 Least squares data ﬁtting

251

deviations. We have

ˆθ2 =

=

=

N (xd)T yd − (1T xd)(1T yd)

N (xd)T xd − (1T xd)2

(xd − avg(xd)1)T (yd − avg(yd)1)

�xd − avg(xd)1�2
ρ.

std(yd)
std(xd)

In the last step we used the deﬁnitions

ρ =

(xd − avg(xd)1)T (yd − avg(yd)1)

N std(xd) std(yd)

,

std(xd) = �xd − avg(xd)1�

√N

from chapter 3. From the ﬁrst of the two normal equations, N θ1+(1T xd)θ2 = 1T yd,
we also obtain a simple expression for ˆθ1:

Putting these results together, we can write the least squares ﬁt as

ˆθ1 = avg(yd) − ˆθ2 avg(xd).

ˆf (x) = avg(yd) + ρ

std(yd)
std(xd)

(x − avg(xd)).

(13.3)

(Note that x and y are generic scalar values, while xd and yd are vectors of the
observed data values.) When std(yd) �= 0, this can be expressed in the more
symmetric form

ˆy − avg(yd)

std(yd)

= ρ

x − avg(xd)

std(xd)

,

which has a nice interpretation. The left-hand side is the diﬀerence between the
predicted response value and the mean response value, divided by its standard de-
viation. The right-hand side is the correlation coeﬃcient ρ times the same quantity,
computed for the dependent variable.

The least squares straight-line ﬁt is used in many application areas.

Asset α and β in ﬁnance.
In ﬁnance, the straight-line ﬁt is used to predict the
return of an individual asset from the return of the whole market. (The return of
the whole market is typically taken to be a sum of the individual asset returns,
weighted by their capitalizations.) The straight-line model ˆf (x) = θ1 +θ2x predicts
the asset return from the market return x. The least squares straight-line ﬁt is
computed from observed market returns rmkt
and individual asset returns
rind
1 , . . . , rind

over some period of length T . We therefore take

, . . . , rmkt

T

1

T

xd = (rmkt

1

, . . . , rmkt

T

),

yd = (rind

1 , . . . , rind
T )

in (13.3). The model is typically written in the form

ˆf (x) = (rrf + α) + β(x − µmkt),

252

13 Least squares data ﬁtting

where rrf
is the risk-free interest rate over the period and µmkt = avg(xd) is
the average market return. Comparing this formula to the straight-line model
ˆf (x) = θ1 + θ2x, we ﬁnd that θ2 = β, and θ1 = rrf + α − βµmkt.
The prediction of asset return ˆf (x) has two components: A constant rrf +α, and
one that is proportional to the de-meaned market performance, β(x − µmkt). The
second component, which has average value zero, relates market return ﬂuctuations
to the asset return ﬂuctuations, and is related to the correlation of the asset and
market returns; see exercise 13.4. The parameter α is the average asset return,
over and above the risk-free interest rate. This model of asset return in terms of
the market return is so common that the terms ‘Alpha’ and ‘Beta’ are widely used
in ﬁnance. (Though not always with exactly the same meaning, since there are a
few variations on how the parameters are deﬁned.)

Time series trend. Suppose the data represents a series of samples of a quantity
y at time (epoch) x(i) = i. The straight-line ﬁt to the time series data,

ˆy(i) = θ1 + θ2i,

i = 1, . . . , N,

is called the trend line. Its slope, which is θ2, is interpreted as the trend in the
quantity over time. Subtracting the trend line from the original time series we
get the de-trended time series, yd − ˆyd. The de-trended time series shows how the
time series compares with its straight-line ﬁt: When it is positive, it means the
time series is above its straight-line ﬁt, and when it is negative, it is below the
straight-line ﬁt.

An example is shown in ﬁgures 13.3 and 13.4. Figure 13.3 shows world petroleum
consumption versus year, along with the straight-line ﬁt. Figure 13.4 shows the
de-trended world petroleum consumption.

Estimation of trend and seasonal component.
In the previous example, we used
least squares to approximate a time series yd = (y(1), . . . , y(N )) of length N by a
sum of two components: yd ≈ ˆyd = ˆyconst + ˆylin where
1
2
...
N

ˆyconst = θ11,

ˆylin = θ2

.



In many applications, the de-trended time series has a clear periodic component,
i.e., a component that repeats itself periodically. As an example, ﬁgure 13.5 shows
an estimate of the road traﬃc (total number of miles traveled in vehicles) in the
US, for each month between January 2000 and December 2014. The most striking
aspect of the time series is the pattern that is (approximately) repeated every year,
with a peak in the summer and a minimum in the winter. In addition there is a
slowly increasing long term trend. The bottom ﬁgure shows the least squares ﬁt of
a sum of two components

yd ≈ ˆyd = ˆylin + ˆyseas,

13.1 Least squares data ﬁtting

253

n
o
i
t
p
m
u
s
n
o
c
m
u
e
l
o
r
t
e
P

)
y
a
d

r
e
p

s
l
e
r
r
a
b

n
o
i
l
l
i

m

(

90

80

70

60

1980 1985 1990 1995 2000 2005 2010

Year

Figure 13.3 World petroleum consumption between 1980 and 2013 (dots)
and least squares straight-line ﬁt (data from www.eia.gov).

n
o
i
t
p
m
u
s
n
o
c
m
u
e
l
o
r
t
e
P

)
y
a
d

r
e
p

s
l
e
r
r
a
b

n
o
i
l
l
i

m

(

6

4

2

0

−2

1980 1985 1990 1995 2000 2005 2010

Year

Figure 13.4 De-trended world petroleum consumption.

254

13 Least squares data ﬁtting

·105

2001

2002

2003

2004

2005

2006

·105

2009

2010

2011

2012

2013

2014

2015

2007

2008
Month

2001

2002

2003

2004

2005

2006

2007

2008

2009

2010

2011

2012

2013

2014

2015

)
s
n
o
i
l
l
i

m

(

s
e
l
i

M

2.6

2.4

2.2

2

Jan.2000

)
s
n
o
i
l
l
i

m

(

s
e
l
i

M

2.6

2.4

2.2

2

Jan.2000

Month

Figure 13.5 Top. Vehicle miles traveled in the US, per month, in the period
January 2000 – December 2014 (U.S. Department of Transportation, Bureau
of Transportation Statistics, www.transtats.bts.gov). Bottom. Least squares
ﬁt of a sum of two time series: A linear trend and a seasonal component
with a 12-month period.

13.1 Least squares data ﬁtting

255

where ˆylin and ˆyseas are deﬁned as

ˆylin = θ1

1
2
...
N

,



ˆyseas =

θ2:(P +1)
θ2:(P +1)

...

θ2:(P +1)

.



The second component is periodic or seasonal, with period P = 12, and consists of
the pattern (θ2, . . . , θP +1), repeated N/P times (we assume N is a multiple of P ).
The constant term is omitted in the model because it would be redundant: It has
the same eﬀect as adding a constant to the parameters θ2, . . . , θP +1.

The least squares ﬁt is computed by minimizing �Aθ−yd�2 where θ is a (P +1)-

vector and the matrix A in (13.1) is given by

A =



1
2
...
P

P + 1
P + 2

...
2P
...

N − P + 1
N − P + 2

...
N

1
0
...
0
1
0
...
0
...
1
0
...
0

0
1
...
0
0
1
...
0
...
0
1
...
0

···
···
. . .
···
···
···
. . .
···

···
···
. . .
···

0
0
...
1
0
0
...
1
...
0
0
...
1

.



In this example, N = 15P = 180. The residual or prediction error in this case is
called the de-trended, seasonally-adjusted series.

Polynomial ﬁt. A simple extension beyond the straight-line ﬁt is a polynomial ﬁt,
with

fi(x) = xi−1,

i = 1, . . . , p,

so ˆf is a polynomial of degree at most p − 1,

ˆf (x) = θ1 + θ2x + ··· + θpxp−1.

(Note that here, xi means the generic scalar value x raised to the ith power; x(i)
means the ith observed scalar data value.) In this case the matrix A in (13.1) has
the form

A =

x(1)
1
x(2)
1
...
...
1 x(N )

(x(1))p−1
(x(2))p−1

···
···

...

(x(N ))p−1

···

,



256

13 Least squares data ﬁtting

ˆf (x)

Degree 2

ˆf (x)

Degree 6

ˆf (x)

Degree 10

Degree 15

x

ˆf (x)

x

x

x

Figure 13.6 Least squares polynomial ﬁts of degree 2, 6, 10, and 15 to 100
points.

i.e., it is a Vandermonde matrix (see (6.7)). Its columns are linearly independent
provided the numbers x(1), . . . , x(N ) include at least p diﬀerent values. Figure 13.6
shows an example of the least squares ﬁt of polynomials of degree 2, 6, 10, and 15
to a set of 100 data points. Since any polynomial of degree less than r is also a
polynomial of degree less than s, for r ≤ s, it follows that the RMS ﬁt attained
by a polynomial with a larger degree is smaller (or at least, no larger) than that
obtained by a ﬁt with a smaller degree polynomial. This suggests that we should
use the largest degree polynomial that we can, since this results in the smallest
residual and the best RMS ﬁt. But we will see in §13.2 that this is not true, and
explore rational methods for choosing a model from among several candidates.

Piecewise-linear ﬁt. A piecewise-linear function, with knot points or kink points
a1 < a2 < ··· < ak, is a continuous function that is aﬃne in between the knot
points. (Such functions should be called piecewise-aﬃne.) We can describe any

13.1 Least squares data ﬁtting

257

(x + 1)+

(x − 1)+

3

2

1

0

3

2

1

0

x

3

1

x

3

1

−1

−3
Figure 13.7 The piecewise-linear functions (x + 1)+ = max{x + 1, 0} and
(x − 1)+ = max{x − 1, 0}.

−3

−1

piecewise-linear function with k knot points using the p = k + 2 basis functions

f1(x) = 1,

f2(x) = x,

fi+2(x) = (x − ai)+,

i = 1, . . . , k,

where (u)+ = max{u, 0}. These basis functions are shown in ﬁgure 13.7 for k = 2
knot points at a1 = −1, a2 = 1. An example of a piecewise-linear ﬁt with these
knot points is shown in ﬁgure 13.8.

13.1.2 Regression

We now return to the general case when x is an n-vector. Recall that the regression
model has the form

ˆy = xT β + v,

where β is the weight vector and v is the oﬀset. We can put this model in our
general data ﬁtting form using the basis functions f1(x) = 1, and

fi(x) = xi−1,

i = 2, . . . , n + 1,

so p = n + 1. The regression model can then be expressed as

ˆy = xT θ2:(n+1) + θ1,

and we see that β = θ2:n+1 and v = θ1.

The N × (n + 1) matrix A in our general data ﬁtting form is given by

A =� 1 X T � ,

where X is the feature matrix with columns x(1), . . . , x(N ). So the regression model
is a special case of our general linear in the parameters model.

258

13 Least squares data ﬁtting

ˆf (x)

−2

−1

0

1

2

x

Figure 13.8 Piecewise-linear ﬁt to 100 points.

General ﬁtting model as regression. The regression model is a special case of
our general data ﬁtting model. Conversely, we can think of our linear in the
parameters model as regression, with a diﬀerent set of feature vectors of dimension
p − 1. Assuming the ﬁrst basis element f1 is the constant function with value one,
we consider the new or generated feature vectors ˜x given by

A regression model for the outcome y and the new generated or mapped features
˜x has the form

ˆy = ˜xT β + v,

where β has dimension p − 1, and v is a number. Comparing this to our linear in
the parameters model

ˆy = θ1f1(x) + ··· + θpfp(x),

we see that they are the same, with v = θ1, β = θ2:p. So we can think of the general
linear in the parameters model as nothing more than simple regression, but applied
to the transformed, mapped, or generated features f1(x), . . . , fp(x). (This idea is
discussed more in §13.3.)
House price regression.
In §2.3 we described a simple regression model for the
selling price of a house based on two attributes, area and number of bedrooms.
The values of the parameters β and the oﬀset v given in (2.9) were computed by
least squares ﬁtting on a set of data consisting of 774 house sales in Sacramento
over a 5 day period. The RMS ﬁtting error for the model is 74.8 (in thousands of
dollars). For comparison, the standard deviation of the prices in the data set is
112.8. So this very basic regression model predicts the prices substantially better
than a constant model (i.e., the mean price of the houses in the data set).

˜x =

f2(x)

...

fp(x)

 .

13.1 Least squares data ﬁtting

259

Auto-regressive time series model. Suppose that z1, z2, . . . is a time series. An
auto-regressive model (also called AR model ) for the time series has the form

ˆzt+1 = θ1zt + ··· + θM zt−M +1,

t = M, M + 1, . . .

where M is the memory or lag of the model. Here ˆzt+1 is the prediction of zt+1
made at time t (when zt, . . . , zt−M +1 are known). This prediction is a linear func-
tion of the previous M values of the time series. With good choice of model
parameters, the AR model can be used to predict the next value in a time series,
given the current and previous M values. This has many practical uses.

We can use least squares (or regression) to choose the AR model parameters,
based on the observed data z1, . . . , zT , by minimizing the sum of squares of the
prediction errors zt − ˆzt over t = M + 1, . . . , T , i.e.,

(zM +1 − ˆzM +1)2 + ··· + (zT − ˆzT )2.

(We must start the predictions at t = M + 1, since each prediction involves the
previous M time series values, and we do not know z0, z−1, . . ..)

The AR model can be put into the general linear in the parameters model form

by taking

y(i) = zM +i,

x(i) = (zM +i−1, . . . , zi),

i = 1, . . . , T − M.

We have N = T − M examples, and n = M features.
As an example, consider the time series of hourly temperature at Los Angeles
International Airport, May 1–31, 2016, with length 31 · 24 = 744. The simple
constant prediction ˆzt+1 = 61.76◦F (the average temperature) has RMS prediction
error 3.05◦F (the standard deviation). The very simple predictor ˆzt+1 = zt, i.e.,
guessing that the temperature next hour is the same as the current temperature, has
RMS error 1.16◦F. The predictor ˆzt+1 = zt−23, i.e., guessing that the temperature
next hour is what is was yesterday at the same time, has RMS error 1.73◦F.
We ﬁt an AR model with memory M = 8 using least squares, with N =
31 · 24 − 8 = 736 samples. The RMS error of this predictor is 0.98◦F, smaller than
the RMS errors for the simple predictors described above. Figure 13.9 shows the
temperature and the predictions for the ﬁrst ﬁve days.

13.1.3 Log transform of dependent variable

When the dependent variable y is positive and varies over a large range, it is
common to replace it with its logarithm w = log y, and then use least squares to
develop a model for w, ˆw = ˆg(x). We then form our estimate of y using ˆy = eˆg(x).
When we ﬁt a model ˆw = ˆg(x) to the logarithm w = log y, the ﬁtting error for w
can be interpreted in terms of the percentage or relative error between ˆy and y,
deﬁned as

η = max{ˆy/y, y/ˆy} − 1.

So η = 0.1 means either ˆy = 1.1y (i.e., we over-estimate by 10%) or ˆy = (1/1.1)y
(i.e., we under-estimate by 10%). The connection between the relative error be-
tween ˆy and y, and the residual r in predicting w, is

η = e|r| − 1

260

13 Least squares data ﬁtting

)
F
◦
(

e
r
u
t
a
r
e
p
m
e
T

70

65

60

55

0

24

48

72

96

120

t

Figure 13.9 Hourly temperature at Los Angeles International Airport be-
tween 12:53AM on May 1, 2016, and 11:53PM on May 5, 2016, shown as
circles. The solid line is the prediction of an auto-regressive model with eight
coeﬃcients.

(see exercise 13.16). For example, a residual with |r| = 0.05 corresponds (ap-
proximately) to a relative error in our prediction ˆy of y of 5%. (Here we use the
approximation e|r| − 1 ≈ |r| for r smaller than, say, 0.15.) So if our RMS error in
predicting w = log y across our examples is 0.05, our predictions of y are typically
within ±5%.

As an example suppose we wish to model house sale prices over an area and
period that includes sale prices varying from under $100k to over $10M. Fitting the
prices directly means that we care equally about absolute errors in price predictions,
so, for example, a prediction error of $20k should bother us equally for a house that
sells for $70k and one that sells for $6.5M. But (at least for some purposes) in the
ﬁrst case we have made a very poor estimate, and the second case, a remarkably
good one. When we ﬁt the logarithm of the house sale price, we are seeking low
percentage prediction errors, not necessarily low absolute errors.

Whether or not to use a logarithmic transform on the dependent variable (when
it is positive) is a judgment call that depends on whether we seek small absolute
prediction errors or small relative or percentage prediction errors.

13.2 Validation

Generalization ability.
In this section we address a key point in model ﬁtting:
The goal of model ﬁtting is typically not to just achieve a good ﬁt on the given
data set, but rather to achieve a good ﬁt on new data that we have not yet seen.

13.2 Validation

261

This leads us to a basic question: How well can we expect a model to predict y for
future or other unknown values of x? Without some assumptions about the future
data, there is no good way to answer this question.

One very common assumption is that the data are described by a formal prob-
ability model. With this assumption, techniques from probability and statistics
can be used to predict how well a model will work on new, unseen data. This
approach has been very successful in many applications, and we hope that you will
learn about these methods in another course. In this book, however, we will take
a simple intuitive approach to this issue.

If a model predicts the outcomes for new unseen data values as well, or nearly
as well, as it predicts the outcomes on the data used to form the model, it is said
to have good generalization ability. In the opposite case, when the model makes
predictions on new unseen data that are much worse than the predictions on the
data used to form the model, the model is said to have poor generalization ability.
So our question is: How can we assess the generalization ability of a model?

Validation on a test set. A simple but eﬀective method for assessing the gener-
alization ability of a model is called out-of-sample validation. We divide the data
we have into two sets: A training set and a test set (also called a validation set).
This is often done randomly, with 80% of the data put into the training set and
20% put into the test set. A common way to describe this is to say that ‘20% of
the data were reserved for validation’. Another common choice for the split ratio
between the training set and the test set is 90%–10%.

To ﬁt our model, we use only the data in the training set. The model that we
come up with is based only on the data in the training set; the data in the test set
has never been ‘seen’ by the model. Then we judge the model by its RMS ﬁt on the
test set. Since the model was developed without any knowledge of the test set data,
the test data are eﬀectively data that are new and unseen, and the performance of
our model on this data gives us at least an idea of how our model will perform on
new, unseen data. If the RMS prediction error on the test set is much larger than
the RMS prediction error on the training set, we conclude that our model has poor
generalization ability. Assuming that the test data are ‘typical’ of future data, the
RMS prediction error on the test set is what we might guess our RMS prediction
error will be on new data.

If the RMS prediction error of the model on the training set is similar to the
RMS prediction error on the test set, we have increased conﬁdence that our model
has reasonable generalization ability.
(A more sophisticated validation method
called cross-validation, described below, can be used to gain even more conﬁdence.)
For example, if our model achieves an RMS prediction error of 10% (compared
to rms(y)) on the training set and 11% on the test set, we can guess that it will have
a similar RMS prediction error on other unseen data. But there is no guarantee
of this, without further assumptions about the data. The basic assumption we are
making here is that the future data will ‘look like’ the test data, or that the test
data were ‘typical’. Ideas from statistics can make this idea more precise, but we
will leave this idea informal and intuitive.

262

13 Least squares data ﬁtting

Over-ﬁtting. When the RMS prediction error on the training set is much smaller
than the RMS prediction error on the test set, we say that the model is over-ﬁt.
It tells us that, for the purposes of making predictions on new, unseen data, the
model is much less valuable than its performance on the training data suggests.
Roughly speaking, an over-ﬁt model trusts the data it has seen (i.e., the training
set) too much; it is too sensitive to the changes in the data that will likely be seen
in the future data. One method for avoiding over-ﬁt is to keep the model simple;
another technique, called regularization, is discussed in chapter 15. Over-ﬁt can be
detected and (one hopes) avoided by validating a model on a test set.

Model prediction quality and generalization ability. Model generalization ability
and training set prediction quality are not the same. A model can perform poorly
and yet have good generalization ability. As an example, consider the (very simple)
model that always makes the prediction ˆy = 0. This model will (likely) perform
poorly on the training set and the test set data, with similar RMS errors, assuming
the two data sets are ‘similar’. So this model has good generalization ability, but
has poor prediction quality. In general, we seek a model that makes good predictions
on the training data set and also makes good predictions on the test data set. In
other words, we seek a model with good performance and generalization ability.
We care much more about a model’s performance on the test data set than the
training data set, since its performance on the test data set is much more likely to
predict how the model will do on (other) unseen data.

Choosing among diﬀerent models. We can use least squares ﬁtting to ﬁt multiple
models to the same data. For example, in univariate ﬁtting, we can ﬁt a constant,
an aﬃne function, a quadratic, or a higher order polynomial. Which is the best
model among these? Assuming that the goal is to make good predictions on new,
unseen data, we should choose the model with the smallest RMS prediction error on
the test set. Since the RMS prediction error on the test set is only a guess about
what we might expect for performance on new, unseen data, we can soften this
advice to we should choose a model that has test set RMS error that is near the
minimum over the candidates. If multiple candidates achieve test set performance
near the minimum, we should choose the ‘simplest’ one among these candidates.

We observed earlier that when we add basis functions to a model, our ﬁtting
error on the training data can only decrease (or stay the same). But this is not
true for the test error. The test error need not decrease when we add more basis
functions. Indeed, when we have too many basis functions, we can expect over-ﬁt,
i.e., larger error on the test set.

If we have a sequence of basis functions f1, f2, . . . we can consider models based
on using just f1 (which is often the constant function 1), then f1 and f2, and so on.
As we increase p, the number of basis functions, our training error will go down
(or stay the same). But the test error typically decreases at ﬁrst and then starts to
increase for larger p. The intuition for this typical behavior is that for p too small,
our model is ‘too simple’ to ﬁt the data well, and so cannot make good predictions;
when p is too large, our model is ‘too complex’ and suﬀers from over-ﬁt, and so
makes poor predictions. Somewhere in the middle, where the model achieves near
minimum test set performance, is a good choice (or several good choices) of p.

13.2 Validation

263

ˆf (x)

Degree 2

ˆf (x)

Degree 6

ˆf (x)

Degree 10

Degree 15

x

ˆf (x)

x

x

x

Figure 13.10 The polynomial ﬁts of ﬁgure 13.6 evaluated on a test set of 100
points.

Example. To illustrate these ideas, we consider the example shown in ﬁgure 13.6.
Using a training set of 100 points, we ﬁnd least squares ﬁts of polynomials of degrees
0, 1, . . . , 20. (The polynomial ﬁts of degrees 2, 6, 10, and 15 are shown in the ﬁgure.)
We now obtain a new set of data for validation, also with 100 points. These test
data are plotted along with the polynomial ﬁts obtained from the training data
in ﬁgure 13.10. This is a real check of our models, since these data points were
not used to develop the models. Figure 13.11 shows the RMS training and test
errors for polynomial ﬁts of diﬀerent degrees. We can see that the RMS training
error decreases with every increase in degree. The RMS test error decreases until
degree 6 and starts to increase for degrees larger than 6. This plot suggests that a
polynomial ﬁt of degree 6 is a reasonable choice. (Degree 4 is another reasonable
choice, considering that it achieves nearly minimum test error, and is ‘simpler’ than
the model with degree 6.) Note also that the models with degrees 0, 1, and 2 have
good generalization ability (i.e., similar performance on the training and test sets),
but worse prediction performance than models with higher degrees.

264

13 Least squares data ﬁtting

Train
Test

r
o
r
r
e

S
M
R
e
v
i
t
a
l
e
R

1

0.8

0.6

0.4

0.2

0

5

10

Degree

15

20

Figure 13.11 RMS error versus polynomial degree for the ﬁtting example
in ﬁgures 13.6 and 13.10. Circles indicate RMS errors on the training set.
Squares show RMS errors on the test set.

With a 6th degree polynomial, the relative RMS test error for both training
and test sets is around 0.3. It is a good sign, in terms of generalization ability,
that the training and test errors are similar. While there are no guarantees, we can
guess that the 6th degree polynomial model will have a relative RMS error around
0.3 on new, unseen data, provided the new, unseen data is suﬃciently similar to
the test set data.

Cross-validation. Cross-validation is an extension of out-of-sample validation that
can be used to get even more conﬁdence in the generalization ability of a model,
or more accurately, a choice of basis functions used to construct a model. We
divide the original data set into 10 sets, called folds. We then ﬁt the model using
folds 1, 2, . . . , 9 as training data, and fold 10 as test data. (So far, this is the same
as out-of-sample validation.) We then ﬁt the model using folds 1, 2, . . . , 8, 10 as
training data and fold 9 as the test data. We continue, ﬁtting a model for each
choice of one of the folds as the test set. We end up with 10 (presumably diﬀerent)
models, and 10 assessments of these models using the fold that was not used to ﬁt
the model. (We have described 10-fold cross-validation here; 5-fold cross-validation
is also commonly used.) If the test ﬁt performance of these 10 models is similar,
we can expect the same, or at least similar, performance on new unseen data. In
cross-validation we can also check for stability of the model coeﬃcients. This means
that the model coeﬃcients found in the diﬀerent folds are similar to each other.
Stability of the model coeﬃcients further enhances our conﬁdence in the model.

To obtain a single number that is our guess of the prediction RMS error we can
expect on new, unseen data, it is common practice to compute the RMS test set
error across all 10 folds. For example, if �1, . . . , �10 are the RMS prediction errors

13.2 Validation

265

Model parameters

RMS error

Fold

v

β1

β2

Train Test

1
2
3
4
5

60.65
54.00
49.06
47.96
60.24

143.36 −18.00
151.11 −20.30
157.75 −21.10
142.65 −14.35
150.13 −21.11

74.00
75.11
76.22
71.16
77.28

78.44
73.89
69.93
88.35
64.20

Table 13.1 Five-fold cross-validation for the simple regression model of the
house sales data set. The RMS cross-validation error is 75.41.

obtained by our models on the test folds, we take

�(�2

1 + ··· + �2

10)/10

(13.4)

as our guess of the RMS error our models might make on new data.
In a plot
like that in ﬁgure 13.11, the RMS test error over all folds is plotted, instead of the
RMS test error on the single data or validation set, as in that plot. The single
number (13.4) is called the RMS cross-validation error, or simply the RMS test
error (when cross-validation is used).

Note that cross-validation does not check a particular model, since it creates
10 diﬀerent (but hopefully not very diﬀerent) models. Cross-validation checks a
selection of basis functions. Once cross-validation is used to verify that a choice
of basis functions produces models that predict and generalize well, there is the
question of which of the 10 models one should use. The models should be not too
diﬀerent, so the choice really should not matter much. One reasonable choice is to
use the parameters obtained by ﬁtting a model over all the data; another option is
to use the average of the model parameters from the diﬀerent folds.

House price regression model. As an example, we apply cross-validation to assess
the generalization ability of the simple regression model of the house sales data
discussed in §2.3 and on page 258. The simple regression model described there,
based on house area and number of bedrooms, has an RMS ﬁtting error of 74.8
thousand dollars. Cross-validation will help us answer the question of how the
model might do on diﬀerent, unseen houses.

We randomly partition the data set of 774 sales records into ﬁve folds, four of
size 155 and one of size 154. Then we ﬁt ﬁve regression models, each of the form

ˆy = v + β1x1 + β2x2

to the data set after removing one of the folds. Table 13.1 summarizes the results.
The model parameters for the 5 diﬀerent regression models are not exactly the
same, but quite similar. The training and test RMS errors are reasonably similar,
which suggests that our model does not suﬀer from over-ﬁt. Scanning the RMS
error on the test sets, we can expect that our prediction error on new houses will

266

13 Least squares data ﬁtting

Fold

v

RMS error (train) RMS error (test)

1
2
3
4
5

230.11
230.25
228.04
225.23
230.23

110.93
113.49
114.47
110.35
114.51

119.91
109.96
105.79
122.27
105.59

Table 13.2 Five-fold cross-validation for the constant model of the house
sales data set. The RMS cross-validation error is 119.93.

be around 70–80 (thousand dollars) RMS. The RMS cross-validation error (13.4) is
75.41. We can also see that the model parameters change a bit, but not drastically,
in each of the folds. This gives us more conﬁdence that, for example, β2 being
negative is not a ﬂuke of the data.

For comparison, table 13.2 shows the RMS errors for the constant model ˆy = v,
where v is the mean price of the training set. The results suggest that the constant
model can predict house prices with a prediction error around 105–120 (thousand
dollars). The RMS cross-validation error for the constant model is 119.93.

Figure 13.12 shows the scatter plots of actual and regression model predicted
prices for each of the ﬁve training and test sets. The results for training and
test sets are reasonably similar in each case, which gives us conﬁdence that the
regression model will have similar performance on new, unseen houses.

Validating time series predictions. When the original data are unordered, for
example, patient records or customer purchase histories, the division of the data
into training and test sets is typically done randomly. This same method can
be used to validate a time series prediction model, such as an AR model, but it
does not give the best emulation of how the model will ultimately be used.
In
practice, the model will be trained on past data and then used to make predictions
on future data. When the training data in a time series prediction model are
randomly chosen, the model is being built with some knowledge of the future, a
phenomenon called look-ahead or peek-ahead. Look-ahead can make a model look
better than it really is at making predictions.

To avoid look-ahead, the training set for a time series prediction model is typ-
ically taken to be the data examples up to some point in time, and the test data
are chosen as points that are past that time (and sometimes, at least M samples
past that time, taking into account the memory of the predictor). In this way we
can say that the model is being tested by making predictions on data it has never
seen. As an example, we might train an AR model for some daily quantity using
data from the years 2006 through 2008, and then test the resulting AR model on
the data from year 2009.

As an example, we return to the AR model of hourly temperatures at Los
Angeles International Airport described on page 259. We divide the one month of
data into a training set (May 1–24) and a test set (May 25–31). The coeﬃcients in

13.2 Validation

267

800

600

400

200

0

0

800

600

400

200

0

0

800

600

400

200

0

0

Fold 2

200

400
Fold 4

600

800

Fold 1

200

400
Fold 3

800

600

400

200

600

800

0

0

800

600

400

200

200

400
Fold 5

600

800

0

0

200

400

600

800

200

400

600

800

Figure 13.12 Scatter plots of actual and predicted prices for the ﬁve simple
regression models of table 13.1. The horizontal axis is the actual selling price
and the vertical axis is the predicted price, both in thousands of dollars. Blue
circles are samples in the training set, red circles samples in the test set.

268

13 Least squares data ﬁtting

65

60

)
F
◦
(

e
r
u
t
a
r
e
p
m
e
T

55

0

24

48

72

96

120

t

Figure 13.13 Hourly temperature at Los Angeles International Airport be-
tween 12:53AM on May 25, 2016, and 11:53PM on May 29, 2016, shown
as circles. The solid line is the prediction of an auto-regressive model with
eight coeﬃcients, developed using training data from May 1 to May 24.

an AR model are computed using the (24)(24)−8 = 568 samples in the training set.
The RMS error on the training set is 1.03◦F. The RMS prediction error on the test
set is 0.98◦F, which is similar to the RMS prediction error on the training set, giving
us conﬁdence that the AR model is not over-ﬁt. (The fact that the test RMS error is
very slightly smaller than the training RMS error has no signiﬁcance.) Figure 13.13
shows the prediction on the ﬁrst ﬁve days of the test set. The predictions look very
similar to those shown in ﬁgure 13.9.

Limitations of out-of-sample and cross-validation. Here we mention a few limi-
tations of out-of-sample and cross-validation. First, the basic assumption that the
test data and future data are similar can (and does) fail in some applications. For
example, a model that predicts consumer demand, trained and validated on this
year’s data, can make much poorer predictions next year, simply because consumer
tastes shift. In ﬁnance, patterns of asset returns periodically shift, so models that
predict well on test data from this year need not predict well next year.

Another limitation arises when the data set is small, which makes it harder to
interpret out-of-sample and cross-validation results. In this case the out-of-sample
test RMS error might be small due to good luck, or large due to bad luck, in the
selection of the test set. In cross-validation the test results can vary considerably,
due to luck of which data points fall into the diﬀerent folds. Here too concepts
from statistics can make this idea more precise, but we leave it as an informal idea:
With small data sets, we can expect to see more variation in test RMS prediction
error than with larger data sets.

Despite these limitations, out-of-sample and cross-validation are powerful and

useful tools for assessing the generalization ability of a model.

13.3 Feature engineering

269

13.3 Feature engineering

In this section we discuss some methods used to ﬁnd appropriate basis functions or
feature mappings f1, . . . , fp. We observed above (in §13.1.2) that ﬁtting a linear in
the parameters model reduces to regression with new features which are the original
features x mapped through the basis (or feature mapping) functions f1, . . . , fp.
Choosing the feature mapping functions is sometimes called feature engineering,
since we are generating features to use in regression.

For a given data set we may consider several, or even many, candidate choices
of the basis functions. To choose among these candidate choices of basis functions,
we use out-of-sample validation or cross-validation.

Adding new features to get a richer model.
In many cases the basis functions
include the constant one, i.e., we have f1(x) = 1. (This is equivalent to having the
oﬀset in the basic regression model.) It is also very common to include the original
features as well, as in fi(x) = xi−1, i = 2, . . . , n + 1. If we do this, we are eﬀectively
starting with the basic regression model; we can then add new features to get a
richer model. In this case we have p > n, so there are more mapped features than
original features. (Whether or not it is a good idea to add the new features can be
determined by out-of-sample validation or cross-validation.)

Dimension reduction.
In some cases, and especially when the number n of the
original features is very large, the feature mappings are used to construct a smaller
set of p < n features. In this case we can think of the feature mappings or basis
functions as a dimension reduction or data aggregation procedure.

13.3.1 Transforming features

Standardizing features.
mon to apply a scaling and oﬀset to each original feature, say,

Instead of using the original features directly, it is com-

fi(x) = (xi − bi)/ai,

i = 2, . . . , n + 1,

so that across the data set, the average value of fi(x) is near zero, and the standard
(This is done by choosing bi to be near the mean of
deviation is around one.
the feature i values over the data set, and choosing ai to be near the standard
deviation of the values.) This is called standardizing or z-scoring the features.
The standardized feature values are easily interpretable since they correspond to
z-values; for example, f2(x) = +3.3 means that the value of original feature 2 is
quite a bit above the typical value. The standardization of each original feature is
typically the ﬁrst step in feature engineering.

Note that the constant feature f1(x) = 1 is not standardized. (In fact, it cannot

be standardized since its standard deviation across the data set is zero.)

Winsorizing features. When the data include some very large values that are
thought to be errors (say, in collecting the data), it is common to clip or win-
sorize the data. This means that we set any data values with absolute value larger

270

13 Least squares data ﬁtting

than some chosen threshold to their sign times the chosen threshold. Assuming,
for example, that a feature entry x5 has already been standardized (so it repre-
sents z-scores across the examples), we replace x5 with its winsorized value (with
threshold 3),

The term winsorize is named after the statistician Charles P. Winsor.

˜x5 =

|x5| ≤ 3
x5
3 x5 > 3
−3 x5 < −3.

Log transform. When feature values are positive and vary over a wide range,
it is common to replace them with their logarithms.
If the feature value also
includes the value 0 (so the logarithm is undeﬁned) a common variation on the log
transformation is to use ˜xk = log(xk + 1). This compresses the range of values that
we encounter. As an example, suppose the original features record the number
of visits to websites over some time period. These can easily vary over a range of
10000:1 (or even more) for a very popular website and a less popular one; taking the
logarithm of the visit counts gives a feature with less variation, which is possibly
more interpretable. (The decision as to whether to use the original feature values
or their logarithms can be decided by validation.)

13.3.2 Creating new features

Expanding categoricals. Some features take on only a few values, such as −1 and
1 or 0 and 1, which might represent some value like presence or absence of some
symptom. (Such features are called Boolean.) A Likert scale response (see page 71)
naturally only takes on a small number of values, such as −2, −1, 0, 1, 2. Another
example is an original feature that takes on the values 1, 2, . . . , 7, representing the
day of the week. Such features are called categorical in statistics, since they specify
which category the example is in, and not some real number.

Expanding a categorical feature with l values means replacing it with a set of
l − 1 new features, each of which is Boolean, and simply records whether or not
the original feature has the associated value. (When all these features are zero,
it means the original feature had the default value.) As an example, suppose the
original feature x1 takes on only the values −1, 0, and 1. Using the feature value
0 as the default feature value, we replace x1 with the two mapped features

f1(x) =� 1 x1 = −1

otherwise,

0

f2(x) =� 1 x1 = 1

0

otherwise.

In words, f1(x) tells us if x1 has the value −1, and f2(x) tells us if x1 has the value 1.
(We do not need a new feature for the default value x1 = 0; this corresponds to
f1(x) = f2(x) = 0.) This feature mapping is shown in table 13.3.

Expanding a categorical feature with l values into l − 1 features that encode
whether the feature has one of the (non-default) values is sometimes called one-hot
encoding, because for any data example, only one of the new feature values is one,
and the others are zero. (When the original feature has the default value, all the
new features are zero.)

13.3 Feature engineering

271

x1
−1
0
1

f1(x)

f2(x)

1
0
0

0
0
1

Table 13.3 The original categorical feature x1 takes on only the three values
listed in the ﬁrst column. This feature is replaced with (expanded into) the
two features f1(x) and f2(x) shown in the second and third columns.

There is no need to expand an original feature that is Boolean (i.e., takes on
two values). If the original Boolean feature is encoded with the values 0 and 1, and
0 is taken as the default value, then the one new feature value will be the same as
the original feature value.

As an example of expanding categoricals, consider a model that is used to
predict house prices based on various features that include the number of bedrooms,
that ranges from 1 to 5 (say). In the basic regression model, we use the number
of bedrooms directly as a feature. In the basic model there is one parameter value
that corresponds to value per bedroom; we multiply this parameter by the number
of bedrooms to get the contribution to our price prediction. In this model, the price
prediction increases (or decreases) by the same amount when we change the number
of bedrooms from 1 to 2 as it does when we change the number of bedrooms from
4 to 5. If we expand this categorical feature, using 2 bedrooms as the default, we
have 4 Boolean features that correspond to a house having 1, 3, 4, and 5 bedrooms.
We then have 4 parameters in our model, which assign diﬀerent amounts to add
to our prediction for houses with 1, 3, 4, and 5 bedrooms, respectively. This more
ﬂexible model can capture the idea that a change from 1 to 2 bedrooms is diﬀerent
from a change from 4 to 5 bedrooms.

Generalized additive model. We introduce new features that are nonlinear func-
tions of the original features, such as, for each xi, the functions min{xi + a, 0}
and max{xi − b, 0}, where a and b are parameters. These new features are readily
interpreted: min{xi + a, 0} is the amount by which feature xi is below −a, and
max{xi − b, 0} is the amount by which feature xi is above b. A common choice,
assuming that xi has already been standardized, is a = b = 1. This leads to the
predictor

where ψi is the piecewise-linear function

ˆy = ψ1(x1) + ··· + ψn(xn),

ψi(xi) = θn+i min{xi + a, 0} + θixi + θ2n+i max{xi − b, 0},

(13.5)

(13.6)

which has kink or knot points at the values −a and +b. The model (13.5) has 3n
parameters, corresponding to the original features, and the two additional features
per original feature. The prediction ˆy is a sum of functions of the original features,
and is called a generalized additive model. (More complex versions add more than
two additional functions of each original feature.)

272

13 Least squares data ﬁtting

ψ1(x1)

0.5

−1

−0.5

ψ2(x2)

0.5

x1

1

−1

x2

1

−0.5

Figure 13.14 The functions ψi in (13.6) for n = 2, a = b = 1, and θ1 = 0.5,
θ2 = −0.4, θ3 = 0.3, θ4 = −0.2, θ5 = −0.3, θ6 = 0.2.

Consider an example with n = 2 original features. Our prediction ˆy is a sum
of two piecewise-linear functions, each depending on one of the original features.
Figure 13.14 shows an example. In this example model, we can say that increasing
x1 increases our prediction ˆy; but for high values of x1 (i.e., above 1) the increase
in the prediction is less pronounced, and for low values (i.e., below −1), it is more
pronounced.

Products and interactions. New features can be developed from pairs of original
features, for example, their product. From the original features we can add xixj,
for i, j = 1, . . . , n, i ≤ j. Products are used to model interactions among the
features. Product features are easily interpretable when the original features are
Boolean, i.e., take the values 0 or 1. Thus xi = 1 means that feature i is present
or has occurred, and the new product feature xixj has the value 1 exactly when
both feature i and j have occurred.

Stratiﬁed models.
In a stratiﬁed model, we have several diﬀerent sub-models,
and choose the one to use depending on the values of the regressors. For example,
instead of treating gender as a regressor in a single model of some medical outcome,
we build two diﬀerent sub-models, one for male patients and one for female patients.
In this case we choose the sub-model to use based on one of the original features,
gender.

As a more general example, we can carry out clustering of the original feature
vectors, and ﬁt a separate model within each cluster. To evaluate ˆy for a new x, we
ﬁrst determine which cluster x is in, and then use the associated model. Whether
or not a stratiﬁed model is a good idea is checked using out-of-sample validation.

13.3 Feature engineering

273

13.3.3 Advanced feature generation methods

Custom mappings.
In many applications custom mappings of the raw data are
used as additional features, in addition to the original features given. For example
in a model meant to predict an asset’s future price using prior prices, we might also
use the highest and lowest prices over the last week. Another well known example
in ﬁnancial models is the price-to-earnings ratio, constructed from the price and
(last) earnings features.

In document analysis applications word count features are typically replaced
with term frequency inverse document frequency (TFIDF) values, which scale the
raw count values by a function of the frequency with which the word appears across
the given set of documents, usually in such a way that uncommon words are given
more weight. (There are many variations on the particular scaling function to use.
Which one to use in a given application can be determined by out-of-sample or
cross-validation.)

Predictions from other models.
In many applications there are existing models
for the data. A common trick is to use the predictions of these models as features
in your model.
In this case you can describe your model as one that combines
or blends the raw data available with predictions made from one or more existing
models to create a new prediction.

Distance to cluster representatives. We can build new features from a clustering
of the data into k groups. One simple method uses the cluster representatives
z1, . . . , zk, and gives k new features, given by f (x) = e−�x−zi�2/σ2
, where σ is a
parameter.

Random features. The new features are given by a nonlinear function of a random
linear combination of the original features. To add K new features of this type,
we ﬁrst generate a random K × n matrix R. We then generate new features as
(Rx)+ or |Rx|, where (·)+ and |·| are applied elementwise to the vector Rx. (Other
nonlinear functions can be used as well.)
This approach to generating new features is quite counter-intuitive, since you
would imagine that feature engineering should be done using detailed knowledge
of, and intuition about, the particular application. Nevertheless this method can
be very eﬀective in some applications.

Neural network features. A neural network computes transformed features using
compositions of linear transformations interspersed with nonlinear mappings such
as the absolute value. This architecture was originally inspired by biology, as a
crude model of how human and animal brains work. The ideas behind neural
networks are very old, but their use has accelerated over the last few years due to
a combination of new techniques, greatly increased computing power, and access
to large amounts of data. Neural networks can ﬁnd good feature mappings directly
from the data, provided there is a very large amount of data available.

274

13 Least squares data ﬁtting

13.3.4 Summary

The discussion above makes it clear that there is much art in choosing features to
use in a model. But it is important to keep several things in mind when creating
new features:

• Try simple models ﬁrst. Start with a constant, then a simple regression model,

and so on. You can compare more sophisticated models against these.

• Compare competing candidate models using validation. Adding new features
will always reduce the RMS error on the training data, but the important
question is whether or not it substantially reduces the RMS error on the test
or validation data sets. (We add the qualiﬁer ‘substantially’ here because a
small reduction in test set error is not meaningful.)

• Adding new features can easily lead to over-ﬁt.

(This will show up when
validating the model.) The most straightforward way to avoid over-ﬁt is to
keep the model simple. We mention here that another approach to avoiding
over-ﬁt, called regularization (covered in chapter 15), can be very eﬀective
when combined with feature engineering.

13.3.5 House price prediction

In this section we use feature engineering to develop a more complicated regres-
sion model for the house sales data, illustrating some of the methods described
above. As mentioned in §2.3, the data set contains records of 774 house sales in
the Sacramento area. For our more complex model we will use four base attributes
or original features:

• x1 is the area of the house (in 1000 square feet),
• x2 is the number of bedrooms,
• x3 is equal to one if the property is a condominium, and zero otherwise,
• x4 is the ﬁve-digit ZIP code.

Only the ﬁrst two attributes were used in the simple regression model

ˆy = β1x1 + β2x2 + v

given in §2.3.
modiﬁcation.

In that model, we do not carry out any feature engineering or

Feature engineering. Here we examine a more complicated model, with 8 basis
functions,

These basis functions are described below.

8�i=1

ˆy =

θifi(x).

13.3 Feature engineering

275

x4

f6(x)

f7(x)

f8(x)

95811, 95814, 95816, 95817, 95818, 95819

95608, 95610, 95621, 95626, 95628, 95655,
95660, 95662, 95670, 95673, 95683, 95691,
95742, 95815, 95821, 95825, 95827, 95833,
95834, 95835, 95838, 95841, 95842, 95843,
95864

95624, 95632, 95690, 95693, 95757, 95758,
95820, 95822, 95823, 95824, 95826, 95828,
95829, 95831, 95832

95603, 95614, 95630, 95635, 95648, 95650,
95661, 95663, 95677, 95678, 95682, 95722,
95746, 95747, 95762, 95765

0

1

0

0

0

0

1

0

0

0

0

1

Table 13.4 Deﬁnition of basis functions f6, f7, f8 as functions of x4 (5-digit
ZIP code).

The ﬁrst basis function is the constant f1(x) = 1. The next two are functions

of x1, the area of the house,

f2(x) = x1,

f3(x) = max{x1 − 1.5, 0}.

In words, f2(x) is the area of the house, and f3(x) is the amount by which the area
exceeds 1.5 (i.e., 1500 square feet). The ﬁrst three basis functions contribute to
the price prediction model a piecewise-linear function of the house area,

θ1f1(x) + θ2f2(x) + θ3f3(x) =� θ1 + θ2x1

x1 ≤ 1.5
θ1 + (θ2 + θ3)x1 x1 > 1.5,

with one knot at 1.5. This is an example of a generalized additive model described
on page 271.

The basis function f4(x) is equal to the number of bedrooms x2. The basis
function f5(x) is equal to x3, i.e., one if the property is a condominium, and
zero otherwise. In these cases we simply using the original feature value, with no
transformation or modiﬁcation.

The last three basis functions are again Boolean, and indicate or encode the
location of the house. We partition the 62 diﬀerent ZIP codes present in the data set
into four groups, corresponding to diﬀerent areas around the center of Sacramento,
as shown in table 13.4. The basis functions f6, f7, and f8 give a one-hot encoding
of the four groups of ZIP codes, as described on page 270.

The resulting model. The coeﬃcients in the least squares ﬁt are

θ1 = 115.62,

θ2 = 175.41,

θ5 = −19.05,

θ6 = −100.91,

θ3 = −42.75,
θ7 = −108.79,

θ4 = −17.88,
θ8 = −24.77.

276

13 Least squares data ﬁtting

800

600

400

200

)
s
r
a
l
l
o
d

d
n
a
s
u
o
h
t
(

ˆy

e
c
i
r
p

d
e
t
c
i
d
e
r
P

0

0

200
600
Actual price y (thousand dollars)

400

800

Figure 13.15 Scatter plot of actual and predicted prices for a model with
eight parameters.

The RMS ﬁtting error is 68.3, a bit better than the simple regression ﬁt, which
achieves RMS ﬁtting error of 74.8. Figure 13.15 shows a scatter plot of predicted
and actual prices.

To validate the model, we use 5-fold cross-validation, using the same folds as in
table 13.1 and ﬁgure 13.12. The results are shown in table 13.5 and ﬁgure 13.16.
The training and test set errors are similar, so our model is not over-ﬁt. We
also see that the test set errors are a bit better than those obtained by our simple
regression model; the RMS cross-validation errors are 69.29 and 75.41, respectively.
We conclude that our more complex model, that uses feature engineering, gives a
modest (around 8%) improvement in prediction ability over the simple regression
model based on only house area and number of bedrooms. (With more data, more
features, and more feature engineering, a much more accurate model of house price
can be developed.)

The table also shows that the model coeﬃcients are reasonably stable across
the diﬀerent folds, giving us more conﬁdence in the model. Another interesting
phenomenon we observe is that the test error for fold 5 is a bit lower on the test
set than on the training set. This occasionally happens, as a consequence of how
the original data were split into folds.

13.3 Feature engineering

277

800

600

400

200

0

0

800

600

400

200

0

0

800

600

400

200

0

0

Fold 2

200

400
Fold 4

600

800

Fold 1

200

400
Fold 3

800

600

400

200

600

800

0

0

800

600

400

200

200

400
Fold 5

600

800

0

0

200

400

600

800

200

400

600

800

Figure 13.16 Scatter plots of actual and predicted prices for the ﬁve models
of table 13.5. The horizontal axis is the actual selling price and the vertical
axis is the predicted price, both in thousands of dollars. Blue circles are
samples in the training set, red circles samples in the test set.

278

13 Least squares data ﬁtting

Model parameters

RMS error

θ4

θ5

θ1

θ2

θ3

Fold
Train Test
1 122.35 166.87 −39.27 −16.31 −23.97 −100.42 −106.66 −25.98 67.29 72.78
2 100.95 186.65 −55.80 −18.66 −14.81 −99.10 −109.62 −17.94 67.83 70.81
3 133.61 167.15 −23.62 −18.66 −14.71 −109.32 −114.41 −28.46 69.70 63.80
4 108.43 171.21 −41.25 −15.42 −17.68 −94.17 −103.63 −29.83 65.58 78.91
5 114.45 185.69 −52.71 −20.87 −23.26 −102.84 −110.46 −23.43 70.69 58.27

θ6

θ7

θ8

Table 13.5 Five-fold validation on the house sales data set. The RMS cross-
validation error is 69.29.

Exercises

Exercises

279

13.1 Error in straight-line ﬁt. Consider the straight-line ﬁt described on page 249, with data
given by the N -vectors xd and yd. Let rd = yd − ˆyd denote the residual or prediction
ρ is the correlation coeﬃcient of xd and yd (assumed non-constant). This shows that the

error using the straight-line model (13.3). Show that rms(rd) = std(yd)�1 − ρ2, where
RMS error with the straight-line ﬁt is a factor�1 − ρ2 smaller than the RMS error with

a constant ﬁt, which is std(yd). It follows that when xd and yd are highly correlated
(ρ ≈ 1) or anti-correlated (ρ ≈ −1), the straight-line ﬁt is much better than the constant
ﬁt. Hint. From (13.3) we have

ˆyd − yd = ρ

std(yd)
std(xd)

(xd − avg(xd)1) − (yd − avg(yd)1).

Expand the norm squared of this expression, and use

ρ =

(xd − avg(xd)1)T (yd − avg(yd)1)
�xd − avg(xd)1��yd − avg(yd)1�

.

13.2 Regression to the mean. Consider a data set in which the (scalar) x(i) is the parent’s
height (average of mother’s and father’s height), and y(i) is their child’s height. Assume
that over the data set the parent and child heights have the same mean value µ, and
the same standard deviation σ. We will also assume that the correlation coeﬃcient ρ
between parent and child heights is (strictly) between zero and one. (These assumptions
hold, at least approximately, in real data sets that are large enough.) Consider the simple
straight-line ﬁt or regression model given by (13.3), which predicts a child’s height from
the parent’s height. Show that this prediction of the child’s height lies (strictly) between
the parent’s height and the mean height µ (unless the parent’s height happens to be
exactly the mean µ). For example, if the parents are tall, i.e., have height above the
mean, we predict that their child will be shorter, but still tall. This phenomenon, called
regression to the mean, was ﬁrst observed by the early statistician Sir Francis Galton (who
indeed, studied a data set of parent’s and child’s heights).

13.3 Moore’s law. The ﬁgure and table below show the number of transistors N in 13 micro-

processors, and the year of their introduction.

Year

Transistors

1971
1972
1974
1978
1982
1985
1989
1993
1997
1999
2000
2002
2003

2,250
2,500
5,000
29,000
120,000
275,000
1,180,000
3,100,000
7,500,000
24,000,000
42,000,000
220,000,000
410,000,000

s
r
o
t
s
i
s
n
a
r
T

108

107

106

105

104

103

1970 1975 1980 1985 1990 1995 2000 2005

Year

280

13 Least squares data ﬁtting

The plot gives the number of transistors on a logarithmic scale. Find the least squares
straight-line ﬁt of the data using the model

log10 N ≈ θ1 + θ2(t − 1970),

where t is the year and N is the number of transistors. Note that θ1 is the model’s
prediction of the log of the number of transistors in 1970, and 10θ2 gives the model’s
prediction of the fractional increase in number of transistors per year.

(a) Find the coeﬃcients θ1 and θ2 that minimize the RMS error on the data, and give

the RMS error on the data. Plot the model you ﬁnd along with the data points.

(b) Use your model to predict the number of transistors in a microprocessor introduced
in 2015. Compare the prediction to the IBM Z13 microprocessor, released in 2015,
which has around 4 × 109 transistors.

(c) Compare your result with Moore’s law, which states that the number of transistors

per integrated circuit roughly doubles every one and a half to two years.

The computer scientist and Intel corporation co-founder Gordon Moore formulated the
law that bears his name in a magazine article published in 1965.

, . . . , rmkt

T

1

13.4 Asset α and β and market correlation. Suppose the T -vectors rind = (rind

T ) and
rmkt = (rmkt
) are return time series for a speciﬁc asset and the whole market,
as described on page 251. We let rrf denote the risk-free interest rate, µmkt and σmkt the
market return and risk (i.e., avg(rmkt) and std(rmkt)), and µ and σ the return and risk
of the asset (i.e., avg(rind) and std(rind)). Let ρ be the correlation coeﬃcient between
the market and asset return time series rmkt and rind. Express the asset α and β in terms
of rrf , µ, σ, µmkt, σmkt, and ρ.

1 , . . . , rind

13.5 Polynomial model with multiple features. The idea of polynomial models can be extended
from the case discussed on page 255 where there is only one feature. In this exercise we
consider a quadratic (degree two) model with 3 features, i.e., x is a 3-vector. This has
the form

ˆf (x) = a + b1x1 + b2x2 + b3x3 + c1x2

1 + c2x2

2 + c3x2

3 + c4x1x2 + c5x1x3 + c6x2x3,

where the scalar a, 3-vector b, and 6-vector c are the zeroth, ﬁrst, and second order
coeﬃcients in the model. Put this model into our general linear in the parameters form,
by giving p, and the basis functions f1, . . . , fp (which map 2-vectors to scalars).

13.6 Average prediction error. Consider a data ﬁtting problem, with ﬁrst basis function φ1(x) =
1, and data set x(1), . . . , x(N ), y(1), . . . , y(N ). Assume the matrix A in (13.1) has linearly
independent columns and let ˆθ denote the parameter values that minimize the mean square
prediction error over the data set. Let the N -vector ˆrd denote the prediction errors using
the optimal model parameter ˆθ. Show that avg(ˆrd) = 0. In other words: With the least
squares ﬁt, the mean of the prediction errors over the data set is zero. Hint. Use the
orthogonality principle (12.9), with z = e1.

13.7 Data matrix in auto-regressive time series model. An auto-regressive model with memory
M is ﬁt by minimizing the sum of the squares of the predictions errors on a data set
with T samples, z1, . . . , zT , as described on page 259. Find the matrix A and vector y for
which �Aβ − y�2 gives the sum of the squares of the prediction errors. Show that A is a
Toeplitz matrix (see page 138), i.e., entries Aij with the same value of i− j are the same.
13.8 Fitting an input-output convolution system. Let u1, . . . , uT and y1, . . . , yT be observed
input and output time series for a system that is thought to be an input-output convolution
system, meaning

yt ≈ ˆyt =

hjut−j+1,

t = 1, . . . , T,

n�j=1

Exercises

281

where we interpret ut as zero for t ≤ 0. Here the n-vector h is the system impulse response;
see page 140. This model of the relation between the input and output time series is also
called a moving average (MA) model. Find a matrix A and vector b for which

Show that A is Toeplitz. (See page 138.)

�Ah − b�2 = (y1 − ˆy1)2 + ··· + (yT − ˆyT )2.

13.9 Conclusions from 5-fold cross-validation. You have developed a regression model for
predicting a scalar outcome y from a feature vector x of dimension 20, using a collection
of N = 600 data points. The mean of the outcome variable y across the given data is
1.85, and its standard deviation is 0.32. After running 5-fold cross-validation we get the
following RMS test errors (based on forming a model based on the data excluding fold i,
and testing it on fold i).

Fold excluded RMS test error

1
2
3
4
5

0.13
0.11
0.09
0.11
0.14

(a) How would you expect your model to do on new, unseen (but similar) data? Respond

brieﬂy and justify your response.

(b) A co-worker observes that the regression model parameters found in the 5 diﬀerent
folds are quite close, but not the same. He says that for the production system, you
should use the regression model parameters found when you excluded fold 3 from
the original data, since it achieved the best RMS test error. Comment brieﬂy.

13.10 Augmenting features with the average. You are ﬁtting a regression model ˆy = xT β + v
to data, computing the model coeﬃcients β and v using least squares. A friend suggests
adding a new feature, which is the average of the original features. (That is, he suggests
using the new feature vector ˜x = (x, avg(x)).) He explains that by adding this new
feature, you might end up with a better model. (Of course, you would test the new model
using validation.) Is this a good idea?

13.11 Interpreting model ﬁtting results. Five diﬀerent models are ﬁt using the same training
data set, and tested on the same (separate) test set (which has the same size as the
training set). The RMS prediction errors for each model, on the training and test sets,
are reported below. Comment brieﬂy on the results for each model. You might mention
whether the model’s predictions are good or bad, whether it is likely to generalize to
unseen data, or whether it is over-ﬁt. You are also welcome to say that you don’t believe
the results, or think the reported numbers are ﬁshy.

Model Train RMS Test RMS

A
B
C
D
E

1.355
9.760
5.033
0.211
0.633

1.423
9.165
0.889
5.072
0.633

13.12 Standardizing Boolean features. (See page 269.) Suppose that the N -vector x gives the
value of a (scalar) Boolean feature across a set of N examples. (Boolean means that each
xi has the value 0 or 1. This might represent the presence or absence of a symptom, or
whether or not a day is a holiday.) How do we standardize such a feature? Express your
answer in terms of p, the fraction of xi that have the value 1. (You can assume that p > 0
and p < 1; otherwise the feature is constant.)

282

13 Least squares data ﬁtting

13.13 Interaction model with Boolean features. Consider a data ﬁtting problem in which all n
original features are Boolean, i.e., entries of x have the value 0 or 1. These features could
be the results of Boolean tests for a patient (or absence or presence of symptoms), or
a person’s response to a survey with yes/no questions. We wish to use these to predict
an outcome, the number y. Our model will include a constant feature 1, the original n
Boolean features, and all interaction terms, which have the form xixj where 1 ≤ i < j ≤ n.
(a) What is p, the total number of basis functions, in this model? Explicitly list the
basis functions for the case n = 3. You can decide their order. Hint. To count the
number of pairs i, j that satisfy 1 ≤ i < j ≤ n, use equation (5.7).

(b) Interpret (together) the following three coeﬃcients of θ: the one associated with
the original feature x3; the one associated with the original feature x5; and the one
associated with the new product feature x3x5. Hint. Consider the four possible
values of the pair x3, x5.

13.14 Least squares timing. A computer takes around one second to ﬁt a regression model (using

least squares) with 20 parameters using 106 data points.

(a) About how long do you guess it will take the same computer to ﬁt the same 20-

(b) About how long do you guess it will take the same computer to ﬁt a 200-parameter

parameter model using 107 data points (i.e., 10× more data points)?
model using 106 data points (i.e., 10× more model parameters)?

13.15 Estimating a matrix. Suppose that the n-vector x and the m-vector y are thought to be
approximately related by a linear function, i.e., y ≈ Ax, where A is an m× n matrix. We
do not know the matrix A, but we do have observed data,

x(1), . . . , x(N ),

y(1), . . . , y(N ).

We can estimate or guess the matrix A by choosing it to minimize

N�i=1

�Ax(i) − y(i)�2 = �AX − Y �2,

where X = [x(1) ··· x(N )] and Y = [y(1) ··· y(N )]. We denote this least squares estimate
as ˆA. (The notation here can be confusing, since X and Y are known, and A is to be
found; it is more conventional to have symbols near the beginning of the alphabet, like
A, denote known quantities, and symbols near the end, like X and Y , denote variables or
unknowns.)

(a) Show that ˆA = Y X†, assuming the rows of X are linearly independent. Hint. Use
�AX − Y �2 = �X T AT − Y T�2, which turns the problem into a matrix least squares
problem; see page 233.
(b) Suggest a good way to compute ˆA, and give the complexity in terms of n, m, and N .

13.16 Relative ﬁtting error and error ﬁtting the logarithm.

(See page 259.) The relative
ﬁtting error between a positive outcome y and its positive prediction ˆy is given by
η = max{ˆy/y, y/ˆy} − 1.
(This is often given as a percentage.) Let r be the residual
between their logarithms, r = log y − log ˆy. Show that η = e|r| − 1.
13.17 Fitting a rational function with a polynomial. Let x1, . . . , x11 be 11 points uniformly
spaced in the interval [−1, 1]. (This means xi = −1.0 + 0.2(i − 1) for i = 1, . . . , 11.)
Take yi = (1 + xi)/(1 + 5x2
i ), for i = 1, . . . , 11. Find the least squares ﬁt of polynomials
of degree 0, 1, . . . , 8 to these points. Plot the ﬁtting polynomials, and the true function
y = (1 + x)/(1 + 5x2), over the interval [−1.1, 1.1] (say, using 100 points). Note that the
interval for the plot, [−1.1, 1.1], extends a bit outside the range of the data used to ﬁt the
polynomials, [−1, 1]; this gives us an idea of how well the polynomial ﬁts can extrapolate.

Exercises

283

Generate a test data set by choosing u1, . . . , u10 uniformly spaced over [−1.1, 1.1], with
vi = (1 + ui)/(1 + 5u2
i ). Plot the RMS error of the polynomial ﬁts found above on this test
data set. On the same plot, show the RMS error of the polynomial ﬁts on the training
data set. Suggest a reasonable value for the degree of the polynomial ﬁt, based on the
RMS ﬁts on the training and test data. Remark. There is no practical reason to ﬁt a
rational function with a polynomial. This exercise is only meant to illustrate the ideas of
ﬁtting with diﬀerent basis functions, over-ﬁt, and validation with a test data set.

13.18 Vector auto-regressive model. The auto-regressive time series model (see page 259) can
be extended to handle times series z1, z2, . . . where zt are n-vectors. This arises in ap-
plications such as econometrics, where the entries of the vector give diﬀerent economic
quantities. The vector auto-regressive model (already mentioned on page 164) has the
same form as a scalar auto-regressive model,

ˆzt+1 = β1zt + ··· + βM zt−M +1,

t = M, M + 1, . . .

where M is the memory of the model; the diﬀerence here is that the model parameters
β1, . . . , βM are all n × n matrices. The total number of (scalar) parameters in the vector
auto-regressive model is M n2. These parameters are chosen to minimize the sum of the
squared norms of the prediction errors over a data set z1, . . . , zT ,

�zM +1 − ˆzM +1�2 + ··· + �zT − ˆzT�2.

(a) Give an interpretation of (β2)13. What would it mean if this coeﬃcient is very small?
(b) Fitting a vector auto-regressive model can be expressed as a matrix least squares
problem (see page 233), i.e., the problem of choosing the matrix X to minimize
�AX − B�2, where A and B are matrices. Find the matrices A and B. You can
consider the case M = 2. (It will be clear how to generalize your answer to larger
values of M .)
Hints. Use the (2n) × n matrix variable X = [ β1
((T − 2) × n matrix) B = [ z3
that �AX − B�2 is the sum of the squared norms of the prediction errors.

β2 ]T , and right-hand side
zT ]T . Your job is to ﬁnd the matrix A so

···

13.19 Sum of sinusoids time series model. Suppose that z1, z2, . . . is a time series. A very

common approximation of the time series is as a sum of K sinusoids

zt ≈ ˆzt =

ak cos(ωkt − φk),

t = 1, 2, . . . .

K�k=1

(The phase is usually chosen to lie in the range from −π to π.)

The kth term in this sum is called a sinusoid signal. The coeﬃcient ak ≥ 0 is called
the amplitude, ωk > 0 is called the frequency, and φk is called the phase of the kth
sinusoid.
In many
applications the frequencies are multiples of ω1, i.e., ωk = kω1 for k = 2, . . . , K, in which
case the approximation is called a Fourier approximation, named for the mathematician
Jean-Baptiste Joseph Fourier.
Suppose you have observed the values z1, . . . , zT , and wish to choose the sinusoid ampli-
tudes a1, . . . , aK and phases φ1, . . . , φK so as to minimize the RMS value of the approxi-
mation error (ˆz1 − z1, . . . , ˆzT − zT ). (We assume that the frequencies are given.) Explain
how to solve this using least squares model ﬁtting.
Hint. A sinusoid with amplitude a, frequency ω, and phase φ can be described by its
cosine and sine coeﬃcients α and β, where

a cos(ωt − φ) = α cos(ωt) + β sin(ωt),

where (using the cosine of sum formula) α = a cos φ, β = a sin φ. We can recover the
amplitude and phase from the cosine and sine coeﬃcients as

Express the problem in terms of the cosine and sine coeﬃcients.

a =�α2 + β2,

φ = arctan(β/α).

284

13 Least squares data ﬁtting

13.20 Fitting with continuous and discontinuous piecewise-linear functions. Consider a ﬁtting
problem with n = 1, so x(1), . . . , x(N ) and y(1), . . . , y(N ) are numbers. We consider two
types of closely related models. The ﬁrst is a piecewise-linear model with knot points
at −1 and 1, as described on page 256, and illustrated in ﬁgure 13.8. The second is a
stratiﬁed model (see page 272), with three independent aﬃne models, one for x < −1,
one for −1 ≤ x ≤ 1, and one for x > 1. (In other words, we stratify on x taking low,
middle, or high values.) Are these two models the same? Is one more general than the
other? How many parameters does each model have? Hint. See problem title.
What can you say about the training set RMS error and test set RMS error that would
be achieved using least squares with these two models?

13.21 Eﬃcient cross-validation. The cost of ﬁtting a model with p basis functions and N
data points (say, using QR factorization) is 2N p2 ﬂops. In this exercise we explore the
complexity of carrying out 10-fold cross-validation on the same data set. We divide
the data set into 10 folds, each with N/10 data points. The na¨ıve method is to ﬁt 10
diﬀerent models, each using 9 of the folds, using the QR factorization, which requires
10 · 2(0.9)N p2 = 18N p2 ﬂops. (To evaluate each of these models on the remaining fold
requires 2(N/10)p ﬂops, which can be ignored compared to the cost of ﬁtting the models.)
So the na¨ıve method of carrying out 10-fold cross-validation requires, not surprisingly,
around 10× the number of ﬂops as ﬁtting a single model.
The method below outlines another method to carry out 10-fold cross-validation. Give the
total ﬂop count for each step, keeping only the dominant terms, and compare the total cost
of the method to that of the na¨ıve method. Let A1, . . . , A10 denote the (N/10)× p blocks
of the data matrix associated with the folds, and let b1, . . . , b10 denote the right-hand
sides in the least squares ﬁtting problem.

(a) Form the Gram matrices Gi = AT
(b) Form G = G1 + ··· + G10 and c = c1 + ··· + c10.
(c) For k = 1, . . . , 10, compute θk = (G − Gk)−1(c − ck).

i Ai and the vectors ci = AT

i bi.

13.22 Prediction contests. Several companies have run prediction contests open to the public.
Netﬂix ran the best known contest, oﬀering a $1M prize for the ﬁrst prediction of user
movie rating that beat their existing method RMS prediction error by 10% on a test set.
The contests generally work like this (although there are several variations on this format,
and most are more complicated). The company posts a public data set, that includes the
regressors or features and the outcome for a large number of examples. They also post the
features, but not the outcomes, for a (typically smaller) test data set. The contestants,
usually teams with obscure names, submit predictions for the outcomes in the test set.
Usually there is a limit on how many times, or how frequently, each team can submit
a prediction on the test set. The company computes the RMS test set prediction error
(say) for each submission. The teams’ prediction performance is shown on a leaderboard,
which lists the 100 or so best predictions in order.
Discuss such contests in terms of model validation. How should a team check a set of pre-
dictions before submitting it? What would happen if there were no limits on the number
of predictions each team can submit? Suggest an obvious method (typically disallowed
by the contest rules) for a team to get around the limit on prediction submissions. (And
yes, it has been done.)

Chapter 14

Least squares classiﬁcation

In this chapter we consider the problem of ﬁtting a model to data where the outcome
takes on values like true or false (as opposed to being numbers, as in chapter 13).
We will see that least squares can be used for this problem as well.

14.1 Classiﬁcation

In the data ﬁtting problem of chapter 13, the goal is to reproduce or predict the
outcome y, which is a (scalar) number, based on an n-vector x. In a classiﬁcation
problem, the outcome or dependent variable y takes on only a ﬁnite number of
values, and for this reason is sometimes called a label, or in statistics, a categorical.
In the simplest case, y has only two values, for example true or false, or spam
or not spam. This is called the two-way classiﬁcation problem, the binary classiﬁ-
cation problem, or the Boolean classiﬁcation problem, since the outcome y can take
on only two values. We start by considering the Boolean classiﬁcation problem.

We will encode y as a real number, taking y = +1 to mean true and y = −1
to mean false.
(It is also possible to encode the outcomes using y = +1 and
y = 0, or any other pair of two diﬀerent numbers.) As in real-valued data ﬁtting,
we assume that an approximate relationship of the form y ≈ f (x) holds, where
f : Rn → {−1, +1}. (This notation means that the function f takes an n-vector
argument, and gives a resulting value that is either +1 or −1.) Our model will
have the form ˆy = ˆf (x), where ˆf : Rn → {−1, +1}. The model ˆf is also called
a classiﬁer, since it classiﬁes n-vectors into those for which ˆf (x) = +1 and those
for which ˆf (x) = −1. As in real-valued data ﬁtting, we choose or construct the
classiﬁer ˆf using some observed data.

Examples. Boolean classiﬁers are widely used in many application areas.

• Email spam detection. The vector x contains features of an email message.
It can include word counts in the body of the email message, other features
such as the number of exclamation points or all-capital words, and features

286

14 Least squares classiﬁcation

related to the origin of the email. The outcome is +1 if the message is spam,
and −1 otherwise. The data used to create the classiﬁer comes from users
who have explicitly marked some messages as junk.

• Fraud detection. The vector x gives a set of features associated with a credit
card holder, such as her average monthly spending levels, median price of
purchases over the last week, number of purchases in diﬀerent categories, av-
erage balance, and so on, as well as some features associated with a particular
proposed transaction. The outcome y is +1 for a fraudulent transaction, and
−1 otherwise. The data used to create the classiﬁer is taken from historical
data, that includes (some) examples of transactions that were later veriﬁed
to be fraudulent and (many) that were veriﬁed to be bona ﬁde.

• Boolean document classiﬁcation. The vector x is a word count (or histogram)
vector for a document, and the outcome y is +1 if the document has some
speciﬁc topic (say, politics) and −1 otherwise. The data used to construct the
classiﬁer might come from a corpus of documents with their topics labeled.

• Disease detection. The examples correspond to patients, with outcome y =
+1 meaning the patient has a particular disease, and y = −1 meaning they
do not. The vector x contains relevant medical features associated with the
patient, including for example age, sex, results of tests, and speciﬁc symp-
toms. The data used to build the model come from hospital records or a
medical study; the outcome is the associated diagnosis (presence or absence
of the disease), conﬁrmed by a doctor.

• Digital communications receiver. In a modern electronic communications sys-
tem, y represents one bit (traditionally represented by the values 0 and 1)
that is to be sent from a transmitter to a receiver. The vector x represents
n measurements of a received signal. The predictor ˆy = ˆf (x) is called the
decoded bit. In communications, the classiﬁer ˆf is called a decoder or detec-
tor. The data used to construct the decoder comes from a training signal, a
sequence of bits known to the receiver, that is transmitted.

Prediction errors. For a given data point x, y, with predicted outcome ˆy = ˆf (x),
there are only four possibilities:

• True positive. y = +1 and ˆy = +1.
• True negative. y = −1 and ˆy = −1.
• False positive. y = −1 and ˆy = +1.
• False negative. y = +1 and ˆy = −1.

In the ﬁrst two cases the predicted label is correct, and in the last two cases, the
predicted label is an error. We refer to the third case as a false positive or type I
error, and we refer to the fourth case as a false negative or type II error. In some
applications we care equally about making the two types of errors; in others we
may care more about making one type of error than another.

14.1 Classiﬁcation

287

Prediction

Outcome

ˆy = +1

y = +1
y = −1

All

Ntp
Nfp

ˆy = −1
Nfn
Ntn

Total

Np
Nn
N

Ntp + Nfp Nfn + Ntp

Table 14.1 Confusion matrix. The oﬀ-diagonal entries Nfn and Nfp give the
numbers of the two types of error.

Error rate and confusion matrix. For a given data set

x(1), . . . , x(N ),

y(1), . . . , y(N ),

and model ˆf , we can count the numbers of each of the four possibilities that occur
across the data set, and display them in a contingency table or confusion matrix,
which is a 2 × 2 table with the columns corresponding to the value of ˆy(i) and the
rows corresponding to the value of y(i). (This is the convention used in machine
learning; in statistics, the rows and columns are sometimes reversed.) The entries
give the total number of each of the four cases listed above, as shown in table 14.1.
The diagonal entries correspond to correct decisions, with the upper left entry the
number of true positives, and the lower right entry the number of true negatives.
The oﬀ-diagonal entries correspond to errors, with the upper right entry the number
of false negatives, and the lower left entry the number of false positives. The total
of the four numbers is N , the number of examples in the data set. Sometimes the
totals of the rows and columns are shown, as in table 14.1.

Various performance metrics are expressed in terms of the numbers in the con-

fusion matrix.

• The error rate is the total number of errors (of both kinds) divided by the

total number of examples, i.e., (Nfp + Nfn)/N .

• The true positive rate (also known as the sensitivity or recall rate) is Ntp/Np.
This gives the fraction of the data points with y = +1 for which we correctly
guessed ˆy = +1.

• The false positive rate (also known as the false alarm rate) is Nfp/Nn. The
false positive rate is the fraction of data points with y = −1 for which we
incorrectly guess ˆy = +1.

• The speciﬁcity or true negative rate is one minus the false positive rate, i.e.,
Ntn/Nn. The true negative rate is the fraction of the data points with y = −1
for which we correctly guess ˆy = −1.

• The precision is Ntp/(Ntp + Nfp), the fraction of true predictions that are

correct.

288

14 Least squares classiﬁcation

Prediction

Outcome

ˆy = +1 (spam)

y = +1 (spam)

y = −1 (not spam)

All

95
19
114

ˆy = −1 (not spam) Total
127
1139
1266

32
1120
1152

Table 14.2 Confusion matrix of a spam detector on a data set of 1266 ex-
amples.

A good classiﬁer will have small (near zero) error rate and false positive rate, and
high (near one) true positive rate, true negative rate, and precision. Which of these
metrics is more important depends on the particular application.

An example confusion matrix is given in table 14.2 for the performance of a
spam detector on a data set of N = 1266 examples (emails) of which 127 are spam
(y = +1) and the remaining 1139 are not spam (y = −1). On the data set,
this classiﬁer has 95 true positives, 1120 true negatives, 19 false positives, and 32
false negatives. Its error rate is (19 + 32)/1266 = 4.03%. Its true positive rate
is 95/127 = 74.8% (meaning it is detecting around 75% of the spam in the data
set), and its false positive rate is 19/1139 = 1.67% (meaning it incorrectly labeled
around 1.7% of the non-spam messages as spam).

Validation in classiﬁcation problems.
In classiﬁcation problems we are concerned
with the error, true positive, and false positive rates. So out-of-sample validation
and cross-validation are carried out using the performance metric or metrics that
we care about, i.e., the error rate or some combination of true positive and false
negative rates. We may care more about one of these metrics than the others.

14.2 Least squares classiﬁer

Many sophisticated methods have been developed for constructing a Boolean model
or classiﬁer from a data set. Logistic regression and support vector machine are
two methods that are widely used, but beyond the scope of this book. Here we
discuss a very simple method, based on least squares, that can work quite well,
though not as well as the more sophisticated methods.

We ﬁrst carry out ordinary real-valued least squares ﬁtting of the outcome,
ignoring for the moment that the outcome y takes on only the values −1 and +1.
We choose basis functions f1, . . . , fp, and then choose the parameters θ1, . . . , θp so
as to minimize the sum squared error

(y(1) − ˜f (x(1)))2 + ··· + (y(N ) − ˜f (x(N )))2,

where ˜f (x) = θ1f1(x) + ··· + θpfp(x). We use the notation ˜f , since this function

14.2 Least squares classiﬁer

289

is not our ﬁnal model ˆf . The function ˜f is the least squares ﬁt over our data set,
and ˜f (x), for a general vector x, is a number.

Our ﬁnal classiﬁer is then taken to be

ˆf (x) = sign( ˜f (x)),

(14.1)

where sign(a) = +1 for a ≥ 0 and −1 for a < 0. We call this classiﬁer the least
squares classiﬁer.
The intuition behind the least squares classiﬁer is simple. The value ˜f (x) is a
number, which (ideally) is near +1 when y(i) = +1, and near −1 when y(i) = −1.
If we are forced to guess one of the two possible outcomes +1 or −1, it is natural to
choose sign( ˜f (x)). (Indeed, sign( ˜f (x)) is the nearest neighbor of ˜f (x) among the
points −1 and +1.) Intuition suggests that the number ˜f (x) can be related to our
conﬁdence in our guess ˆy = sign( ˜f (x)): When ˜f (x) is near 1 we have conﬁdence
in our guess ˆy = +1; when it is small and negative (say, ˜f (x) = −0.03), we guess
ˆy = −1, but our conﬁdence in the guess will be low. We won’t pursue this idea
further in this book, except in multi-class classiﬁers, which we discuss in §14.3.
The least squares classiﬁer is often used with a regression model, i.e., ˜f (x) =
xT β + v, in which case the classiﬁer has the form

ˆf (x) = sign(xT β + v).

(14.2)

We can easily interpret the coeﬃcients in this model. For example, if β7 is negative,
it means that the larger the value of x7 is, the more likely we are to guess ˆy = −1.
If β4 is the coeﬃcient with the largest magnitude, then we can say that x4 is the
feature that contributes the most to our classiﬁcation decision.

14.2.1

Iris ﬂower classiﬁcation

We illustrate least squares classiﬁcation with a famous data set, ﬁrst used in the
1930s by the statistician Ronald Fisher. The data are measurements of four at-
tributes of three types of iris ﬂowers: Iris Setosa, Iris Versicolour, and Iris Vir-
ginica. The data set contains 50 examples of each class. The four attributes are:

• x1 is the sepal length in cm,
• x2 is the sepal width in cm,
• x3 is the petal length in cm,
• x4 is the petal width in cm.

We compute a Boolean classiﬁer of the form (14.2) that distinguishes the class Iris
Virginica from the other two classes. Using the entire set of 150 examples we ﬁnd
the coeﬃcients

v = −2.39,

β1 = −0.0918,

β2 = 0.406,

β3 = 0.00798,

β4 = 1.10.

The confusion matrix associated with this classiﬁer is shown in table 14.3. The
error rate is 7.3%.

290

14 Least squares classiﬁcation

Prediction

Outcome

ˆy = +1

y = +1
y = −1

All

46
7
53

ˆy = −1 Total
50
100
150

4
93
97

Table 14.3 Confusion matrix for a Boolean classiﬁer of the Iris data set.

Model parameters

Error rate (%)

Fold

v

β1

1
2
3
4
5

−2.45
0.0240
−2.38 −0.0657
−2.63
0.0340
−1.89 −0.3338
−2.42 −0.1464

β3

β2
0.264 −0.00571
0.398 −0.07593
0.326 −0.08869
0.09902
0.577
0.456
0.11200

β4

Train

Test

0.994
1.251
1.189
1.151
0.944

6.7
5.8
7.5
6.7
8.3

3.3
10.0
3.3
16.7
3.3

Table 14.4 Five-fold validation for the Boolean classiﬁer of the Iris data set.

Validation. To test our least squares classiﬁcation method, we apply 5-fold cross-
validation. We randomly divide the data set into 5 folds of 30 examples (10 for
each class). The results are shown in table 14.4. The test data sets contain only
30 examples, so a single prediction error changes the test error rate signiﬁcantly
(i.e., by 3.3%). This explains what would seem to be large variation seen in the
test set error rates. We might guess that the classiﬁer will perform on new unseen
data with an error rate in the 7–10% range, but our test sets are not large enough
to predict future performance more accurately than this. (This is an example of
the limitation of cross-validation when the data set is small; see the discussion on
page 268.)

14.2.2 Handwritten digit classiﬁcation

We now consider a much larger example, the MNIST data set described in §4.4.1.
The (training) data set contains 60000 images of size 28 by 28. (A few samples are
shown in ﬁgure 4.6.) The number of examples per digit varies between 5421 (for
digit ﬁve) and 6742 (for digit one). The pixel intensities are scaled to lie between 0
and 1. We remove the pixels that are nonzero in fewer than 600 training examples.
The remaining 493 pixels are shown as the white area in ﬁgure 14.1. There is also
a separate test set containing 10000 images. Here we will consider classiﬁers to
distinguish the digit zero from the other nine digits.

In this ﬁrst experiment, we use the 493 pixel intensities, plus an additional
feature with value 1, as the n = 494 features in the least squares classiﬁer (14.1).

14.2 Least squares classiﬁer

291

Figure 14.1 Location of the pixels used as features in the handwritten digit
classiﬁcation example.

Prediction

Outcome

ˆy = +1

y = +1
y = −1

All

5158
167
5325

ˆy = −1 Total
5923
765
53910
54077
60000
54675

Table 14.5 Confusion matrix for a classiﬁer for recognizing the digit zero,
on a training set of 60000 examples.

The performance on the (training) data set is shown in the confusion matrix in
table 14.5. The error rate is 1.6%, the true positive rate is 87.1%, and the false
positive rate is 0.3%.

Figure 14.2 shows the distribution of the values of ˜f (x(i)) for the two classes in
the training set. The interval [−2.1, 2.1] is divided in 100 intervals of equal width.
For each interval, the height of the blue bar is the fraction of the total number of
training examples x(i) from class +1 (digit zero) that have a value ˜f (x(i)) in the
interval. The height of the red bar is the fraction of the total number of training
examples from class −1 (digits 1–9) with ˜f (x(i)) in the interval. The vertical dashed
line shows the decision boundary: For ˜f (x(i)) to the left (i.e., negative) we guess
that digit i is from class −1, i.e., digits 1–9; for ˜f (x(i)) to the right of the dashed
line, we guess that digit i is from class +1, i.e., digit 0. False positives correspond
to red bars to the right of the dashed line, and false negatives correspond to blue
bars to the left of the line.

Figure 14.3 shows the values of the coeﬃcients βk, displayed as an image. We
can interpret this image as a map of the sensitivity of our classiﬁer to the pixel

292

14 Least squares classiﬁcation

Positive
Negative

0.1

0.08

n
o
i
t
c
a
r
F

0.06

0.04

0.02

0

−2 −1.5 −1 −0.5

0

0.5

1

1.5

2

˜f (x(i))

Figure 14.2 The distribution of the values of ˜f (x(i)) in the Boolean classi-
ﬁer (14.1) for recognizing the digit zero, over all elements x(i) of the training
set. The red bars correspond to the digits from class −1, i.e., the digits 1–9;
the blue bars correspond to the digits from class +1, i.e., the digit zero.

14.2 Least squares classiﬁer

293

≥ 0.1

0.05

0

−0.05

≤ −0.1

Figure 14.3 The coeﬃcients βk in the least squares classiﬁer that distin-
guishes the digit zero from the other nine digits.

Prediction

Outcome

ˆy = +1

y = +1
y = −1

All

864
42
906

ˆy = −1 Total
980
116
9020
8978
9094
10000

Table 14.6 Confusion matrix for the classier for recognizing the digit zero,
on a test set of 10000 examples.

values. Pixels with βi = 0 are not used at all; pixels with larger positive values of
βi are locations where the larger the image pixel value, the more likely we are to
guess that the image represents the digit zero.

Validation. The performance of the least squares classiﬁer on the test set is shown
in the confusion matrix in table 14.6. For the test set the error rate is 1.6%, the
true positive rate is 88.2%, and the false positive rate is 0.5%. These performance
metrics are similar to those for the training data, which suggests that our classiﬁer
is not over-ﬁt, and gives us some conﬁdence in our classiﬁer.

Feature engineering. We now do some simple feature engineering (as in §13.3)
to improve our classiﬁer. As described on page 273, we add 5000 new features
to the original 494 features, as follows. We ﬁrst generate a 5000 × 494 matrix
R, with randomly chosen entries ±1. The 5000 new functions are then given by
max{0, (Rx)j}, for j = 1, . . . , 5000. After the addition of the 5000 new features (so

294

14 Least squares classiﬁcation

Prediction

Prediction

Outcome

ˆy = +1

y = +1
y = −1

All

5813
15
5828

ˆy = −1 Total
5923
110
54077
54062
54172
60000

Outcome

ˆy = +1

y = +1
y = −1

All

963
7
970

ˆy = −1 Total
980
9020
10000

17
9013
9030

Table 14.7 Confusion matrices for the Boolean classiﬁer to recognize the
digit zero after addition of 5000 new features. The table on the left is for
the training set. The table on the right is for the test set.

the total number is 5494), we get the confusion matrices for the training and test
data sets shown in table 14.7. The error rates are consistent, and equal to 0.21%
for the training set and 0.24% for the test set, a very substantial improvement
compared to the 1.6% in the ﬁrst experiment. A comparison of the distributions in
ﬁgures 14.4 and 14.2 also shows how much better the new classiﬁer distinguishes
between the two classes of the training set. We conclude that this was a successful
exercise in feature engineering.

14.2.3 Receiver operating characteristic

One useful modiﬁcation of the least squares classiﬁer (14.1) is to skew the decision
boundary, by subtracting a constant α from ˜f (x) before taking the sign:

The classiﬁer is then

ˆf (x) = sign( ˜f (x) − α).
ˆf (x) =� +1

˜f (x) ≥ α
˜f (x) < α.

−1

(14.3)

We call α the decision threshold for the modiﬁed classiﬁer. The basic least squares
classiﬁer (14.1) has decision threshold α = 0.

By choosing α positive, we make the guess ˆf (x) = +1 less frequently, so the
numbers in the ﬁrst column of the confusion matrix go down, and the numbers in
the second column go up (since the sum of the numbers in each row is always the
same). This means that choosing α positive decreases the true positive rate (which
is bad), but it also decreases the false positive rate (which is good). Choosing α
negative has the opposite eﬀect, increasing the true positive rate (which is good)
and increasing the false positive rate (which is bad). The parameter α is chosen
depending on how much we care about the two competing metrics, in the particular
application.

By sweeping α over a range, we obtain a family of classiﬁers that vary in their
true positive and false positive rates. We can plot the false positive and negative
rates, as well as the error rate, as a function of α. A more common way to plot this
data has the strange name receiver operating characteristic (ROC). The ROC shows

14.2 Least squares classiﬁer

295

Positive
Negative

0.15

0.10

n
o
i
t
c
a
r
F

0.05

0

−2 −1.5 −1 −0.5

0

0.5

1

1.5

2

˜f (x(i))

Figure 14.4 The distribution of the values of ˜f (x(i)) in the Boolean classi-
ﬁer (14.1) for recognizing the digit zero, after addition of 5000 new features.

the true positive rate on the vertical axis and false positive rate on the horizontal
axis. The name comes from radar systems deployed during World War II, where
y = +1 means that an enemy vehicle (or ship or airplane) is present, and ˆy = +1
means that an enemy vehicle is detected.

Example. We examine the skewed threshold least squares classiﬁer (14.3) for the
example described above, where we attempt to detect whether or not a handwritten
digit is zero. Figure 14.5 shows how the error, true positive, and false positive rates
depend on the decision threshold α, for the training set data. We can see that as
α increases, the true positive rate decreases, as does the false positive rate. We
can see that for this particular case the total error rate is minimized by choosing
α = −0.1, which gives error rate 1.4%, slightly lower than the basic least squares
classiﬁer. The limiting cases when α is negative enough, or positive enough, are
readily understood. When α is very negative, the prediction is always ˆy = +1; our
error rate is then the fraction of the data set with y = −1. When α is very positive,
the prediction is always ˆy = −1, which gives an error rate equal to the fraction of
the data set with y = +1.
The same information (without the total error rate) is plotted in the traditional
ROC curve shown in ﬁgure 14.6. The dots show the basic least squares classiﬁer,
with α = 0, and the skewed threshold least squares classiﬁers for α = −0.25 and
α = 0.25. These curves are for the training data; the same curves for the test
data look similar, giving us some conﬁdence that our classiﬁers will have similar
performance on new, unseen data.

296

14 Least squares classiﬁcation

0.1

0.08

0.06

0.04

0.02

n
o
i
t
c
a
r
F

0

1

0.8

0.6

0.4

0.2

0

e
t
a
R

Positive
Negative

−2 −1.5 −1 −0.5

0

0.5

1

1.5

2

˜f (x(i))

True positive

False positive

Total error

−2 −1.5 −1 −0.5

0
α

0.5

1

1.5

2

Figure 14.5 True positive, false positive, and total error rate versus decision
threshold α. The vertical dashed line is shown for decision threshold α =
0.25.

14.3 Multi-class classiﬁers

297

α = −0.25

α = 0

1

e
t
a
r

e
v
i
t
i
s
o
p

e
u
r
T

0.9

0.8

α = 0.25

0.7

0

0.05

0.1

0.15

0.2

False positive rate

Figure 14.6 ROC curve.

14.3 Multi-class classiﬁers

In a multi-class classiﬁcation problem, we have K > 2 possible labels. This is
sometimes referred to more compactly as K-class classiﬁcation. (The case K = 2
is Boolean classiﬁcation, discussed above.) For our generic discussion of multi-class
classiﬁers, we will encode the labels as y = 1, 2, . . . , K. In some applications there
are more natural encodings; for example, the Likert scale labels Strongly Disagree,
Disagree, Neutral, Agree, and Strongly Agree are typically encoded as −2,−1, 0, 1, 2,
respectively.
A multi-class classiﬁer is a function ˆf : Rn → {1, . . . , K}. Given a feature vector
x, ˆf (x) (which is an integer between 1 and K) is our prediction of the associated
outcome. A multi-class classiﬁer classiﬁes n-vectors into K groups, corresponding
to the values 1, . . . , K.

Examples. Multi-class classiﬁers are used in many application areas.

• Handwritten digit classiﬁcation. We are given an image of a hand-written
digit (and possibly other features generated from the images), and wish to
guess which of ten digits it represents. This classiﬁer is used to do automatic
(computer-based) reading of handwritten digits.

• Marketing demographic classiﬁcation. Data from purchases made, or web
sites visited, is used to train a multi-class classiﬁer for a set of market seg-
ments, such as college-educated women aged 25–30, men without college de-
grees aged 45–55, and so on. This classiﬁer guesses the demographic segment
of a new customer, based only on their purchase history. This can be used to
select which promotions to oﬀer a customer for whom we only have purchase
data. The classiﬁer is trained using data from known customers.

298

14 Least squares classiﬁcation

• Disease diagnosis. The labels are a set of diseases (including one label that
corresponds to disease-free), and the features are medically relevant values,
such as patient attributes and the results of tests. Such a classiﬁer carries
out diagnosis (of the diseases corresponding to the labels). The classiﬁer is
trained on cases in which a deﬁnitive diagnosis has been made.

• Translation word choice. A machine translation system translates a word in
the source language to one of several possible words in the target language.
The label corresponds to a particular choice of translation for the word in the
source language. The features contain information about the context around
the word, for example, words counts or occurrences in the same paragraph.
As an example, the English word ‘bank’ might be translated into another
language one way if the word ‘river’ appears nearby, and another way if
‘ﬁnancial’ or ‘reserves’ appears nearby. The classiﬁer is trained on data taken
from translations carried out by (human) experts.

• Document topic prediction. Each example corresponds to a document or
article, with the feature vector containing word counts or histograms, and
the label corresponding to the topic or category, such as Politics, Sports,
Entertainment, and so on.

• Detection in communications. Many electronic communications systems trans-
mit messages as a sequence of K possible symbols. The vector x contains
measurements of the received signal. In this context the classiﬁer ˆf is called
a detector or decoder ; the goal is to correctly determine which of the K
symbols was transmitted.

Prediction errors and confusion matrix. For a multi-class classiﬁer ˆf and a given
data point (x, y), with predicted outcome ˆy = ˆf (x), there are K 2 possibilities, cor-
responding to all the pairs of values of y, the actual outcome, and ˆy, the predicted
outcome. For a given data set (training or validation set) with N elements, the
numbers of each of the K 2 occurrences are arranged into a K×K confusion matrix,
where Nij is the number of data points for which y = i and ˆy = j.
The K diagonal entries N11, . . . , NKK correspond to the cases when the predic-
tion is correct; the K 2 − K oﬀ-diagonal entries Nij, i �= j, correspond to prediction
errors. For each i, Nii is the number of data points with label i for which we cor-
rectly guessed ˆy = i. For i �= j, Nij is the number of data points for which we have
mistaken label i (its true value) for the label j (our incorrect guess). For K = 2
(Boolean classiﬁcation) there are only two types of prediction errors, false positive
and false negative. For K > 2 the situation is more complicated, since there are
many more types of errors a predictor can make. From the entries of the confusion
matrix we can derive various measures of the accuracy of the predictions. We let
Ni (with one index) denote the total number of data points for which y = i, i.e.,
Ni = Ni1 + ··· + NiK. We have N = N1 + ··· + NK.
The simplest measure is the overall error rate, which is the total number of
errors (the sum of all oﬀ-diagonal entries in the confusion matrix) divided by the

14.3 Multi-class classiﬁers

299

ˆy

y

Dislike Neutral

Like

Dislike
Neutral
Like

183
7
3

10
61
13

5
8
210

Table 14.8 Example confusion matrix of a multi-class classiﬁer with three
classes.

data set size (the sum of all entries in the confusion matrix):

(1/N )�i�=j

Nij = 1 − (1/N )�i

Nii.

This measure implicitly assumes that all errors are equally bad. In many applica-
tions this is not the case; for example, some medical mis-diagnoses might be worse
for a patient than others.

We can also look at the rate with which we predict each label correctly. The
quantity Nii/Ni is called the true label i rate. It is the fraction of data points with
label y = i for which we correctly predicted ˆy = i. (The true label i rates reduce
to the true positive and true negative rates for Boolean classiﬁers.)

A simple example, with K = 3 labels (Dislike, Neutral, and Like), and a total
number N = 500 data points, is shown in table 14.8. Out of 500 data points, 454
(the sum of the diagonal entries) were classiﬁed correctly. The remaining 46 data
points (the sum of the oﬀ-diagonal entries) correspond to the 6 diﬀerent types of
errors. The overall error rate is 46/500 = 9.2%. The true label Dislike rate is
183/(183 + 10 + 5) = 92.4%, i.e., among the data points with label Dislike, we
correctly predicted the label on 92.4% of the data. The true label Neutral rate is
61/(7 + 61 + 8) = 80.3%, and the true label Like rate is 210/(3 + 13 + 210) = 92.9%.

14.3.1 Least squares multi-class classiﬁer

The idea behind the least squares Boolean classiﬁer can be extended to handle
multi-class classiﬁcation problems. For each possible label value, we construct a
new data set with the Boolean label +1 if the label has the given value, and −1
otherwise. (This is sometimes called a one-versus-others or one-versus-all classi-
ﬁer.) From these K Boolean classiﬁers we must create a classiﬁer that chooses
one of the K possible labels. We do this by selecting the label for which the least
squares regression ﬁt has the highest value, which roughly speaking is the one with
the highest level of conﬁdence. Our classiﬁer is then
˜fk(x),

ˆf (x) = argmax
k=1,...,K

where ˜fk is the least squares regression model for label k against the others. The
notation argmax means the index of the largest value among the numbers ˜fk(x),

300

14 Least squares classiﬁcation

for k = 1, . . . , K. Note that ˜fk(x) is the real-valued prediction for the Boolean
classiﬁer for class k versus not class k; it is not the Boolean classiﬁer, which is
sign( ˜fk(x)).

As an example consider a multi-class classiﬁcation problem with 3 labels. We
construct 3 diﬀerent least squares classiﬁers, for 1 versus 2 or 3, for 2 versus 1 or
3, and for 3 versus 1 or 2. Suppose for a given feature vector x, we ﬁnd that

˜f1(x) = −0.7,

˜f2(x) = +0.2,

˜f3(x) = +0.8.

The largest of these three numbers is ˜f3(x), so our prediction is ˆf (x) = 3. We can
interpret these numbers and our ﬁnal decision. The ﬁrst classiﬁer is fairly conﬁdent
that the label is not 1. According to the second classiﬁer, the label could be 2,
but it does not have high conﬁdence in this prediction. Finally, the third classiﬁer
predicts the label is 3, and moreover has relatively high conﬁdence in this guess.
So our ﬁnal guess is label 3. (This interpretation suggests that if we had to make a
second guess, it should be label 2.) Of course here we are anthropomorphizing the
individual label classiﬁers, since they do not have beliefs or levels of conﬁdence in
their predictions. But the story is helpful in understanding the motivation behind
the classiﬁer above.

Skewed decisions.
In a Boolean classiﬁer we can skew the decision threshold (see
§14.2.3) to trade oﬀ the true positive and false positive rates. In a K-class classiﬁer,
an analogous method can be used to trade oﬀ the K true label i rates. We apply
an oﬀset αk to ˜fl(x) before ﬁnding the largest value. This gives the predictor

ˆf (x) = argmax

k=1,...,K� ˜fk(x) − αk�,

where αk are constants chosen to trade oﬀ the true label k rates. If we decrease αk,
we predict ˆf (x) = k more often, so all entries of the kth column in the confusion
matrix increase. This increases our rate of true positives for label k (since Nkk
increases), which is good. But it can decrease the true positive rates for the other
labels.

Complexity.
In least squares multi-class classiﬁcation, we solve K least squares
problems, each with N rows and p variables. The na¨ıve method of computing
θ1, . . . , θK, the coeﬃcients in our one-versus-others classiﬁers, costs 2KN p2 ﬂops.
But the K least squares problems we solve all involve the same matrix; only the
right-hand side vector changes. This means that we can carry out the QR factoriza-
tion just once, and use it for computing all K classiﬁer coeﬃcients. Alternatively,
we can say that ﬁnding the coeﬃcients of all the one-versus-others classiﬁers can
be done by solving a matrix least squares problem (see page 233.) When K (the
number of classes or labels) is small compared to p (the number of basis functions
or coeﬃcients in the classiﬁer), the cost is about the same as solving just one least
squares problem.

Another simpliﬁcation in K-class least squares classiﬁcation arises due to the
special form of the right-hand sides in the K least squares problems to be solved.
The right-hand sides in these K problems are Boolean vectors with entries +1 for

14.3 Multi-class classiﬁers

301

Prediction

Class

Setosa Versicolour Virginica Total

Setosa
Versicolour
Virginica
All

40
0
0
40

0
27
4
31

0
13
36
49

40
40
40
120

Table 14.9 Confusion matrix for a 3-class classiﬁer of the Iris data set, on a
training set of 120 examples.

one of the classes and −1 for all others. It follows that the sum of these K right-
hand sides is the vector with all entries equal to 2 − K, i.e., (2 − K)1. Since the
mapping from the right-hand sides to the least squares approximate solutions ˆθk is
linear (see page 229), we have ˆθ1 +··· + ˆθk = (2− K)a, where a is the least squares
approximate solution when the right-hand side is 1. Assuming that the ﬁrst basis
function is f1(x) = 1, we have a = e1. So we have

ˆθ1 + ··· + ˆθK = (2 − K)e1,

where ˆθk is the coeﬃcient vector for distinguishing class k from the others. Once
we have computed ˆθ1, . . . , ˆθK−1, we can ﬁnd ˆθK by simple vector subtraction.

This explains why for the Boolean classiﬁcation case we have K = 2, but we
only have to solve one least squares problem. In §14.2 we compute one coeﬃcient
vector θ; if the same problem were to be considered a K-class problem with K = 2,
we would have θ1 = θ. (This one distinguishes class 1 versus class 2.) The other
coeﬃcient vector is then θ2 = −θ1. (This one distinguishes class 2 versus class 1.)

14.3.2

Iris ﬂower classiﬁcation

We compute a 3-class classiﬁer for the Iris data set described on page 289. The
examples are randomly partitioned into a training set of size 120, containing 40
examples of each class, and a test set of size 30, with 10 examples of each class.
The 3 × 3 confusion matrix for the training set is given in table 14.9. The error
rate is 14.2%. The results for the test set are in table 14.10. The error rate is
13.3%, similar enough to the training error rate to give us some conﬁdence in our
classiﬁer. The true Setosa rate is 100% for both train and test sets, suggesting
that our classiﬁer can detect this type well. The true Versicolour rate is 67.5% for
the training data, and 60% for the test set. The true Virginica rate is 90% for the
training data, and 100% for the test set. This suggests that our classiﬁer can detect
Virginica well, but perhaps not as well as Setosa. (The 100% true Virginica rate
on the test set is a matter of luck, due to the very small number of test examples
of each type; see the discussion on page 268.)

302

14 Least squares classiﬁcation

Prediction

Class

Setosa Versicolour Virginica Total

Setosa
Versicolour
Virginica
All

10
0
0
10

0
6
0
6

0
4
10
14

10
10
10
30

Table 14.10 Confusion matrix for a 3-class classiﬁer of the Iris data set, on
a test set of 30 examples.

14.3.3 Handwritten digit classiﬁcation

We illustrate the least squares multi-class classiﬁcation method by applying it to
the MNIST data set. For each of the ten digits 0, . . . , 9 (which we encode as
k = 1, . . . , 10) we compute a least squares Boolean classiﬁer

ˆfk(x) = sign(xT βk + vk),

to distinguish digit k from the other digits. The ten Boolean classiﬁers are combined
into a multi-class classiﬁer

ˆf (x) = argmax
k=1,...,10

(xT βk + vk).

The 10 × 10 confusion matrix for the data set and the test set are given in ta-
bles 14.11 and 14.12.
The error rate on the training set is 14.5%; on the test set it is 13.9%. The true
label rates on the test set range from 73.5% for digit 5 to 97.5% for digit 1. Many of
the entries of the confusion matrix make sense. From the ﬁrst row of the matrix, we
see a handwritten 0 was rarely mistakenly classiﬁed as a 1, 2, or 9; presumably these
digits look diﬀerent enough that they are easily distinguished. The most common
error (80) corresponds to y = 9, ˆy = 4, i.e., mistakenly identifying a handwritten
9 as a 4. This makes sense since these two digits can look very similar.

Feature engineering. After adding the 5000 randomly generated new features (as
described on page 293), the training set error is reduced to about 1.5%, and the
test set error to 2.6%. The confusion matrices are given in tables 14.13 and 14.14.
Since we have (substantially) reduced the error in the test set, we conclude that
adding these 5000 new features was a successful exercise in feature engineering.

You could reasonably wonder how much performance improvement is possible
for this example, using feature engineering. For the handwritten digit data set,
humans have an error rate around 2% (with the true digits veriﬁed by checking
actual addresses, ZIP codes, and so on). Further feature engineering (i.e., introduc-
ing even more additional random features, or using neural network features) brings
the error rate down well below 2%, i.e., well below human ability. This should give
you some idea of how powerful the ideas in this book are.

14.3 Multi-class classiﬁers

303

Prediction

Digit

0

0
1
2
3
4
5
6
7
8
9
All

5669

2
99
38
13
164
104
55
69
67
6280

1

8

6543
278
172
104
94
78
191
492
66
8026

2

3

4

5

6

21
36
4757
174
41
30
77
36
64
26
5262

19
17
153
5150

5
448
2
48
225
115
6182

25
20
116
31
5189
103
64
165
102
365
6180

46
30
17
122
52
3974
106
9
220
12
4588

65
14
234
59
45
185
5448

4
64
4

6122

7

4
14
92
122
24
44
0

5443
21
513
6277

8

9

6
60
6
60
22
190
128
135
309
60
142
237
3
36
301
13
177
4417
4742
39
5247 5836

Total

5923
6742
5958
6131
5842
5421
5918
6265
5851
5949
60000

Table 14.11 Confusion matrix for least squares multi-class classiﬁcation of
handwritten digits (training set).

Prediction

Digit

0

0
1
2
3
4
5
6
7
8
9
All

944

0
18
4
0
24
17
5
14
16
1042

1

0

1107

54
18
22
19
9
43
48
10
1330

2

1
2

815
22
6
3
10
14
11
3
887

3

2
2
26
884
0
74
0
6
31
17
1042

4

2
3
16
5

883
24
22
25
26
80
1086

5

6

8
1
0
16
3
656
17
1
40
0
742

13
5
38
10
9
24
876

1
17
1
994

7

2
1
22
22
1
13
0

883
13
75
1032

8

9

Total

1
7
0
14
4
39
9
20
46
12
17
38
0
7
49
1
18
756
4
803
898 947

980
1135
1032
1010
982
892
958
1028
974
1009
10000

Table 14.12 Confusion matrix for least squares multi-class classiﬁcation of
handwritten digits (test set).

304

14 Least squares classiﬁcation

Digit

0

1

2

3

4

5

6

7

8

9

Prediction

0
1
2
3
4
5
6
7
8
9
All

5988

5748

5888

1

6679

2
2
1
5
11
22
1
52
1
7
6
1
8
37
3
14
5
5785
10
5934 6759 5968 6072 5842 5413 5938 6263 5884 5927

2
27
5866
31
3
4
0
23
11
1

10
0
3
0
13
23
5875

3
11
12
0

7
3
8
9
41

0
10
22
24
4
2
0

7
35

5335
15
0
17
16

1
6
6

0
26
0
4
12
29

14
6
26
34
5
9
11
5

5749
25

7
4
15
2
5
25
16
5

2
0
0
27
1

6159

1
11
2

Total

5923
6742
5958
6131
5842
5421
5918
6265
5851
5949
60000

Table 14.13 Confusion matrix for least squares multi-class classiﬁcation of
handwritten digits, after addition of 5000 features (training set).

Digit

0

0
1
2
3
4
5
6
7
8
9
All

972
0
6
0
2
2
8
0
3
4
997

1

0

1126

0
0
1
0
3
8
1
3

2

0
3
998
3
3
1
0
12
3
1

1142

1024

Prediction

3

2
1
3

977

0
5
0
0
6
12
1006

4

0
1
2
0

953

0
4
2
4
11
977

5

1
0
0
13
0

875

6
0
3
7
905

6

1
3
4
0
6
5

933

1
2
1
956

7

1
0
7
5
3
0
0

992
2
3

1013

8

9

Total

0
3
0
1
1
11
4
8
13
1
1
3
0
4
10
3
4
946
3
964
983 997

980
1135
1032
1010
982
892
958
1028
974
1009
10000

Table 14.14 Confusion matrix for least squares multi-class classiﬁcation of
handwritten digits, after addition of 5000 features (test set).

Exercises

Exercises

305

14.1 Chebyshev bound. Let ˜f (x) denote the continuous prediction of the Boolean outcome y,
and ˆf (x) = sign( ˜f (x)) the actual classiﬁer. Let σ denote the RMS error in the continuous
prediction over some set of data, i.e.,

σ2 =

( ˜f (x(1)) − y(1))2 + ··· + ( ˜f (x(N )) − y(N ))2

N

.

Use the Chebyshev bound to argue that the error rate over this data set, i.e., the fraction
of data points for which ˆf (x(i)) �= y(i), is no more than σ2, assuming σ < 1.
Remark. This bound on the error rate is usually quite bad; that is, the actual error rate
in often much lower than this bound. But it does show that if the continuous prediction
is good, then the classiﬁer must perform well.

14.2 Interpreting the parameters in a regression classiﬁer. Consider a classiﬁer of the form
ˆy = sign(xT β + v), where ˆy is the prediction, the n-vector x is the feature vector, and the
n-vector β and scalar v are the classiﬁer parameters. We will assume here that v �= 0 and
β �= 0. Evidently ˆy = sign(v) is the prediction when the feature vector x is zero. Show
that when �x� < |v|/�β�, we have ˆy = sign(v). This shows that when all the features
are small (i.e., near zero), the classiﬁer makes the prediction ˆy = sign(v). Hint. If two
numbers a and b satisfy |a| < |b|, then sign(a + b) = sign(b).
This means that we can interpret sign(v) as the classiﬁer prediction when the features are
small. The ratio |v|/�β� tells us how big the feature vector must be before the classiﬁer
‘changes its mind’ and predicts ˆy = − sign(v).

14.3 Likert classiﬁer. A response to a question has the options Strongly Disagree, Disagree,
Neutral, Agree, or Strongly Agree, encoded as −2,−1, 0, 1, 2, respectively. You wish to
build a multi-class classiﬁer that takes a feature vector x and predicts the response.
A multi-class least squares classiﬁer builds a separate (continuous) predictor for each
response versus the others. Suggest a simpler classiﬁer, based on one continuous regression
model ˜f (x) that is ﬁt to the numbers that code the responses, using least squares.

14.4 Multi-class classiﬁer via matrix least squares. Consider the least squares multi-class clas-
siﬁer described in §14.3, with a regression model ˜fk(x) = xT βk for the one-versus-others
classiﬁers. (We assume that the oﬀset term is included using a constant feature.) Show
that the coeﬃcient vectors β1, . . . , βK can be found by solving the matrix least squares
problem of minimizing �X T β−Y �2, where β is the n×K matrix with columns β1, . . . , βK ,
and Y is an N × K matrix.
(a) Give Y , i.e., describe its entries. What is the ith row of Y ?

(b) Assuming the rows of X (i.e., the data feature vectors) are linearly independent,

show that the least squares estimate is given by ˆβ = (X T )†Y .

14.5 List classiﬁer. Consider a multi-class classiﬁcation problem with K classes. A standard
multi-class classiﬁer is a function ˆf that returns a class (one of the labels 1, . . . , K), when
given a feature n-vector x. We interpret ˆf (x) as the classiﬁer’s guess of the class that
corresponds to x. A list classiﬁer returns a list of guesses, typically in order from ‘most
likely’ to ‘least likely’. For example, for a speciﬁc feature vector x, a list classiﬁer might
return 3, 6, 2, meaning (roughly) that its top guess is class 3, its next guess is class 6, and
its third guess is class 2. (The lists can have diﬀerent lengths for diﬀerent values of the
feature vector.) How would you modify the least squares multi-class classiﬁer described
in §14.3.1 to create a list classiﬁer? Remark. List classiﬁers are widely used in electronic
communication systems, where the feature vector x is the received signal, and the class
corresponds to which of K messages was sent. In this context they are called list decoders.
List decoders produce a list of probable or likely messages, and allow a later processing
stage to make the ﬁnal decision or guess.

306

14 Least squares classiﬁcation

14.6 Polynomial classiﬁer with one feature. Generate 200 points x(1), . . . , x(200), uniformly

spaced in the interval [−1, 1], and take

y(i) =� +1 −0.5 ≤ x(i) < 0.1 or 0.5 ≤ x(i)

otherwise

−1

for i = 1, . . . , 200. Fit polynomial least squares classiﬁers of degrees 0, . . . , 8 to this
training data set.

(a) Evaluate the error rate on the training data set. Does the error rate decrease when

you increase the degree?

(b) For each degree, plot the polynomial ˜f (x) and the classiﬁer ˆf (x) = sign( ˜f (x)).
(c) It is possible to classify this data set perfectly using a classiﬁer ˆf (x) = sign( ˜f (x))

and a cubic polynomial

˜f (x) = c(x + 0.5)(x − 0.1)(x − 0.5),

for any positive c. Compare this classiﬁer with the least squares classiﬁer of degree
3 that you found and explain why there is a diﬀerence.

14.7 Polynomial classiﬁer with two features. Generate 200 random 2-vectors x(1), . . . , x(200) in

a plane, from a standard normal distribution. Deﬁne

y(i) =� +1 x(i)

−1

1 x(i)
2 ≥ 0
otherwise

for i = 1, . . . , 200. In other words, y(i) is +1 when x(i) is in the ﬁrst or third quadrant,
and −1 otherwise. Fit a polynomial least squares classiﬁer of degree 2 to the data set,
i.e., use a polynomial

˜f (x) = θ1 + θ2x1 + θ3x2 + θ4x2

1 + θ5x1x2 + θ6x2
2.

Give the error rate of the classiﬁer. Show the regions in the plane where ˆf (x) = 1 and
ˆf (x) = −1. Also compare the computed coeﬃcients with the polynomial ˜f (x) = x1x2,
which classiﬁes the data points with zero error.
14.8 Author attribution. Suppose that the N feature n-vectors x(1), . . . , x(N ) are word count
histograms, and the labels y(1), . . . , y(N ) give the document authors (as one of 1, . . . , K).
A classiﬁer guesses which of the K authors wrote an unseen document, which is called
author attribution. A least squares classiﬁer using regression is ﬁt to the data, resulting
in the classiﬁer

ˆf (x) = argmax
k=1,...,K

(xT βk + vk).

For each author (i.e., k = 1, . . . , K) we ﬁnd the ten largest (most positive) entries in the
n-vector βk and the ten smallest (most negative) entries. These correspond to two sets
of ten words in the dictionary, for each author. Interpret these words, brieﬂy, in English.
14.9 Nearest neighbor interpretation of multi-class classiﬁer. We consider the least squares
K-class classiﬁer of §14.3.1. We associate with each data point the n-vector x, and the
label or class, which is one of 1, . . . , K. If the class of the data point is k, we associate
it with a K-vector y, whose entries are yk = +1 and yj = −1 for j �= k.
(We can
write this vector as y = 2ek − 1.) Deﬁne ˜y = ( ˜f1(x), . . . , ˜fK (x)), which is our (real-
valued or continuous) prediction of the label y. Our multi-class prediction is given by
ˆf (x) = argmaxk=1,...,K
˜fk(x). Show that ˆf (x) is also the index of the nearest neighbor of
˜y among the vectors 2ek − 1, for k = 1, . . . , K. In other words, our guess ˆy for the class is
the nearest neighbor of our continuous prediction ˜y, among the vectors that encode the
class labels.

Exercises

307

14.10 One-versus-one multi-class classiﬁer. In §14.3.1 we construct a K-class classiﬁer from K
Boolean classiﬁers that attempt to distinguish each class from the others. In this exercise
we describe another method for constructing a K-class classiﬁer. We ﬁrst develop a
Boolean classiﬁer for every pair of classes i and j, i < j. There are K(K − 1)/2 such
pairs of classiﬁers, called one-versus-one classiﬁers. Given a feature vector x, we let ˆyij
be the prediction of the i-versus-j classiﬁer, with ˆyij = 1 meaning that the one-versus-one
classiﬁer is guessing that y = i. We consider ˆyij = 1 as one ‘vote’ for class i, and ˆyij = −1
as one ‘vote’ for class j. We obtain the ﬁnal estimated class by majority voting: We take
ˆy as the class that has the most votes. (We can break ties in some simple way, like taking
the smallest index that achieves the largest number of votes.)

(a) Construct the least squares classiﬁer, and the one-versus-one classiﬁer, for a multi-
class (training) data set. Find the confusion matrices, and the error rates, of the
two classiﬁers on both the training data set and a separate test data set.

(b) Compare the complexity of computing the one-versus-one multi-class classiﬁer with
the complexity of the least squares multi-class classiﬁer (see page 300). Assume the
training set contains N/K examples of each class and that N/K is much greater than
the number of features p. Distinguish two methods for the one-versus-one multi-class
classiﬁer. The ﬁrst, na¨ıve, method solves K(K − 1)/2 least squares problem with
N/K rows and p columns. The second, more eﬃcient, method precomputes the
Gram matrices Gi = AiAT
for i = 1, . . . , K, where the rows of the (N/K) × p
i
matrix Ai are the training example for class i, and uses the pre-computed Gram
matrices to speed up the solution of the K(K − 1)/2 least squares problems.

14.11 Equalizer design from training message. We consider an electronic communication system,
with message to be sent given by an N -vector s, whose entries are −1 or 1, and received
signal y, where y = c∗s, where c is an n-vector, the channel impulse response. The receiver
applies equalization to the received signal, which means that it computes ˜y = h∗y = h∗c∗s,
where h is an n-vector, the equalizer impulse response. The receiver then estimates the
original message using ˆs = sign(˜y1:N ). This works well if h ∗ c ≈ e1. (See exercise 7.15.)
If the channel impulse response c is known or can be measured, we can design or choose
h using least squares, as in exercise 12.6.
In this exercise we explore a method for choosing h directly, without estimating or mea-
suring c. The sender ﬁrst sends a message that is known to the receiver, called the training
message, strain. (From the point of view of communications, this is wasted transmission,
and is called overhead.) The receiver receives the signal ytrain = c ∗ strain from the train-
ing message, and then chooses h to minimize �(h ∗ ytrain)1:N − strain�2. (In practice, this
equalizer is used until the bit error rate increases, which means the channel has changed,
at which point another training message is sent.) Explain how this method is the same as
least squares classiﬁcation. What are the training data x(i) and y(i)? What is the least
squares problem that must be solved to determine the equalizer impulse response h?

Chapter 15

Multi-objective least squares

In this chapter we consider the problem of choosing a vector that achieves a com-
promise in making two or more norm squared objectives small. The idea is widely
used in data ﬁtting, image reconstruction, control, and other applications.

15.1 Multi-objective least squares

In the basic least squares problem (12.1), we seek the vector ˆx that minimizes
the single objective function �Ax − b�2.
In some applications we have multiple
objectives, all of which we would like to be small:

J1 = �A1x − b1�2,

. . . ,

Jk = �Akx − bk�2.

Here Ai is an mi × n matrix, and bi is an mi-vector. We can use least squares to
ﬁnd the x that makes any one of these objectives as small as possible (provided the
associated matrix has linearly independent columns). This will give us (in general)
k diﬀerent least squares approximate solutions. But we seek a single ˆx that gives
a compromise, and makes them all small, to the extent possible. We call this the
multi-objective (or multi-criterion) least squares problem, and refer to J1, . . . , Jk
as the k objectives.

Multi-objective least squares via weighted sum. A standard method for ﬁnding
a value of x that gives a compromise in making all the objectives small is to choose
x to minimize a weighted sum objective:

J = λ1J1 + ··· + λkJk = λ1�A1x − b1�2 + ··· + λk�Akx − bk�2,

(15.1)

where λ1, . . . , λk are positive weights, that express our relative desire for the terms
to be small. If we choose all λi to be one, the weighted sum objective is the sum
of the objective terms; we give each of them equal weight. If λ2 is twice as large
as λ1, it means that we attach twice as much weight to the objective J2 as to J1.
Roughly speaking, we care twice as strongly that J2 should be small, compared

310

15 Multi-objective least squares

to our desire that J1 should be small. We will discuss later how to choose these
weights.

Scaling all the weights in the weighted sum objective (15.1) by any positive
number is the same as scaling the weighted sum objective J by the number, which
does not change its minimizers. Since we can scale the weights by any positive
number, it is common to choose λ1 = 1. This makes the ﬁrst objective term J1
our primary objective; we can interpret the other weights as being relative to the
primary objective.

Weighted sum least squares via stacking. We can minimize the weighted sum
objective function (15.1) by expressing it as a standard least squares problem. We
start by expressing J as the norm squared of a single vector:

where we use the property that �(a1, . . . , ak)�2 = �a1�2 +···+�ak�2 for any vectors
a1, . . . , ak. So we have

2

,

�������


�������

2

= � ˜Ax − ˜b�2,

√λ1(A1x − b1)
√λk(Akx − bk)

...

J =�������


 x −
 ,

where ˜A and ˜b are the matrix and vector

J =�������
√λ1A1

...√λkAk
˜A =

√λ1A1
...√λkAk

√λ1b1

...√λkbk
˜b =

√λ1b1
...√λkbk

 .

(15.2)

The matrix ˜A is m × n, and the vector ˜b has length m, where m = m1 + ··· + mk.
We have now reduced the problem of minimizing the weighted sum least squares
objective to a standard least squares problem. Provided the columns of ˜A are
linearly independent, the minimizer is unique, and given by

ˆx = ( ˜AT ˜A)−1 ˜AT ˜b

= (λ1AT

1 A1 + ··· + λkAT

k Ak)−1(λ1AT

1 b1 + ··· + λkAT

k bk).

(15.3)

This reduces to our standard formula for the solution of a least squares problem
when k = 1 and λ1 = 1. (In fact, when k = 1, λ1 does not matter.) We can
compute ˆx via the QR factorization of ˜A.

Independent columns of stacked matrix. Our assumption (12.2) that the columns
of ˜A in (15.2) are linearly independent is not the same as assuming that each of
A1, . . . , Ak has linearly independent columns. We can state the condition that ˜A
has linearly independent columns as: There is no nonzero vector x that satisﬁes

15.1 Multi-objective least squares

311

Aix = 0 for i = 1, . . . , k. This implies that if just one of the matrices A1, . . . , Ak
has linearly independent columns, then ˜A does.

The stacked matrix ˜A can have linearly independent columns even when none
of the matrices A1, . . . , Ak do. This can happen when mi < n for all i, i.e., all
Ai are wide. However, we must have m1 + ··· + mk ≥ n, since ˜A must be tall or
square for the linearly independent columns assumption to hold.

Optimal trade-oﬀ curve. We start with the special case of two objectives (also
called the bi-criterion problem), and write the weighted sum objective as

J = J1 + λJ2 = �A1x − b1�2 + λ�A2x − b2�2,

where λ > 0 is the relative weight put on the second objective, compared to the
ﬁrst. For small λ, we care much more about J1 being small than J2 being small;
for large λ, we care much less about J1 being small than J2 being small.

Let ˆx(λ) denote the weighted sum least squares solution ˆx as a function of λ,
assuming the stacked matrices have linearly independent columns. These points
are called Pareto optimal (after the economist Vilfredo Pareto) which means there
is no point z that satisﬁes

�A1z − b1�2 ≤ �A1 ˆx(λ) − b1�2,

�A2z − b2�2 ≤ �A2 ˆx(λ) − b2�2,

with one of the inequalities holding strictly. In other words, there is no point z
that is as good as ˆx(λ) in one of the objectives, and beats it on the other one. To
see why this is the case, we note that any such z would have a value of J that is
less than that achieved by ˆx(λ), which minimizes J, a contradiction.

We can plot the two objectives �A1 ˆx(λ)− b1�2 and �A2 ˆx(λ)− b2�2 against each
other, as λ varies over (0,∞), to understand the trade-oﬀ of the two objectives.
This curve is called the optimal trade-oﬀ curve of the two objectives. There is no
point z that achieves values of J1 and J2 that lies below and to the left of the
optimal trade-oﬀ curve.

Simple example. We consider a simple example with two objectives, with A1 and
A2 both 10 × 5 matrices. The entries of the weighted least squares solution ˆx(λ)
are plotted against λ in ﬁgure 15.1. On the left, where λ is small, ˆx(λ) is very close
to the least squares approximate solution for A1, b1. On the right, where λ is large,
ˆx(λ) is very close to the least squares approximate solution for A2, b2. In between
the behavior of ˆx(λ) is very interesting; for instance, we can see that ˆx(λ)3 ﬁrst
increases with increasing λ before eventually decreasing.

Figure 15.2 shows the values of the two objectives J1 and J2 versus λ. As
expected, J1 increases as λ increases, and J2 decreases as λ increases.
(It can
be shown that this always holds.) Roughly speaking, as λ increases we put more
emphasis on making J2 small, which comes at the expense of making J1 bigger.
The optimal trade-oﬀ curve for this bi-criterion problem is plotted in ﬁgure 15.3.
The left end-point corresponds to minimizing �A1x− b1�2, and the right end-point
corresponds to minimizing �A2x − b2�2. We can conclude, for example, that there
is no vector z that achieves �A1z − b1�2 ≤ 8 and �A2z − b2�2 ≤ 5.

312

15 Multi-objective least squares

0.6

0.4

0.2

ˆx1(λ)

ˆx2(λ)

ˆx5(λ)

ˆx4(λ)

0

ˆx3(λ)

−0.2

10−4

10−2

100
λ

102

104

Figure 15.1 Weighted-sum least squares solution ˆx(λ) as a function of λ for
a bi-criterion least squares problem with ﬁve variables.

J2(λ)

J1(λ)

14

12

10

8

6

4

10−4

10−2

100
λ

102

104

Figure 15.2 Objective functions J1 = �A1 ˆx(λ) − b1�2 (blue line) and J2 =
�A2 ˆx(λ) − b2�2 (red line) as functions of λ for the bi-criterion problem in
ﬁgure 15.1.

15.1 Multi-objective least squares

313

14

12

10

8

6

4

)
λ
(
2
J

λ = 0.1

λ = 1

λ = 10

6

8

12

14

10
J1(λ)

Figure 15.3 Optimal trade-oﬀ curve for the bi-criterion least squares problem
of ﬁgures 15.1 and 15.2.

The steep slope of the optimal trade-oﬀ curve near the left end-point means
that we can achieve a substantial reduction in J2 with only a small increase in J1.
The small slope of the optimal trade-oﬀ curve near the right end-point means that
we can achieve a substantial reduction in J1 with only a small increase in J2. This
is quite typical, and indeed, is why multi-criterion least squares is useful.

Optimal trade-oﬀ surface. Above we described the case with k = 2 objectives.
When we have more than 2 objectives, the interpretation is similar, although it is
harder to plot the objectives, or the values of ˆx, versus the weights. For example
with k = 3 objectives, we have two weights, λ2 and λ3, which give the relative
weight of J2 and J3 compared to J1. Any solution ˆx(λ) of the weighted least squares
problem is Pareto optimal, which means that there is no point that achieves values
of J1, J2, J3 less than or equal to those obtained by ˆx(λ), with strict inequality
holding for at least one of them. As the parameters λ2 and λ3 vary over (0,∞),
the values of J1, J2, J3 sweep out the optimal trade-oﬀ surface.

Using multi-objective least squares.
In the rest of this chapter we will see several
speciﬁc applications of multi-objective least squares. Here we give some general
remarks on how it is used in applications.

First we identify a primary objective J1 that we would like to be small. The
objective J1 is typically the one that would be used in an ordinary single-objective
least squares approach, such as the mean square error of a model on some training
data, or the mean square deviation from some target or goal.

314

15 Multi-objective least squares

We also identify one or more secondary objectives J2, J3, . . . , Jk, that we would
also like to be small. These secondary objectives are typically generic ones, like
the desire that some parameters be ‘small’ or ‘smooth’, or close to some previous
or prior value. In estimation applications these secondary objectives typically cor-
respond to some kind of prior knowledge or assumption about the vector x that
we seek. We wish to minimize our primary objective, but are willing to accept an
increase in it, if this gives a suﬃcient decrease in the secondary objectives.

The weights are treated like ‘knobs’ in our method, that we change (‘turn’ or
‘tune’ or ‘tweak’) to achieve a value of ˆx that we like (or can live with). For given
candidate values of λ we evaluate the objectives; if we decide that J2 is larger
than we would like, but we can tolerate a somewhat larger J3, then we increase
λ2 and decrease λ3, and ﬁnd ˆx and the associated values of J1, J2, J3 using the
new weights. This is repeated until a reasonable trade-oﬀ among them has been
obtained. In some cases we can be principled in how we adjust the weights; for
example, in data ﬁtting, we can use validation to help guide us in the choice of
the weights. In many other applications, it comes down to (application-speciﬁc)
judgment or even taste.

The additional terms λ2J2, . . . , λkJk that we add to the primary objective J1,
are sometimes called regularization (terms). The secondary objectives are some-
times described by name, as in ‘least squares ﬁtting with smoothness regulariza-
tion’.

In exploring the trade-oﬀs among the objectives, the weights are typically varied
over a wide range of values, by choosing a ﬁnite number of values (perhaps ten or
a few tens) that are logarithmically spaced, as in ﬁgures 15.1 and 15.2. This means
that for N values of λ between λmin and λmax, we use the values

λmin,

θλmin,

θ2λmin,

. . . ,

θN−1λmin = λmax,

with θ = (λmax/λmin)1/(N−1).

15.2 Control

In control applications, the goal is to decide on a set of actions or inputs, speciﬁed
by an n-vector x, that achieve some goals. The actions result in some outputs or
eﬀects, given by an m-vector y. We consider here the case when the inputs and
outputs are related by an aﬃne model

y = Ax + b.

The m × n matrix A and m-vector b characterize the input-output mapping of
the system. The model parameters A and b are found from analytical models,
experiments, computer simulations, or ﬁt to past (observed) data. Typically the
input or action x = 0 has some special meaning. The m-vector b gives the output
when the input is zero. In many cases the vectors x and y represent deviations of
the inputs and outputs from some standard values.

15.2 Control

315

We typically have a desired or target output, denoted by the m-vector ydes.

The primary objective is

J1 = �Ax + b − ydes�2,

the norm squared deviation of the output from the desired output. The main
objective is to choose an action x so that the output is as close as possible to the
desired value.

There are many possible secondary objectives. The simplest one is the norm
squared value of the input, J2 = �x�2, so the problem is to optimally trade oﬀ
missing the target output (measured by �y − ydes�2), and keeping the input small
(measured by �x�2).
Another common secondary objective has the form J2 = �x − xnom�2, where
xnom is a nominal or standard value for the input. In this case the secondary ob-
jective it to keep the input close to the nominal value. This objective is sometimes
used when x represents a new choice for the input, and xnom is the current value.
In this case the goal is to get the output near its target, while not changing the
input much from its current value.

Control of heating and cooling. As an example, x could give the vector of n
heating (or cooling) power levels in a commercial building with n air handling
units (with xi > 0 meaning heating and xi < 0 meaning cooling) and y could
represent the resulting temperature at m locations in the building. The matrix
A captures the eﬀect of each of n heating/cooling units on the temperatures in
the building at each of m locations; the vector b gives the temperatures at the m
locations when no heating or cooling is applied. The desired or target output might
be ydes = T des1, assuming the target temperature is the same at all locations. The
primary objective �y− ydes�2 is the sum of squares of the deviations of the location
temperatures from the target temperature. The secondary objective J2 = �x�2,
the norm squared of the vector of heating/cooling powers, would be reasonable,
since it is at least roughly related to the energy cost of the heating and cooling.

We ﬁnd tentative choices of the input by minimizing J1 +λ2J2 for various values
of λ2. If for the current value of λ2 the heating/cooling powers are larger than we’d
like, we increase λ2 and re-compute ˆx.

Product demand shaping.
In demand shaping, we adjust or change the prices of
a set of n products in order to move the demand for the products towards some
given target demand vector, perhaps to better match the available supply of the
products. The standard price elasticity of demand model is δdem = Edδprice, where
δdem is the vector of fractional demand changes, δprice is the vector of fractional
price changes, and Ed is the price elasticity of demand matrix.
(These are all
described on page 150.) In this example the price change vector δprice represents
the action that we take; the result is the change in demand, δdem. The primary
objective could be

J1 = �δdem − δtar�2 = �Edδprice − δtar�2,

where δtar is the target change in demand.

316

15 Multi-objective least squares

At the same time, we want the price changes to be small. This suggests the
secondary objective J2 = �δprice�2. We then minimize J1 + λJ2 for various values
of λ, which trades oﬀ how close we come to the target change in demand with how
much we change the prices.

Dynamics. The system can also be dynamic, meaning that we take into account
time variation of the input and output. In the simplest case x is the time series of
a scalar input, so xi is the action taken in period i, and yi is the (scalar) output in
period i. In this setting, ydes is a desired trajectory for the output. A very common
model for modeling dynamic systems, with x and y representing scalar input and
output time series, is a convolution: y = h ∗ x. In this case, A is Toeplitz, and b
represents a time series, which is what the output would be with x = 0.
As a typical example in this category, the input xi can represent the torque
applied to the drive wheels of a locomotive (say, over one second intervals), and yi
is the locomotive speed.

In addition to the usual secondary objective J2 = �x�2, it is common to have
an objective that the input should be smooth, i.e., not vary too rapidly over time.
This is achieved with the objective �Dx�2, where D is the (n−1)×n ﬁrst diﬀerence
matrix

D =

.

(15.4)



−1
1
0 −1
...
...
0
0
0
0

0
1
...
0
0

···
···

0
0
...
··· −1
···

0
0
...
1
0 −1

0
0
...
0
1



15.3 Estimation and inversion

In the broad application area of estimation (also called inversion), the goal is to
estimate a set of n values (also called parameters), the entries of the n-vector x. We
are given a set of m measurements, the entries of an m-vector y. The parameters
and measurements are related by

y = Ax + v,

where A is a known m × n matrix, and v is an unknown m-vector. The matrix A
describes how the measured values (i.e., yi) depend on the unknown parameters
(i.e., xj). The m-vector v is the measurement error or measurement noise, and is
unknown but presumed to be small. The estimation problem is to make a sensible
guess as to what x is, given y (and A), and prior knowledge about x.

If the measurement noise were zero, and A has linearly independent columns,
we could recover x exactly, using x = A†y. (This is called exact inversion.) Our
job here is to guess x, even when these strong assumptions do not hold. Of course
we cannot expect to ﬁnd x exactly, when the measurement noise is nonzero, or
when A does not have linearly independent columns. This is called approximate
inversion, or in some contexts, just inversion.

15.3 Estimation and inversion

317

The matrix A can be wide, square, or tall; the same methods are used to esti-
mate x in all three cases. When A is wide, we would not have enough measurements
to determine x from y, even without the noise (i.e., with v = 0). In this case we
have to also rely on our prior information about x to make a reasonable guess.
When A is square or tall, we would have enough measurements to determine x, if
there were no noise present. Even in this case, judicious use of multiple-objective
least squares can incorporate our prior knowledge in the estimation, and yield far
better results.

15.3.1 Regularized inversion

If we guess that x has the value ˆx, then we are implicitly making the guess that
v has the value y − Aˆx. If we assume that smaller values of v (measured by �v�)
are more plausible than larger values, then a sensible choice for ˆx is the least
squares approximate solution, which minimizes �Aˆx− y�2. We will take this as our
primary objective. Our prior information about x enters in one or more secondary
objectives. Simple examples are listed below.

• �x�2: x should be small. This corresponds to the (prior) assumption that x

is more likely to be small than large.

• �x − xprior�2: x should be near xprior. This corresponds to the assumption

that x is near some known vector xprior.

• �Dx�2, where D is the ﬁrst diﬀerence matrix (15.4). This corresponds to
the assumption that x should be smooth, i.e., xi+1 should be near xi. This
regularization is often used when x represents a time series.

• The Dirichlet energy D(x) = �AT x�2, where A is the incidence matrix of
a graph (see page 135). This corresponds to the assumption that x varies
smoothly across the graph, i.e., xi is near xj when i and j are connected by
an edge of the graph. When the Dirichlet energy is used as a regularizer, it
is sometimes called Laplacian regularization. (The previous example, �Dx�2,
is special case of Dirichlet energy, for the chain graph.)

Finally, we will choose our estimate ˆx by minimizing

�Ax − y�2 + λ2J2(x) + ··· + λpJp(x),

where λi > 0 are weights, and J2, . . . , Jp are the regularization terms. This is called
regularized inversion or regularized estimation. We may repeat this for several
choices of the weights, and choose the best estimate for the particular application.

Tikhonov regularized inversion. Choosing ˆx to minimize

�Ax − y�2 + λ�x�2

for some choice of λ > 0 is called Tikhonov regularized inversion, after the math-
ematician Andrey Tikhonov. Here we seek a guess ˆx that is consistent with the
measurements (i.e., �Aˆx − y�2 is small), but not too big.

318

15 Multi-objective least squares

The stacked matrix in this case,

˜A =� A√λI � ,

always has linearly independent columns, without any assumption about A, which
can have any dimensions, and need not have linearly independent columns. To see
this we note that ˜Ax = (Ax,√λx) = 0 implies that √λx = 0, which implies x = 0.
The Gram matrix associated with ˜A,

˜AT ˜A = AT A + λI,

is therefore always invertible (provided λ > 0). The Tikhonov regularized approx-
imate solution is then

ˆx = (AT A + λI)−1AT b.

Equalization. The vector x represents a transmitted signal or message, consisting
of n real values. The matrix A represents the mapping from the transmitted signal
to what is received (called the channel ); y = Ax + v includes noise as well as the
action of the channel. Guessing what x is, given y, can be thought of as un-doing
the eﬀects of the channel. In this context, estimation is called equalization.

15.3.2 Estimating a periodic time series

Suppose that the T -vector y is a (measured) time series, that we believe is a noisy
version of a periodic time series, i.e., one that repeats itself every P periods. We
might also know or assume that the periodic time series is smooth, i.e., its adjacent
values are not too far apart.

Periodicity arises in many time series. For example, we would expect a time
series of hourly temperature at some location to approximately repeat itself every
24 hours, or the monthly snowfall in some region to approximately repeat itself
every 12 months. (Periodicity with a 24 hour period is called diurnal ; periodicity
with a yearly period is called seasonal or annual.) As another example, we might
expect daily total sales at a restaurant to approximately repeat itself weekly. The
goal is to get an estimate of Tuesday’s total sales, given some historical daily sales
data.

The periodic time series will be represented by a P -vector x, which gives its

values over one period. It corresponds to the full time series

ˆy = (x, x, . . . , x)

which just repeats x, where we assume here for simplicity that T is a multiple of P .
(If this is not the case, the last x is replaced with a slice of the form x1:k.) We can
express ˆy as ˆy = Ax, where A is the T × P selector matrix

A =

I
...
I

 .

Dcirc =



−1
1 0
0 −1
1
...
...
...
0 0
0
0
0 0
0 0
1

···
···

0
0
0
0
...
...
··· −1
1
···
0 −1
···
0

0
0
...
0
1
0 −1

.



15.3 Estimation and inversion

319

Our total square estimation error is �Ax − y�2.
We can minimize this objective analytically. The solution ˆx is found by av-
eraging the values of y associated with the diﬀerent entries in x. For example,
we estimate Tuesday sales by averaging all the entries in y that correspond to
Tuesdays. (See exercise 15.10.) This simple averaging works well if we have many
periods worth of data, i.e., if T /P is large.

A more sophisticated estimate can be found by adding regularization for x to

be smooth, based on the assumption that

x1 ≈ x2,

. . . ,

xP−1 ≈ xP ,

xP ≈ x1.

(Note that we include the ‘wrap-around’ pair xP and x1 here.) We measure non-
smoothness as �Dcircx�2, where Dcirc is the P × P circular diﬀerence matrix

We estimate the periodic time series by minimizing

�Ax − y�2 + λ�Dcircx�2.

For λ = 0 we recover the simple averaging mentioned above; as λ gets bigger, the
estimated signal becomes smoother, ultimately converging to a constant (which is
the mean of the original time series data).

The time series Aˆx is called the extracted seasonal component of the given time
series data y (assuming we are considering yearly variation). Subtracting this from
the original data yields the time series y−Aˆx, which is called the seasonally adjusted
time series.
The parameter λ can be chosen using validation. This can be done by selecting
a time interval over which to build the estimate, and another one to validate it.
For example, with 4 years of data, we might train our model on the ﬁrst 3 years of
data, and test it on the last year of data.

Example.
In ﬁgure 15.4 we apply this method to a series of hourly ozone mea-
surements. The top ﬁgure shows hourly measurements over a period of 14 days
(July 1–14, 2014). We represent these values by a 336-vector c, with c24(j−1)+i,
i = 1, . . . , 24, deﬁned as the hourly values on day j, for j = 1, . . . , 14. As indicated
by the gaps in the graph, a number of measurements are missing from the record
(only 275 of the 336 = 24 × 14 measurements are available). We use the notation
Mj ⊆ {1, 2, . . . , 24} to denote the set containing the indices of the available mea-
surements on day j. For example, M8 = {1, 2, 3, 4, 6, 7, 8, 23, 24}, because on July
8, the measurements at 4AM, and from 8AM to 9PM are missing. The middle and
bottom ﬁgures show two periodic time series. The time series are parametrized

320

15 Multi-objective least squares

1

2

3

4

5

6

7

8

9

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

0
1

y
l
u
J

1
1

y
l
u
J

2
1

y
l
u
J

3
1

y
l
u
J

4
1

y
l
u
J

5
1

y
l
u
J

1

2

3

4

5

6

7

8

9

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

0
1

y
l
u
J

1
1

y
l
u
J

2
1

y
l
u
J

3
1

y
l
u
J

4
1

y
l
u
J

5
1

y
l
u
J

10−1

10−2

)

m
p
p
(

l
e
v
e
l

e
n
o
z
O

10−1

10−2

)

m
p
p
(

l
e
v
e
l

e
n
o
z
O

10−1

10−2

)

m
p
p
(

l
e
v
e
l

e
n
o
z
O

1

2

3

4

5

6

7

8

9

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

y
l
u
J

0
1

y
l
u
J

1
1

y
l
u
J

2
1

y
l
u
J

3
1

y
l
u
J

4
1

y
l
u
J

5
1

y
l
u
J

Time

Figure 15.4 Top. Hourly ozone level at Azusa, California, during the ﬁrst
14 days of July 2014 (California Environmental Protection Agency, Air Re-
sources Board, www.arb.ca.gov). Measurements start at 12AM on July 1st,
and end at 11PM on July 14. Note the large number of missing measure-
ments. In particular, all 4AM measurements are missing. Middle. Smooth
periodic least squares ﬁt to logarithmically transformed measurements, using
λ = 1. Bottom. Smooth periodic least squares ﬁt using λ = 100.

15.3 Estimation and inversion

321

by a 24-vector x, repeated 14 times to get the full series (x, x, . . . , x). The two
estimates of x in the ﬁgure were computed by minimizing

14�j=1 �i∈Mj

(xi − log(c24(j−1)+i))2 + λ� 23�i=1

(xi+1 − xi)2 + (x1 − x24)2�

for λ = 1 and λ = 100.

15.3.3

Image de-blurring

The vector x is an image, and the matrix A gives blurring, so y = Ax+v is a blurred,
noisy image. Our prior information about x is that it is smooth; neighboring pixels
values are not very diﬀerent from each other. Estimation is the problem of guessing
what x is, and is called de-blurring.

In least squares image deblurring we form an estimate ˆx by minimizing a cost

function of the form

�Ax − y�2 + λ(�Dhx�2 + �Dvx�2).

(15.5)

Here Dv and Dh represent vertical and horizontal diﬀerencing operations, and
the role of the second term in the weighted sum is to penalize non-smoothness in
the reconstructed image. Speciﬁcally, suppose the vector x has length M N and
contains the pixel intensities of an M × N image X stored column-wise. Let Dh
be the M (N − 1) × M N matrix
−I
I
0 −I
...
...
0
0
0
0

0
0
...
··· −I
···

0
0
...
I
0 −I

···
···

0
I
...
0
0

0
0
...
0
I

Dh =

,



where all blocks have size M × M , and let Dv be the (M − 1)N × M N matrix

where each of the N diagonal blocks D is an (M − 1) × M diﬀerence matrix





D =

Dv =

···
D 0
0
0 D ···
0
...
...
. . .
··· D
0

...
0

,



−1
1
0
0 −1
1
...
...
...
0 0
0
0
0 0

···
···

0
0
...
··· −1
···

0
0
...
1
0 −1

0
0
...
0
1

.



322

15 Multi-objective least squares

Figure 15.5 Left: Blurred, noisy image. Right: Result of regularized least
squares deblurring with λ = 0.007. Image credit: NASA.

With these deﬁnitions the penalty term in (15.5) is the sum of squared diﬀerences
of intensities at adjacent pixels in a row or column:

�Dhx�2 + �Dvx�2 =

M�i=1

N−1�j=1

(Xi,j+1 − Xij)2 +

M−1�i=1

N�j=1

(Xi+1,j − Xij)2.

This quantity is the Dirichlet energy (see page 135), for the graph that connects
each pixel to its left and right, and up and down, neighbors.

Example.
In ﬁgures 15.5 and 15.6 we illustrate this method for an image of size
512 × 512. The blurred, noisy image is shown in the left part of ﬁgure 15.5.
Figure 15.6 shows the estimates ˆx, obtained by minimizing (15.5), for four diﬀerent
values of the parameter λ. The best result (in this case, judged by eye) is obtained
for λ around 0.007 and is shown on the right in ﬁgure 15.5.

15.3.4 Tomography

In tomography, the vector x represents the values of some quantity (such as density)
in a region of interest in n voxels (or pixels) over a 3-D (or 2-D) region. The entries
of the vector y are measurements obtained by passing a beam of radiation through
the region of interest, and measuring the intensity of the beam after it exits the
region.

A familiar application is the computer-aided tomography (CAT) scan used in
medicine.
In this application, beams of X-rays are sent through a patient, and
an array of detectors measure the intensity of the beams after passing through
the patient. These intensity measurements are related to the integral of the X-
ray absorption along the beam. Tomography is also used in applications such as
manufacturing, to assess internal damage or certify quality of a welded joint.

15.3 Estimation and inversion

323

λ = 10−6

λ = 10−4

λ = 10−2

λ = 1

Figure 15.6 Deblurred images for λ = 10−6, 10−4, 10−2, 1. Image credit:
NASA.

324

15 Multi-objective least squares

Line integral measurements. For simplicity we will assume that each beam is a
single line, and that the received value yi is the integral of the quantity over the
region, plus some measurement noise. (The same method can be used when more
complex beam shapes are used.) We consider the 2-D case.

Let d(x, y) denote the density (say) at the position (x, y) in the region. (Here
x and y are the scalar 2-D coordinates, not the vectors x and y in the estimation
problem.) We assume that d(x, y) = 0 outside the region of interest. A line through
the region is deﬁned by the set of points

p(t) = (x0, y0) + t(cos θ, sin θ),

where (x0, y0) denotes a (base) point on the line, and θ is the angle of the line with
respect to the x-axis. The parameter t gives the distance along the line from the
point (x0, y0). The line integral of d is given by

� ∞

−∞

d(p(t)) dt.

We assume that m lines are speciﬁed (e.g., by their base points and angles), and
the measurement yi is the line integral of d, plus some noise, which is presumed
small.

We divide the region of interest into n pixels (or voxels in the 3-D case), and
assume that the density has the constant value xi over pixel i. Figure 15.7 illus-
trates this for a simple example with n = 25 pixels. (In applications the number
of pixels or voxels is in the thousands or millions.) The line integral is then given
by the sum of xi (the density in pixel i) times the length of the intersection of the
line with pixel i. In ﬁgure 15.7, with the pixels numbered row-wise starting at the
top left corner, with width and height one, the line integral for the line shown is

1.06x16 + 0.80x17 + 0.27x12 + 1.06x13 + 1.06x14 + 0.53x15 + 0.54x10.

The coeﬃcient of xi is the length of the intersection of the line with pixel i.

Measurement model. We can express the vector of m line integral measurements,
without the noise, as Ax, where the m × n matrix A has entries

Aij = length of line i in pixel j,

i = 1, . . . , m,

j = 1, . . . , n,

with Aij = 0 if line i does not intersect voxel j.

Tomographic reconstruction.
called tomographic reconstruction or tomographic inversion.

In tomography, estimation or inversion is often

The objective term �Ax− y�2 is the sum of squares of the residual between the
predicted (noise-free) line integrals Ax and the actual measured line integrals y.
Regularization terms capture prior information or assumptions about the voxel
values, for example, that they vary smoothly over the region. A simple regularizer
commonly used is the Dirichlet energy (see page 135) associated with the graph
that connects each voxel to its 6 neighbors (in the 3-D case) or its 4 neighbors (in
the 2-D case). Using the Dirichlet energy as a regularizer is also called Laplacian
regularization.

15.4 Regularized data ﬁtting

325

x2

x1

x6

(x0, y0)

θ

Figure 15.7 A square region of interest divided into 25 pixels, and a line
passing through it.

Example. A simple 2-D example is shown in ﬁgures 15.8–15.10. Figure 15.8 shows
the geometry of the m = 4000 lines and the square region, shown as the square.
The square is divided into 100 × 100 pixels, so n = 10000.
The density of the object we are imaging is shown in ﬁgure 15.9. In this object
the density of each pixel is either 0 or 1 (shown as white or black, respectively).
We reconstruct or estimate the object density from the 4000 (noisy) line integral
measurements by solving the regularized least squares problem

minimize

�Ax − y�2 + λ�Dx�2,

where �Dx�2 is the sum of squares of the diﬀerences of the pixel values from their
neighbors. Figure 15.10 shows the results for six diﬀerent values of λ. We can see
that for small λ the reconstruction is relatively sharp, but suﬀers from noise. For
large λ the noise in the reconstruction is smaller, but it is too smooth.

15.4 Regularized data ﬁtting

We consider least squares data ﬁtting, as described in chapter 13. In §13.2 we con-
sidered the issue of over-ﬁtting, where the model performs poorly on new, unseen
data, which occurs when the model is too complicated for the given data set. The
remedy is to keep the model simple, e.g., by ﬁtting with a polynomial of not too
high a degree.

Regularization is another way to avoid over-ﬁtting, diﬀerent from simply choos-
ing a model that is simple (i.e., does not have too many basis functions). Regular-
ization is also called de-tuning, shrinkage, or ridge regression, for reasons we will
explain below.

326

15 Multi-objective least squares

Figure 15.8 The square region at the center of the picture is surrounded
by 100 points shown as circles. 40 lines (beams) emanate from each point.
(The lines are shown for two points only.) This gives a total of 4000 lines
that intersect the region.

.

Figure 15.9 Density of object used in the tomography example.

15.4 Regularized data ﬁtting

327

λ = 10−2

λ = 10−1

λ = 1

λ = 5

λ = 10

λ = 100

Figure 15.10 Regularized least squares reconstruction for six values of the
regularization parameter.

328

15 Multi-objective least squares

Motivation. To motivate regularization, consider the model

ˆf (x) = θ1f1(x) + ··· + θpfp(x).

(15.6)

We can interpret θi as the amount by which our prediction depends on fi(x), so
if θi is large, our prediction will be very sensitive to changes or variations in the
value of fi(x), such as those we might expect in new, unseen data. This suggests
that we should prefer that θi be small, so our model is not too sensitive. There
is one exception here: if fi(x) is constant (for example, the number one), then we
should not worry about the size of θi, since fi(x) never varies. But we would like
all the others to be small, if possible.

This suggests the bi-criterion least squares problem with primary objective
�y − Aθ�2, the sum of squares of the prediction errors, and secondary objective
�θ2:p�2, assuming that f1 is the constant function one. Thus we should minimize
(15.7)

�y − Aθ�2 + λ�θ2:p�2,
where λ > 0 is called the regularization parameter.

For the regression model, this weighted objective can be expressed as

�y − X T β − v1�2 + λ�β�2.

Here we penalize β being large (because this leads to sensitivity of the model), but
not the oﬀset v. Choosing β to minimize this weighted objective is called ridge
regression.

Eﬀect of regularization. The eﬀect of the regularization is to accept a worse value
of sum square ﬁt (�y−Aθ�2) in return for a smaller value of �θ2:p�2, which measures
the size of the parameters (except θ1, which is associated with the constant basis
function). This explains the name shrinkage: The parameters are smaller than they
would be without regularization, i.e., they are shrunk. The term de-tuned suggests
that with regularization, the model is not excessively ‘tuned’ to the training data
(which would lead to over-ﬁt).

Regularization path. We get a diﬀerent model for every choice of λ. The way
the parameters change with λ is called the regularization path. When p is small
enough (say, less than 15 or so) the parameter values can be plotted, with λ on the
horizontal axis. Usually only 30 or 50 values of λ are considered, typically spaced
logarithmically over a large range (see page 314).

An appropriate value of λ can be chosen via out-of-sample or cross-validation.
As λ increases, the RMS ﬁt on the training data worsens (increases). But (as with
model order) the test set RMS prediction error typically decreases as λ increases,
and then, when λ gets too big, it increases. A good choice of regularization pa-
rameter is one which approximately minimizes the test set RMS prediction error.
When multiple values of λ approximately minimize the RMS error, common prac-
tice is to take the largest value of λ. The idea here is to use the model of minimum
sensitivity, as measured by �β�2, among those that make good predictions on the
test set.

15.4 Regularized data ﬁtting

329

)
t
(
s

3

2

1

0

−1

Train
Test

0

0.2

0.4

0.6

0.8

1

t

Figure 15.11 A signal s(t) and 30 noisy samples. Ten of the samples are
used as training set, the other 20 as test set.

Example. We illustrate these ideas with a small example with synthetic (simu-
lated) data. We start with a signal, shown in ﬁgure 15.11, consisting of a constant
plus four sinusoids:

s(t) = c +

4�k=1

αk cos(ωkt + φk),

with coeﬃcients

c = 1.54, α1 = 0.66, α2 = −0.90, α3 = −0.66, α4 = 0.89.

(15.8)

(The other parameters are ω1 = 13.69, ω2 = 3.55, ω3 = 23.25, ω4 = 6.03, and
φ1 = 0.21, φ2 = 0.02, φ3 = −1.87, φ4 = 1.72.) We will ﬁt a model of the
form (15.6) with p = 5 and

f1(x) = 1,

fk+1(x) = cos(ωkx + φk),

k = 1, . . . , 4.

(Note that the model is exact when the parameters are chosen as θ1 = c, θk = αk−1,
k = 2, . . . , 5. This rarely occurs in practice.) We ﬁt our model using regularized
least squares on 10 noisy samples of the function, shown as the open circles in
ﬁgure 15.11. We will test the model obtained on the 20 noisy data points shown
as the ﬁlled circles in ﬁgure 15.11.

Figure 15.12 shows the regularization path and the RMS training and test errors
as functions of the regularization parameter λ, as it varies over a large range. The
regularization path shows that as λ increases, the parameters θ2, . . . , θ5 get smaller
(i.e., shrink), converging towards zero as λ gets very large. We can see that the
training prediction error increases with increasing λ (since we are trading oﬀ model
sensitivity for sum square ﬁtting error). The test error follows a typical pattern: It
ﬁrst decreases to a minimum value, then increases again. The minimum test error

330

15 Multi-objective least squares

occurs around λ = 0.079; any choice between around λ = 0.065 and 0.100 (say)
would be reasonable. The horizontal dashed lines show the ‘true’ values of the
coeﬃcients (i.e., the ones we used to synthesize the data) given in (15.8). We can
see that for λ near 0.079, our estimated parameters are close to the ‘true’ values.

Linear independence of columns. One side beneﬁt of adding regularization to
the basic least squares data ﬁtting method, as in (15.7), is that the columns of the
associated stacked matrix are always linearly independent, even if the columns of
the matrix A are not. To see this, suppose that

� A√λB � x = 0,

where B is the (p − 1) × p selector matrix
B = (eT

2 , . . . , eT

p ),

so Bθ = θ2:p. From the last p−1 entries in the equation above, we get √λxi = 0 for
i = 2, . . . , p, which implies that x2 = ··· = xp = 0. Using these values of x2, . . . , xp,
and the fact that the ﬁrst column of A is 1, the top m equations become 1x1 = 0,
and we conclude that x1 = 0 as well. So the columns of the stacked matrix are
always linearly independent.

Feature engineering and regularized least squares. The simplest method of
avoiding over-ﬁt is to keep the model simple, which usually means that we should
not use too many features. A typical and rough rule of thumb is that the number
of features should be small compared to the number of data points (say, no more
than 10% or 20%). The presence of over-ﬁt can be detected using out-of-sample
validation or cross-validation, which is always done when you ﬁt a model to data.
Regularization is a powerful alternative method to avoid over-ﬁtting a model.
With regularization, you can ﬁt a model with more features than would be appro-
priate without regularization. You can even ﬁt a model using more features than
you have data points, in which case the matrix A is wide. Regularization is often
the key to success in feature engineering, which can greatly increase the number of
features.

15.5 Complexity

In the general case we can minimize the weighted sum objective (15.1) by creating
the stacked matrix and vector ˜A and ˜b in (15.2), and then using the QR factorization
to solve the resulting least squares problem. The complexity of this method is order
mn2 ﬂops, where m = m1+···+mk is the sum of heights of the matrices A1, . . . , Ak.
When using multi-objective least squares, it is common to minimize the weighted
sum objective for some, or even many, diﬀerent choices of weights. Assuming that
the weighted sum objective is minimized for L diﬀerent values of the weights, the
total complexity is order Lmn2 ﬂops.

15.5 Complexity

331

r
o
r
r
e

S
M
R

1.2

1

0.8

0.6

0.4

0.2

0

Test
Train

10−5

10−3

10−1

101

103

105

2 θ1

1

0

−1

θ5

θ2

θ4

θ3

s
t
n
e
i
c
ffi
e
o
C

10−5

10−3

10−1

101

103

105

λ

Figure 15.12 Top. RMS training and test errors as a function of the reg-
ularization parameter λ. Bottom. The regularization path. The dashed
horizontal lines show the values of the coeﬃcients used to generate the data.

332

15 Multi-objective least squares

15.5.1 Gram caching

We start from the formula (15.3) for the minimizer of the weighted sum objective,

ˆx = (λ1AT

1 A1 + ··· + λkAT

k Ak)−1(λ1AT

1 b1 + ··· + λkAT

k bk).

The matrix appearing in the inverse is a weighted sum of the Gram matrices Gi =
AT
i Ai associated with the matrices Ai. We can compute ˆx by forming these Gram
matrices Gi, along with the vectors hi = AT

G = λ1G1 + ··· + λkGk,

i bi, then forming the weighted sums
h = λ1h1 + ··· + λkhk,

and ﬁnally, solving the n × n set of equations Gˆx = h. Forming Gi and hi costs
min2 and 2min ﬂops, respectively. (We save a factor of two in forming the Gram
matrix; see page 182.) Ignoring the second term and adding over i = 1, . . . , k we
get a total of mn2 ﬂops. Forming the weighted sums G and h costs 2kn2 ﬂops.
Solving Gˆx = h costs order 2n3 ﬂops.

Gram caching is the simple trick of computing Gi (and hi) just once, and re-
using these matrices and vectors for the L diﬀerent choices of weights. This leads
to a complexity of

mn2 + L(k + 2n)n2

ﬂops. When m is much larger than k + n, which is a common occurrence, this cost
is smaller than Lmn2, the cost for the simple method.

As a simple example consider Tikhonov regularization. We will compute

ˆx(i) = (AT A + λ(i)I)−1AT b

for i = 1, . . . , L, where A is m × n. The cost of the simple method is 2Lmn2 ﬂops;
using Gram caching the cost is mn2 + 2Ln3 = (m + 2Ln)n2 ﬂops. (We drop the
term Lkn2, since k = 2 here.) With m = 100n and L = 100, Gram caching reduces
the computational cost by more than a factor of 50. This means that the entire
regularization path (i.e., the solution for 100 values of λ) can be computed in not
much more time than it takes to compute the solution for one value of λ.

15.5.2 The kernel trick

In this section we focus on another special case, which arises in many applications:

J = �Ax − b�2 + λ�x − xdes�2,

(15.9)

where the m × n matrix A is wide, i.e., m < n, and λ > 0. (Here we drop the
subscripts on A, b, and m since we have only one matrix in this problem.) The
associated (m + n) × n stacked matrix (see (15.2))

˜A =� A√λI �

always has linearly independent columns. Using the QR factorization to solve the
stacked least squares problem requires 2(m + n)n2 ﬂops, which grows like n3. We

15.5 Complexity

333

will show now how this special problem can be solved far more eﬃciently when m
is much smaller than n, using something called the kernel trick. Recall that the
minimizer of J is given by (see (15.3))

ˆx = (AT A + λI)−1(AT b + λxdes)

= (AT A + λI)−1(AT b + (λI + AT A)xdes − (AT A)xdes)
= (AT A + λI)−1AT (b − Axdes) + xdes.

The matrix inverse here has size n × n.

We will use the identity

(AT A + λI)−1AT = AT (AAT + λI)−1

(15.10)

which holds for any matrix A and any λ > 0. Note that the left-hand side of the
identity involves the inverse of an n×n matrix, whereas the right-hand side involves
the inverse of a (smaller) m × m matrix. (This is a variation on the push-through
identity from exercise 11.9.)
To show the identity (15.10), we ﬁrst observe that the matrices AT A + λI and

AAT + λI are invertible. We start with the equation

AT (AAT + λI) = (AT A + λI)AT ,

and multiply each side by (AT A+λI)−1 on the left and (AAT +λI)−1 on the right,
which yields the identity above.

Using (15.10) we can express the minimizer of J as

ˆx = AT (AAT + λI)−1(b − Axdes) + xdes.

We can compute the term (AAT + λI)−1(b − Axdes) by computing the QR factor-
ization of the (m + n) × m matrix

¯A =� AT

√λI � ,

which has a cost of 2(m + n)m2 ﬂops. The other operations involve matrix-vector
products and have order (at most) mn ﬂops, so we can use this method to compute
ˆx in around 2(m + n)m2 ﬂops. This complexity grows only linearly in n.

To summarize, we can minimize the regularized least squares objective J in (15.9)
two diﬀerent ways. One requires a QR factorization of the (m + n) × n matrix ˜A,
which has cost 2(m + n)n2 ﬂops. The other (which uses the kernel trick) requires
a QR factorization of the (m + n)× m matrix ¯A, which has cost 2(m + n)m2 ﬂops.
We should evidently use the kernel trick when m < n. The complexity can then
be expressed as

(m + n) min{m2, n2} ≈ min{mn2, nm2} = (max{m, n})(min{m, n})2.

where ≈ means we ignore non-dominant terms.
states that many operations involving a matrix A can be carried out with order

This is an instance of the big-times-small-squared rule or mnemonic, which

where ‘big’ and ‘small’ refer to the big and small dimensions of the matrix. Several
other examples are listed in appendix B.

(big) × (small)2 ﬂops,

334

15 Multi-objective least squares

Exercises

15.1 A scalar multi-objective least squares problem. We consider the special case of the multi-
objective least squares problem in which the variable x is a scalar, and the k matrices Ai
are all 1×1 matrices with value Ai = 1, so Ji = (x−bi)2. In this case our goal is to choose
a number x that is simultaneously close to all the numbers b1, . . . , bk. Let λ1, . . . , λk be
positive weights, and ˆx the minimizer of the weighted objective (15.1). Show that ˆx is a
weighted average (or convex combination; see page 17) of the numbers b1, . . . , bk, i.e., it
has the form

where wi are nonnegative and sum to one. Give an explicit formula for the combination
weights wi in terms of the multi-objective least squares weights λi.

x = w1b1 + ··· + wkbk,

15.2 Consider the regularized data ﬁtting problem (15.7). Recall that the elements in the ﬁrst

column of A are one. Let ˆθ be the solution of (15.7), i.e., the minimizer of

and let ˜θ be the minimizer of

�Aθ − y�2 + λ(θ2

2 + ··· + θ2
p),

�Aθ − y�2 + λ�θ�2 = �Aθ − y�2 + λ(θ2

1 + θ2

2 + ··· + θ2
p),

in which we also penalize θ1. Suppose columns 2 through p of A have mean zero (for
example, because features 2, . . . , p have been standardized on the data set; see page 269).
Show that ˆθk = ˜θk for k = 2, . . . , p.

15.3 Weighted Gram matrix. Consider a multi-objective least squares problems with matrices

A1, . . . , Ak and positive weights λ1, . . . , λk. The matrix
1 A1 + ··· + λkAT

G = λ1AT

k Ak

is called the weighted Gram matrix ; it is the Gram matrix of the stacked matrix ˜A (given
in (15.2)) associated with the multi-objective problem. Show that G is invertible provided
there is no nonzero vector x that satisﬁes A1x = 0, . . . , Akx = 0.

15.4 Robust approximate solution of linear equations. We wish to solve the square set of n
linear equations Ax = b for the n-vector x. If A is invertible the solution is x = A−1b.
In this exercise we address an issue that comes up frequently: We don’t know A exactly.
One simple method is to just choose a typical value of A and use it. Another method,
which we explore here, takes into account the variation in the matrix A. We ﬁnd a set of
K versions of A, and denote them as A(1), . . . , A(K). (These could be found by measuring
the matrix A at diﬀerent times, for example.) Then we choose x so as to minimize

�A(1)x − b�2 + ··· + �A(K)x − b�2,

the sum of the squares of residuals obtained with the K versions of A. This choice of x,
which we denote xrob, is called a robust (approximate) solution. Give a formula for xrob, in
terms of A(1), . . . , A(K) and b. (You can assume that a matrix you construct has linearly
independent columns.) Verify that for K = 1 your formula reduces to xrob = (A(1))−1b.
15.5 Some properties of bi-objective least squares. Consider the bi-objective least squares prob-

lem with objectives

J1(x) = �A1x − b1�2,

J2(x) = �A2x − b2�2.

For λ > 0, let ˆx(λ) denote the minimizer of J1(x) + λJ2(x). (We assume the columns of
the stacked matrix are linearly independent.) We deﬁne J �
2 (λ) =
J2(ˆx(λ)), the values of the two objectives as functions of the weight parameter. The
optimal trade-oﬀ curve is the set of points (J �
2 (λ)), as λ varies over all positive
numbers.

1 (λ) = J1(ˆx(λ)) and J �

1 (λ), J �

Exercises

335

(a) Bi-objective problems without trade-oﬀ. Suppose µ and γ are diﬀerent positive
weights, and ˆx(µ) = ˆx(γ). Show that ˆx(λ) is constant for all λ > 0. Therefore
the point (J �
2 (λ)) is the same for all λ and the trade-oﬀ curve collapses to a
single point.

1 (λ), J �

(b) Eﬀect of weight on objectives in a bi-objective problem. Suppose ˆx(λ) is not constant.

Show the following: for λ < µ, we have

J �
1 (λ) < J �

1 (µ),

J �
2 (λ) > J �

2 (µ).

This means that if you increase the weight (on the second objective), the second
objective goes down, and the ﬁrst objective goes up. In other words the trade-oﬀ
curve slopes downward.
Hint. Resist the urge to write out any equations or formulas. Use the fact that
ˆx(λ) is the unique minimizer of J1(x) + λJ2(x), and similarly for ˆx(µ), to deduce
the inequalities

J �
1 (µ) + λJ �

2 (µ) > J �

1 (λ) + λJ �

2 (λ),

J �
1 (λ) + µJ �

2 (λ) > J �

1 (µ) + µJ �

2 (µ).

Combine these inequalities to show that J �

1 (λ) < J �

1 (µ) and J �

2 (λ) > J �

2 (µ).

(c) Slope of the trade-oﬀ curve. The slope of the trade-oﬀ curve at the point (J �

1 (λ), J �

2 (λ))

is given by

S = lim
µ→λ

J �
2 (µ) − J �
J �
1 (µ) − J �

2 (λ)
1 (λ)

.

(This limit is the same if µ approaches λ from below or from above.) Show that
S = −1/λ. This gives another interpretation of the parameter λ: (J �
2 (λ)) is
the point on the trade-oﬀ curve where the curve has slope −1/λ.
Hint. First assume that µ approaches λ from above (meaning, µ > λ) and use the
inequalities in the hint for part (b) to show that S ≥ −1/λ. Then assume that µ
approaches λ from below and show that S ≤ −1/λ.

1 (λ), J �

15.6 Least squares with smoothness regularization. Consider the weighted sum least squares

objective

�Ax − b�2 + λ�Dx�2,

where the n-vector x is the variable, A is an m× n matrix, D is the (n− 1)× n diﬀerence
matrix, with ith row (ei+1−ei)T , and λ > 0. Although it does not matter in this problem,
this objective is what we would minimize if we want an x that satisﬁes Ax ≈ b, and has
entries that are smoothly varying. We can express this objective as a standard least
squares objective with a stacked matrix of size (m + n − 1) × n.
Show that the stacked matrix has linearly independent columns if and only if A1 �= 0,
i.e., the sum of the columns of A is not zero.

15.7 Greedy regulation policy. Consider a linear dynamical system given by xt+1 = Axt + But,
where the n-vector xt is the state at time t, and the m-vector ut is the input at time t. The
goal in regulation is to choose the input so as to make the state small. (In applications,
the state xt = 0 corresponds to the desired operating point, so small xt means the state
is close to the desired operating point.) One way to achieve this goal is to choose ut so
as to minimize

�xt+1�2 + ρ�ut�2,

where ρ is a (given) positive parameter that trades oﬀ using a small input versus making
the (next) state small. Show that choosing ut this way leads to a state feedback policy
ut = Kxt, where K is an m × n matrix. Give a formula for K (in terms of A, B, and ρ).
If an inverse appears in your formula, state the conditions under which the inverse exists.
Remark. This policy is called greedy or myopic since it does not take into account the
eﬀect of the input ut on future states, beyond xt+1. It can work very poorly in practice.

336

15 Multi-objective least squares

15.8 Estimating the elasticity matrix.

In this problem you create a standard model of how
demand varies with the prices of a set of products, based on some observed data. There
are n diﬀerent products, with (positive) prices given by the n-vector p. The prices are
held constant over some period, say, a day. The (positive) demands for the products over
the day are given by the n-vector d. The demand in any particular day varies, but it is
thought to be (approximately) a function of the prices.
The nominal prices are given by the n-vector pnom. You can think of these as the prices
that have been charged in the past for the products. The nominal demand is the n-vector
dnom. This is the average value of the demand, when the prices are set to pnom. (The
actual daily demand ﬂuctuates around the value dnom.) You know both pnom and dnom.
We will describe the prices by their (fractional) variations from the nominal values, and
the same for demands. We deﬁne δp and δd as the (vectors of) relative price change and
demand change:

δp
i =

pi − pnom

i
pnom
i

,

δd
i =

di − dnom

i
dnom
i

,

i = 1, . . . , n.

3 = +0.05 means that the price for product 3 has been increased by 5% over its
5 = −0.04 means that the demand for product 5 in some day is 4%

So δp
nominal value, and δd
below its nominal value.
Your task is to build a model of the demand as a function of the price, of the form

δd ≈ Eδp,

where E is the n × n elasticity matrix. You don’t know E, but you do have the results
of some experiments in which the prices were changed a bit from their nominal values for
one day, and the day’s demands were recorded. This data has the form

(p1, d1), . . . , (pN , dN ),

where pi is the price for day i, and di is the observed demand.
Explain how you would estimate E, given this price-demand data. Be sure to explain how
you will test for, and (if needed) avoid over-ﬁt. Hint. Formulate the problem as a matrix
least squares problem; see page 233.
Remark. Note the diﬀerence between elasticity estimation and demand shaping, discussed
on page 315. In demand shaping, we know the elasticity matrix and are choosing prices;
in elasticity estimation, we are guessing the elasticity matrix from some observed price
and demand data.

15.9 Regularizing stratiﬁed models. In a stratiﬁed model (see page 272), we divide the data
into diﬀerent sets, depending on the value of some (often Boolean) feature, and then ﬁt
a separate model for each of these two data sets, using the remaining features. As an
example, to develop a model of some health outcome we might build a separate model for
women and for men. In some cases better models are obtained when we encourage the
diﬀerent models in a stratiﬁed model to be close to each other. For the case of stratifying
on one Boolean feature, this is done by choosing the two model parameters θ(1) and θ(2)
to minimize

�A(1)θ(1) − y(1)�2 + �A(2)θ(2) − y(2)�2 + λ�θ(1) − θ(2)�2,

where λ ≥ 0 is a parameter. The ﬁrst term is the least squares residual for the ﬁrst model
on the ﬁrst data set (say, women); the second term is the least squares residual for the
second model on the second data set (say, men); the third term is a regularization term
that encourages the two model parameters to be close to each other. Note that when
λ = 0, we simply ﬁt each model separately; when λ is very large, we are basically ﬁtting
one model to all the data. Of course the choice of an appropriate value of λ is obtained
using out-of-sample validation (or cross-validation).

ˆyi =

yi+(k−1)P .

1
K

K�k=1

In other words, each entry of the periodic estimate is the average of the entries of the
original vector over the corresponding indices.

15.11 General pseudo-inverse. In chapter 11 we encountered the pseudo-inverse of a tall matrix
with linearly independent columns, a wide matrix with linearly independent rows, and
a square invertible matrix. In this exercise we describe the pseudo-inverse of a general
matrix, i.e., one that does not ﬁt these categories. The general pseudo-inverse can be
deﬁned in terms of Tikhonov regularized inversion (see page 317). Let A be any matrix,
and λ > 0. The Tikhonov regularized approximate solution of Ax = b, i.e., unique
minimizer of �Ax − b�2 + λ�x�2, is given by (AT A + λI)−1AT b. The pseudo-inverse of A
is deﬁned as

Exercises

337

(a) Give a formula for the optimal (ˆθ(1), ˆθ(2)). (If your formula requires one or more

matrices to have linearly independent columns, say so.)

(b) Stratifying across age groups. Suppose we ﬁt a model with each data point repre-
senting a person, and we stratify over the person’s age group, which is a range of
consecutive ages such as 18–24, 24–32, 33–45, and so on. Our goal is to ﬁt a model
for each age of k groups, with the parameters for adjacent age groups similar, or not
too far, from each other. Suggest a method for doing this.

15.10 Estimating a periodic time series. (See §15.3.2.) Suppose that the T -vector y is a mea-
sured time series, and we wish to approximate it with a P -periodic T -vector. For simplic-
ity, we assume that T = KP , where K is an integer. Let ˆy be the simple least squares ﬁt,
with no regularization, i.e., the P -periodic vector that minimizes �ˆy − y�2. Show that for
i = 1, . . . , P − 1, we have

A† = lim
λ→0

(AT A + λI)−1AT .

A† = lim
λ→0

AT (AAT + λI)−1.

In other words, A†b is the limit of the Tikhonov-regularized approximate solution of
Ax = b, as the regularization parameter converges to zero. (It can be shown that this
limit always exists.) Using the kernel trick identity (15.10), we can also express the
pseudo-inverse as

(a) What is the pseudo-inverse of the m × n zero matrix?
(b) Suppose A has linearly independent columns. Explain why the limits above reduce

to our previous deﬁnition, A† = (AT A)−1AT .

(c) Suppose A has linearly independent rows. Explain why the limits above reduce to

our previous deﬁnition, A† = AT (AAT )−1.

Hint. For parts (b) and (c), you can use the fact that the matrix inverse is a continuous
function, which means that the limit of the inverse of a matrix is the inverse of the limit,
provided the limit matrix is invertible.

Chapter 16

Constrained least squares

In this chapter we discuss a useful extension of the least squares problem that
includes linear equality constraints. Like least squares, the constrained least squares
problem can be reduced to a set of linear equations, which can be solved using the
QR factorization.

16.1 Constrained least squares problem

In the basic least squares problem, we seek x that minimizes the objective function
�Ax − b�2. We now add constraints to this problem, by insisting that x satisfy
the linear equations Cx = d, where the matrix C and the vector d are given.
The linearly constrained least squares problem (or just constrained least squares
problem) is written as

�Ax − b�2
minimize
subject to Cx = d.

(16.1)

Here x, the variable to be found, is an n-vector. The problem data (which are
given) are the m × n matrix A, the m-vector b, the p × n matrix C, and the
p-vector d.
We refer to the function �Ax − b�2 as the objective of the problem, and the set
of p linear equality constraints Cx = d as the constraints of the problem. They
can be written out as p scalar constraints (equations)

cT
i x = di,

i = 1, . . . , p,

where cT
i

is the ith row of C.

An n-vector x is called feasible (for the problem (16.1)) if it satisﬁes the con-
straints, i.e., Cx = d. An n-vector ˆx is called an optimal point or solution of the
optimization problem (16.1) if it is feasible, and if �Aˆx− b�2 ≤ �Ax− b�2 holds for
any feasible x. In other words, ˆx solves the problem (16.1) if it is feasible and has
the smallest possible value of the objective function among all feasible vectors.

The constrained least squares problem combines the problems of solving a set
of linear equations (ﬁnd x that satisﬁes Cx = d) with the least squares problem

340

16 Constrained least squares

ˆf (x)

p(x)

q(x)

a

x

Figure 16.1 Least squares ﬁt of two cubic polynomials to 140 points, with
continuity constraints p(a) = q(a) and p�(a) = q�(a).

(ﬁnd x that minimizes �Ax−b�2). Indeed each of these problems can be considered
a special case of the constrained least squares problem (16.1).
The constrained least squares problem can also be thought of as a limit of a bi-
objective least squares problem, with primary objective �Ax − b�2 and secondary
objective �Cx − d�2. Roughly speaking, we put inﬁnite weight on the second
objective, so that any nonzero value is unacceptable (which forces x to satisfy
Cx = d). So we would expect (and it can be veriﬁed) that minimizing the weighted
objective

�Ax − b�2 + λ�Cx − d�2,

for a very large value of λ yields a vector close to a solution of the constrained least
squares problem (16.1). We will encounter this idea again in chapter 19, when we
consider the nonlinear constrained least squares problem.

Example.
N = 140 points (xi, yi) in the plane. The function ˆf (x) is deﬁned as

In ﬁgure 16.1 we ﬁt a piecewise-polynomial function ˆf (x) to a set of

ˆf (x) =� p(x) x ≤ a

q(x) x > a,

with a given, and p(x) and q(x) polynomials of degree three or less,

p(x) = θ1 + θ2x + θ3x2 + θ4x3,

q(x) = θ5 + θ6x + θ7x2 + θ8x3.

We also impose the condition that p(a) = q(a) and p�(a) = q�(a), so that ˆf (x) is
continuous and has a continuous ﬁrst derivative at x = a. Suppose the N data

16.1 Constrained least squares problem

341

points (xi, yi) are numbered so that x1, . . . , xM ≤ a and xM +1, . . . , xN > a. The
sum of squares of the prediction errors is

M�i=1

(θ1 + θ2xi + θ3x2

i + θ4x3

i − yi)2 +

N�i=M +1

(θ5 + θ6xi + θ7x2

i + θ8x3

i − yi)2.

The conditions p(a) − q(a) = 0 and p�(a) − q�(a) = 0 are two linear equations

θ1 + θ2a + θ3a2 + θ4a3 − θ5 − θ6a − θ7a2 − θ8a3 = 0
θ2 + 2θ3a + 3θ4a2 − θ6 − 2θ7a − 3θ8a2 = 0.

We can determine the coeﬃcients ˆθ = (ˆθ1, . . . , ˆθ8) that minimize the sum of squares
of the prediction errors, subject to the continuity constraints, by solving a con-
strained least squares problem

�Aθ − b�2
minimize
subject to Cθ = d.

The matrices and vectors A, b, C, d are deﬁned as

0
0
...
0

x1
x2
...

x3
x2
1
0
1
1
x3
x2
0
1
2
2
...
...
...
...
M x3
1 xM x2
M 0
0
0
0
0
0
0
...
...
...
0
0
0

0
0
...
0

1 xM +1 x2
1 xM +2 x2
...
1

...
xN

0
0
...
0

0
0
...
0
M +1 x3
M +2 x3
...
x2
N

M +1

M +2

...
x3
N

,

b =



y1
y2
...
yM
yM +1
yM +2

...
yN



,





A =

and

C =� 1 a a2

0

1

a3 −1 −a −a2 −a3

0 −1 −2a −3a2 � ,

2a 3a2

d =� 0
0 � .

This method is easily extended to piecewise-polynomial functions with more than
two intervals. Functions of this kind are called splines.

Advertising budget allocation. We continue the example described on page 234,
where the goal is to purchase advertising in n diﬀerent channels so as to achieve
(or approximately achieve) a target set of customer views or impressions in m
diﬀerent demographic groups. We denote the n-vector of channel spending as s;
this spending results in a set of views (across the demographic groups) given by the
m-vector Rs. We will minimize the sum of squares of the deviation from the target
set of views, given by vdes. In addition, we ﬁx our total advertising spending, with
the constraint 1T s = B, where B is a given total advertising budget. (This can
also be described as allocating a total budget B across the n diﬀerent channels.)
This leads to the constrained least squares problem
�Rs − vdes�2

minimize
subject to 1T s = B.

342

16 Constrained least squares

Optimal
Scaled

s
n
o
i
s
s
e
r
p
m

I

1,000

500

0

1

2

3

4

6
5
Group

7

8

9

10

Figure 16.2 Advertising with budget constraint. The ‘optimal’ views vector
is the solution of the constrained least squares problem with budget con-
straint. The ‘scaled’ views vector is obtained by scaling the unconstrained
least squares solution so that it satisﬁes the budget constraint. This is a
scalar multiple of the views vector of ﬁgure 12.4.

(The solution ˆs of this problem is not guaranteed to have nonnegative entries, as it
must to make sense in this application. But we ignore this aspect of the problem
here.)

We consider the same problem instance as on page 234, with m = 10 demo-
graphic groups and n = 3 channels, and reach matrix R given there. The least
squares method yields an RMS error of 133 (around 13.3%), with a total budget
of 1T sls = 1605. We seek a spending plan with a budget that is 20% smaller,
B = 1284. Solving the associated constrained least squares problem yields the
spending vector scls = (315, 110, 859), which has RMS error of 161 in the target
views. We can compare this spending vector to the one obtained by simply scaling
the least squares spending vector by 0.80. The RMS error for this allocation is 239.
The resulting impressions for both spending plans are shown in ﬁgure 16.2.

16.1.1 Least norm problem

An important special case of the constrained least squares problem (16.1) is when
A = I and b = 0:

(16.2)

minimize
subject to Cx = d.

�x�2

In this problem we seek the vector of smallest or least norm that satisﬁes the linear
equations Cx = d. For this reason the problem (16.2) is called the least norm
problem or minimum-norm problem.

16.1 Constrained least squares problem

343

1

0

e
c
r
o
F

−1

0

1

n
o
i
t
i
s
o
P

0.5

0

0

2

4

Time

6

8

10

2

4

Time

6

8

10

Figure 16.3 Left: A force sequence f bb = (1, −1, 0, . . . , 0) that transfers the
mass over a unit distance in 10 seconds. Right: The resulting position of
the mass p(t).

Example. The 10-vector f represents a series of forces applied, each for one sec-
ond, to a unit mass on a surface with no friction. The mass starts with zero velocity
and position. By Newton’s laws, its ﬁnal velocity and position are given by

vﬁn = f1 + f2 + ··· + f10
pﬁn = (19/2)f1 + (17/2)f2 + ··· + (1/2)f10.

(See exercise 2.3.)

Now suppose we want to choose a force sequence that results in vﬁn = 0, pﬁn = 1,
i.e., a force sequence that moves the mass to a resting position one meter to the
right. There are many such force sequences; for example f bb = (1,−1, 0, . . . , 0).
This force sequence accelerates the mass to velocity 0.5 after one second, then
decelerates it over the next second, so it arrives after two seconds with velocity 0,
at the destination position 1. After that it applies zero force, so the mass stays
where it is, at rest at position 1. The superscript ‘bb’ refers to bang-bang, which
means that a large force is applied to get the mass moving (the ﬁrst ‘bang’) and
another large force (the second ‘bang’) is then applied to slow it to zero velocity.
The force and position versus time for this choice of f are shown in ﬁgure 16.3.

Now we ask, what is the smallest force sequence that can achieve vﬁn = 0,
pﬁn = 1, where smallest is measured by the sum of squares of the applied forces,
�f�2 = f 2

10? This problem can be posed as a least norm problem,

1 + ··· + f 2
minimize

�f�2
1

subject to �

1

19/2

17/2

···
···

1
3/2

1

1/2 � f =� 0
1 � ,

with variable f . The solution f ln, and the resulting position, are shown in ﬁg-
ure 16.4. The norm square of the least norm solution f ln is 0.0121; in contrast,
the norm square of the bang-bang force sequence is 2, a factor of 165 times larger.
(Note the very diﬀerent vertical axis scales in ﬁgures 16.4 and 16.3.)

344

16 Constrained least squares

0.05

e
c
r
o
F

0

−0.05

0

2

4

Time

6

8

10

1

n
o
i
t
i
s
o
P

0.5

0

0

2

4

Time

6

8

10

Figure 16.4 Left: The smallest force sequence f ln that transfers the mass
over a unit distance in 10 steps. Right: The resulting position of the mass
p(t).

16.2 Solution

Optimality conditions via Lagrange multipliers. We will use the method of La-
grange multipliers (developed by the mathematician Joseph-Louis Lagrange, and
summarized in §C.3) to solve the constrained least squares problem (16.1). Later
we give an independent veriﬁcation, that does not rely on calculus or Lagrange
multipliers, that the solution we derive is correct.

We ﬁrst write the constrained least squares problem with the constraints given

as a list of p scalar equality constraints:

minimize
subject to

�Ax − b�2
cT
i x = di,

i = 1, . . . , p,

where cT

i are the rows of C. We form the Lagrangian function
1 x − d1) + ··· + zp(cT

L(x, z) = �Ax − b�2 + z1(cT

p x − dp),

where z is the p-vector of Lagrange multipliers. The method of Lagrange multipliers
tells us that if ˆx is a solution of the constrained least squares problem, then there
is a set of Lagrange multipliers ˆz that satisfy

∂L
∂xi

(ˆx, ˆz) = 0,

i = 1, . . . , n,

∂L
∂zi

(ˆx, ˆz) = 0,

i = 1, . . . , p.

(16.3)

These are the optimality conditions for the constrained least squares problem. Any
solution of the constrained least squares problem must satisfy them. We will now
see that the optimality conditions can be expressed as a set of linear equations.

The second set of equations in the optimality conditions can be written as

∂L
∂zi

(ˆx, ˆz) = cT

i ˆx − di = 0,

i = 1, . . . , p,

16.2 Solution

345

which states that ˆx satisﬁes the equality constraints C ˆx = d (which we already
knew). The ﬁrst set of equations, however, is more informative. Expanding the
objective �Ax − b�2 as a sum of terms involving the entries of x (as was done on
page 229) and taking the partial derivative of L with respect to xi we obtain

∂L
∂xi

(ˆx, ˆz) = 2

n�j=1

(AT A)ij ˆxj − 2(AT b)i +

ˆzj(cj)i = 0.

p�j=1

These equations can be written in compact matrix-vector form as

2(AT A)ˆx − 2AT b + C T ˆz = 0.

Combining this set of linear equations with the feasibility conditions C ˆx = d, we
can write the optimality conditions (16.3) as one set of n + p linear equations in
the variables (ˆx, ˆz):

� 2AT A C T

0 �� ˆx

ˆz � =� 2AT b

C

d

� .

(16.4)

These equations are called the KKT equations for the constrained least squares
problem.
(KKT are the initials of the last names of William Karush, Harold
Kuhn, and Albert Tucker, the three researchers who derived the optimality con-
ditions for a more general form of constrained optimization problem.) The KKT
equations (16.4) are an extension of the normal equations (12.4) for a least squares
problem with no constraints. So we have reduced the constrained least squares
problem to the problem of solving a (square) set of n + p linear equations in n + p
variables (ˆx, ˆz).

Invertibility of KKT matrix. The (n + p) × (n + p) coeﬃcient matrix in (16.4) is
called the KKT matrix. It is invertible if and only if

C has linearly independent rows, and � A

C � has linearly independent columns.

(16.5)
The ﬁrst condition requires that C is wide (or square), i.e., that there are fewer
constraints than variables. The second condition depends on both A and C, and
it can be satisﬁed even when the columns of A are linearly dependent. The con-
dition (16.5) is the generalization of our assumption (12.2) for unconstrained least
squares (i.e., that A has linearly independent columns).

Before proceeding, let us verify that the KKT matrix is invertible if and only
if (16.5) holds. First suppose that the KKT matrix is not invertible. This means
that there is a nonzero vector (¯x, ¯z) with

� 2AT A C T

0 �� ¯x

¯z � = 0.

C

Multiply the top block equation 2AT A¯x + C T ¯z = 0 on the left by ¯xT to get

2�A¯x�2 + ¯xT C T ¯z = 0.

346

16 Constrained least squares

The second block equation, C ¯x = 0, implies (by taking the transpose) ¯xT C T = 0,
so the equation above becomes 2�A¯x�2 = 0, i.e., A¯x = 0. We also have C ¯x = 0, so

� A
C � ¯x = 0.

Since the matrix on the left has linearly independent columns (by assumption),
we conclude that ¯x = 0. The ﬁrst block equation above then becomes C T ¯z = 0.
But by our assumption that the columns of C T are linearly independent, we have
¯z = 0. So (¯x, ¯z) = 0, which is a contradiction.

The converse is also true. First suppose that the rows of C are linearly depen-

dent. Then there is a nonzero vector ¯z with C T ¯z = 0. Then

� 2AT A C T

0 �� 0

¯z � = 0,

C

which shows the KKT matrix is not invertible. Now suppose that the stacked
matrix in (16.5) has linearly dependent columns, which means there is a nonzero
vector ¯x for which

Direct calculation shows that

� A
C � ¯x = 0.
� 2AT A C T
0 �� ¯x

C

0 � = 0,

which shows that the KKT matrix is not invertible.

When the conditions (16.5) hold, the constrained least squares problem (16.1)

has the (unique) solution ˆx, given by

� ˆx
ˆz � =� 2AT A C T

0 �−1� 2AT b

C

d

� .

(16.6)

(This formula also gives us ˆz, the set of Lagrange multipliers.) From (16.6), we
observe that the solution ˆx is a linear function of (b, d).

Direct veriﬁcation of constrained least squares solution. We will now show
directly, without using calculus, that the solution ˆx given in (16.6) is the unique
vector that minimizes �Ax − b�2 over all x that satisfy the constraints Cx = d,
when the conditions (16.5) hold. Let ˆx and ˆz denote the vectors given in (16.6), so
they satisfy

2AT Aˆx + C T ˆz = 2AT b,

C ˆx = d.

Suppose that x �= ˆx is any vector that satisﬁes Cx = d. We will show that
�Ax − b�2 > �Aˆx − b�2.

We proceed in the same way as for the least squares problem:

�Ax − b�2 = �(Ax − Aˆx) + (Aˆx − b)�2

= �Ax − Aˆx�2 + �Aˆx − b�2 + 2(Ax − Aˆx)T (Aˆx − b).

16.3 Solving constrained least squares problems

347

Now we expand the last term:

2(Ax − Aˆx)T (Aˆx − b) = 2(x − ˆx)T AT (Aˆx − b)

= −(x − ˆx)T C T ˆz
= −(C(x − ˆx))T ˆz
= 0,

where we use 2AT (Aˆx − b) = −C T ˆz in the second line and Cx = C ˆx = d in the
last line. So we have, exactly as in the case of unconstrained least squares,

�Ax − b�2 = �A(x − ˆx)�2 + �Aˆx − b�2,

from which we conclude that �Ax − b�2 ≥ �Aˆx − b�2. So ˆx minimizes �Ax − b�2
subject to Cx = d.
It remains to show that for x �= ˆx, we have the strict inequality �Ax − b�2 >
�Aˆx − b�2, which by the equation above is equivalent to �A(x − ˆx)�2 > 0. If this
is not the case, then A(x − ˆx) = 0. We also have C(x − ˆx) = 0, and so

� A
C � (x − ˆx) = 0.

By our assumption that the matrix on the left has linearly independent columns,
we conclude that x = ˆx.

16.3 Solving constrained least squares problems

We can compute the solution (16.6) of the constrained least squares problem by
forming and solving the KKT equations (16.4).

Algorithm 16.1 Constrained least squares via KKT equations
given an m × n matrix A and a p × n matrix C that satisfy (16.5), an m-vector b,
and a p-vector d.

1. Form Gram matrix. Compute AT A.

2. Solve KKT equations. Solve KKT equations (16.4) by QR factorization and

back substitution.

The second step cannot fail, provided the assumption (16.5) holds. Let us
analyze the complexity of this algorithm. The ﬁrst step, forming the Gram matrix,
requires mn2 ﬂops (see page 182). The second step requires the solution of a square
system of n + p equations, which costs 2(n + p)3 ﬂops, so the total is

mn2 + 2(n + p)3

ﬂops. This grows linearly in m and cubicly in n and p. The assumption (16.5)
implies p ≤ n, so in terms of order, (n + p)3 can be replaced with n3.

348

16 Constrained least squares

Solving constrained least squares problems via QR factorization. We now give
a method for solving the constrained least squares problem that generalizes the QR
factorization method for least squares problems (algorithm 12.1). We assume that
A and C satisfy the conditions (16.5).

We start by rewriting the KKT equations (16.4) as

2(AT A + C T C)ˆx + C T w = 2AT b,

C ˆx = d

(16.7)

with a new variable w = ˆz − 2d. To obtain (16.7) we multiplied the equation
C ˆx = d on the left by 2C T , then added the result to the ﬁrst equation of (16.4),
and replaced the variable ˆz with w + 2d.

Next we use the QR factorization

� A
C � = QR =� Q1

Q2 � R

(16.8)

to simplify (16.7). This factorization exists because the stacked matrix has linearly
independent columns, by our assumption (16.5). In (16.8) we also partition Q in
two blocks Q1 and Q2, of size m × n and p × n, respectively.
If we make the
substitutions A = Q1R, C = Q2R, and AT A + C T C = RT R in (16.7) we obtain

2RT Rˆx + RT QT

2 w = 2RT QT

1 b,

Q2Rˆx = d.

We multiply the ﬁrst equation on the left by R−T (which we know exists) to get

Rˆx = QT

1 b − (1/2)QT

2 w.

Substituting this expression into Q2Rˆx = d gives an equation in w:

Q2QT

2 w = 2Q2QT

1 b − 2d.

(16.9)

(16.10)

2 = R−T C T has linearly independent columns. Suppose QT

We now use the second part of the assumption (16.5) to show that the matrix
2 z = R−T C T z = 0.
QT
Multiplying with RT gives C T z = 0. Since C has linearly independent rows, this
implies z = 0, and we conclude that the columns of QT
2 are linearly independent.
2 = ˜Q ˜R. Substituting this
2 therefore has a QR factorization QT

The matrix QT

into (16.10) gives

which we can write as

˜RT ˜Rw = 2 ˜RT ˜QT QT

1 b − 2d,

˜Rw = 2 ˜QT QT

1 b − 2 ˜R−T d.

We can use this to compute w, ﬁrst by computing ˜R−T d (by forward substitution),
then forming the right-hand side, and then solving for w using back substitution.
Once we know w, we can ﬁnd ˆx from (16.9). The method is summarized in the
following algorithm.

16.3 Solving constrained least squares problems

349

Algorithm 16.2 Constrained least squares via QR factorization
given an m × n matrix A and a p × n matrix C that satisfy (16.5), an m-vector b,
and a p-vector d.

1. QR factorizations. Compute the QR factorizations

� A
C � =� Q1

Q2 � R,

QT

2 = ˜Q ˜R.

2. Compute ˜R−T d by forward substitution.

3. Form right-hand side and solve

˜Rw = 2 ˜QT QT

1 b − 2 ˜R−T d

via back substitution.

4. Compute ˆx. Form right-hand side and solve

by back substitution.

Rˆx = QT

1 b − (1/2)QT
2 w

In the unconstrained case (when p = 0), step 1 reduces to computing the QR
factorization of A, steps 2 and 3 are not needed, and step 4 reduces to solving
Rˆx = QT
1 b. This is the same as algorithm 12.1 for solving (unconstrained) least
squares problems.

We now give a complexity analysis. Step 1 involves the QR factorizations of
an (m + p) × n and an n × p matrix, which costs 2(m + p)n2 + 2np2 ﬂops. Step 2
requires p2 ﬂops. In step 3, we ﬁrst evaluate QT
1 b (2mn ﬂops), multiply the result
by ˜QT (2pn ﬂops), and then solve for w using forward substitution (p2 ﬂops). Step 4
requires 2mn + 2pn ﬂops to form the right-hand side, and n2 ﬂops to compute ˆx via
back substitution. The costs of steps 2, 3, and 4 are quadratic in the dimensions,
and so are negligible compared to the cost of step 1, so our ﬁnal complexity is

2(m + p)n2 + 2np2

ﬂops. The assumption (16.5) implies the inequalities

p ≤ n ≤ m + p,

and therefore (m + p)n2 ≥ np2. So the ﬂop count above is no more than 4(m + p)n2
ﬂops. In particular, its order is (m + p)n2.

Sparse constrained least squares. Constrained least squares problems with sparse
matrices A and C arise in many applications; we will see several examples in the
next chapter. Just as for solving linear equations, or (unconstrained) least squares
problems, there are methods that exploit the sparsity in A and C to solve con-
strained least squares problems more eﬃciently than the generic algorithms 16.1

AT

0
A −(1/2)I
C

0



C T
0

0 

ˆx
ˆy

ˆz  =

0
b

d  .

(16.11)

350

16 Constrained least squares

or 16.2. The simplest such methods follow these basic algorithms, replacing the
QR factorizations with sparse QR factorizations (see page 190).

One potential problem with forming the KKT matrix as in algorithm 16.1 is
that the Gram matrix AT A can be far less sparse than the matrix A. This problem
can be avoided using a trick analogous to the one used on page 232 to solve sparse
(unconstrained) least squares problems. We form the square set of m + n + p linear
equations

If (ˆx, ˆy, ˆz) satisﬁes these equations, it is easy to see that (ˆx, ˆz) satisﬁes the KKT
equations (16.4); conversely, if (ˆx, ˆz) satisﬁes the KKT equations (16.4), (ˆx, ˆy, ˆz)
satisﬁes the equations above, with ˆy = 2(Aˆx − b). Provided A and C are sparse,
the coeﬃcient matrix above is sparse, and any method for solving a sparse system
of linear equations can be used to solve it.

Solution of least norm problem. Here we specialize the solution of the general
constrained least squares problem (16.1) given above to the special case of the least
norm problem (16.2).

We start with the conditions (16.5). The stacked matrix is in this case

� I
C � ,

which always has linearly independent columns. So the conditions (16.5) reduce
to: C has linearly independent rows. We make this assumption now.
For the least norm problem, the KKT equations (16.4) reduce to

� 2I C T

0 �� ˆx

ˆz � =� 0
d � .

C

We can solve this using the methods for general constrained least squares, or derive
the solution directly, which we do now. The ﬁrst block row of this equation is
2ˆx + C T ˆz = 0, so

We substitute this into the second block equation, C ˆx = d, to obtain

ˆx = −(1/2)C T ˆz.

−(1/2)CC T ˆz = d.

Since the rows of C are linearly independent, CC T is invertible, so we have

Substituting this expression for ˆz into the formula for ˆx above gives

ˆz = −2(CC T )−1d.

ˆx = C T (CC T )−1d.

(16.12)

16.3 Solving constrained least squares problems

351

We have seen the matrix in this formula before: It is the pseudo-inverse of a wide
matrix with linearly independent rows. So we can express the solution of the least
norm problem (16.2) in the very compact form

ˆx = C†d.

In §11.5, we saw that C† is a right inverse of C; here we see that not only does
ˆx = C†d satisfy Cx = d, but it gives the vector of least norm that satisﬁes Cx = d.
In §11.5, we also saw that the pseudo-inverse of C can be expressed as C† =
QR−T , where C T = QR is the QR factorization of C T . The solution of the least
norm problem can therefore be expressed as

ˆx = QR−T d

and this leads to an algorithm for solving the least norm problem via the QR
factorization.

Algorithm 16.3 Least norm via QR factorization
given a p × n matrix C with linearly independent rows and a p-vector d.

1. QR factorization. Compute the QR factorization C T = QR.
2. Compute ˆx. Solve RT y = d by forward substitution.

3. Compute ˆx = Qy.

The complexity of this algorithm is dominated by the cost of the QR factoriza-

tion in step 1, i.e., 2np2 ﬂops.

352

16 Constrained least squares

Exercises

16.1 Smallest right inverse. Suppose the m × n matrix A is wide, with linearly independent
rows. Its pseudo-inverse A† is a right inverse of A. In fact, there are many right inverses
of A and it turns out that A† is the smallest one among them, as measured by the matrix
norm. In other words, if X satisﬁes AX = I, then �X� ≥ �A†�. You will show this in
this problem.

(a) Suppose AX = I, and let x1, . . . , xm denote the columns of X. Let bj denote the
jth column of A†. Explain why �xj�2 ≥ �bj�2. Hint. Show that z = bj is the vector
of smallest norm that satisﬁes Az = ej, for j = 1, . . . , m.

(b) Use the inequalities from part (a) to establish �X� ≥ �A†�.

16.2 Matrix least norm problem. The matrix least norm problem is

minimize
subject to CX = D,

�X�2

where the variable to be chosen is the n × k matrix X; the p × n matrix C and the
p × k matrix D are given. Show that the solution of this problem is ˆX = C†D, assuming
the rows of C are linearly independent. Hint. Show that we can ﬁnd the columns of X
independently, by solving a least norm problem for each one.

16.3 Closest solution to a given point. Suppose the wide matrix A has linearly independent
rows. Find an expression for the point x that is closest to a given vector y (i.e., minimizes
�x − y�2) among all vectors that satisfy Ax = b.
Remark. This problem comes up when x is some set of inputs to be found, Ax = b
represents some set of requirements, and y is some nominal value of the inputs. For
example, when the inputs represent actions that are re-calculated each day (say, because b
changes every day), y might be yesterday’s action, and the today’s action x found as above
gives the least change from yesterday’s action, subject to meeting today’s requirements.

16.4 Nearest vector with a given average. Let a be an n-vector and β a scalar. How would you
ﬁnd the n-vector x that is closest to a among all n-vectors that have average value β?
Give a formula for x and describe it in English.

16.5 Checking constrained least squares solution. Generate a random 20 × 10 matrix A and a
random 5×10 matrix C. Then generate random vectors b and d of appropriate dimensions
for the constrained least squares problem

�Ax − b�2
minimize
subject to Cx = d.

Compute the solution ˆx by forming and solving the KKT equations. Verify that the
constraints very nearly hold, i.e., C ˆx − d is very small. Find the least norm solution xln
of Cx = d. The vector xln also satisﬁes Cx = d (very nearly). Verify that �Axln − b�2 >
�Aˆx − b�2.
(Continuation of exercise 8.9.) The
current daily diet is speciﬁed by the n-vector dcurr. Explain how to ﬁnd the closest diet
dmod to dcurr that satisﬁes the nutrient requirements given by the m-vector ndes, and has
the same cost as the current diet dcurr.

16.6 Modifying a diet to meet nutrient requirements.

16.7 Minimum cost trading to achieve target sector exposures. A current portfolio is given
by the n-vector hcurr, with the entries giving the dollar value invested in the n assets.
The total value (or net asset value) of the portfolio is 1T hcurr. We seek a new portfolio,
given by the n-vector h, with the same total value as hcurr. The diﬀerence h − hcurr is
called the trade vector ; it gives the amount of each asset (in dollars) that we buy or sell.
The n assets are divided into m industry sectors, such as pharmaceuticals or consumer

Exercises

353

electronics. We let the m-vector s denote the (dollar value) sector exposures to the m
sectors. (See exercise 8.13.) These are given by s = Sh, where S is the m × n sector
exposure matrix deﬁned by Sij = 1 if asset j is in sector i and Sij = 0 if asset j is not
in sector i. The new portfolio must have a given sector exposure sdes. (The given sector
exposures are based on forecasts of whether companies in the diﬀerent sectors will do well
or poorly in the future.)
Among all portfolios that have the same value as our current portfolio and achieve the
desired exposures, we wish to minimize the trading cost, given by

n�i=1

κi(hi − hcurr

i

)2,

a weighted sum of squares of the asset trades. The weights κi are positive. (These depend
on the daily trading volumes of assets, as well as other quantities. In general, it is cheaper
to trade assets that have high trading volumes.)
Explain how to ﬁnd h using constrained least squares. Give the KKT equations that you
would solve to ﬁnd h.

16.8 Minimum energy regulator. We consider a linear dynamical system with dynamics xt+1 =
Axt + But, where the n-vector xt is the state at time t and the m-vector ut is the input
at time t. We assume that x = 0 represents the desired operating point; the goal is to
ﬁnd an input sequence u1, . . . , uT−1 that results in xT = 0, given the initial state x1.
Choosing an input sequence that takes the state to the desired operating point at time T
is called regulation.
Find an explicit formula for the sequence of inputs that yields regulation, and minimizes
�u1�2 + ··· + �uT−1�2, in terms of A, B, T , and x1. This sequence of inputs is called the
minimum energy regulator.
Hint. Express xT in terms of x1, A, the controllability matrix

C =� AT−2B AT−3B ··· AB B � ,

and (u1, u2, . . . , uT−1) (which is the input sequence stacked). You may assume that C is
wide and has linearly independent rows.

16.9 Smoothest force sequence to move a mass. We consider the same setup as the example
given on page 343, where the 10-vector f represents a sequence of forces applied to a unit
mass over 10 1-second intervals. As in the example, we wish to ﬁnd a force sequence f
that achieves zero ﬁnal velocity and ﬁnal position one. In the example on page 343, we
choose the smallest f , as measured by its norm (squared). Here, though, we want the
smoothest force sequence, i.e., the one that minimizes

f 2
1 + (f2 − f1)2 + ··· + (f10 − f9)2 + f 2
10.

(This is the sum of the squares of the diﬀerences, assuming that f0 = 0 and f11 = 0.)
Explain how to ﬁnd this force sequence. Plot it, and give a brief comparison with the
force sequence found in the example on page 343.

16.10 Smallest force sequence to move a mass to a given position. We consider the same setup
as the example given on page 343, where the 10-vector f represents a sequence of forces
applied to a unit mass over 10 1-second intervals. In that example the goal is to ﬁnd
the smallest force sequence (measured by �f�2) that achieves zero ﬁnal velocity and
ﬁnal position one. Here we ask, what is the smallest force sequence that achieves ﬁnal
position one? (We impose no condition on the ﬁnal velocity.) Explain how to ﬁnd this
force sequence. Compare it to the force sequence found in the example, and give a brief
intuitive explanation of the diﬀerence. Remark. Problems in which the ﬁnal position of
an object is speciﬁed, but the ﬁnal velocity doesn’t matter, generally arise in applications
that are not socially positive, for example control of missiles.

354

16 Constrained least squares

16.11 Least distance problem. A variation on the least norm problem (16.2) is the least distance

problem,

�x − a�2
minimize
subject to Cx = d,

where the n-vector x is to be determined, the n-vector a is given, the p × n matrix C is
given, and the p-vector d is given. Show that the solution of this problem is

ˆx = a − C†(Ca − d),

assuming the rows of C are linearly independent. Hint. You can argue directly from the
KKT equations for the least distance problem, or solve for the variable y = x − a instead
of x.

16.12 Least norm polynomial interpolation. (Continuation of exercise 8.7.) Find the polynomial
of degree 4 that satisﬁes the interpolation conditions given in exercise 8.7, and minimizes
the sum of the squares of its coeﬃcients. Plot it, to verify that if satisﬁes the interpolation
conditions.

16.13 Steganography via least norm.

In steganography, a secret message is embedded in an
image in such a way that the image looks the same, but an accomplice can decode the
message. In this exercise we explore a simple approach to steganography that relies on
constrained least squares. The secret message is given by a k-vector s with entries that
are all either +1 or −1 (i.e., it is a Boolean vector). The original image is given by the
n-vector x, where n is usually much larger than k. We send (or publish or transmit) the
modiﬁed message x + z, where z is an n-vector of modiﬁcations. We would like z to be
small, so that the original image x and the modiﬁed one x+z look (almost) the same. Our
accomplice decodes the message s by multiplying the modiﬁed image by a k × n matrix
D, which yields the k-vector y = D(x + z). The message is then decoded as ˆs = sign(y).
(We write ˆs to show that it is an estimate, and might not be the same as the original.)
The matrix D must have linearly independent rows, but otherwise is arbitrary.

(a) Encoding via least norm. Let α be a positive constant. We choose z to minimize
�z�2 subject to D(x+z) = αs. (This guarantees that the decoded message is correct,
i.e., ˆs = s.) Give a formula for z in terms of D†, α, and x.

(b) Complexity. What is the complexity of encoding a secret message in an image?
(You can assume that D† is already computed and saved.) What is the complexity
of decoding the secret message? About how long would each of these take with a
computer capable of carrying out 1 Gﬂop/s, for k = 128 and n = 5122 = 262144 (a
512 × 512 image)?

(c) Try it out. Choose an image x, with entries between 0 (black) and 1 (white),
and a secret message s with k small compared to n, for example, k = 128 for a
512 × 512 image. (This corresponds to 16 bytes, which can encode 16 characters,
i.e., letters, numbers, or punctuation marks.) Choose the entries of D randomly, and
compute D†. The modiﬁed image x+z may have entries outside the range [0, 1]. We
replace any negative values in the modiﬁed image with zero, and any values greater
than one with one. Adjust α until the original and modiﬁed images look the same,
but the secret message is still decoded correctly. (If α is too small, the clipping of
the modiﬁed image values, or the round-oﬀ errors that occur in the computations,
can lead to decoding error, i.e., ˆs �= s. If α is too large, the modiﬁcation will be
visually apparent.) Once you’ve chosen α, send several diﬀerent secret messages
embedded in several diﬀerent original images.

16.14 Invertibility of matrix in sparse constrained least squares formulation. Show that the
(m + n + p) × (m + n + p) coeﬃcient matrix appearing in equation (16.11) is invertible if
and only if the KKT matrix is invertible, i.e., the conditions (16.5) hold.

Exercises

355

16.15 Approximating each column of a matrix as a linear combination of the others. Suppose A
is an m × n matrix with linearly independent columns a1, . . . , an. For each i we consider
the problem of ﬁnding the linear combination of a1, . . . , ai−1, ai+1, . . . , an that is closest
to ai. These are n standard least squares problems, which can be solved using the methods
of chapter 12. In this exercise we explore a simple formula that allows us to solve these n
least squares problem all at once. Let G = AT A denote the Gram matrix, and H = G−1
its inverse, with columns h1, . . . , hn.
(a) Explain why minimizing �Ax(i)�2 subject to x(i)

i = −1 solves the problem of ﬁnding
the linear combination of a1, . . . , ai−1, ai+1, . . . , an that is closest to ai. These are n
constrained least squares problems.

(b) Solve the KKT equations for these constrained least squares problems,

� 2AT A ei

eT
i

0 �� x(i)

zi � =�

0

−1 � ,

to conclude that x(i) = −(1/Hii)hi. In words: x(i) is the ith column of (AT A)−1,
scaled so its ith entry is −1.
(c) Each of the n original least squares problems has n − 1 variables, so the complexity
is n(2m(n − 1)2) ﬂops, which we can approximate as 2mn3 ﬂops. Compare this
to the complexity of a method based on the result of part (b): First ﬁnd the QR
factorization of A; then compute H.

(d) Let di denote the distance between ai and the linear combination of the other

columns that is closest to it. Show that di = 1/√Hii.

Remark. When the matrix A is a data matrix, with Aij the value of the jth feature on the
ith example, the problem addressed here is the problem of predicting each of the features
from the others. The numbers di tells us how well each feature can be predicted from the
others.

Chapter 17

Constrained least squares
applications

In this chapter we discuss several applications of equality constrained least squares.

17.1 Portfolio optimization

In portfolio optimization (also known as portfolio selection), we invest in diﬀerent
assets, typically stocks, over some investment periods. The goal is to make invest-
ments so that the combined return on all our investments is consistently high. (We
must accept the idea that for our average return to be high, we must tolerate some
variation in the return, i.e., some risk.) The idea of optimizing a portfolio of assets
was proposed in 1953 by Harry Markowitz, who won the Nobel prize in economics
for this work in 1990. In this section we will show that a version of this problem
can be formulated and solved as a linearly constrained least squares problem.

17.1.1 Portfolio risk and return

Portfolio allocation weights. We allocate a total amount of money to be invested
in n diﬀerent assets. The allocation across the n assets is described by an allocation
n-vector w, which satisﬁes 1T w = 1, i.e., its entries sum to one. If a total (dollar)
amount V is to be invested in some period, then V wj is the amount invested in
asset j. (This can be negative, meaning a short position of |V wj| dollars on asset j.)
The entries of w are called by various names including fractional allocations, asset
weights, asset allocations, or just weights.

For example, the asset allocation w = ej means that we invest everything in
asset j. (In this way, we can think of the individual assets as simple portfolios.)
The asset allocation w = (−0.2, 0.0, 1.2) means that we take a short position in
asset 1 of one ﬁfth of the total amount invested, and put the cash derived from the

358

17 Constrained least squares applications

short position plus our initial amount to be invested into asset 3. We do not invest
in asset 2 at all.

The leverage L of the portfolio is given by

L = |w1| + ··· + |wn|,

the sum of the absolute values of the weights. If all entries of w are nonnegative
(which is a called a long-only portfolio), we have L = 1; if some entries are negative,
then L > 1. If a portfolio has a leverage of 5, it means that for every $1 of portfolio
value, we have $3 of total long holdings, and $2 of total short holdings. (Other
deﬁnitions of leverage are used, for example, (L − 1)/2.)

Multi-period investing with allocation weights. The investments are held for T
periods of, say, one day each. (The periods could just as well be hours, weeks, or
months). We describe the investment returns by the T × n matrix R, where Rtj
is the fractional return of asset j in period t. Thus R61 = 0.02 means that asset 1
gained 2% in period 6, and R82 = −0.03 means that asset 2 lost 3%, over period 8.
The jth column of R is the return time series for asset j; the tth row of R gives
the returns of all assets in period t. It is often assumed that one of the assets is
cash, which has a constant (positive) return µrf , where the superscript stands for
risk-free. If the risk-free asset is asset n, then the last column of R is µrf 1.

Suppose we invest a total (positive) amount Vt at the beginning of period t,
so we invest Vtwj in asset j. At the end of period t, the dollar value of asset j is
Vtwj(1 + Rtj), and the dollar value of the whole portfolio is

Vt+1 =

n�j=1

Vtwj(1 + Rtj) = Vt(1 + ˜rT

t w),

where ˜rT
is the tth row of R. We assume Vt+1 is positive; if the total portfolio
t
value becomes negative we say that the portfolio has gone bust and stop trading.
The total (fractional) return of the portfolio over period t, i.e., its fractional

increase in value, is

Vt+1 − Vt

Vt

=

Vt(1 + ˜rT
t w) − Vt
Vt

= ˜rT

t w.

Note that we invest the total portfolio value in each period according to the
weights w. This entails buying and selling assets so that the dollar value frac-
tions are once again given by w. This is called re-balancing the portfolio.

The portfolio return in each of the T periods can be expressed compactly using

matrix-vector notation as

r = Rw,

where r is the T -vector of portfolio returns in the T periods, i.e., the time series
of portfolio returns. (Note that r is a T -vector, which represents the time series
of total portfolio return, whereas ˜rt is an n-vector, which gives the returns of the
n assets in period t.) If asset n is risk-free, and we choose the allocation w = en,
then r = Ren = µrf 1, i.e., we obtain a constant return in each period of µrf .

17.1 Portfolio optimization

359

We can express the total portfolio value in period t as

Vt = V1(1 + r1)(1 + r2)··· (1 + rt−1),

(17.1)

where V1 is the total amount initially invested in period t = 1. This total value time
series is often plotted using V1 = $10000 as the initial investment by convention.
The product in (17.1) arises from re-investing our total portfolio value (including
any past gains or losses) in each period. In the simple case when the last asset is
risk-free and we choose w = en, the total value grows as Vt = V1(1 + µrf )t−1. This
is called compounded interest at rate µrf .

When the returns rt are small (say, a few percent), and T is not too big (say, a
few hundred), we can approximate the product above using the sum or average of
the returns. To do this we expand the product in (17.1) into a sum of terms, each
of which involves a product of some of the returns. One term involves none of the
returns, and is V1. There are t − 1 terms that involve just one return, which have
the form V1rs, for s = 1, . . . , t− 1. All other terms in the expanded product involve
the product of at least two returns, and so can be neglected since we assume that
the returns are small. This leads to the approximation

Vt ≈ V1 + V1(r1 + ··· + rt−1),

which for t = T + 1 can be written as

VT +1 ≈ V1 + T avg(r)V1.

This approximation suggests that to maximize our total ﬁnal portfolio value, we
should seek high return, i.e., a large value for avg(r).

Portfolio return and risk. The choice of weight vector w is judged by the result-
ing portfolio return time series r = Rw. The portfolio mean return (over the T
periods), often shortened to just the return, is given by avg(r). The portfolio risk
(over the T periods) is the standard deviation of portfolio return, std(r).

The quantities avg(r) and std(r) give the per-period return and risk. They
are often converted to their equivalent values for one year, which are called the
annualized return and risk, and reported as percentages. If there are P periods in
one year, these are given by

P avg(r),

√P std(r),

respectively. For example, suppose each period is one (trading) day. There are
about 250 trading days in one year, so the annualized return and risk are given
by 250 avg(r) and 15.81 std(r). Thus a daily return sequence r with per-period
(daily) return 0.05% (0.0005) and risk 0.5% (0.005) has an annualized return and
risk of 12.5% and 7.9%, respectively. (The squareroot of P in the risk annualization
comes from the assumption that the ﬂuctuations in the returns vary randomly and
independently from period to period.)

360

17 Constrained least squares applications

17.1.2 Portfolio optimization

We want to choose w so that we achieve high return and low risk. This means
that we seek portfolio returns rt that are consistently high. This is an optimization
problem with two objectives, return and risk. Since there are two objectives, there
is a family of solutions, that trade oﬀ return and risk. For example, when the
last asset is risk-free, the portfolio weight w = en achieves zero risk (which is the
smallest possible value), and return µrf . We will see that other choices of w can
lead to higher return, but higher risk as well. Portfolio weights that minimize risk
for a given level of return (or maximize return for a given level of risk) are called
Pareto optimal. The risk and return of this family of weights are typically plotted
on a risk-return plot, with risk on the horizontal axis and return on the vertical
axis. Individual assets can be considered (very simple) portfolios, corresponding
to w = ej. In this case the corresponding portfolio return and risk are simply the
return and risk of asset j (over the same T periods).

One approach is to ﬁx the return of the portfolio to be some given value ρ, and
minimize the risk over all portfolios that achieve the required return. Doing this
for many values of ρ produces (diﬀerent) portfolio allocation vectors that trade oﬀ
risk and return. Requiring that the portfolio return be ρ can be expressed as

avg(r) = (1/T )1T (Rw) = µT w = ρ,

where µ = RT 1/T is the n-vector of the average asset returns. This is a single
linear equation in w. Assuming that it holds, we can express the square of the risk
as

std(r)2 = (1/T )�r − avg(r)1�2 = (1/T )�r − ρ1�2.

Thus to minimize risk (squared), with return value ρ, we must solve the linearly
constrained least squares problem

minimize

�Rw − ρ1�2

subject to � 1T

µT � w =� 1
ρ � .

(17.2)

(We dropped the factor 1/T from the objective, which does not aﬀect the solution.)
This is a constrained least squares problem with two linear equality constraints.
The ﬁrst constraint sets the sum of the allocation weights to one, and the second
requires that the mean portfolio return is ρ.

The portfolio optimization problem has the solution

w
z1
z2



 =

2RT R 1 µ
0

1T
µT

0
0

(17.3)

0 

−1

2ρT µ

1
ρ

 ,

where z1 and z2 are Lagrange multipliers for the equality constraints (which we
don’t care about).

As a historical note, the portfolio optimization problem (17.2) is not exactly
the same as the one proposed by Markowitz. His formulation used a statistical
model of returns, where instead we are using a set of actual (or realized ) returns.
(See exercise 17.2 for a formulation of the problem that is closer to the original
formulation by Markowitz.)

17.1 Portfolio optimization

361

Future returns and the big assumption. The portfolio optimization problem (17.2)
suﬀers from what would appear to be a serious conceptual ﬂaw: It requires us to
know the asset returns over the periods t = 1, . . . , T , in order to compute the op-
timal allocation to use over those periods. This is silly: If we knew any future
returns, we would be able to achieve as large a portfolio return as we like, by sim-
ply putting large positive weights on the assets with positive returns and negative
weights on those with negative returns. The whole challenge in investing is that
we do not know future returns.

Assume the current time is period T , so we know the (so-called realized ) return
matrix R. The portfolio weight w found by solving (17.2), based on the observed
returns in periods t = 1, . . . , T , can still be useful, when we make one (big) as-
sumption:

Future asset returns are similar to past returns.

(17.4)

In other words, if the asset returns for future periods T + 1, T + 2, . . . are similar
in nature to the past periods t = 1, . . . , T , then the portfolio allocation w found by
solving (17.2) could be a wise choice to use in future periods.

Every time you invest, you are warned that the assumption (17.4) need not
hold; you are required to acknowledge that past performance is no guarantee of
future performance. The assumption (17.4) often holds well enough to be useful,
but in times of ‘market shift’ it need not.

This situation is similar to that encountered when ﬁtting models to observed
data, as in chapters 13 and 14. The model is trained on past data that you have
observed; but it will be used to make predictions on future data that you have not
yet seen. A model is useful only to the extent that future data looks like past data.
And this is an assumption which often (but not always) holds reasonably well.

Just as in model ﬁtting, investment allocation vectors can (and should) be
validated before being used. For example, we determine the weight vector by
solving (17.2) using past returns data over some past training period, and check
the performance on some other past testing period. If the portfolio performance
over the training and testing periods are reasonably consistent, we gain conﬁdence
(but no guarantee) that the weight vector will work in future periods. For example,
we might determine the weights using the realized returns from two years ago, and
then test these weights by the performance of the portfolio over last year. If the test
works out, we use the weights for next year. In portfolio optimization, validation
is sometimes called back-testing, since you are testing the investment method on
previous realized returns, to get an idea of how the method will work on (unknown)
future returns.

The basic assumption (17.4) often holds less well than the analogous assumption
in data ﬁtting, i.e., that future data looks like past data. For this reason we expect
less coherence between the training and test performance of a portfolio, compared
to a generic data ﬁtting application. This is especially so when the test period has
a small number of periods in it, like 100; see the discussion on page 268.

362

17 Constrained least squares applications

n
r
u
t
e
R

0.4

0.3

0.2

0.1

0

1/n

Risk-free

0

0.1

0.2

0.3

0.4

0.5

0.6

0.7

Risk

Figure 17.1 The open circles show annualized risk and return for 20 assets
(19 stocks and one risk-free asset with a return of 1%). The solid line shows
risk and return for the Pareto optimal portfolios. The dots show risk and
return for three Pareto optimal portfolios with 10%, 20%, and 40% return,
and the portfolio with weights wi = 1/n.

17.1.3 Example

We use daily return data for 19 stocks over a period of 2000 days (8 years). After
adding a risk-free asset with a 1% annual return, we obtain a 2000 × 20 return
matrix R. The circles in ﬁgure 17.1 show the annualized risk and return for the 20
assets, i.e., the points

� √250 std(Rei)
250 avg(Rei) � ,

i = 1, . . . , 20.

It also shows the Pareto-optimal risk-return curve, and the risk and return for
the uniform portfolio with equal weights wi = 1/n. The annualized risk, return,
and leverage for ﬁve portfolios (the four Pareto-optimal portfolios indicated in the
ﬁgure, and the 1/n portfolio) are given in table 17.1.

Figure 17.2 shows the total portfolio value (17.1) for the ﬁve portfolios. Fig-
ure 17.3 shows the portfolio values for a diﬀerent test period of 500 days (two
years).

17.1 Portfolio optimization

363

Return

Risk

Portfolio Train Test Train Test Leverage

Risk-free
10%
20%
40%
1/n

0.01
0.10
0.20
0.40
0.10

0.01
0.08
0.15
0.30
0.21

0.00
0.09
0.18
0.38
0.23

0.00
0.07
0.15
0.31
0.13

1.00
1.96
3.03
5.48
1.00

Table 17.1 Annualized risk, return, and leverage for ﬁve portfolios.

150

100

50

10

)
s
r
a
l
l
o
d

d
n
a
s
u
o
h
t
(

e
u

l
a
V

0

400

800

Day

1200

1600

2000

40%

20%

10%

1/n

Risk-free

Figure 17.2 Total value over time for ﬁve portfolios: the risk-free portfolio
with 1% annual return, the Pareto optimal portfolios with 10%, 20%, and
40% return, and the uniform portfolio. The total value is computed using
the 2000 × 20 daily return matrix R.

364

17 Constrained least squares applications

18

16

14

12

10

)
s
r
a
l
l
o
d

d
n
a
s
u
o
h
t
(

e
u

l
a
V

40%

1/n

20%

10%

Risk-free

0

100

200

Day

300

400

500

Figure 17.3 Value over time for the ﬁve portfolios in ﬁgure 17.2 over a test
period of 500 days.

17.1.4 Variations

There are many variations on the basic portfolio optimization problem (17.2). We
describe a few of them here; a few others are explored in the exercises.

Regularization. Just as in data ﬁtting, our formulation of portfolio optimization
can suﬀer from over-ﬁt, which means that the chosen weights perform very well on
past (realized) returns, but poorly on new (future) returns. Over-ﬁt can be avoided
or reduced by adding regularization, which here means to penalize investments in
assets other than cash. (This is analogous to regularization in model ﬁtting, where
we penalize the size of the model coeﬃcients, except for the coeﬃcient associated
with the constant feature.) One natural way to incorporate regularization in the
portfolio optimization problem (17.2) is to add a positive multiple λ of the weighted
sum of squares term

σ2
1w2

n−1

1 + ··· + σ2

n−1w2

to the objective in (17.2). Note that we do not penalize wn, which is the weight
associated with the risk-free asset. The constants σi are the standard deviations of
the (realized) returns, i.e., σi = std(Rei). This regularization penalizes weights as-
sociated with risky assets more than those associated with less risky assets. A good
choice of λ can be found by back-testing.

17.1 Portfolio optimization

365

Time-varying weights. Markets do shift, so it is not uncommon to periodically
update or change the allocation weights that are used. In one extreme version of
this, a new allocation vector is used in every period. The allocation weight for any
period is obtained by solving the portfolio optimization problem over the preceding
M periods. (This scheme can be modiﬁed to include testing periods as well.) The
parameter M in this method would be chosen by validation on previous realized
returns, i.e., back-testing.

When the allocation weights are changed over time, we can add a (regulariza-
tion) term of the form κ�wcurr−w�2 to the objective, where κ is a positive constant.
Here wcurr is the currently used allocation, and w is the proposed new allocation
vector. The additional regularization term encourages the new allocation vector
to be near the current one. (When this is not the case, the portfolio will require
excessive buying and selling of assets. This is called turnover, which leads to trad-
ing costs not included in our simple model.) The parameter κ would be chosen by
back-testing, taking into account an approximation of trading cost.

17.1.5 Two-fund theorem

We can express the solution (17.3) of the portfolio optimization problem in the
form

w
z1
z2



 =

2RT R 1 µ
0

1T
µT

0
0

0 

−1

0
1

0  + ρ

2RT R 1 µ
0

1T
µT

0
0

0 

−1

2T µ

0

1  .

Taking the ﬁrst n components of this, we obtain

where w0 and v are the ﬁrst n components of the (n + 2)-vectors

w = w0 + ρv,

(17.5)



2RT R 1 µ
0

1T
µT

0
0

0 

−1

0
1

0  ,



2RT R 1 µ
0

1T
µT

0
0

0 

−1

2T µ

0

1  ,

respectively. The equation (17.5) shows that the Pareto optimal portfolios form a
line in weight space, parametrized by the required return ρ. The portfolio w0 is a
point on the line, and the vector v, which satisﬁes 1T v = 0, gives the direction of
the line. This equation tells us that we do not need to solve the equation (17.3) for
each value of ρ. We ﬁrst compute w0 and v (by factoring the matrix once and using
two solve steps), and then form the optimal portfolio with return ρ as w0 + ρv.

Any point on a line can be expressed as an aﬃne combination of two diﬀerent
points on the line. So if we ﬁnd two diﬀerent Pareto optimal portfolios, then we
can express a general Pareto optimal portfolio as an aﬃne combination of them.
In other words, all Pareto optimal portfolios are aﬃne combinations of just two
portfolios (indeed, any two diﬀerent Pareto optimal portfolios). This is the two-
fund theorem. (Fund is another term for portfolio.)

366

17 Constrained least squares applications

Now suppose that the last asset is risk-free. The portfolio w = en is Pareto
optimal, since it achieves return µrf with zero risk. We then ﬁnd one other Pareto
optimal portfolio, for example, the one w2 that achieves return 2µrf , twice the
risk-free return. (We could choose here any return other than µrf .) Then we can
express the general Pareto optimal portfolio as

w = (1 − θ)en + θw2,

where θ = ρ/µrf − 1.

17.2 Linear quadratic control

We consider a time-varying linear dynamical system with state n-vector xt and
input m-vector ut, with dynamics equations

xt+1 = Atxt + Btut,

t = 1, 2, . . . .

(17.6)

The system has an output, the p-vector yt, given by

yt = Ctxt,

t = 1, 2, . . . .

(17.7)

Usually, m ≤ n and p ≤ n, i.e., there are fewer inputs and outputs than states.
In control applications, the input ut represents quantities that we can choose
or manipulate, like control surface deﬂections or engine thrust on an airplane. The
state xt, input ut, and output yt typically represent deviations from some standard
or desired operating condition, for example, the deviation of aircraft speed and
altitude from the desired values. For this reason it is desirable to have xt, yt, and
ut small.

Linear quadratic control refers to the problem of choosing the input and state
sequences, over a time period t = 1, . . . , T , so as to minimize a sum of squares
objective, subject to the dynamics equations (17.6), the output equations (17.7),
and additional linear equality constraints. (In ‘linear quadratic’, ‘linear’ refers to
the linear dynamics, and ‘quadratic’ refers to the objective function, which is a
sum of squares.)

Most control problems include an initial state constraint, which has the form
x1 = xinit, where xinit is a given initial state. Some control problems also include a
ﬁnal state constraint xT = xdes, where xdes is a given (‘desired’) ﬁnal (also called
terminal or target) state.

The objective function has the form J = Joutput + ρJinput, where

Joutput = �y1�2 + ··· + �yT�2 = �C1x1�2 + ··· + �CT xT�2,
Jinput = �u1�2 + ··· + �uT−1�2.

The positive parameter ρ weights the input objective Jinput relative to the output
objective Joutput.

17.2 Linear quadratic control

367

The linear quadratic control problem (with initial and ﬁnal state constraints) is

minimize
subject to xt+1 = Atxt + Btut,

Joutput + ρJinput

x1 = xinit,

xT = xdes,

t = 1, . . . , T − 1,

(17.8)

where the variables to be chosen are x1, . . . , xT and u1, . . . , uT−1.

Formulation as constrained least squares problem. We can solve the linear
quadratic control problem (17.8) by setting it up as a big linearly constrained
least squares problem. We deﬁne the vector z of all these variables, stacked:

z = (x1, . . . , xT , u1, . . . , uT−1).

The dimension of z is T n + (T − 1)m. The control objective can be expressed as
� ˜Az − ˜b�2, where ˜b = 0 and ˜A is the block matrix

˜A =



C1

C2

. . .

CT √ρI

. . .

√ρI

.



In this matrix, (block) entries not shown are zero, and the identity matrices in
the lower right corner have dimension m. (The lines in the matrix delineate the
portions related to the states and the inputs.) The dynamics constraints, and the
initial and ﬁnal state constraints, can be expressed as ˜Cz = ˜d, with

˜C =



B1

B2

A1 −I

A2 −I
. . .

I

. . .
AT−1 −I
I

. . .

BT−1

,

˜d =



0
0
...
0

xinit
xdes



,



where (block) entries not shown are zero. (The vertical line separates the portions
of the matrix associated with the states and the inputs, and the horizontal lines
separate the dynamics equations and the initial and ﬁnal state constraints.)

The solution ˆz of the constrained least squares problem

minimize
subject to

� ˜Az − ˜b�2
˜Cz = ˜d

(17.9)

gives us the optimal input trajectory and the associated optimal state (and output)
trajectory. The solution ˆz is a linear function of ˜b and ˜d; since here ˜b = 0, it is a
linear function of xinit and xdes.

368

17 Constrained least squares applications

Complexity. The large constrained least squares problem (17.9) has dimensions

˜n = T n + (T − 1)m,

˜m = T p + (T − 1)m,

˜p = (T − 1)n + 2n,

so using one of the standard methods described in §16.2 would require order

(˜p + ˜m)˜n2 ≈ T 3(m + p + n)(m + n)2,

ﬂops, where the symbol ≈ means we have dropped terms with smaller exponents.
But the matrices ˜A and ˜C are very sparse, and by exploiting this sparsity (see
page 349), the large constrained least squares problem can be solved in order T (m+
p + n)(m + n)2 ﬂops, which grows only linearly in T .

17.2.1 Example

We consider the time-invariant linear dynamical system with

A =

0.855
0.015
−0.084

0.667
0.053

1.161
1.073
0.059

1.022  ,

B =
C =� 0.218 −3.597 −1.683 � ,

−0.076
−0.139

0.342  ,

with initial condition xinit = (0.496,−0.745, 1.394), target or desired ﬁnal state
xdes = 0, and T = 100. In this example, both the input ut and the output yt have
dimension one, i.e., are scalar. Figure 17.4 shows the output when the input is
zero,

yt = CAt−1xinit,

t = 1, . . . , T.

which is called the open-loop output. Figure 17.5 shows the optimal trade-oﬀ curve
of the objectives Jinput and Joutput, found by varying the parameter ρ, solving the
problem (17.9), and evaluating the objectives Jinput and Joutput. The points corre-
sponding to the values ρ = 0.05, ρ = 0.2, and ρ = 1 are shown as circles. As always,
increasing ρ has the eﬀect of decreasing Jinput, at the cost of increasing Joutput.

The optimal input and output trajectories for these three values of ρ are shown
in ﬁgure 17.6. Here too we see that for larger ρ, the input is smaller but the output
is larger.

17.2.2 Variations

There are many variations on the basic linear quadratic control problem described
above. We describe some of them here.

Tracking. We replace yt in Joutput with yt − ydes
is a given desired
output trajectory. In this case the objective function Joutput is called the tracking
error. Decreasing the parameter ρ leads to better output tracking, at the cost
of larger input trajectory. This variation on the linear quadratic control problem
can be expressed as a linearly constrained least squares problem with the same
big matrices ˜A and ˜C, the same vector ˜d, and a nonzero vector ˜b. The desired
trajectory ydes

appears in the vector ˜b.

, where ydes

t

t

t

17.2 Linear quadratic control

369

t
u
p
t
u
O

0.4

0.3

0.2

0.1

0

0

t
u
p
t
u
o
J

4.4

4.2

4

3.8

3.6

20

40

60

80

100

t

Figure 17.4 Open-loop response CAt−1xinit.

ρ = 1

ρ = 0.2

ρ = 0.05

0

1

2

Jinput

3

4

Figure 17.5 Optimal trade-oﬀ curve of the objectives Jinput and Joutput.

370

17 Constrained least squares applications

0.5

t
u

0

−0.5

0.5

t
u

0

−0.5

0.5

t
u

0

−0.5

0

20

40

0

20

40

0

20

40

t

t

t

0.4

t
y

0.2

0

60

80

100

0

20

40

0.4

t
y

0.2

0

60

80

100

0

20

40

0.4

t
y

0.2

0

60

80

100

0

20

40

60

80

100

60

80

100

60

80

100

t

t

t

Figure 17.6 Optimal inputs (left) and outputs (right) for ρ = 0.05 (top),
ρ = 0.2 (center), and ρ = 1 (bottom).

17.2 Linear quadratic control

371

Time-weighted objective. We replace Joutput with

Joutput = w1�y1�2 + ··· + wT�yT�2,

where w1, . . . , wT are given positive constants. This allows us to weight earlier or
later output values diﬀerently. A common choice, called exponential weighting, is
wt = θt, where θ > 0. For θ > 1 we weight later values of yt more than earlier
values; the opposite is true for θ < 1 (in which case θ is sometimes called the
discount or forgetting factor ).

Way-point constraints. A way-point constraint speciﬁes that yτ = ywp, where
ywp is a given p-vector, and τ is a given way-point time. This constraint is typically
used when yt represents a position of a vehicle; it requires that the vehicle pass
through the position ywp at time t = τ . Way-point constraints can be expressed as
linear equality constraints on the big vector z.

17.2.3 Linear state feedback control

In the linear quadratic control problem we work out a sequence of inputs u1, . . . , uT−1
to apply to the system, by solving the constrained least squares problem (17.8). It
is typically used in cases where t = T has some signiﬁcance, like the time of landing
or docking for a vehicle.

We have already mentioned (on page 185) another simpler approach to the
control of a linear dynamical system. In linear state feedback control we measure
the state in each period and use the input

ut = Kxt

for t = 1, 2, . . .. The matrix K is called the state feedback gain matrix. State
feedback control is very widely used in practical applications, especially ones where
there is no ﬁxed future time T when the state must take on some desired value;
instead, it is desired that both xt and ut should be small and converge to zero. One
practical advantage of linear state feedback control is that we can ﬁnd the state
feedback matrix K ahead of time; when the system is operating, we determine
the input values using one simple matrix-vector multiply. Here we show how an
appropriate state feedback gain matrix K can be found using linear quadratic
control.

Let ˆz denote the solution of the linear quadratic control problem, i.e., the
solution of the linearly constrained least squares problem (17.8), with xdes = 0.
The solution ˆz is a linear function of xinit and xdes; since here xdes = 0, ˆz is a linear
function of xinit = x1. Since ˆu1, the optimal input at t = 1, is a slice or subvector of
ˆz, we conclude that ˆu1 is a linear function of x1, and so can be written as u1 = Kx1
for some m × n matrix K. The columns of K can be found by solving (17.8) with
initial conditions xinit = e1, . . . , en. This can be done eﬃciently by factoring the
coeﬃcient matrix once, and then carrying out n solves.

This matrix generally provides a good choice of state feedback gain matrix.
With this choice, the input u1 with state feedback control and under linear quadratic

372

17 Constrained least squares applications

0.1

t
u

0

−0.1

0.4

State feedback

Optimal

t
y

0.2

Optimal

State feedback

0

50

t

100

150

0

0

50

t

100

150

Figure 17.7 The blue curves are the solutions of (17.8) for ρ = 1. The
red curves are the inputs and outputs that result from the constant state
feedback ut = Kxt.

control are the same; for t > 1, the two inputs diﬀer. An interesting phenomenon,
beyond the scope of this book, is that the state feedback gain matrix K found this
way does not depend very much on T , provided it is chosen large enough.

Example. For the example in §17.2.1 the state feedback gain matrix for ρ = 1 is

K =� 0.308 −2.659 −1.446 � .

In ﬁgure 17.7, we plot the trajectories with linear quadratic control (in blue) and
using the simpler linear state feedback control ut = Kxt. We can see that the input
sequence found using linear quadratic control achieves yT = 0 exactly; the input
sequence found by linear state feedback control makes yT small, but not zero.

17.3 Linear quadratic state estimation

The setting is a linear dynamical system of the form

xt+1 = Atxt + Btwt,

yt = Ctxt + vt,

t = 1, 2, . . . .

(17.10)

Here the n-vector xt is the state of the system, the p-vector yt is the measurement,
the m-vector wt is the input or process noise, and the p-vector vt is the measurement
noise or residual. The matrices At, Bt, and Ct are the dynamics, input, and output
matrices, respectively.

In state estimation, we know the matrices At, Bt, and Ct over the time period
t = 1, . . . , T , as well as the measurements y1, . . . , yT , but we do not know the
process or measurement noises. The goal is to guess or estimate the state sequence

17.3 Linear quadratic state estimation

373

x1, . . . , xT . State estimation is widely used in many application areas, including all
guidance and navigation systems, such as the Global Positioning System (GPS).

Since we do not know the process or measurement noises, we cannot exactly
deduce the state sequence. Instead we will guess or estimate the state sequence
x1, . . . , xT and process noise sequence w1, . . . , wT−1, subject to the requirement
that they satisfy the dynamic system model (17.10). When we guess the state
sequence, we implicitly guess that the measurement noise is vt = yt − Ctxt. We
make one fundamental assumption: The process and measurement noises are both
small, or at least, not too large.

Our primary objective is the sum of squares of the norms of the measurement

residuals,

Jmeas = �v1�2 + ··· + �vT�2 = �C1x1 − y1�2 + ··· + �CT xT − yT�2.

If this quantity is small, it means that the proposed state sequence guess is consis-
tent with our measurements. Note that the quantities in the squared norms above
are the same as −vt.

The secondary objective is the sum of squares of the norms of the process noise,

Jproc = �w1�2 + ··· + �wT−1�2.

Our prior assumption that the process noise is small corresponds to this objective
being small.

Least squares state estimation. We will make our guesses of x1, . . . , xT and
w1, . . . , wT−1 so as to minimize a weighted sum of our objectives, subject to the
dynamics constraints:

minimize
subject to xt+1 = Atxt + Btwt,

Jmeas + λJproc

t = 1, . . . , T − 1,

(17.11)

where λ is a positive parameter that allows us to put more emphasis on making
our measurement discrepancies small (by choosing λ small), or the process noises
small (by choosing λ large). Roughly speaking, small λ means that we trust the
measurements more, while large λ means that we trust the measurements less, and
put more weight on choosing a trajectory consistent with the dynamics, with small
process noise. We will see later how λ can be chosen using validation.

Estimation versus control. The least squares state estimation problem is very
similar to the linear quadratic control problem, but the interpretation is quite
diﬀerent. In the control problem, we can choose the inputs; they are under our
control. Once we choose the inputs, we know the state sequence. The inputs
are typically actions that we take to aﬀect the state trajectory. In the estimation
problem, the inputs (called process noise in the estimation problem) are unknown,
and the problem is to guess them. Our job is to guess the state sequence, which
we do not know. This is a passive task. We are not choosing inputs to aﬀect
the state; rather, we are observing the outputs and hoping to deduce the state
sequence. The mathematical formulations of the two problems, however, are very
closely related. The close connection between the two problems is sometimes called
control/estimation duality.

˜A =

C1


˜C =

C2

. . .

CT √λI

. . .

√λI

,

˜b =



y1
y2
...
yT
0
...
0



.



.



374

17 Constrained least squares applications

Formulation as constrained least squares problem. The least squares state esti-
mation problem (17.11) can be formulated as a linearly constrained least squares
problem, using stacking. We deﬁne the stacked vector

z = (x1, . . . , xT , w1, . . . , wT−1).

The objective in (17.11) can be expressed as � ˜Az − ˜b�2, with

The constraints in (17.11) can be expressed as ˜Cz = ˜d, with ˜d = 0 and

A1 −I

A2 −I
. . .

B1

B2

. . .
AT−1 −I

. . .

BT−1

The constrained least squares problem has dimensions

˜n = T n + (T − 1)m,

˜m = T p + (T − 1)m,

˜p = (T − 1)n

so using one of the standard methods described in §16.2 would require order

(˜p + ˜m)˜n2 ≈ T 3(m + p + n)(m + n)2

ﬂops. As in the case of linear quadratic control, the matrices ˜A and ˜C are very
sparse, and by exploiting this sparsity (see page 349), the large constrained least
squares problem can be solved in order T (m + p + n)(m + n)2 ﬂops, which grows
only linearly in T .

The least squares state estimation problem was formulated in around 1960 by
Rudolf Kalman and others (in a statistical framework). He and others developed a
particular recursive algorithm for solving the problem, and the whole method has
come to be known as Kalman ﬁltering. For this work Kalman was awarded the
Kyoto Prize in 1985.

17.3.1 Example

We consider a system with n = 4, p = 2, and m = 2, and time-invariant matrices

A =

1
0
0
0

0
1
0
0

1
0
1
0

0
1
0
1

 ,

B =

0
0
1
0

0
0
0
1

 ,

C =� 1

0

0
1

0
0

0

0 � .

17.3 Linear quadratic state estimation

375

This is a very simple model of motion of a mass moving in 2-D. The ﬁrst two
components of xt represent the position coordinates; components 3 and 4 represent
the velocity coordinates. The input wt acts like a force on the mass, since it adds to
the velocity. We think of the 2-vector Cxt as the exact or true position of the mass
at period t. The measurement yt = Cxt + vt is a noisy measurement of the mass
position. We will estimate the state trajectory over t = 1, . . . , T , with T = 100.

In ﬁgure 17.8 the 100 measured positions yt are shown as circles in 2-D. The
solid black lines show Cxt, i.e., the actual position of the mass. We solve the least
squares state estimation problem (17.11) for a range of values of λ. The estimated
trajectories C ˆxt for three values of λ are shown as blue lines. We can see that λ = 1
is too small for this example: The estimated state places too much trust in the
measurements, and is following measurement noise. We can also see that λ = 105
is too large: The estimated state is very smooth (since the estimated process noise
is small), but the imputed noise measurements are too high. In this example the
choice of λ is simple, since we have the true position trajectory. We will see later
how λ can be chosen using validation in the general case.

17.3.2 Variations

Known initial state. There are several interesting variations on the state estima-
tion problem. For example, we might know the initial state x1. In this case we
simply add an equality constraint x1 = xknown

.

1

Missing measurements. Another useful variation on the least squares state esti-
mation problem allows for missing measurements, i.e., we only know yt for t ∈ T ,
where T is the set of times for which we have a measurement. We can han-
t=1 �vt�2 with
(Both lead to the same state sequence estimate.) When there are missing mea-
surements, we can estimate what the missing measurements might have been, by
taking

dle this variation two (equivalent) ways: We can either replace �T
�t∈T �vt�2, or we can consider yt for t �∈ T to be optimization variables as well.

(Here we assume that vt = 0.)

ˆyt = Ct ˆxt,

t �∈ T .

17.3.3 Validation

The technique of estimating what a missing measurement might have been directly
gives us a method to validate a quadratic state estimation method, and in partic-
ular, to choose λ. To do this, we remove some of the measurements (say, 20%),
and carry out least squares state estimation pretending that those measurements
are missing. Our state estimate produces predicted values for the missing (really,
held back) measurements, which we can compare to the actual measurements. We
choose a value of λ which approximately minimizes this (test) prediction error.

376

17 Constrained least squares applications

λ = 1

0

t = 1

2
)
t
x
(

−500

−1000

−1500

0

100

200

300

(xt)1

λ = 103

λ = 105

Figure 17.8 The circles show 100 noisy measurements in 2-D. The solid
black line in each plot is the exact position Cxt. The blue lines in plots 2–4
are estimated trajectories C ˆxt for three values of λ.

17.3 Linear quadratic state estimation

377

r
o
r
r
e

S
M
R

80

60

40

20

0

Test

Train

10−3

10−1

101

λ

103

105

Figure 17.9 Training and test errors for the state estimation example.

Example. Continuing the previous example, we randomly remove 20 of the 100
measurement points. We solve the same problem (17.11) for a range of values of
λ, but with Jmeas deﬁned as

Jmeas =�t∈T

�Cxt − yt�2,

i.e., we only sum the measurement errors over the measurements we have. For each
value of λ we compute the RMS train and test errors

Etrain =

1

√80p��t∈T

�C ˆxt − yt�2�1/2

,

Etest =

1/2

.

1

√20p�t�∈T

�C ˆxt − yt�2

The training error (squared and scaled) appears directly in our minimization prob-
lem. The test error, however, is a good test of our estimation method, since it
compares predictions of positions (in this example) with measurements of position
that were not used to form the estimates. The errors are shown in ﬁgure 17.9, as
functions of the parameter λ. We can clearly see that for λ < 100 or so, we are
over-ﬁt, since the test RMS error substantially exceeds the train RMS error. We
can also see that λ around 103 is a good choice.

378

17 Constrained least squares applications

Exercises

17.1 A variation on the portfolio optimization formulation. Consider the following variation

on the linearly constrained least squares problem (17.2):

�Rw�2

minimize

subject to � 1T

µT � w =� 1
ρ � ,

(17.12)

with variable w. (The diﬀerence is that here we drop the term ρ1 that appears inside the
norm square objective in (17.2).) Show that this problem is equivalent to (17.2). This
means w is a solution of (17.12) if and only if it is a solution of (17.2).
Hint. You can argue directly by expanding the objective in (17.2) or via the KKT systems
of the two problems.

17.2 A more conventional formulation of the portfolio optimization problem. In this problem
we derive an equivalent formulation of the portfolio optimization problem (17.2) that
appears more frequently in the literature than our version. (Equivalent means that the
two problems always have the same solution.) This formulation is based on the return
covariance matrix, which we deﬁne below. (See also exercise 10.16.)
The means of the columns of the asset return matrix R are the entries of the vector µ.
The de-meaned returns matrix is given by ˜R = R − 1µT . (The columns of the matrix
˜R = R − 1µT are the de-meaned return time series for the assets.) The return covariance
matrix, traditionally denoted Σ, is its Gram matrix Σ = (1/T ) ˜RT ˜R.
(a) Show that σi = √Σii is the standard deviation (risk) of asset i return. (The symbol

σi is a traditional one for the standard deviation of asset i.)

(b) Show that the correlation coeﬃcient between asset i and asset j returns is given by
ρij = Σij/(σiσj). (Assuming neither asset has constant return; if either one does,
they are uncorrelated.)

(c) Portfolio optimization using the return covariance matrix. Show that the following

problem is equivalent to our portfolio optimization problem (17.2):

wT Σw

minimize

subject to � 1T

µT � w =� 1
ρ � ,

(17.13)

with variable w. This is the form of the portfolio optimization problem that you
will ﬁnd in the literature. Hint. Show that the objective is the same as � ˜Rw�2, and
that this is the same as �Rw − ρ1�2 for any feasible w.

17.3 A simple portfolio optimization problem.

(a) Find an analytical solution for the portfolio optimization problem with n = 2 assets.
You can assume that µ1 �= µ2, i.e., the two assets have diﬀerent mean returns. Hint.
The optimal weights depend only on µ and ρ, and not (directly) on the return
matrix R.

(b) Find the conditions under which the optimal portfolio takes long positions in both
assets, a short position in one and a long position in the other, or a short position
in both assets. You can assume that µ1 < µ2, i.e., asset 2 has the higher return.
Hint. Your answer should depend on whether ρ < µ1, µ1 < ρ < µ2, or µ2 < ρ, i.e.,
how the required return compares to the two asset returns.

17.4 Index tracking. Index tracking is a variation on the portfolio optimization problem de-
scribed in §17.1. As in that problem we choose a portfolio allocation weight vector w that
satisﬁes 1T w = 1. This weight vector gives a portfolio return time series Rw, which is

Exercises

379

a T -vector. In index tracking, the goal is for this return time series to track (or follow)
as closely as possible a given target return time series rtar. We choose w to minimize
the RMS deviation between the target return time series rtar and the portfolio return
time series r. (Typically the target return is the return of an index, like the Dow Jones
Industrial Average or the Russell 3000.) Formulate the index tracking problem as a lin-
early constrained least squares problem, analogous to (17.2). Give an explicit solution,
analogous to (17.3).

17.5 Portfolio optimization with market neutral constraint. In the portfolio optimization prob-
lem (17.2) the portfolio return time series is the T -vector Rw. Let rmkt denote the
T -vector that gives the return of the whole market over the time periods t = 1, . . . , T .
(This is the return associated with the total value of the market, i.e., the sum over the
assets of asset share price times number of outstanding shares.) A portfolio is said to be
market neutral if Rw and rmkt are uncorrelated.
Explain how to formulate the portfolio optimization problem, with the additional con-
straint of market neutrality, as a constrained least squares problem. Give an explicit
solution, analogous to (17.3).

17.6 State feedback control of the longitudinal motions of a Boeing 747 aircraft. In this exercise
we consider the control of the longitudinal motions of a Boeing 747 aircraft in steady level
ﬂight, at an altitude of 40000 ft, and speed 774 ft/s, which is 528 MPH or 460 knots,
around Mach 0.8 at that altitude. (Longitudinal means that we consider climb rate and
speed, but not turning or rolling motions.) For modest deviations from these steady
state or trim conditions, the dynamics is given by the linear dynamical system xt+1 =
Axt + But, with

A =

0.03 −0.02 −0.32
0.99
0.00
0.01
0.47
0.02 −0.06
0.00
0.01 −0.04
0.99

4.70
0.40
0.72

 ,

B =

0.01
−3.44
−0.83
−0.47

0.99
1.66
0.44
0.25

 ,

with time unit one second. The state 4-vector xt consists of deviations from the trim
conditions of the following quantities.

• (xt)1 is the velocity along the airplane body axis, in ft/s, with forward motion

positive.

• (xt)2 is the velocity perpendicular to the body axis, in ft/s, with positive down.
• (xt)3 is the angle of the body axis above horizontal, in units of 0.01 radian (0.57◦).
• (xt)4 is the derivative of the angle of the body axis, called the pitch rate, in units of

0.01 radian/s (0.57◦/s).

The input 2-vector ut (which we can control) consists of deviations from the trim condi-
tions of the following quantities.

• (ut)1 is the elevator (control surface) angle, in units of 0.01 radian.
• (ut)2 is the engine thrust, in units of 10000 lbs.

You do not need to know these details; we mention them only so you know what the
entries of xt and ut mean.

(a) Open loop trajectory. Simulate the motion of the Boeing 747 with initial condition
x1 = e4, in open-loop (i.e., with ut = 0). Plot the state variables over the time
interval t = 1, . . . , 120 (two minutes). The oscillation you will see in the open-loop
simulation is well known to pilots, and called the phugoid mode.

(b) Linear quadratic control. Solve the linear quadratic control problem with C = I,
ρ = 100, and T = 100, with initial state x1 = e4, and desired terminal state xdes = 0.
Plot the state and input variables over t = 1, . . . , 120. (For t = 100, . . . , 120, the
state and input variables are zero.)

380

17 Constrained least squares applications

(c) Find the 2×4 state feedback gain K obtained by solving the linear quadratic control
problem with C = I, ρ = 100, T = 100, as described in §17.2.3. Verify that it is
almost the same as the one obtained with T = 50.

(d) Simulate the motion of the Boeing 747 with initial condition x1 = e4, under state
feedback control (i.e., with ut = Kxt). Plot the state and input variables over the
time interval t = 1, . . . , 120.

17.7 Bio-mass estimation. A bio-reactor is used to grow three diﬀerent bacteria. We let xt
be the 3-vector of the bio-masses of the three bacteria, at time period (say, hour) t, for
t = 1, . . . , T . We believe that they each grow, independently, with growth rates given by
the 3-vector r (which has positive entries). This means that (xt+1)i ≈ (1 + ri)(xt)i, for
i = 1, 2, 3. (These equations are approximate; the real rate is not constant.) At every
time sample we measure the total bio-mass in the reactor, i.e., we have measurements
yt ≈ 1T xt, for t = 1, . . . , T . (The measurements are not exactly equal to the total mass;
there are small measurement errors.) We do not know the bio-masses x1, . . . , xT , but wish
to estimate them based on the measurements y1, . . . , yT .
Set this up as a linear quadratic state estimation problem as in §17.3.
Identify the
matrices At, Bt, and Ct. Explain what eﬀect the parameter λ has on the estimated
bio-mass trajectory ˆx1, . . . , ˆxT .

Chapter 18

Nonlinear least squares

In previous chapters we studied the problems of solving a set of linear equations
or ﬁnding a least squares approximate solution to them. In this chapter we study
extensions of these problems in which linear is replaced with nonlinear. These
nonlinear problems are in general hard to solve exactly, but we describe a heuristic
algorithm that often works well in practice.

18.1 Nonlinear equations and least squares

18.1.1 Nonlinear equations

Consider a set of m possibly nonlinear equations in n unknowns (or variables)
x = (x1, . . . , xn), written as

fi(x) = 0,

i = 1, . . . , m,

where fi : Rn → R is a scalar-valued function. We refer to fi(x) = 0 as the ith
equation. For any x we call fi(x) the ith residual, since it is a quantity we want to
be zero. Many interesting practical problems can be expressed as the problem of
solving, possibly approximately, a set of nonlinear equations.

We take the right-hand side of the equations to be zero to simplify the problem
notation. If we need to solve fi(x) = bi, i = 1, . . . , m, where bi are some given
nonzero numbers, we deﬁne ˜fi(x) = fi(x) − bi, and solve ˜fi(x) = 0, i = 1, . . . , m,
which gives us a solution of the original equations. Assuming the right-hand sides
of the equations are zero will simplify formulas and equations.

We often write the set of equations in the compact vector form

f (x) = 0,

(18.1)

where f (x) = (f1(x), . . . , fm(x)) is an m-vector, and the zero vector on the right-
hand side has dimension m. We can think of f as a function that maps n-vectors
to m-vectors, i.e., f : Rn → Rm. We refer to the m-vector f (x) as the residual

382

18 Nonlinear least squares

(vector) associated with the choice of the n-vector x; our goal is to ﬁnd x with
associated residual zero.

When f is an aﬃne function, the set of equations (18.1) is a set of m linear
equations in n unknowns, which can be solved (or approximately solved in a least
squares sense when m > n), using the techniques covered in previous chapters. We
are interested here in the case when f is not aﬃne.

We extend the ideas of under-determined, square, and over-determined equa-
tions to the nonlinear case. When m < n, there are fewer equations than unknowns,
and the system of equations (18.1) is called under-determined. When m = n, so
there are as many equations as unknowns, the system of equations is called square.
When m > n, there are more equations than unknowns, and the system of equa-
tions is called over-determined.

18.1.2 Nonlinear least squares

When we cannot ﬁnd a solution of the equations (18.1), we can seek an approximate
solution, by ﬁnding x that minimizes the sum of squares of the residuals,

f1(x)2 + ··· + fm(x)2 = �f (x)�2.

This means ﬁnding ˆx for which �f (x)�2 ≥ �f (ˆx)�2 holds for all x. We refer to
such a point as a least squares approximate solution of (18.1), or more directly, as
a solution of the nonlinear least squares problem

minimize

�f (x)�2,

(18.2)

where the n-vector x is the variable to be found. When the function f is aﬃne, the
nonlinear least squares problem (18.2) reduces to the (linear) least squares problem
from chapter 12.

The nonlinear least squares problem (18.2) includes the problem of solving
nonlinear equations (18.1) as a special case, since any x that satisﬁes f (x) = 0 is
also a solution of the nonlinear least squares problem. But as in the case of linear
equations, the least squares approximate solution of a set of nonlinear equations is
often very useful even when it does not solve the equations. So we will focus on
the nonlinear least squares problem (18.2).

18.1.3 Optimality condition

Calculus gives us a necessary condition for ˆx to be a solution of (18.2), i.e., to
minimize �f (x)�2. (This means that the condition must hold for a solution, but
it may also hold for other points that are not solutions.) The partial derivative of
�f (x)�2 with respect to each of x1, . . . , xn must vanish at ˆx:

∂
∂xi�f (ˆx)�2 = 0,

i = 1, . . . , n,

18.1 Nonlinear equations and least squares

383

or, in vector form, ∇�f (ˆx)�2 = 0 (see §C.2). This gradient can be expressed as

∇�f (x)�2 = ∇� m�i=1

fi(x)2� = 2

m�i=1

fi(x)∇fi(x) = 2Df (x)T f (x),

where the m× n matrix Df (x) is the derivative or Jacobian matrix of the function
f at the point x, i.e., the matrix of its partial derivatives (see §8.2.1 and C.1). So
if ˆx minimizes �f (x)�2, it must satisfy

2Df (ˆx)T f (ˆx) = 0.

(18.3)

This optimality condition must hold for any solution of the nonlinear least squares
problem (18.2). But the optimality condition can also hold for other points that
are not solutions of the nonlinear least squares problem. For this reason the opti-
mality condition (18.3) is called a necessary condition for optimality, because it is
necessarily satisﬁed for any solution ˆx. It is a not a suﬃcient condition for opti-
mality, since the optimality condition (18.3) is not enough (i.e., is not suﬃcient)
to guarantee that the point is a solution of the nonlinear least squares problem.

When the function f is aﬃne, the optimality conditions (18.3) reduce to the
normal equations (12.4), the optimality conditions for the (linear) least squares
problem.

18.1.4 Diﬃculty of solving nonlinear equations

Solving a set of nonlinear equations (18.1), or solving the nonlinear least squares
problem (18.2), is in general much more diﬃcult than solving a set of linear equa-
tions or a linear least squares problem. For nonlinear equations, there can be no
solution, or any number of solutions, or an inﬁnite number of solutions. Unlike
linear equations, it is a very diﬃcult computational problem to determine which
one of these cases holds for a particular set of equations; there is no analog of the
QR factorization that we can use for linear equations and least squares problems.
Even the simple sounding problem of determining whether or not there are any
solutions to a set of nonlinear equations is very diﬃcult computationally. There
are advanced non-heuristic algorithms for exactly solving nonlinear equations, or
exactly solving nonlinear least squares problems, but they are complicated and
very computationally demanding, and rarely used in applications.

Given the diﬃculty of solving a set of nonlinear equations, or solving a nonlinear
least squares problem, we must lower our expectations. We can only hope for an
algorithm that often ﬁnds a solution (when one exists), or produces a value of x
with small residual norm, if not the smallest that is possible. Algorithms like this,
that often work, or tend to produce a good if not always the best possible point, are
called heuristics. The k-means algorithm of chapter 4 is an example of a heuristic
algorithm. Solving linear equations or linear least squares problems using the QR
factorization are not heuristics; these algorithms always work.

Many heuristic algorithms for the nonlinear least squares problem, including
those we describe later in this chapter, compute a point ˆx that satisﬁes the op-
timality condition (18.3). Unless f (ˆx) = 0, however, such a point need not be a
solution of the nonlinear least squares problem (18.2).

384

18 Nonlinear least squares

18.1.5 Examples

In this section we list a few applications that reduce to solving a set of nonlinear
equations, or a nonlinear least squares problem.

Computing equilibrium points. The idea of an equilibrium, where some type
of consumption and generation balance each other, arises in many applications.
Consumption and generation depend, often nonlinearly, on the values of some pa-
rameters, and the goal is to ﬁnd values of the parameters that lead to equilibrium.
These examples typically have m = n, i.e., the system of nonlinear equations is
square.

• Equilibrium prices. We consider n commodities or goods, with associated
prices given by the n-vector p. The demand for the n goods (an n-vector) is
a nonlinear function of the prices, given by D(p). (In an example on page 150
we described an approximate model for demand that is accurate when the
prices change from nominal values by a few percent; here we consider the
demand over a large range of prices.) The supply of the goods (an n-vector)
also depends on the prices, and is given by S(p). (When the price for a good
is high, for example, more producers are willing to produce it, so the supply
increases.)

A set of commodity prices p is an equilibrium price vector if it results in
supply balancing demand, i.e., S(p) = D(p). Finding a set of equilibrium
prices is the same as solving the square set of nonlinear equations

f (p) = S(p) − D(p) = 0.

(The vector f (p) is called the excess supply, at the set of prices p.) This is
shown in ﬁgure 18.1 for a simple case with n = 1.

• Chemical equilibrium. We consider n chemical species in a solution. The
n-vector c denotes the concentrations of the n species. Reactions among
the species consume some of them (the reactants) and generate others (the
products). The rate of each reaction is a function of the concentrations of
its reactants (and other parameters we assume are ﬁxed, like temperature or
presence of catalysts). We let C(c) denote the vector of total consumption of
the n reactants, over all the reactions, and we let G(c) denote the vector of
generation of the n reactants, over all reactions.

A concentration vector c is in chemical equilibrium if C(c) = G(c), i.e., the
rate of consumption of all species balances the rate of generation. Computing
a set of equilibrium concentrations is the same as solving the square set of
nonlinear equations

f (c) = C(c) − G(c) = 0.

• Mechanical equilibrium. A mechanical system in 3-D with N nodes is char-
acterized by the positions of the nodes, given by a 3N -vector q of the stacked
node positions, called the generalized position. The net force on each node is

18.1 Nonlinear equations and least squares

385

S(p)

D(p)
p

Figure 18.1 Supply and demand as functions of the price, shown on the hori-
zontal axis. They intersect at the point shown as a circle. The corresponding
price is the equilibrium price.

a 3-vector, which depends on q, i.e., the node positions. We describe this as
a 3N -vector of forces, F (q).

The system is in mechanical equilibrium if the net force on each node is zero,
i.e., F (q) = 0, a set of 3N nonlinear equations in 3N unknowns. (A more
complex mechanical equilibrium model takes into account angular displace-
ments and torques at each node.)

• Nash equilibrium. We consider a simple setup for a mathematical game. Each
of n competing agents or participants chooses a number xi. Each agent is
given a (numerical) reward (say, money) that depends not only on her own
choice, but also on the choice of all the other agents. The reward for agent i
is given by the function Ri(x), called the payoﬀ function. Each agent wishes
to make a choice that maximizes her reward. This is complicated since the
reward depends not only on her choice, but the choices of the other agents.

A Nash equilibrium (named after the mathematician John Forbes Nash, Jr.)
is a set of choices given by the n-vector x where no agent can improve (in-
crease) her reward by changing her choice. Such a choice is argued to be
‘stable’ since no agent is incented to change her choice. At a Nash equilib-
rium xi maximizes Ri(x), so we must have

∂Ri
∂xi

(x) = 0,

i = 1, . . . , n.

This necessary condition for a Nash equilibrium is a square set of nonlinear
equations.

The idea of a Nash equilibrium is widely used in economics, social science,
and engineering. Nash was awarded the Nobel Prize in economics for this
work in 1994.

386

18 Nonlinear least squares

Nonlinear least squares examples. Nonlinear least squares problems arise in
many of the same settings and applications as linear least squares problems.

• Location from range measurements. The 3-vector (or 2-vector) x represents
the location of some object or target in 3-D (or 2-D), which we wish to
determine or guess. We are given m range measurements, i.e., the distance
from x to some known locations a1, . . . , am,

ρi = �x − ai� + vi,

i = 1, . . . , m,

where vi is an unknown measurement error, assumed to be small. Our esti-
mate ˆx of the location is found by minimizing the sum of the squares of the
range residuals,

m�i=1

(�x − ai� − ρi)2 .

A similar method is used in GPS devices, where ai are the known locations
of GPS satellites that are in view.

• Nonlinear model ﬁtting. We consider a model y ≈ ˆf (x; θ) where x denotes
a feature vector, y a scalar outcome, ˆf a model of some function relating x
and y, and θ is a vector of model parameters that we seek. In chapter 13, ˆf
is an aﬃne function of the model parameter p-vector θ; but here it need not
be. As in chapter 13, we choose the model parameter by minimizing the sum
of the squares of the residuals over a data set with N examples,

N�i=1

( ˆf (x(i); θ) − y(i))2.

(18.4)

(As in linear least squares model ﬁtting, we can add a regularization term
to this objective function.) This is a nonlinear least squares problem, with
variable θ.

18.2 Gauss–Newton algorithm

In this section we describe a powerful heuristic algorithm for the nonlinear least
squares problem (18.2) that bears the names of the two famous mathematicians
Carl Friedrich Gauss and Isaac Newton. In the next section we will also describe a
variation of the Gauss–Newton algorithm known as the Levenberg–Marquardt algo-
rithm, which addresses some shortcomings of the basic Gauss–Newton algorithm.

The Gauss–Newton and Levenberg–Marquardt algorithms are iterative algo-
rithms that generate a sequence of points x(1), x(2), . . .. The vector x(1) is called
the starting point of the algorithm, and x(k) is called the kth iterate. Moving from
x(k) to x(k+1) is called an iteration of the algorithm. We judge the iterates by
the norm of the associated residuals, �f (x(k))�, or its square. The algorithm is
terminated when �f (x(k))� is small enough, or x(k+1) is very near x(k), or when a
maximum number of iterations is reached.

18.2 Gauss–Newton algorithm

387

18.2.1 Basic Gauss–Newton algorithm

The idea behind the Gauss–Newton algorithm is simple: We alternate between
ﬁnding an aﬃne approximation of the function f at the current iterate, and then
solving the associated linear least squares problem to ﬁnd the next iterate. This
combines two of the most powerful ideas in applied mathematics: Calculus is used
to form an aﬃne approximation of a function near a given point, and least squares
is used to compute an approximate solution of the resulting aﬃne equations.

We now describe the algorithm in more detail. At each iteration k, we form
the aﬃne approximation ˆf of f at the current iterate x(k), given by the Taylor
approximation

ˆf (x; x(k)) = f (x(k)) + Df (x(k))(x − x(k)),

(18.5)
where the m×n matrix Df (x(k)) is the Jacobian or derivative matrix of f (see §8.2.1
and §C.1). The aﬃne function ˆf (x; x(k)) is a very good approximation of f (x)
provided x is near x(k), i.e., �x − x(k)� is small.
The next iterate x(k+1) is then taken to be the minimizer of � ˆf (x; x(k))�2, the
norm squared of the aﬃne approximation of f at x(k). Assuming that the derivative
matrix Df (x(k)) has linearly independent columns (which requires m ≥ n), we have
(18.6)

Df (x(k))T f (x(k)).

x(k+1) = x(k) −�Df (x(k))T Df (x(k))�−1

This iteration gives the basic Gauss–Newton algorithm.

Algorithm 18.1 Basic Gauss–Newton algorithm for nonlinear least squares
given a diﬀerentiable function f : Rn → Rm, an initial point x(1).
For k = 1, 2, . . . , kmax

1. Form aﬃne approximation at current iterate using calculus. Evaluate the Ja-

cobian Df (x(k)) and deﬁne

ˆf (x; x(k)) = f (x(k)) + Df (x(k))(x − x(k)).

2. Update iterate using linear least squares.

Set x(k+1) as the minimizer of

� ˆf (x; x(k))�2,

x(k+1) = x(k) −�Df (x(k))T Df (x(k))�−1

Df (x(k))T f (x(k)).

The Gauss–Newton algorithm is terminated early if f (x) is very small, or x(k+1) ≈
x(k). It terminates with an error if the columns of Df (x(k)) are linearly dependent.
The condition x(k+1) = x(k) (the exact form of our stopping condition) holds

when

�Df (x(k))T Df (x(k))�−1

Df (x(k))T f (x(k)) = 0,

which occurs if and only if Df (x(k))T f (x(k)) = 0 (since we assume that Df (x(k))
has linearly independent columns). So the Gauss–Newton algorithm stops only
when the optimality condition (18.3) holds.

388

18 Nonlinear least squares

We can also observe that

� ˆf (x(k+1); x(k))�2 ≤ � ˆf (x(k); x(k))�2 = �f (x(k))�2

(18.7)
holds, since x(k+1) minimizes � ˆf (x; x(k))�2, and ˆf (x(k); x(k)) = f (x(k)). The norm
of the residual of the approximation goes down in each iteration. This is not the
same as

�f (x(k+1))�2 ≤ �f (x(k))�2,

(18.8)

i.e., the norm of the residual goes down in each iteration, which is what we would
like.

Shortcomings of the basic Gauss–Newton algorithm. We will see in examples
that the Gauss–Newton algorithm can work well, in the sense that the iterates
x(k) converge very quickly to a point with small residual. But the Gauss–Newton
algorithm has two related serious shortcomings.

The ﬁrst is that it can fail, by producing a sequence of points with the norm of
the residual �f (x(k))� increasing to large values, as opposed to decreasing to a small
value, which is what we want. (In this case the algorithm is said to diverge.) The
mechanism behind this failure is related to the diﬀerence between (18.7) and (18.8).
The approximation

�f (x)�2 ≈ � ˆf (x; x(k))�2

is guaranteed to hold only when x is near x(k). So when x(k+1) is not near x(k),
�f (x(k+1))�2 and � ˆf (x(k+1); x(k))�2 can be very diﬀerent. In particular, the (true)
residual at x(k+1) can be larger than the residual at x(k).
The second serious shortcoming of the basic Gauss–Newton algorithm is the
assumption that the columns of the derivative matrix Df (x(k)) are linearly inde-
pendent. In some applications, this assumption never holds; in others, it can fail to
hold at some iterate x(k), in which case the Gauss–Newton algorithm stops, since
x(k+1) is not deﬁned.

We will see that a simple modiﬁcation of the Gauss–Newton algorithm, de-

scribed below in §18.3, addresses both of these shortcomings.

18.2.2 Newton algorithm

For the special case m = n, the Gauss–Newton algorithm reduces to another famous
algorithm for solving a set of n nonlinear equations in n variables, called the Newton
algorithm.
(The algorithm is sometimes called the Newton-Raphson algorithm,
since Newton developed the method only for the special case n = 1, and Joseph
Raphson later extended it to the case n > 1.)

When m = n, the matrix Df (x(k)) is square, so the basic Gauss–Newton up-

date (18.6) can be simpliﬁed to

x(k+1) = x(k) − (Df (x(k)))−1(Df (x(k))T )−1Df (x(k))T f (x(k))

= x(k) − (Df (x(k)))−1f (x(k)).

This iteration gives the Newton algorithm.

18.2 Gauss–Newton algorithm

389

ˆf (x; x(k))

f (x)

x(k+1)

x(k)

Figure 18.2 One iteration of the Newton algorithm for solving an equation
f (x) = 0 in one variable.

Algorithm 18.2 Newton algorithm for solving nonlinear equations
given a diﬀerentiable function f : Rn → Rn, an initial point x(1).
For k = 1, 2, . . . , kmax

1. Form aﬃne approximation at current iterate. Evaluate the Jacobian Df (x(k))

and deﬁne

ˆf (x; x(k)) = f (x(k)) + Df (x(k))(x − x(k)).

2. Update iterate by solving linear equations.

Set x(k+1) as the solution of

ˆf (x; x(k)) = 0,

x(k+1) = x(k) −�Df (x(k))�−1

f (x(k)).

The basic Newton algorithm shares the same shortcomings as the basic Gauss–
Newton algorithm, i.e., it can diverge, and the iterations terminate if the derivative
matrix is not invertible.

Newton algorithm for n = 1. The Newton algorithm is easily understood for
n = 1. The iteration is

x(k+1) = x(k) − f (x(k))/f�(x(k))

(18.9)

and is illustrated in ﬁgure 18.2. To update x(k) we form the Taylor approximation

ˆf (x; x(k)) = f (x(k)) + f�(x(k))(x − x(k))

and set it to zero to ﬁnd the next iterate x(k+1). If f�(x(k)) �= 0, the solution of
ˆf (x; x(k)) = 0 is given by the right-hand side of (18.9). If f�(x(k)) = 0, the Newton
algorithm terminates with an error.

390

18 Nonlinear least squares

1

1

−3 −2 −1

1

2

3

−3 −2 −1

1

2

3

−1

−1

Figure 18.3 The ﬁrst iterations in the Newton algorithm for solving f (x) = 0,
for two starting points: x(1) = 0.95 and x(1) = 1.15.

1

0

)
)
k
(
x
(
f

−1

1

0

)
)
k
(
x
(
f

−1

1

2

3

4

5

6

1

2

3

4

5

6

k

k

Figure 18.4 Value of f (x(k)) versus iteration number k for Newton’s method
in the example of ﬁgure 18.3, started at x(1) = 0.95 and x(1) = 1.15.

Example. The function

f (x) =

ex − e−x
ex + e−x

(18.10)

has a unique zero at the origin, i.e., the only solution of f (x) = 0 is x = 0. (This
function is called the sigmoid function, and will make another appearance later.)
The Newton iteration started at x(1) = 0.95 converges quickly to the solution x = 0.
With x(1) = 1.15, however, the iterates diverge. This is shown in ﬁgures 18.3
and 18.4.

18.3 Levenberg–Marquardt algorithm

391

18.3 Levenberg–Marquardt algorithm

In this section we describe a variation on the basic Gauss–Newton algorithm (as
well as the Newton algorithm) that addresses the shortcomings described above.
The variation comes directly from ideas we have encountered earlier in this book. It
was ﬁrst proposed by Kenneth Levenberg and Donald Marquardt, and is called the
Levenberg–Marquardt algorithm. It is also sometimes called the Gauss–Newton
algorithm, since it is a natural extension of the basic Gauss–Newton algorithm
described above.

Multi-objective update formulation. The main problem with the Gauss–Newton
algorithm is that the minimizer of the approximation � ˆf (x; x(k))�2 may be far from
the current iterate x(k), in which case the approximation ˆf (x; x(k)) ≈ f (x) need
not hold, which implies that � ˆf (x; x(k))�2 ≈ �f (x)�2 need not hold. In choosing
x(k+1), then, we have two objectives: We would like � ˆf (x; x(k))�2 small, and we
would also like �x − x(k)�2 small. The ﬁrst objective is an approximation of what
we really want to minimize; the second objective expresses the idea that we should
not move so far that we cannot trust the aﬃne approximation. This suggests that
we should choose x(k+1) as the minimizer of

� ˆf (x; x(k))�2 + λ(k)�x − x(k)�2,

(18.11)

where λ(k) is a positive parameter. We add an iteration superscript to the pa-
rameter λ since it can take diﬀerent values in diﬀerent iterations. For λ(k) small,
we primarily minimize the ﬁrst term, the squared norm of the approximation; for
λ(k) large, we choose x(k+1) near x(k). (For λ(k) = 0, this coincides with the next
iterate in the basic Gauss–Newton algorithm.) The second term in (18.11) is some-
times called a trust penalty term, since it penalizes choices of x that are far from
x(k), where we cannot trust the aﬃne approximation. The parameter λ(k) is some-
times called the trust parameter (although ‘distrust parameter’ is perhaps more
accurate).

Computing the minimizer of (18.11) is a multi-objective least squares or regu-

larized least squares problem, and equivalent to minimizing

����� Df (x(k))
√λ(k)I � x −� Df (x(k))x(k) − f (x(k))

√λ(k)x(k)

Since λ(k) is positive, the stacked matrix in this least squares problem has linearly
independent columns, even when Df (x(k)) does not. It follows that the solution of
the least squares problem exists and is unique. From the normal equations of the
least squares problem we can derive a useful expression for x(k+1):

2

.

�����

�Df (x(k))T Df (x(k)) + λ(k)I� x(k+1)
= Df (x(k))T�Df (x(k))x(k) − f (x(k))� + λ(k)x(k)
= �Df (x(k))T Df (x(k)) + λ(k)I� x(k) − Df (x(k))T f (x(k)),

392

18 Nonlinear least squares

and therefore

x(k+1) = x(k) −�Df (x(k))T Df (x(k)) + λ(k)I�−1

The matrix inverse here always exists.

Df (x(k))T f (x(k)).

(18.12)

From (18.12), we see that x(k+1) = x(k) only if 2Df (x(k))T f (x(k)) = 0, i.e.,
only when the optimality condition (18.3) holds for x(k). So like the Gauss–Newton
algorithm, the Levenberg–Marquardt algorithm stops (or more accurately, repeats
itself with x(k+1) = x(k)) only when the optimality condition (18.3) holds.

Updating the trust parameter. The ﬁnal issue is how to choose the trust pa-
rameter λ(k). When λ(k) is too small, x(k+1) can be far enough away from x(k)
that �f (x(k+1))�2 > �f (x(k)�2 can hold, i.e., our true objective function increases,
which is not what we want. When λ(k) is too large, x(k+1) − x(k) is small, so the
aﬃne approximation is good, and the objective decreases (which is good). But in
this case we have x(k+1) very near x(k), so the decrease in objective is small, and
it will take many iterations to make progress. We want λ(k) in between these two
cases, big enough that the approximation holds well enough to get a decrease in
objective, but not much bigger, which slows convergence.

Several algorithms can be used to adjust λ. One simple method forms x(k+1)
using the current value of λ and checks if the objective has decreased. If it has, we
accept the new point and decrease λ a bit for the next iteration. If the objective
has not decreased, which means λ is too small, we do not update the point x(k+1),
and increase the trust parameter λ substantially.

Levenberg–Marquardt algorithm. The ideas above can be formalized as the al-
gorithm given below.

Algorithm 18.3 Levenberg–Marquardt algorithm for nonlinear least
squares
given a diﬀerentiable function f : Rn → Rm, an initial point x(1), an initial trust
parameter λ(1) > 0.
For k = 1, 2, . . . , kmax

1. Form aﬃne approximation at current iterate. Evaluate the Jacobian Df (x(k))

and deﬁne

ˆf (x; x(k)) = f (x(k)) + Df (x(k))(x − x(k)).

2. Compute tentative iterate. Set x(k+1) as minimizer of
� ˆf (x; x(k))�2 + λ(k)�x − x(k)�2.

3. Check tentative iterate.

If �f (x(k+1))�2 < �f (x(k))�2, accept iterate and reduce λ: λ(k+1) = 0.8λ(k).
Otherwise, increase λ and do not update x: λ(k+1) = 2λ(k) and x(k+1) = x(k).

18.3 Levenberg–Marquardt algorithm

393

Stopping criteria. The algorithm is stopped before the maximum number of it-
erations kmax if either of the following two conditions hold.

• Small residual : �f (x(k+1))�2 is small enough. This means we have (almost)

solved the equations f (x) = 0, and therefore (almost) minimized �f (x)�2.

• Small optimality condition residual : �2Df (ˆx)T f (ˆx)� is small enough, i.e.,

the optimality condition (18.3) almost holds.

When the algorithm terminates with small optimality condition residual, we
can say very little for sure about the point x(k+1) computed. This point found may
be a minimizer of �f (x)�2, or perhaps not. Since the algorithm does not always
ﬁnd a minimizer of �f (x)�2, it is a heuristic. Like the k-means algorithm, which
is also a heuristic, the Levenberg–Marquardt algorithm is widely used in many
applications, even when we cannot be sure that it has found a point that gives the
smallest possible residual norm.

Warm start.
In many applications a sequence of similar or related nonlinear least
squares problems are solved. In these cases it is common to start the Levenberg–
Marquardt algorithm at the solution of the previously solved problem. If the prob-
lem to be solved is not much diﬀerent from the previous problem, this can greatly
reduce the number of iterations required to converge. This technique is called warm
starting. It is commonly used in nonlinear model ﬁtting, when multiple models are
ﬁt as we vary a regularization parameter.

Multiple runs.
It is common to run the Levenberg–Marquardt algorithm from
several diﬀerent starting points x(1). If the ﬁnal points found by running the algo-
rithm from these diﬀerent starting points are the same, or very close, it increases
our conﬁdence that we have found a solution of the nonlinear least squares prob-
lem, but we cannot be sure. If the diﬀerent runs of the algorithm produce diﬀerent
points, we use the best one found, i.e., the one with the smallest value of �f (x)�2.
Complexity. Each execution of step 1 requires evaluating the derivative matrix
of f . The complexity of this step depends on the particular function f . Each
execution of step 2 requires the solution of a regularized least squares problem.
Using the QR factorization of the stacked matrix this requires 2(m + n)n2 ﬂops
(see §15.5). When m is on the order of n, or larger, this is the same order as mn2.
When m is much smaller than n, x(k+1) can be computed using the kernel trick
described in §15.5, which requires 2nm2 ﬂops.
Levenberg–Marquardt update for n = 1. The Newton update for solving f (x) = 0
when n = 1 is given in (18.9). The Levenberg–Marquardt update for minimizing
f (x)2 is

x(k+1) = x(k) −

f�(x(k))

λ(k) + (f�(x(k)))2 f (x(k)).

(18.13)

For λ(k) = 0 they agree; but when f�(x(k)) = 0, for example, the Levenberg–
Marquardt update makes sense (since λ(k) > 0), whereas the Newton update is
undeﬁned.

394

18 Nonlinear least squares

)
)
k
(
x
(
f

0.8

0.6

0.4

0.2

0

1

)
k
(
λ

0.5

2

4

6

k

8

10

0

2

4

8

10

6

k

Figure 18.5 Values of f (x(k)) and λ(k) versus the iteration number k
for the Levenberg–Marquardt algorithm applied to f (x) = (exp(x) −
exp(−x))/(exp(x)+exp(−x)). The starting point is x(1) = 1.15 and λ(1) = 1.

18.3.1 Examples

Nonlinear equation. The ﬁrst example is the sigmoid function (18.10) from the
example on page 390. We saw in ﬁgures 18.3 and 18.4 that the Gauss–Newton
method, which reduces to Newton’s method in this case, diverges when the ini-
tial value x(1) is 1.15. The Levenberg–Marquardt algorithm, however, solves this
problem. Figure 18.5 shows the value of the residual f (x(k)), and the value of λ(k),
for the Levenberg–Marquardt algorithm started from x(1) = 1.15 and λ(1) = 1. It
converges to the solution x = 0 in around 10 iterations.

Equilibrium prices. We illustrate algorithm 18.3 with a small instance of the equi-
librium price problem, with supply and demand functions

D(p) = exp�Ed(log p − log pnom) + dnom� ,
S(p) = exp (Es(log p − log pnom) + snom) ,

where Ed and Es are the demand and supply elasticity matrices, dnom and snom
are the nominal demand and supply vectors, and the log and exp appearing in
the equations apply to vectors elementwise. Figure 18.6 shows the contour lines of
�f (p)�2, where f (p) = S(p) − D(p) is the excess supply, for

pnom = (2.8, 10),

dnom = (3.1, 2.2),

snom = (2.2, 0.3)

and

Ed =� −0.5

0

0.2

−0.5 � ,

Es =�

0.5
−0.15

−0.3

0.8 � .

Figure 18.7 shows the iterates of the algorithm 18.3, started at p = (3, 9) and
λ(1) = 1. The values of �f (p(k))�2 and the trust parameter λ(k) versus iteration k
are shown in ﬁgure 18.8.

18.3 Levenberg–Marquardt algorithm

395

10

8

2
p

6

4

2

2

3

4

5

6
p1

7

8

9

10

Figure 18.6 Contour lines of the square norm of the excess supply f (p) =
S(p) − D(p) for a small example with two commodities. The point marked
with a star is the equilibrium prices, for which f (p) = 0.

10

8

2
p

6

4

2

2

3

4

5

6
p1

7

8

9

10

Figure 18.7 Iterates of the Levenberg–Marquardt algorithm started at p =
(3, 9).

396

150

100

2
�
)
)
k
(
p
(
f
�

18 Nonlinear least squares

1

)
k
(
λ

0.5

50

0

0

5

10

15

k

0

0

5

10

15

k

Figure 18.8 Cost function �f (p(k)�2 and trust parameter λ(k) versus iteration
number k in the example of ﬁgure 18.7.

Location from range measurements. We illustrate algorithm 18.3 with a small
instance of the location from range measurements problem, with ﬁve points ai in a
plane, shown in ﬁgure 18.9. The range measurements ρi are the distances of these
points to the ‘true’ point (1, 1), plus some measurement errors. Figure 18.9 also
shows the level curves of �f (x)�2, and the point (1.18, 0.82) (marked with a star)
that minimizes �f (x)�2. (This point is close to, but not equal to, the ‘true’ value
(1, 1), due to the noise added to the range measurements.) Figure 18.10 shows the
graph of �f (x)�.

We run algorithm 18.3 from three diﬀerent starting points,

x(1) = (1.8, 3.5),

x(1) = (2.2, 3.5),

x(1) = (3.0, 1.5),

with λ(1) = 0.1. Figure 18.11 shows the iterates x(k) for the three starting points.
When started at (1.8, 3.5) (blue circles) or (3.0, 1.5) (brown diamonds) the al-
gorithm converges to (1.18, 0.82), the point that minimizes �f (x)�2. When the
algorithm is started at (2.2, 3.5) the algorithm converges to a non-optimal point
(2.98, 2.12) (which gives a poor estimate of the ‘true’ location (1, 1)).

The values of �f (x(k))�2 and the trust parameter λ(k) during the iteration are
shown in ﬁgure 18.12. As can be seen from this ﬁgure, in the ﬁrst run of the
algorithm (blue circles), λ(k) is increased in the third iteration. Correspondingly,
x(3) = x(4) in ﬁgure 18.12. For the second starting point (red squares) λ(k) decreases
monotonically. For the third starting point (brown diamonds) λ(k) increases in
iterations 2 and 4.

18.3 Levenberg–Marquardt algorithm

397

4

3

2
x

2

1

0

0

1

2
x1

3

4

Figure 18.9 Contour lines of �f (x)�2 where fi(x) = �x − ai� − ρi. The
dots show the points ai, and the point marked with a star is the point that
minimizes �f (x)�2.

�f (x)�

4

2

0
0

4

x2

2

1

2
x1

3

4 0

Figure 18.10 Graph of �f (x)� in the location from range measurements
example.

398

18 Nonlinear least squares

4

3

2
x

2

1

0

0

1

2
x1

3

4

Figure 18.11 Iterates of the Levenberg–Marquardt algorithm started at three
diﬀerent starting points.

4

2
�
)
)
k
(
x
(
f
�

2

0

1 2 3 4 5 6 7 8 9 10

k

)
k
(
λ

0.3

0.2

0.1

0

1 2 3 4 5 6 7 8 9 10

k

Figure 18.12 Cost function �f (x(k))�2 and trust parameter λ(k) versus iter-
ation number k for the three starting points.

18.4 Nonlinear model ﬁtting

399

ˆf (x; θ)

x

Figure 18.13 Least squares ﬁt of a function ˆf (x; θ) = θ1eθ2x cos(θ3x + θ4) to
N = 60 points (x(i), y(i)).

18.4 Nonlinear model ﬁtting

The Levenberg–Marquardt algorithm is widely used for nonlinear model ﬁtting.
As in §13.1 we are given a set of data, x(1), . . . , x(N ), y(1), . . . , y(N ), where the n-
vectors x(1), . . . , x(N ) are the feature vectors, and the scalars y(1), . . . , y(N ) are the
associated outcomes. (So here the superscript indexes the given data; previously
in this chapter the superscript denoted the iteration number.)

In nonlinear model ﬁtting, we ﬁt a model of the general form y ≈ ˆf (x; θ) to the
given data, where the p-vector θ contains the model parameters. In linear model
ﬁtting, ˆf (x; θ) is a linear function of the parameters, so it has the special form

ˆf (x; θ) = θ1f1(x) + ··· + θpfp(x),

where f1, . . . , fp are scalar-valued functions, called the basis functions (See §13.1.)
In nonlinear model ﬁtting the dependence of ˆf (x; θ) on θ is not linear (or aﬃne),
so it does not have the simple form of a linear combination of p basis functions.

As in linear model ﬁtting, we choose the parameter θ by (approximately) min-

imizing the sum of the squares of the prediction residuals,

N�i=1

( ˆf (x(i); θ) − y(i))2,

which is a nonlinear least squares problem, with variable θ. (We can also add a
regularization term to this objective.)

Example. Figure 18.13 shows a nonlinear model ﬁtting example. The model is
an exponentially decaying sinusoid

ˆf (x; θ) = θ1eθ2x cos(θ3x + θ4),

400

18 Nonlinear least squares

ˆf (x; θ)

Figure 18.14 The solid line minimizes the sum of the squares of the orthog-
onal distances of points to the graph of the polynomial.

x

with four parameters θ1, θ2, θ3, θ4. (This model is an aﬃne function of θ1, but it is
not an aﬃne function of θ2, θ3, or θ4.) We ﬁt the model to N = 60 points (x(i), y(i))
by minimizing the sum of the squared residuals (18.4) over the four parameters.

Orthogonal distance regression. Consider the linear in the parameters model

ˆf (x; θ) = θ1f1(x) + ··· + θpfp(x),

with basis functions fi : Rn → R, and a data set of N pairs (x(i), y(i)). The usual
objective is the sum of squares of the diﬀerence between the model prediction
ˆf (x(i)) and the observed value y(i), which leads to a linear least squares problem.
In orthogonal distance regression we use another objective, the sum of the squared
distances of N points (x(i), y(i)) to the graph of ˆf , i.e., the set of points of the form
(u, ˆf (u)). This model can be found by solving the nonlinear least squares problem

( ˆf (u(i); θ) − y(i))2 +

�u(i) − x(i)�2

N�i=1

minimize

N�i=1

with variables θ1, . . . , θp, and u(1), . . . , u(N ). In orthogonal distance regression, we
are allowed to choose the parameters in the model, and also, to modify the feature
vectors from x(i) from u(i) to obtain a better ﬁt. (Orthogonal distance regression is
an example of an error-in-variables model, since it takes into account errors in the
regressors or independent variables.) Figure 18.14 shows a cubic polynomial ﬁt to
25 points using this method. The open circles are the points (x(i), y(i)). The small
circles on the graph of the polynomial are the points (u(i), ˆf (u(i); θ)). Roughly
speaking, we ﬁt a curve that passes near all the data points, as measured by the
(minimum) distance from the data points to the curve. In contrast, ordinary least
squares regression ﬁnds a curve that minimizes the sum of squares of the vertical
errors between the curve and the data points.

18.5 Nonlinear least squares classiﬁcation

401

18.5 Nonlinear least squares classiﬁcation

In this section we describe a nonlinear extension of the least squares classiﬁcation
method discussed in chapters 14 and 15, that typically out-performs the basic least
squares classiﬁer in practice.

The Boolean classiﬁer of chapter 14 ﬁts a linearly parametrized function

˜f (x) = θ1f1(x) + ··· + θpfp(x)

to the data points (x(i), y(i)), i = 1, . . . , N , where y(i) ∈ {−1, 1}, using linear
least squares. The parameters θ1, . . . , θp are chosen to minimize the sum squares
objective

N�i=1

( ˜f (x(i)) − y(i))2,

(18.14)

plus, optionally, a regularization term. This (hopefully) results in ˜f (x(i)) ≈ y(i),
which is roughly what we want. We can think of ˜f (x) as the continuous prediction
of the Boolean outcome y. The classiﬁer itself is given by ˆf (x) = sign( ˜f (x)); this
is the Boolean prediction of the outcome.

Instead of the sum square prediction error for the continuous prediction, con-

sider the sum square prediction error for the Boolean prediction,

N�i=1

( ˆf (x(i)) − y(i))2 =

N�i=1

(sign( ˜f (x(i))) − y(i))2.

(18.15)

This is 4 times the number of classiﬁcation errors we make on the training set. To
see this, we note that when ˆf (x(i)) = y(i), which means that a correct prediction
was made on the ith data point, we have ( ˆf (x(i))− y(i))2 = 0. When ˆf (x(i)) �= y(i),
which means that an incorrect prediction was made on the ith data point, one of
the values is +1 and the other is −1, so we have ( ˆf (x(i)) − y(i))2 = 4.
The objective (18.15) is what we really want; the least squares objective (18.14)
is a surrogate for what we want. But we cannot use the Levenberg–Marquardt algo-
rithm to minimize the objective (18.15), since the sign function is not diﬀerentiable.
To get around this, we replace the sign function with a diﬀerentiable approximation,
for example the sigmoid function

φ(u) =

eu − e−u
eu + e−u ,

(18.16)

shown in ﬁgure 18.15. We choose θ by solving the nonlinear least squares problem
of minimizing

N�i=1

(φ( ˜f (x(i))) − y(i))2,

(18.17)

using the Levenberg–Marquardt algorithm.
(We can also add regularization to
this objective.) Minimizing the nonlinear least squares objective (18.17) is a good
approximation for choosing the parameter vector θ so as to minimize the number
of classiﬁcation errors made on the training set.

402

18 Nonlinear least squares

φ(u)

1

−4

−2

2

u

4

−1

Figure 18.15 The sigmoid function φ.

Loss function interpretation. We can interpret the objective functions (18.14),
(18.15), and (18.17) in terms of loss functions that depend on the continuous
prediction ˜f (x(i)) and the outcome y(i). Each of the three objectives has the form

�( ˜f (x(i)), y(i)),

N�i=1

where � is a loss function. The ﬁrst argument of the loss function is a real number,
and the second argument is Boolean, with values −1 or +1. For the linear least
squares objective (18.14), the loss function is �(u, y) = (u − y)2. For the nonlinear
least squares objective with the sign function (18.15), the loss function is �(u, y) =
(sign(u) − y)2. For the diﬀerentiable nonlinear least squares objective (18.15), the
loss function is �(u, y) = (φ(u) − y)2. Roughly speaking, the loss function �(u, y)
tells us how bad it is to have ˜f (x(i)) = u when y = y(i).

Since the outcome y takes on only two values, −1 and +1, we can plot the loss
functions as functions of u for these two values of y. Figure 18.16 shows these three
functions, with the value for y = −1 in the left column and the value for y = +1
in the right column. We can see that all three loss functions discourage prediction
errors, since their values are higher for sign(u) �= y than when sign(u) = y.
The loss function for nonlinear least squares classiﬁcation with the sign function
(shown in the middle row) assesses a cost of 0 for a correct prediction and 4 for
an incorrect prediction. The loss function for nonlinear least squares classiﬁcation
with the sigmoid function (shown in the bottom row) is a smooth approximation
of this.

18.5 Nonlinear least squares classiﬁcation

403

(u + 1)2

4

3

2

1

−3 −1

1

3

(sign(u) + 1)2

4

3

2

1

−3 −1

1

3

(φ(u) + 1)2

4

3

2

1

−3 −1

1

3

u

u

u

(u − 1)2
4

3

2

1

−3 −1

1

3

(sign(−u) + 1)2

4

3

2

1

−3 −1

1

3

(φ(u) − 1)2

4

3

2

1

−3 −1

1

3

u

u

u

Figure 18.16 The loss functions �(u, y) for linear least squares classiﬁcation
(top), nonlinear least squares classiﬁcation with the sign function (middle),
and nonlinear least squares classiﬁcation with the sigmoid function (bottom).
The left column shows �(u,−1) and the right columns shows �(u, +1).

404

18 Nonlinear least squares

Train
Test

5

4

3

2

1

)

%

(

r
o
r
r
e

n
o
i
t
a
c
fi

i
s
s
a
l
C

10−9

10−6

10−3

100

103

106

λ

Figure 18.17 Boolean classiﬁcation error in percent versus λ.

18.5.1 Handwritten digit classiﬁcation

We apply nonlinear least squares classiﬁcation on the MNIST set of handwritten
digits used in chapter 14. We ﬁrst consider the Boolean problem of recognizing the
digit zero. We use linear features, i.e.,

˜f (x) = xT β + v,

where x is the 493-vector of pixel intensities. To determine the parameters v and
β we solve the nonlinear least squares problem

minimize

N�i=1

(φ((x(i))T β + v) − y(i))2 + λ�β�2,

(18.18)

where φ is the sigmoid function (18.16) and λ is a positive regularization parameter.
(This λ is the regularization parameter in the classiﬁcation problem; it has no
relation to the trust parameter λ(k) in the iterates of the Levenberg–Marquardt
algorithm.)

Figure 18.17 shows the classiﬁcation error on the training and test sets as a
function of the regularization parameter λ. For λ = 100, the classiﬁcation errors
on the training and test sets are about 0.7%. This is less than half the 1.6%
error of the Boolean least squares classiﬁer that used the same features, discussed
in chapter 14. This improvement in performance, by more than a factor of two,
comes from minimizing an objective that is closer to what we want (i.e., the number
of prediction errors on the training set) than the surrogate linear least squares
objective. The confusion matrices for the training set and test set are given in
table 18.1. Figure 18.18 shows the distribution of the values of ˜f (x(i)) for the two
classes of the data set.

18.5 Nonlinear least squares classiﬁcation

405

Prediction

Prediction

Outcome

ˆy = +1

y = +1
y = −1

All

5627
148
5775

ˆy = −1 Total
5923
296
54077
53929
54225
60000

Outcome

ˆy = +1

y = +1
y = −1

All

945
40
985

ˆy = −1 Total
980
9020
10000

35
8980
9015

Table 18.1 Confusion matrices for a Boolean classiﬁer to recognize the digit
zero. The table on the left is for the training set. The table on the right is
for the test set.

Positive
Negative

0.08

0.06

0.04

0.02

n
o
i
t
c
a
r
F

0

−10

−5

0
˜f (x(i))

5

Figure 18.18 The distribution of the values of ˜f (x(i)) used in the Boolean
classiﬁer (14.1) for recognizing the digit zero. The function ˜f was computed
by solving the nonlinear least squares problem (18.17).

406

18 Nonlinear least squares

Train
Test

102

101

100

)

%

(

r
o
r
r
e

n
o
i
t
a
c
fi

i
s
s
a
l
C

0

2

4
6
Iteration

8

10

Figure 18.19 Training and test error versus Levenberg–Marquardt iteration
for λ = 100.

Convergence of Levenberg–Marquardt algorithm. The Levenberg–Marquardt
algorithm is used to compute the parameters in the nonlinear least squares classiﬁer.
In this example the algorithm takes several tens of iterations to converge, i.e., until
the stopping criterion for the nonlinear least squares problem is satisﬁed. But in
this application we are more interested in the performance of the classiﬁer, and
not minimizing the objective of the nonlinear least squares problem. Figure 18.19
shows the classiﬁcation error of the classiﬁer (on the training and test data sets)
with parameter θ(k), the kth iterate of the Levenberg–Marquardt algorithm. We
can see that the classiﬁcation errors reach their ﬁnal values of 0.7% after just a few
iterations. This phenomenon is very typical in nonlinear data ﬁtting problems. Well
before convergence, the Levenberg–Marquardt algorithm ﬁnds model parameters
that are just as good (as judged by test error) as the parameters obtained when
the algorithm converges.

Feature engineering. After adding the 5000 random features used in chapter 14,
we obtain the training and test classiﬁcation errors shown in ﬁgure 18.20. The
error on the training set is zero for small λ. For λ = 1000, the error on the test
set is 0.24%, with the confusion matrix in table 18.2. The distribution of ˜f (x(i))
on the training set in ﬁgure 18.21 shows why the training error is zero.

Figure 18.22 shows the classiﬁcation errors versus Levenberg–Marquardt iter-
ation, if we start the Levenberg–Marquardt algorithm with β = 0, v = 0. (This
implies that the values computed in the ﬁrst iteration are the coeﬃcients of the
linear least squares classiﬁer.) The error on the training set is exactly zero at itera-
tion 5. The error on the test set is almost equal to its ﬁnal value after one iteration.

18.5 Nonlinear least squares classiﬁcation

407

Train
Test

)

%

(

r
o
r
r
e

n
o
i
t
a
c
fi

i
s
s
a
l
C

0.6

0.4

0.2

0

10−2

100

104

106

102
λ

Figure 18.20 Boolean classiﬁcation error in percent versus λ, after adding
5000 random features.

Prediction

Outcome

ˆy = +1

y = +1
y = −1

All

967
11
978

ˆy = −1 Total
980
9020
10000

13
9009
9022

Table 18.2 Confusion matrix on the test set for the Boolean classiﬁer to
recognize the digit zero after addition of 5000 new features.

408

18 Nonlinear least squares

0.06

Positive
Negative

n
o
i
t
c
a
r
F

0.04

0.02

0

−10

−5

0
˜f (x(i))

5

Figure 18.21 The distribution of the values of ˜f (x(i)) used in the Boolean
classiﬁer (14.1) for recognizing the digit zero, after addition of 5000 new
features. The function ˜f was computed by solving the nonlinear least squares
problem (18.17).

Train
Test

)

%

(

r
o
r
r
e

n
o
i
t
a
c
fi

i
s
s
a
l
C

102

101

100

10−1

10−2

10−3

0

2

4
6
Iteration

8

10

Figure 18.22 Training and test error versus Levenberg–Marquardt iteration
for λ = 1000.

18.5 Nonlinear least squares classiﬁcation

409

)

%

(

r
o
r
r
e

n
o
i
t
a
c
fi

i
s
s
a
l
C

10

9

8

7

6

Train
Test

10−5

10−3

10−1

λ

101

103

Figure 18.23 Multiclass classiﬁcation error in percent versus λ.

Multi-class classiﬁer. Next we apply the nonlinear least squares method to the
multi-class classiﬁcation of recognizing the ten digits in the MNIST data set. For
each digit k, we compute a Boolean classiﬁer ˜fk(x) = xT βk + vk by solving a
regularized nonlinear least squares problem (18.18). The same value of λ is used
in the ten nonlinear least squares problems. The Boolean classiﬁers are combined
into a multi-class classiﬁer

ˆf (x) = argmax
k=1,...,10

(xT βk + vk).

Figure 18.23 shows the classiﬁcation errors versus λ. The test set confusion matrix
(for λ = 1) is given in table 18.3. The classiﬁcation error on the test set is 7.6%,
down from the 13.9% error we obtained for the same set of features with the least
squares method of chapter 14.

Feature engineering. Figure 18.24 shows the error rates when we add the 5000
randomly generated features. The training and test error rates are now 0.02% and
2%. The test set confusion matrix for λ = 1000 is given in table 18.4. This classiﬁer
has matched human performance in classifying digits correctly. Further, or more
sophisticated, feature engineering can bring the test performance well below what
humans can achieve.

410

18 Nonlinear least squares

Digit

0

0
1
2
3
4
5
6
7
8
9
All

1

0

1112

5
0
2
2
3
6
10
6

964
0
5
3
1
10
8
2
13
8

1014

1146

Prediction

2

0
4
934
19
4
2
3
25
4
0
995

3

4

5

6

7

8

9

Total

2
3
13
926
2
31
1
5
18
12
1013

0
0
7
1
917
10
5
11
16
43
1010

2
1
3
21
0
782
20
5
27
11
872

5
4
13
2
7
17
910
0
8
1
967

3
1
10
8
1
7
1
947
9
19
1006

1
3
0
10
4
38
9
21
38
10
8
23
0
7
23
4
4
865
886
23
1004 973

980
1135
1032
1010
982
892
958
1028
974
1009
10000

Table 18.3 Confusion matrix for test set. The error rate is 7.6%.

.

Train
Test

)

%

(

r
o
r
r
e

n
o
i
t
a
c
fi

i
s
s
a
l
C

3

2

1

0

10−2

100

102

λ

104

Figure 18.24 Multiclass classiﬁcation error in percent versus λ after adding
5000 random features.

18.5 Nonlinear least squares classiﬁcation

411

Digit

0

0
1
2
3
4
5
6
7
8
9
All

972
0
5
0
0
2
7
1
3
2
992

1

1

1124

0
0
0
0
2
7
0
5

2

1
2

1006

3
4
2
0
6
0
0

3

1
2
1

986

1
5
1
1
4
5

1139

1024

1007

Prediction

4

0
0
3
0

966

2
3
2
4
6
986

5

1
0
0
5
0

875

2
0
5
4
892

6

1
3
2
0
4
5

941

0
3
1
960

7

1
1
6
3
1
0
0

1003

4
6

1025

8

2
3
9
7
0
1
2
3

949

9

0
0
0
6
6
0
0
5
2

2
978
978 997

Total

980
1135
1032
1010
982
892
958
1028
974
1009
10000

Table 18.4 Confusion matrix for test set after adding 5000 features. The
error rate is 2.0%.

412

18 Nonlinear least squares

Exercises

18.1 Lambert W -function. The Lambert W -function, denoted W : [0,∞) → R, is deﬁned as
W (u) = x, where x is the unique number x ≥ 0 for which xex = u. (The notation just
means that we restrict the argument x to be nonnegative.) The Lambert function arises in
a variety of applications, and is named after the mathematician Johann Heinrich Lambert.
There is no analytical formula for W (u); it must be computed numerically. In this exercise
you will develop a solver to compute W (u), given a nonnegative number u, using the
Levenberg–Marquardt algorithm, by minimizing f (x)2 over x, where f (x) = xex − u.
(a) Give the Levenberg–Marquardt update (18.13) for f .
(b) Implement the Levenberg–Marquardt algorithm for minimizing f (x)2. You can start
with x(1) = 1 and λ(1) = 1 (but it should work with other initializations). You can
stop the algorithm when |f (x)| is small, say, less than 10−6.

18.2 Internal rate of return. Let the n-vector c denote a cash ﬂow over n time periods, with
positive entries meaning cash received and negative entries meaning payments. We assume
that its NPV (net present value; see page 22) with interest rate r ≥ 0 is given by

N (r) =

ci

(1 + r)i−1 .

n�i=1

The internal rate of return (IRR) of the cash ﬂow is deﬁned as the smallest positive value
of r for which N (r) = 0. The Levenberg–Marquardt algorithm can be used to compute
the IRR of a given cash ﬂow sequence, by minimizing N (r)2 over r.

(a) Work out a speciﬁc formula for the Levenberg–Marquardt update for r, i.e., (18.13).
(b) Implement the Levenberg–Marquardt algorithm to ﬁnd the IRR of the cash ﬂow

sequence

c = (−13, 0.3 15, 0.6 16),

where the subscripts give the dimensions.
(This corresponds to three periods in
which you make investments, which pay oﬀ at one rate for 5 periods, and a higher
rate for the next 6 periods.) You can initialize with r(0) = 0, and stop when N (r(k))2
is small. Plot N (r(k))2 versus k.

18.3 A common form for the residual. In many nonlinear least squares problems the residual

function f : Rn → Rm has the speciﬁc form

fi(x) = φi(aT

i x − bi),

i = 1, . . . , m,

where ai is an n-vector, bi is a scalar, and φi : R → R is a scalar-valued function of a
scalar. In other words, fi(x) is a scalar function of an aﬃne function of x. In this case
the objective of the nonlinear least squares problem has the form

�f (x)�2 =

m�i=1�φi(aT

i x − bi)�2

.

We deﬁne the m× n matrix A to have rows aT
m, and the m-vector b to have entries
b1, . . . , bm. Note that if the functions φi are the identity function, i.e., φi(u) = u for all u,
then the objective becomes �Ax−b�2, and in this case the nonlinear least squares problem
reduces to the linear least squares problem. Show that the derivative matrix Df (x) has
the form

1 , . . . , aT

Df (x) = diag(d)A,

where di = φ�i(ri) for i = 1, . . . , m, with r = Ax − b.
Remark. This means that in each iteration of the Gauss–Newton method we solve a
weighted least squares problem (and in the Levenberg–Marquardt algorithm, a regularized
weighted least squares problem); see exercise 12.4. The weights change in each iteration.

Exercises

413

18.4 Fitting an exponential to data. Use the Levenberg–Marquardt algorithm to ﬁt an expo-

nential function of the form ˆf (x; θ) = θ1eθ2x to the data

0, 1, . . . , 5,

5.2, 4.5, 2.7, 2.5, 2.1, 1.9.

(The ﬁrst list gives x(i); the second list gives y(i).) Plot your model ˆf (x; ˆθ) versus x, along
with the data points.

18.5 Mechanical equilibrium. A mass m, at position given by the 2-vector x, is subject to three
forces acting on it. The ﬁrst force F grav is gravity, which has value F grav = −mg(0, 1),
where g = 9.8 is the acceleration of gravity. (This force points straight down, with a force
that does not depend on the position x.) The mass is attached to two cables, whose other
ends are anchored at (2-vector) locations a1 and a2. The force Fi applied to the mass by
cable i is given by

where Ti is the cable tension. (This means that each cable applies a force on the mass
that points from the mass to the cable anchor, with magnitude given by the tension Ti.)
The cable tensions are given by

Fi = Ti(ai − x)/�ai − x�,

Ti = k

max{�ai − x� − Li, 0}

Li

,

where k is a positive constant, and Li is the natural or unloaded length of cable i (also
positive). In words: The tension is proportional to the fractional stretch of the cable,
above its natural length. (The max appearing in this formula means the tension is not
a diﬀerentiable function of the position, when �ai − x� = Li, but we will simply ignore
this.) The mass is in equilibrium at position x if the three forces acting on it sum to zero,

F grav + F1 + F2 = 0.

We refer to the left-hand side as the residual force. It is a function of mass position x,
and we write it as f (x).
Compute an equilibrium position for

a1 = (3, 2),

a2 = (−1, 1),

L1 = 3,

L2 = 2,

m = 1,

k = 100,

by applying the Levenberg–Marquardt algorithm to the residual force f (x). Use x(1) =
(0, 0) as starting point. (Note that it is important to start at a point where T1 > 0 and
T2 > 0, because otherwise the derivative matrix Df (x(1)) is zero, and the Levenberg–
Marquardt update gives x(2) = x(1).) Plot the components of the mass position and the
residual force versus iterations.

18.6 Fitting a simple neural network model. A neural network is a widely used model of the
form ˆy = ˆf (x; θ), where the n-vector x is the feature vector and the p-vector θ is the
model parameter. In a neural network model, the function ˆf is not an aﬃne function of
the parameter vector θ. In this exercise we consider a very simple neural network, with
two layers, three internal nodes, and two inputs (i.e., n = 2). This model has p = 13
parameters, and is given by

ˆf (x; θ) = θ1φ(θ2x1 + θ3x2 + θ4) + θ5φ(θ6x1 + θ7x2 + θ8)

+ θ9φ(θ10x1 + θ11x2 + θ12) + θ13

where φ : R → R is the sigmoid function deﬁned in (18.16). This function is shown as
a signal ﬂow graph in ﬁgure 18.25. In this graph each edge from an input to an internal
node, or from an internal node to the output node, corresponds to multiplication by one
of the parameters. At each node (shown as the small ﬁlled circles) the incoming values
and the constant oﬀset are added together, then passed through the sigmoid function, to
become the outgoing edge value.

414

18 Nonlinear least squares

θ4

θ8

θ12

x1

x2

θ10

θ3

θ2

θ6

θ7

θ11

θ13

ˆy = ˆf (x; θ)

θ1

θ5

θ9

Figure 18.25 Signal ﬂow graph of a simple neural network.

Fitting such a model to a data set consisting of the n-vectors x(1), . . . , x(N ) and the
associated scalar outcomes y(1), . . . , y(N ) by minimizing the sum of the squares of the
residuals is a nonlinear least squares problem with objective (18.4).

(a) Derive an expression for ∇θ ˆf (x; θ). Your expression can use φ and φ�, the sigmoid
function and its derivative. (You do not need to express these in terms of exponen-
tials.)

(b) Derive an expression for the derivative matrix Dr(θ), where r : Rp → RN is the

vector of model ﬁtting residuals,

r(θ)i = ˆf (x(i); θ) − y(i),

i = 1, . . . , N.

Your expression can use the gradient found in part (a).

(c) Try ﬁtting this neural network to the function g(x1, x2) = x1x2. First generate
N = 200 random points x(i) and take y(i) = (x(i))1(x(i))2 for i = 1, . . . , 200. Use
the Levenberg–Marquardt algorithm to try to minimize

f (θ) = �r(θ)�2 + γ�θ�2

with γ = 10−5. Plot the value of f and the norm of its gradient versus iteration.
Report the RMS ﬁtting error achieved by the neural network model. Experiment
with choosing diﬀerent starting points to see the eﬀect on the ﬁnal model found.

(d) Fit the same data set with a (linear) regression model ˆf lin(x; β, v) = xT β + v and
report the RMS ﬁtting error achieved. (You can add regularization in your ﬁtting,
but it won’t improve the results.) Compare the RMS ﬁtting error with the neural
network model RMS ﬁtting error from part (c).

Remarks. Neural networks used in practice employ many more regressors, layers, and in-
ternal modes. Specialized methods and software are used to minimize the ﬁtting objective,
and evaluate the required gradients and derivatives.

Exercises

415

p

L2

θ2

L1

θ1

Figure 18.26 Two-link robot manipulator in a plane.

18.7 Robot manipulator. Figure 18.26 shows a two-link robot manipulator in a plane. The

robot manipulator endpoint is at the position

p = L1� cos θ1

sin θ1 � + L2� cos(θ1 + θ2)
sin(θ1 + θ2) � ,

where L1 and L2 are the lengths of the ﬁrst and second links, θ1 is the ﬁrst joint angle, and
θ2 is second joint angle. We will assume that L2 < L1, i.e., the second link is shorter than
the ﬁrst. We are given a desired endpoint position pdes, and seek joint angles θ = (θ1, θ2)
for which p = pdes.
This problem can be solved analytically. We ﬁnd θ2 using the equation

�pdes�2 = (L1 + L2 cos θ2)2 + (L2 sin θ2)2

= L2

1 + L2

2 + 2L1L2 cos θ2.

When L1 − L2 < �pdes� < L1 + L2, there are two choices of θ2 (one positive and one
negative). For each solution θ2 we can then ﬁnd θ1 using

pdes = L1� cos θ1

sin(θ1 + θ2) �
sin θ1 � + L2� cos(θ1 + θ2)
L1 + L2 cos θ2 �� cos θ1
sin θ1 � .

−L2 sin θ2

= � L1 + L2 cos θ2

L2 sin θ2

In this exercise you will use the Levenberg–Marquardt algorithm to ﬁnd joint angles, by
minimizing �p − pdes�2.
(a) Identify the function f (θ) in the nonlinear least squares problem, and give its deriva-

tive Df (θ).

(b) Implement the Levenberg–Marquardt algorithm to solve the nonlinear least squares
problem. Try your implementation on a robot with L1 = 2, L2 = 1, and the desired
endpoints

(1.0, 0.5),

For each endpoint, plot the cost function �f (θ(k))�2 versus iteration number k.
Note that the norm of the last endpoint exceeds L1 + L2 = 3, so there are no joint
angles for which p = pdes. Explain the angles your algorithm ﬁnds in this case.

(−2.0, 1.0),

(−0.2, 3.1).

416

18 Nonlinear least squares

r + δ

r − δ

c

α

Figure 18.27 Ellipse with center (c1, c2), and radii r + δ and r − δ. The
largest semi-axis makes an angle α with respect to horizontal.

Figure 18.28 Ellipse ﬁt to 50 points in a plane.

18.8 Fitting an ellipse to points in a plane. An ellipse in a plane can be described as the set

of points

ˆf (t; θ) =� c1 + r cos(α + t) + δ cos(α − t)
c2 + r sin(α + t) + δ sin(α − t) � ,

where t ranges from 0 to 2π. The vector θ = (c1, c2, r, δ, α) contains ﬁve parameters, with
geometrical meanings illustrated in ﬁgure 18.27. We consider the problem of ﬁtting an
ellipse to N points x(1), . . . , x(N ) in a plane, as shown in ﬁgure 18.28. The circles show
the N points. The short lines connect each point to the nearest point on the ellipse. We
will ﬁt the ellipse by minimizing the sum of the squared distances of the N points to the
ellipse.
(a) The squared distance of a data point x(i) to the ellipse is the minimum of � ˆf (t(i); θ)−
x(i)�2 over the scalar t(i). Minimizing the sum of the squared distances of the data
points x(1), . . . , x(N ) to the ellipse is therefore equivalent to minimizing

N�i=1

� ˆf (t(i); θ) − x(i)�2

over t(1), . . . , t(N ) and θ. Formulate this as a nonlinear least squares problem. Give
expressions for the derivatives of the residuals.

Exercises

417

(b) Use the Levenberg–Marquardt algorithm to ﬁt an ellipse to the 10 points:

(0.5, 1.5),

(0.7, 0.1),

(−0.3, 0.6),
(2.3, 0.8),

(1.0, 1.8),

(1.4, 0.5),

(−0.4, 0.2),
(0.0, 0.2),

(0.2, 1.3)

(2.4, 1.7).

To select a starting point, you can choose parameters θ that describe a circle with
radius one and center at the average of the data points, and initial values of t(i) that
minimize the objective function for those initial values of θ.

Chapter 19

Constrained nonlinear
least squares

In this chapter we consider an extension of the nonlinear least squares problem
that includes nonlinear constraints. Like the problem of solving a set of nonlinear
equations, or ﬁnding a least squares approximate solution to a set of nonlinear
equations, the constrained nonlinear least squares problem is in general hard to
solve exactly. We describe a heuristic algorithm that often works well in practice.

19.1 Constrained nonlinear least squares

In this section we consider an extension of the nonlinear least squares problem (18.2)
that includes equality constraints:

minimize
subject to

�f (x)�2
g(x) = 0,

(19.1)

where the n-vector x is the variable to be found. Here f (x) is an m-vector, and
g(x) is a p-vector. We sometimes write out the components of f (x) and g(x), to
express the problem as

minimize
subject to

f1(x)2 + ··· + fm(x)2
gi(x) = 0,

i = 1, . . . , p.

We refer to fi(x) as the ith (scalar) residual, and gi(x) = 0 as the ith (scalar)
equality constraint. When the functions f and g are aﬃne, the equality constrained
nonlinear least squares problem (19.1) reduces to the (linear) least squares problem
with equality constraints from chapter 16.

We say that a point x is feasible for the problem (19.1) if it satisﬁes g(x) = 0.
A point ˆx is a solution of the problem (19.1) if it is feasible and has the smallest
objective among all feasible points, i.e., if whenever g(x) = 0, we have �f (x)�2 ≥
�f (ˆx)�2.

420

19 Constrained nonlinear least squares

Like the nonlinear least squares problem, or solving a set of nonlinear equations,
the constrained nonlinear least squares problem is in general hard to solve exactly.
But the Levenberg–Marquardt algorithm for solving the (unconstrained) nonlinear
least squares problem (18.2) can be leveraged to handle the problem with equality
constraints. We will describe a basic algorithm below, the penalty algorithm, and
a variation on it that works much better in practice, the augmented Lagrangian al-
gorithm. These algorithms are heuristics for (approximately) solving the nonlinear
least squares problem (19.1).

Linear equality constraints. One special case of the constrained nonlinear least
squares problem (19.1) is when the constraint function g is aﬃne, in which case
the constraints g(x) = 0 can be written Cx = d for some p × n matrix C and
a p-vector d.
In this case the problem (19.1) is called a nonlinear least squares
problem with linear equality constraints. It can be (approximately) solved by the
Levenberg–Marquardt algorithm described in chapter 18, by simply adding the
linear equality constraints to the linear least squares problem that is solved in
step 2. The more challenging problem is the case when g is not aﬃne.

19.1.1 Optimality condition

Using Lagrange multipliers (see §C.3) we can derive a condition that any solu-
tion of the constrained nonlinear least squares problem (19.1) must satisfy. The
Lagrangian for the problem (19.1) is

L(x, z) = �f (x)�2 + z1g1(x) + ··· + zpgp(x) = �f (x)�2 + g(x)T z,

(19.2)

where the p-vector z is the vector of Lagrange multipliers. The method of Lagrange
multipliers tells us that for any solution ˆx of (19.1), there is a set of Lagrange
multipliers ˆz that satisfy

∂L
∂xi

(ˆx, ˆz) = 0,

i = 1, . . . , n,

∂L
∂zi

(ˆx, ˆz) = 0,

i = 1, . . . , p

(provided the rows of Dg(ˆx) are linearly independent). The p-vector ˆz is called an
optimal Lagrange multiplier.

The second set of equations can be written as gi(ˆx) = 0, i = 1, . . . , p, in vector

form

g(ˆx) = 0,

(19.3)

i.e., ˆx is feasible, which we already knew. The ﬁrst set of equations can be written
in vector form as

2Df (ˆx)T f (ˆx) + Dg(ˆx)T ˆz = 0.

(19.4)

This equation is the extension of the condition (18.3) for the unconstrained non-
linear least squares problem (18.2). The equation (19.4), together with (19.3), i.e.,
ˆx is feasible, form the optimality conditions for the problem (19.1).

If ˆx is a solution of the constrained nonlinear least squares problem (19.1), then
it satisﬁes the optimality condition (19.4) for some Lagrange multiplier vector ˆz

19.2 Penalty algorithm

421

(provided the rows of Dg(ˆx) are linearly independent). So ˆx and ˆz satisfy the
optimality conditions.

These optimality conditions are not suﬃcient, however; there can be choices of
x and z that satisfy them, but x is not a solution of the constrained nonlinear least
squares problem.

19.2 Penalty algorithm

We start with the observation (already made on page 340) that the equality con-
strained problem can be thought of as a limit of a bi-objective problem with ob-
jectives �f (x)�2 and �g(x)�2, as the weight on the second objective increases to
inﬁnity. Let µ be a positive number, and consider the composite objective

�f (x)�2 + µ�g(x)�2.

(19.5)

This can be (approximately) minimized using the Levenberg–Marquardt algorithm
applied to

�����

2

f (x)

√µg(x) �����

.

(19.6)

By minimizing the composite objective (19.5), we do not insist that g(x) is zero,
but we assess a cost or penalty µ�g(x)�2 on the residual. If we solve this for large
enough µ, we should obtain a choice of x for which g(x) is very small, and �f (x)�2
is small, i.e., an approximate solution of (19.1). The second term µ�g(x)�2 is a
penalty imposed on choices of x with nonzero g(x).
Minimizing the composite objective (19.5) for an increasing sequence of values

of µ is known as the penalty algorithm.

Algorithm 19.1 Penalty algorithm for constrained nonlinear least squares
given diﬀerentiable functions f : Rn → Rm and g : Rn → Rp, and an initial
point x(1). Set µ(1) = 1.
For k = 1, 2, . . . , kmax

1. Solve unconstrained nonlinear least squares problem. Set x(k+1) to be the (ap-

proximate) minimizer of

�f (x)�2 + µ(k)�g(x)�2

using the Levenberg–Marquardt algorithm, starting from initial point x(k).

2. Update µ(k): µ(k+1) = 2µ(k).

The penalty algorithm is stopped early if �g(x(k))� is small enough, i.e., the equality
constraint is almost satisﬁed.
The penalty algorithm is simple and easy to implement, but has an important
drawback: The parameter µ(k) rapidly increases with iterations (as it must, to drive

422

19 Constrained nonlinear least squares

g(x) to zero). When the Levenberg–Marquardt algorithm is used to minimize (19.5)
for very high values of µ, it can take a large number of iterations or simply fail.
The augmented Lagrangian algorithm described below gets around this drawback,
and gives a much more reliable algorithm.

We can connect the penalty algorithm iterates to the optimality condition (19.4).
The iterate x(k+1) (almost) satisﬁes the optimality condition for minimizing (19.5),

2Df (x(k+1))T f (x(k+1)) + 2µ(k)Dg(x(k+1))T g(x(k+1)) = 0.

Deﬁning

z(k+1) = 2µ(k)g(x(k+1))

as our estimate of a suitable Lagrange multiplier in iteration k + 1, we see that the
optimality condition (19.4) (almost) holds for x(k+1) and z(k+1). (The feasibility
condition g(x(k)) = 0 only holds in the limit as k → ∞.)

19.3 Augmented Lagrangian algorithm

The augmented Lagrangian algorithm is a modiﬁcation of the penalty algorithm
that addresses the diﬃculty associated with the penalty parameter µ(k) becoming
very large. It was proposed by Magnus Hestenes and Michael Powell in the 1960s.

Augmented Lagrangian. The augmented Lagrangian for the problem (19.1), with
parameter µ > 0, is deﬁned as

Lµ(x, z) = L(x, µ) + µ�g(x)�2 = �f (x)�2 + g(x)T z + µ�g(x)�2.

(19.7)

This is the Lagrangian, augmented with the new term µ�g(x)�2; alternatively, it
can be interpreted as the composite objective function (19.5) used in the penalty
algorithm, with the Lagrange multiplier term g(x)T z added.

The augmented Lagrangian (19.7) is also the ordinary Lagrangian associated

with the problem

minimize
subject to

�f (x)�2 + µ�g(x)�2
g(x) = 0.

This problem is equivalent to the original constrained nonlinear least squares prob-
lem (19.1): A point x is a solution of one if and only if it is a solution of the other.
(This follows since the term µ�g(x)�2 is zero for any feasible x.)

Minimizing the augmented Lagrangian.
In the augmented Lagrangian algorithm
we minimize the augmented Lagrangian over the variable x for a sequence of values
of µ and z. We show here how this can be done using the Levenberg–Marquardt
algorithm. We ﬁrst establish the identity

Lµ(x, z) = �f (x)�2 + µ�g(x) + z/(2µ)�2 − µ�z/(2µ)�2.

(19.8)

19.3 Augmented Lagrangian algorithm

423

We expand the second term on the right-hand side to get

µ�g(x) + z/(2µ)�2

= µ�g(x)�2 + 2µg(x)T (z/(2µ)) + µ�z/(2µ)�2
= g(x)T z + µ�g(x)�2 + µ�z/(2µ)�2.

Substituting this into the right-hand side of (19.8) veriﬁes the identity.

When we minimize Lµ(x, z) over the variable x, the term −µ�z/(2µ)�2 in (19.8)
is a constant (i.e., does not depend on x), and does not aﬀect the choice of x. It
follows that we can minimize Lµ(x, z) over x by minimizing the function

�f (x)�2 + µ�g(x) + z/(2µ)�2,

which in turn can be expressed as

�����

f (x)

√µg(x) + z/(2√µ) �����

(19.9)

(19.10)

2

.

This can be be (approximately) minimized using the Levenberg–Marquardt algo-
rithm.

Any minimizer ˜x of Lµ(x, z) (or equivalently, (19.9)) satisﬁes the optimality

condition

0 = 2Df (˜x)T f (˜x) + 2µDg(˜x)T (g(˜x) + z/(2µ))

= 2Df (˜x)T f (˜x) + Dg(˜x)T (2µg(˜x) + z).

From this equation we can observe that if ˜x minimizes the augmented Lagrangian
and is also feasible (i.e., g(˜x) = 0), then it satisﬁes the optimality condition (19.4)
with the vector z as the Lagrange multiplier. The bottom equation also suggests
a good choice for updating the Lagrange multiplier vector z if ˜x is not feasible. In
this case the choice

˜z = z + 2µg(˜x)

(19.11)

satisﬁes the optimality condition (19.4) with ˜x and ˜z.

The augmented Lagrangian algorithm alternates between minimizing the aug-
mented Lagrangian (approximately, using the Levenberg–Marquardt algorithm),
and updating the parameter z (our estimate of a suitable Lagrange multiplier) us-
ing the suggestion (19.11) above. The penalty parameter µ is increased only when
needed, when �g(x)� does not suﬃciently decrease.

424

19 Constrained nonlinear least squares

Algorithm 19.2 Augmented Lagrangian algorithm
given diﬀerentiable functions f : Rn → Rm and g : Rn → Rp, and an initial
point x(1). Set z(1) = 0, µ(1) = 1.
For k = 1, 2, . . . , kmax

1. Solve unconstrained nonlinear least squares problem. Set x(k+1) to be the (ap-

proximate) minimizer of

�f (x)�2 + µ(k)�g(x) + z(k)/(2µ(k))�2

using Levenberg–Marquardt algorithm, starting from initial point x(k).

2. Update z(k).

3. Update µ(k).

z(k+1) = z(k) + 2µ(k)g(x(k+1)).

µ(k+1) =� µ(k)

2µ(k)

�g(x(k+1))� < 0.25�g(x(k))�
�g(x(k+1))� ≥ 0.25�g(x(k))�.

The augmented Lagrangian algorithm is stopped early if g(x(k)) is very small.
Note that due to our particular choice of how z(k) is updated, the iterate x(k+1)
(almost) satisﬁes the optimality condition (19.4) with z(k+1).

The augmented Lagrangian algorithm is not much more complicated than the
penalty algorithm, but it works much better in practice. In part this is because
the penalty parameter µ(k) does not need to increase as much as the algorithm
proceeds.

Example. We consider an example with two variables and

1 + 2x2 + 1 � ,
f (x1, x2) =� x1 + exp(−x2)

x2

g(x1, x2) = x1 + x3

1 + x2 + x2
2.

Figure 19.1 shows the contour lines of the cost function �f (x)�2 (solid lines) and
the constraint function g(x) (dashed lines). The point ˆx = (0, 0) is optimal with
corresponding Lagrange multiplier ˆz = −2. One can verify that g(ˆx) = 0 and

2Df (ˆx)T f (ˆx) + Dg(ˆx)T ˆz = 2� 1

−1

0

2 �� 1

1 � − 2� 1

1 � = 0.

The circle at x = (−0.666,−0.407) indicates the position of the unconstrained
minimizer of �f (x)�2.
The augmented Lagrangian algorithm is started from the point x(1) = (0.5,−0.5).
Figure 19.2 illustrates the ﬁrst six iterations. The solid lines are the contour lines
for Lµ(x, z(k)), the augmented Lagrangian with the current value of the Lagrange
multiplier. For comparison, we also show in ﬁgure 19.3 the ﬁrst six iterations of
the penalty algorithm, started from the same point. The solid lines are the contour
lines of �f (x)�2 + µ(k)�g(x)�2.
In ﬁgure 19.4 we show how the algorithms converge. The horizontal axis is
the cumulative number of Levenberg–Marquardt iterations. Each of these requires

19.4 Nonlinear control

425

1

g(x) = 1

g(x) = 0

2
x

0

g(x) = −1

−1

−1

0
x1

1

Figure 19.1 Contour lines of the cost function �f (x)�2 (solid line) and the
constraint function g(x) (dashed line) for a nonlinear least squares problem
in two variables with one equality constraint.

the solution of one linear least squares problem (minimizing (19.10) and (19.6),
respectively). The two lines show the absolute value of the feasibility residual
|g(x(k))|, and the norm of the optimality condition residual,
�2Df (x(k))T f (x(k)) + Dg(x(k))T z(k)�.

The vertical jumps in the optimality condition norm occur in steps 2 and 3 of the
augmented Lagrangian algorithm, and in step 2 of the penalty algorithm, when the
parameters µ and z are updated.

Figure 19.5 shows the value of the penalty parameter µ versus the cumulative

number of Levenberg–Marquardt iterations in the two algorithms.

19.4 Nonlinear control

A nonlinear dynamical system has the form of an iteration

xk+1 = f (xk, uk),

k = 1, 2, . . . , N,

where the n-vector xk is the state, and the m-vector uk is the input or control, at
time period k. The function f : Rn+m → Rn speciﬁes what the next state is, as a
function of the current state and the current input. When f is an aﬃne function,
this reduces to a linear dynamical system.

In nonlinear control, the goal is to choose the inputs u1, . . . , uN−1 to achieve
some goal for the state and input trajectories. In many problems the initial state

426

19 Constrained nonlinear least squares

µ(1) = 1, z(1) = 0

0.5

0

x(2)

0.5

0

µ(2) = 2, z(2) = −0.893

x(3)

−0.5

−0.5

0

0.5

−0.5

−0.5

0

0.5

µ(3) = 4, z(3) = −1.569

x(4)

0

µ(5) = 4, z(5) = −1.976

x(6)

0

0.5

0

−0.5

−0.5

0.5

0

−0.5

−0.5

0.5

0

µ(4) = 4, z(4) = −1.898

x(5)

0.5

−0.5

−0.5

0

0.5

0.5

0

µ(6) = 4, z(6) = −1.994

x(7)

0.5

−0.5

−0.5

0

0.5

Figure 19.2 First six iterations of the augmented Lagrangian algorithm.

19.4 Nonlinear control

427

0.5

0

x(2)

−0.5

−0.5

0.5

0

−0.5

−0.5

0.5

0

−0.5

−0.5

µ(1) = 1

0

µ(3) = 4

µ(2) = 2

0

0.5

µ(4) = 8

0.5

0

x(3)

0.5

−0.5

−0.5

0.5

0

x(4)

x(5)

0

µ(5) = 16

x(6)

0

0.5

−0.5

−0.5

0.5

0

0

0.5

µ(6) = 32

x(7)

0.5

−0.5

−0.5

0

0.5

Figure 19.3 First six iterations of the penalty algorithm.

428

19 Constrained nonlinear least squares

l
a
u
d

i
s
e
R

l
a
u
d

i
s
e
R

101

100

10−1

10−2

10−3

10−4

10−5

10−6

0

101

100

10−1

10−2

10−3

10−4

10−5

10−6

Feasibility
Opt. cond.

20

40

60

80

100

120

140

Feasibility
Opt. cond.

20

40

60

0
Cumulative Levenberg–Marquardt iterations

100

80

120

140

Figure 19.4 Feasibility and optimality condition errors versus the cumulative
number of Levenberg–Marquardt iterations in the augmented Lagrangian
algorithm (top) and the penalty algorithm (bottom).

19.4 Nonlinear control

429

Aug. Lag.
Penalty

µ

r
e
t
e
m
a
r
a
p

y
t
l
a
n
e
P

104

103

102

101

100

20

0
Cumulative Levenberg–Marquardt iterations

120

40

60

80

100

140

Figure 19.5 Penalty parameter µ versus cumulative number of Levenberg–
Marquardt iterations in the augmented Lagrangian algorithm and the
penalty algorithm.

x1 is given, and the ﬁnal state xN is speciﬁed. Subject to these constraints, we may
wish the control inputs to be small and smooth, which suggests that we minimize

N�k=1

�uk�2 + γ

N−1�k=1

�uk+1 − uk�2,

where γ > 0 is a parameter used to trade oﬀ input size and smoothness. (In many
nonlinear control problems the objective also involves the state trajectory.)

We can formulate the nonlinear control problem, with a norm squared objective
that involves the state and input, as a large constrained least problem, and then
solve it using the augmented Lagrangian algorithm. We illustrate this with a
speciﬁc example.

Control of a car. Consider a car with position p = (p1, p2) and orientation (an-
gle) θ. The car has wheelbase (length) L, steering angle φ, and speed s (which can
be negative, meaning the car moves in reverse). This is illustrated in ﬁgure 19.6.

The wheelbase L is a known constant; all of the other quantities p, θ, φ, and s
are functions of time. The dynamics of the car motion are given by the diﬀerential
equations

dp1
dt
dp2
dt
dθ
dt

(t) = s(t) cos θ(t),

(t) = s(t) sin θ(t),

(t) = (s(t)/L) tan φ(t).

430

19 Constrained nonlinear least squares

L

φ

(p1, p2)

θ

Figure 19.6 Simple model of a car.

Here we assume that the steering angle is always less than 90◦, so the tangent
term in the last equation makes sense. The ﬁrst two equations state that the car is
moving in the direction θ(t) (its orientation) at speed s(t). The last equation gives
the change in orientation as a function of the car speed and the steering angle. For
a ﬁxed steering angle and speed, the car moves in a circle.

We can control the speed s and the steering angle φ; the goal is to move the car
over some time period from a given initial position and orientation to a speciﬁed
ﬁnal position and orientation.

We now discretize the equations in time. We take a small time interval h, and

obtain the approximations

p1(t + h) ≈ p1(t) + hs(t) cos θ(t),
p2(t + h) ≈ p2(t) + hs(t) sin θ(t),
θ(t + h) ≈ θ(t) + h(s(t)/L) tan φ(t).

We will use these approximations to derive nonlinear state equations for the car
motion, with state xk = (p1(kh), p2(kh), θ(kh)) and input uk = (s(kh), φ(kh)). We
have

xk+1 = f (xk, uk),

with

We now consider the nonlinear optimal control problem

cos(xk)3
sin(xk)3

f (xk, uk) = xk + h(uk)1
N�k=1�uk�2 + γ

(tan(uk)2)/L  .
N−1�k=1 �uk+1 − uk�2

minimize

subject to x2 = f (0, u1)

(19.12)

xk+1 = f (xk, uk),
xﬁnal = f (xN , uN ),

k = 2, . . . , N − 1

with variables u1, . . . , uN , and x2, . . . , xN .

19.4 Nonlinear control

431

xﬁnal = (0, 1, 0)

xﬁnal = (0, 1, π/2)

1

0.5

0

0.6

0.4

0.2

0

−0.5

0

0.5

xﬁnal = (0, 0.5, 0)

1

0.5

0

0.4

0.2

0

−0.2

−0.5

0

0.5

xﬁnal = (0.5, 0.5,−π/2)

−0.6 −0.4 −0.2

0

0.2

−0.2

0

0.2

0.4

0.6

0.8

Figure 19.7 Solution trajectories of 19.12 for diﬀerent end states xﬁnal. The
outline of the car shows the position (p1(kh), p2(kh)), orientation θ(kh), and
the steering angle φ(kh) at time kh.

Figure 19.7 shows solutions for

L = 0.1,

N = 50,

h = 0.1,

γ = 10,

and diﬀerent values of xﬁnal. They are computed using the augmented Lagrangian
algorithm. The algorithm is started at the same starting point for each example.
The starting point for the input variables uk is randomly chosen, the starting point
for the states xk is zero.

432

19 Constrained nonlinear least squares

Angle

0.5

k
u

0

−0.5

Speed

0.5

k
u

0

−0.5

Speed

Angle

0

10

20

30

40

50

0

10

20

30

40

50

k

k

Angle

0.5

k
u

0

−0.5

Speed

Speed

Angle

0.5

k
u

0

−0.5

0

10

20

30

40

50

0

10

20

30

40

50

k

k

Figure 19.8 The two inputs (speed and steering angle) for the trajectories
in ﬁgure 19.7.

19.4 Nonlinear control

433

Feasibility
Opt. cond.

100

200

400
Cumulative L.-M. iterations

300

102

l
a
u
d

i
s
e
R

10−1

10−4

10−7

0

102

l
a
u
d

i
s
e
R

10−1

10−4

102

l
a
u
d

i
s
e
R

10−1

10−4

10−7

0

102

l
a
u
d

i
s
e
R

10−1

10−4

100

300
Cumulative L.-M. iterations

200

10−7

0

200

400

600

800

Cumulative L.-M. iterations

10−7

0

500

1000

1500

Cumulative L.-M. iterations

Figure 19.9 Feasibility and optimality condition residuals in the augmented
Lagrangian algorithm for computing the trajectories in ﬁgure 19.7.

434

19 Constrained nonlinear least squares

Exercises

19.1 Projection on a curve. We consider a constrained nonlinear least squares problem with

three variables x = (x1, x2, x3) and two equations:

minimize
subject to x2

(x1 − 1)2 + (x2 − 1)2 + (x3 − 1)2
1 + 0.5x2
0.8x2

3 − 1 = 0

2 + x2

1 + 2.5x2

2 + x2

3 + 2x1x3 − x1 − x2 − x3 − 1 = 0.

The solution is the point closest to (1, 1, 1) on the nonlinear curve deﬁned by the two
equations.

(a) Solve the problem using the augmented Lagrangian method. You can start the
algorithm at x(1) = 0, z(1) = 0, µ(1) = 1, and start each run of the Levenberg–
Marquardt method with λ(1) = 1. Stop the augmented Lagrangian method when
the feasibility residual �g(x(k))� and the optimality condition residual

�2Df (x(k))T f (x(k)) + Dg(x(k))T z(k)�

are less than 10−5. Make a plot of the two residuals and of the penalty parameter
µ versus the cumulative number of Levenberg–Marquardt iterations.

(b) Solve the problem using the penalty method, started at x(1) = 0 and µ(1) = 1,
and with the same stopping condition. Compare the convergence and the value of
the penalty parameter with the results for the augmented Lagrangian method in
part (a).

19.2 Portfolio optimization with downside risk.

In standard portfolio optimization (as de-
scribed in §17.1) we choose the weight vector w to achieve a given target mean return,
and to minimize deviations from the target return value (i.e., the risk). This leads to the
constrained linear least squares problem (17.2). One criticism of this formulation is that
it treats portfolio returns that exceed our target value the same as returns that fall short
of our target value, whereas in fact we should be delighted to have a return that exceeds
our target value. To address this deﬁciency in the formulation, researchers have deﬁned
the downside risk of a portfolio return time series T -vector r, which is sensitive only to
portfolio returns that fall short of our target value ρtar. The downside risk of a portfolio
return time series (T -vector) r is given by

D =

1
T

T�t=1�max{ρtar − rt, 0}�2

.

The quantity max{ρtar − rt, 0} is the shortfall, i.e., the amount by which the return in
period t falls short of the target; it is zero when the return exceeds the target value. The
downside risk is the mean square value of the return shortfall.

(a) Formulate the portfolio optimization problem, using downside risk in place of the
usual risk, as a constrained nonlinear least squares problem. Be sure to explain what
the functions f and g are.

(b) Since the function g is aﬃne (if you formulate the problem correctly), you can use the
Levenberg–Marquardt algorithm, modiﬁed to handle linear equality constraints, to
approximately solve the problem. (See page 420.) Find an expression for Df (x(k)).
You can ignore the fact that the function f is not diﬀerentiable at some points.

(c) Implement the Levenberg–Marquardt algorithm to ﬁnd weights that minimize down-
side risk for a given target annualized return. A very reasonable starting point is the
solution of the standard portfolio optimization problem with the same target return.
Check your implementation with some simulated or real return data (available on-
line). Compare the weights, and the risk and downside risk, for the minimum risk
and the minimum downside risk portfolios.

Exercises

435

19.3 Boolean least squares. The Boolean least squares problem is a special case of the con-

strained nonlinear least squares problem (19.1), with the form

minimize
subject to x2

�Ax − b�2
i = 1,

i = 1, . . . , n,

where the n-vector x is the variable to be chosen, and the m × n matrix A and the m-
vector b are the (given) problem data. The constraints require that each entry of x is
either −1 or +1, i.e., x is a Boolean vector. Since each entry can take one of two values,
there are 2n feasible values for the vector x. The Boolean least squares problem arises in
many applications.
One simple method for solving the Boolean least squares problem, sometimes called the
brute force method, is to evaluate the objective function �Ax − b�2 for each of the 2n
possible values, and choose one that has the least value. This method is not practical for
n larger than 30 or so. There are many heuristic methods that are much faster to carry
out than the brute force method, and approximately solve it, i.e., ﬁnd an x for which the
objective is small, if not the smallest possible value over all 2n feasible values of x. One
such heuristic is the augmented Lagrangian algorithm 19.2.

(a) Work out the details of the update step in the Levenberg–Marquardt algorithm
used in each iteration of the augmented Lagrangian algorithm, for the Boolean least
squares problem.

(b) Implement the augmented Lagrangian algorithm for the Boolean least squares prob-
lem. You can choose the starting point x(1) as the minimizer of �Ax − b�2. At
each iteration, you can obtain a feasible point ˜x(k) by rounding the entries of x(k)
to the values ±1, i.e., ˜x(k) = sign(x(k)). You should evaluate and plot the objective
value of these feasible points, i.e., �A˜x(k) − b�2. Your implementation can return
the best rounded value found during the iterations. Try your method on some small
problems, with n = m = 10 (say), for which you can ﬁnd the actual solution by the
brute force method. Try it on much larger problems, with n = m = 500 (say), for
which the brute force method is not practical.

Appendices

Appendix A

Notation

Vectors

x1
...
xn





(x1, . . . , xn)
xi
xr:s
0
1
ei
xT y
�x�
rms(x)
avg(x)
std(x)
dist(x, y)
� (x, y)
x ⊥ y

Matrices



X11
...
Xm1

··· X1n
...
··· Xmn

Xij
Xr:s,p:q
0
I
X T

n-vector with entries x1, . . . , xn.

n-vector with entries x1, . . . , xn.
The ith entry of a vector x.
Subvector with entries from r to s.
Vector with all entries zero.
Vector with all entries one.
The ith standard unit vector.
Inner product of vectors x and y.
Norm of vector x.
Root-mean-square value of a vector x.
Average of entries of a vector x.
Standard deviation of a vector x.
Distance between vectors x and y.
Angle between vectors x and y.
Vectors x and y are orthogonal.

 m × n matrix with entries X11, . . . , Xmn.

The i, jth entry of a matrix X.
Submatrix with rows r, . . . , s and columns p . . . , q.
Matrix with all entries zero.
Identity matrix.
Transpose of matrix X.

3

3
3
4
5
5
5
19
45
46
20
52
48
56
58

107

107
109
113
113
115

440

A Notation

�X�
X k
X−1
X−T
X†
diag(x)

Norm of matrix X.
(Square) matrix X to the kth power.
Inverse of (square) matrix X.
Inverse of transpose of matrix X.
Pseudo-inverse of matrix X.
Diagonal matrix with diagonal entries x1, . . . , xn.

117
186
202
205
215
114

Functions and derivatives

f : A → B
∇f (z)
Df (z)

f is a function on the set A into the set B.
Gradient of function f : Rn → R at z.
Derivative (Jacobian) matrix of function f : Rn → Rm at z.

Ellipsis notation

In this book we use standard mathematical ellipsis notation in lists and sums. We
write k, . . . , l to mean the list of all integers from k to l. For example, 3, . . . , 7
means 3, 4, 5, 6, 7. This notation is used to describe a list of numbers or vectors,

or in sums, as in�i=1,...,n ai, which we also write as a1 + ··· + an. Both of these

mean the sum of the n terms a1, a2, . . . , an.

Sets

In a few places in this book we encounter the mathematical concept of sets. The
notation {a1, . . . , an} refers to a set with elements a1, . . . , an. This is not the same
as the vector with entries a1, . . . , an, which is denoted (a1, . . . , an). For sets the
order does not matter, so, for example, we have {1, 2, 6} = {6, 1, 2}. Unlike a
vector, a set cannot have repeated elements. We can also specify a set by giving
conditions that its entries must satisfy, using the notation {x | condition(x)}, which
means the set of x that satisfy the condition, which depends on x. We say that a set
contains its elements, or that the elements are in the set, using the symbol ∈, as in
2 ∈ {1, 2, 6}. The symbol �∈ means not in, or not an element of, as in 3 �∈ {1, 2, 6}.
We can use sets to describe a sum over some elements in a list. The notation

�i∈S xi means the sum over all xi for which i is in the set S. As an example,
�i∈{1,2,6}
A few sets have speciﬁc names: R is the set of real numbers (or scalars), and
Rn is the set of all n-vectors. So α ∈ R means that α is a number, and x ∈ Rn
means that x is an n-vector.

ai means a1 + a2 + a6.

Appendix B

Complexity

Here we summarize approximate complexities or ﬂop counts of various operations
and algorithms encountered in the book. We drop terms of lower order. When
the operands or arguments are sparse, and the operation or algorithm has been
modiﬁed to take advantage of sparsity, the ﬂop counts can be dramatically lower
than those given here.

Vector operations

In the table below, x and y are n-vectors and a is a scalar.

ax
x + y
xT y
�x�
�x − y�
rms(x)
std(x)
� (x, y)

n
n
2n
2n
3n
2n
4n
6n

The convolution a∗ b of an n-vector a and m-vector b can be computed by a special
algorithm that requires 5(m + n) log2(m + n) ﬂops.

Matrix operations

In the table below, A and B are m × n matrices, C is an m × p matrix, x is an
n-vector, and a is a scalar.

aA
A + B
Ax
AC
AT A
�A�

mn
mn
2mn
2mnp
mn2
2mn

442

B Complexity

Factorization and inverses
In the table below, A is a tall or square m × n matrix, R is an n × n triangular
matrix, and b is an n-vector. We assume the factorization or inverses exist; in
particular in any expression involving A−1, A must be square.

QR factorization of A
R−1b
A−1b
A−1
A†

2mn2
n2
2n3
3n3
3mn2

The pseudo-inverse A† of a wide m × n matrix (with linearly independent rows)
can be computed in 3m2n ﬂops.

Solving least squares problems
In the table below, A is an m × n matrix, C is a wide p × n matrix, and b is an
m-vector. We assume the associated independence conditions hold.

minimize �Ax − b�2
minimize �Ax − b�2 subject to Cx = d
minimize �x�2 subject to Cx = d
Big-times-small-squared mnemonic

2mn2
2(m + p)n2 + 2np2
2np2

Many of the complexities listed above that involve two dimensions can be remem-
bered using a simple mnemonic: The cost is order
(big) × (small)2 ﬂops,

where ‘big’ and ‘small’ refer to big and small problem dimensions. We list some
examples below.

m is the big dimension and n is the small dimension.

• Computing the Gram matrix of a tall m× n matrix requires mn2 ﬂops. Here
• In the QR factorization of an m × n matrix, we have m ≥ n, so m is the big
• Computing the pseudo-inverse A† of an m × n matrix A when A is tall
(and has independent columns) costs 3mn2 ﬂops. When A is wide (and has
independent rows), it is 3nm2 ﬂops.

dimension and n is the small dimension. The complexity is 2mn2 ﬂops.

• For least squares, we have m ≥ n, so m is the big dimension and n is the small
dimension. The cost of computing the least squares approximate solution is
2mn2 ﬂops.

is the small dimension. The cost is 2np2 ﬂops.

• For the least norm problem, we have p ≤ n, so n is the big dimension and p
• The constrained least squares problem involves two matrices A and C, and
three dimensions that satisfy m + p ≥ n. The numbers m + p and n are the

big and small dimensions of the stacked matrix� A

C �. The cost of solving

the constrained least squares problem is 2(m + p)n2 + 2np2 ﬂops, which is
between 2(m + p)n2 ﬂops and 4(m + p)n2 ﬂops, since n ≤ m + p.

Appendix C

Derivatives and optimization

Calculus does not play a big role in this book, except in chapters 18 and 19 (on
nonlinear least squares and constrained nonlinear least squares), where we use
derivatives, Taylor approximations, and the method of Lagrange multipliers. In
this appendix we collect some basic material about derivatives and optimization,
focusing on the few results and formulas we use.

C.1 Derivatives

C.1.1 Scalar-valued function of a scalar

Deﬁnition. Suppose f : R → R is a real-valued function of a real (scalar) variable.
For any number x, the number f (x) is the value of the function, and x is called
the argument of the function. The number

f (z + t) − f (z)

t

,

lim
t→0

(if the limit exists) is called the derivative of the function f at the point z. It gives
the slope of the graph of f at the point (z, f (z)). We denote the derivative of f at
z as f�(z). We can think of f� as a scalar-valued function of a scalar variable; this
function is called the derivative (function) of f .

Taylor approximation. Let us ﬁx the number z. The (ﬁrst order) Taylor approx-
imation of the function f at the point z is deﬁned as

ˆf (x) = f (z) + f�(z)(x − z)

for any x. Here f (z) is the value of f at z, x − z is the deviation of x from z,
and f�(z)(x − z) is the approximation of the change in value of the function due
to the deviation of x from z. Sometimes the Taylor approximation is shown with

444

C Derivatives and optimization

a second argument, separated by a semicolon, to denote the point z where the ap-
proximation is made. Using this notation, the left-hand side of the equation above
is written ˆf (x; z). The Taylor approximation is sometimes called the linearized
approximation of f at z. (Here linear uses informal mathematical language, where
aﬃne is sometimes called linear.) The Taylor approximation function ˆf is an aﬃne
function of x, i.e., a linear function of x plus a constant.

The Taylor approximation ˆf satisﬁes ˆf (z; z) = f (z), i.e., at the point z it agrees
with the function f . For x near z, ˆf (x; z) is a very good approximation of f (x).
For x not close enough to z, however, the approximation can be poor.

Finding derivatives.
In a basic calculus course, the derivatives of many common
functions are worked out. For example, with f (x) = x2, we have f�(z) = 2z, and
for f (x) = ex, we have f�(z) = ez. Derivatives of more complex functions can be
found using these known derivatives of common functions, along with a few rules
for ﬁnding the derivative of various combinations of functions. For example, the
chain rule gives the derivative of a composition of two functions. If f (x) = g(h(x)),
where g and h are scalar-valued functions of a scalar variable, we have

f�(z) = g�(h(z))h�(z).

Another useful rule is the derivative of product rule, for f (x) = g(x)h(x), which is

f�(z) = g�(z)h(z) + g(z)h�(z).

The derivative operation is linear, which means that if f (x) = ag(x) + bh(x), where
a and b are constants, we have

f�(z) = ag�(z) + bh�(z).

Knowledge of the derivative of just a few common functions, and a few combination
rules like the ones above, is enough to determine the derivatives of many functions.

C.1.2 Scalar-valued function of a vector

Suppose f : Rn → R is a scalar-valued function of an n-vector argument. The
number f (x) is the value of the function f at the n-vector (argument) x. We
sometimes write out the argument of f to show that it can be considered a function
of n scalar arguments, x1, . . . , xn:

f (x) = f (x1, . . . , xn).

Partial derivative. The partial derivative of f at the point z, with respect to its
ith argument, is deﬁned as

∂f
∂xi

(z) = lim
t→0

= lim
t→0

f (z1, . . . , zi−1, zi + t, zi+1, . . . , zn) − f (z)

t

f (z + tei) − f (z)

t

,

(if the limit exists). Roughly speaking the partial derivative is the derivative with
respect to the ith argument, with all other arguments ﬁxed.

C.1 Derivatives

445

Gradient. The partial derivatives of f with respect to its n arguments can be
collected into an n vector called the gradient of f (at z):

∇f (z) =

∂f
∂x1

(z)
...

∂f
∂xn

(z)

 .

Taylor approximation. The (ﬁrst-order) Taylor approximation of f at the point
z is the function ˆf : Rn → R deﬁned as

ˆf (x) = f (z) +

∂f
∂x1

(z)(x1 − z1) + ··· +

∂f
∂xn

(z)(xn − zn)

for any x. We interpret xi − zi as the deviation of xi from zi, and the term
∂f
(z)(xi − zi) as an approximation of the change in f due to the deviation of xi
∂xi
from zi. Sometimes ˆf is written with a second vector argument, as ˆf (x; z), to show
the point z at which the approximation is developed. The Taylor approximation
can be written in compact form as

ˆf (x; z) = f (z) + ∇f (z)T (x − z).

The Taylor approximation ˆf is an aﬃne function of x.

The Taylor approximation ˆf agrees with the function f at the point z, i.e.,
ˆf (z; z) = f (z). When all xi are near the associated zi, ˆf (x; z) is a very good
approximation of f (x). The Taylor approximation is sometimes called the linear
approximation or linearized approximation of f (at z), even though in general it is
aﬃne, and not linear.

Finding gradients. The gradient of a function can be found by evaluating the
partial derivatives using the common functions and rules for derivatives of scalar-
valued functions, and assembling the result into a vector. In many cases the result
can be expressed in a more compact matrix-vector form. As an example let us ﬁnd
the gradient of the function

which is the sum of squares of the arguments. The partial derivatives are

f (x) = �x�2 = x2

1 + ··· + x2
n,

∂f
∂xi

(z) = 2zi,

i = 1, . . . , n.

This leads to the very simple vector formula

∇f (z) = 2z.

(Note the resemblance to the formula for the derivative of the square of a scalar
variable.)

There are rules for the gradient of a combination of functions similar to those

for functions of a scalar. For example if f (x) = ag(x) + bh(x), we have

∇f (z) = a∇g(z) + b∇h(z).

446

C Derivatives and optimization

C.1.3 Vector-valued function of a vector

Suppose f : Rn → Rm is a vector-valued function of a vector. The n-vector x is
the argument; the m-vector f (x) is the value of the function f at x. We can write
out the m components of f as

f (x) =

f1(x)

...

fm(x)

 ,

where fi is a scalar-valued function of x = (x1, . . . , xn).

Jacobian. The partial derivatives of the components of f (x) with respect to the
components of x, evaluated at z, are arranged into an m×n matrix denoted Df (z),
called the derivative matrix or Jacobian of f at z. (In the notation Df (z), the
D and f go together; Df does not represent, say, a matrix-vector product.) The
derivative matrix is deﬁned by

Df (z)ij =

∂fi
∂xj

(z),

i = 1, . . . , m,

j = 1, . . . , n.

The rows of the Jacobian are ∇fi(z)T , for i = 1, . . . , m. For m = 1, i.e., when
f is a scalar-valued function, the derivative matrix is a row vector of size n, the
transpose of the gradient of the function. The derivative matrix of a vector-valued
function of a vector is a generalization of the derivative of a scalar-valued function
of a scalar.

Taylor approximation. The (ﬁrst-order) Taylor approximation of f near z is given
by

ˆf (x)i = fi(z) +

∂fi
∂x1

(z)(x1 − z1) + ··· +

∂fi
∂xn

(z)(xn − zn)

= fi(z) + ∇fi(z)T (x − z),

for i = 1, . . . , m. We can express this approximation in compact notation as

ˆf (x) = f (z) + Df (z)(x − z).

For x near z, ˆf (x) is a very good approximation of f (x). As in the scalar case, the
Taylor approximation is sometimes written with a second argument as ˆf (x; z) to
show the point z around which the approximation is made. The Taylor approxi-
mation ˆf is an aﬃne function of x, sometimes called a linear approximation of f ,
even though it is not, in general, a linear function.

Finding Jacobians. We can always ﬁnd the derivative matrix by calculating par-
tial derivatives of the entries of f with respect to the components of the argument
vector. In many cases the result simpliﬁes using matrix-vector notation. As an
example, let us ﬁnd the derivative of the (scalar-valued) function

h(x) = �f (x)�2 = f1(x)2 + ··· + fm(x)2,

C.2 Optimization

447

where f : Rn → Rm. The partial derivative with respect to xj, at z, is

∂h
∂xj

(z) = 2f1(z)

∂f1
∂xj

(z) + ··· + 2fm(z)

∂fm
∂xj

(z).

Arranging these to form the row vector Dh(z), we see we can write this using
matrix multiplication as

Dh(z) = 2f (z)T Df (z).

The gradient of h is the transpose of this expression,
∇h(z) = 2Df (z)T f (z).

(C.1)

(Note the analogy to the formula for the scalar-valued function of a scalar variable
h(x) = f (x)2, which is h�(z) = 2f�(z)f (z).)

Many of the formulas for derivatives in the scalar case also hold for the vector
case, with scalar multiplication replaced with matrix multiplication (provided the
order of the terms is correct). As an example, consider the composition function
f (x) = g(h(x)), where h : Rn → Rk and g : Rk → Rm. The Jacobian or derivative
matrix of f at z is given by

Df (z) = Dg(h(z))Dh(z).

(This is matrix multiplication; compare it to composition formula for scalar-valued
functions of scalars given above.) This chain rule is described on page 184.

C.2 Optimization

Derivative condition for minimization. Suppose h is a scalar-valued function of
a scalar argument. If ˆx minimizes h(x), we must have h�(ˆx) = 0. This fact is easily
understood: If h�(ˆx) �= 0, then by taking a point ˜x slightly less than ˆx (if h�(ˆx) > 0)
or slightly more than ˆx (if h�(ˆx) < 0), we would obtain h(˜x) < h(ˆx), which shows
that ˆx does not minimize h(x). This leads to the classic calculus-based method
for ﬁnding a minimizer of a function f : Find the derivative, and set it equal to
zero. One subtlety here is that there can be (and generally are) points that satisfy
h�(z) = 0, but are not minimizers of h. So we generally need to check which of the
solutions of h�(z) = 0 are in fact minimizers of h.

Gradient condition for minimization. This basic calculus-based method for ﬁnd-
ing a minimizer of a scalar-valued function can be generalized to functions with
vector arguments. If the n-vector ˆx minimizes h : Rn → R, then we must have

∂h
∂xi

(ˆx) = 0,

i = 1, . . . , n.

In vector notation, we must have

∇h(ˆx) = 0.

448

C Derivatives and optimization

Like the case of a scalar argument, this is easily seen to hold if ˆx minimizes h. Also
as in the case of a scalar argument, there can be points that satisfy ∇h(z) = 0 but
are not minimizers of h. So we need to check if points found this way are in fact
minimizers of h.

Nonlinear least squares. As an example, consider the nonlinear least squares
problem, with objective h(x) = �f (x)�2, where f : Rn → Rm. The optimality
condition ∇h(ˆx) = 0 is

2Df (ˆx)T f (ˆx) = 0

(using the expression (C.1) for the gradient, derived above). This equation will
hold for a minimizer, but there can be points that satisfy the equation, but are not
solutions of the nonlinear least squares problem.

C.3 Lagrange multipliers

Constrained optimization. We now consider the problem of minimizing a scalar-
valued function h : Rn → R, subject to the requirements, or constraints, that

g1(x) = 0,

. . . , gp(x) = 0

must hold, where gi : Rn → R are given functions. We can write the constraints
in compact vector form g(x) = 0, where g(x) = (g1(x), . . . , gp(x)), and express the
problem as

minimize
subject to

f (x)
g(x) = 0.

We seek a solution of this optimization problem, i.e., a point ˆx that satisﬁes g(ˆx) =
0 (i.e., is feasible) and, for any other x that satisﬁes g(x) = 0, we have h(x) ≥ h(ˆx).
The method of Lagrange multipliers is an extension of the derivative or gradient
conditions for (unconstrained) minimization, that handles constrained optimization
problems.

Lagrange multipliers. The Lagrangian function associated with the constrained
problem is deﬁned as

L(x, z) = h(x) + z1g1(x) + ··· + zpgp(x)

= h(x) + g(x)T z,

with arguments x (the original variable to be determined in the optimization prob-
lem), and a p-vector z, called the (vector of) Lagrange multipliers. The Lagrangian
function is the original objective, with one term added for each constraint func-
tion. Each term is the constraint function value multiplied by zi, hence the name
multiplier.

C.3 Lagrange multipliers

449

KKT conditions. The KKT conditions (named for Karush, Kuhn, and Tucker)
state that if ˆx is a solution of the constrained optimization problem, then there is
a vector ˆz that satisﬁes

∂L
∂xi

(ˆx, ˆz) = 0,

i = 1, . . . , n,

∂L
∂zi

(ˆx, ˆz) = 0,

i = 1, . . . , p.

(This is provided the rows of Dg(ˆx) are linearly independent, a technical condition
we ignore.) As in the unconstrained case, there can be pairs x, z that satisfy the
KKT conditions but ˆx is not a solution of the constrained optimization problem.
The KKT conditions give us a method for solving the constrained optimization
problem that is similar to the approach for the unconstrained optimization problem.
We attempt to solve the KKT equations for ˆx and ˆz; then we check to see if any
of the points found are really solutions.

We can simplify the KKT conditions, and express them compactly using matrix
notation. The last p equations can be expressed as gi(ˆx) = 0, which we already
knew. The ﬁrst n can be expressed as

∇xL(ˆx, ˆz) = 0,

where ∇x denotes the gradient with respect to the xi arguments. This can be
written as

∇h(ˆx) + ˆz1∇g1(ˆx) + ··· + ˆzpgp(ˆx) = ∇h(ˆx) + Dg(ˆx)T ˆz = 0.
So the KKT conditions for the constrained optimization problem are

∇h(ˆx) + Dg(ˆx)T ˆz = 0,

g(ˆx) = 0.

This is the extension of the gradient condition for unconstrained optimization to
the constrained case.

Constrained nonlinear least squares. As an example, consider the constrained
least squares problem

minimize
subject to

�f (x)�2
g(x) = 0,

where f : Rn → Rm and g : Rn → Rp. Deﬁne h(x) = �f (x)�2. Its gradient at ˆx
is 2Df (ˆx)T f (ˆx) (see above) so the KKT conditions are

2Df (ˆx)T f (ˆx) + Dg(ˆx)T ˆz = 0,

g(ˆx) = 0.

These conditions will hold for a solution of the problem (assuming the rows of
Dg(ˆx) are linearly independent). But there can be points that satisfy them and
are not solutions.

