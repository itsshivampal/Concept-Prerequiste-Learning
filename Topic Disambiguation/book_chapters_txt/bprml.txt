1

Introduction

The problem of searching for patterns in data is a fundamental one and has a long and
successful history. For instance, the extensive astronomical observations of Tycho
Brahe in the 16th century allowed Johannes Kepler to discover the empirical laws of
planetary motion, which in turn provided a springboard for the development of clas-
sical mechanics. Similarly, the discovery of regularities in atomic spectra played a
key role in the development and veriﬁcation of quantum physics in the early twenti-
eth century. The ﬁeld of pattern recognition is concerned with the automatic discov-
ery of regularities in data through the use of computer algorithms and with the use of
these regularities to take actions such as classifying the data into different categories.
Consider the example of recognizing handwritten digits, illustrated in Figure 1.1.
Each digit corresponds to a 28×28 pixel image and so can be represented by a vector
x comprising 784 real numbers. The goal is to build a machine that will take such a
vector x as input and that will produce the identity of the digit 0, . . . , 9 as the output.
This is a nontrivial problem due to the wide variability of handwriting. It could be

1

2

1. INTRODUCTION

Figure 1.1 Examples of hand-written dig-

its taken from US zip codes.

tackled using handcrafted rules or heuristics for distinguishing the digits based on
the shapes of the strokes, but in practice such an approach leads to a proliferation of
rules and of exceptions to the rules and so on, and invariably gives poor results.

Far better results can be obtained by adopting a machine learning approach in
which a large set of N digits {x1, . . . , xN} called a training set is used to tune the
parameters of an adaptive model. The categories of the digits in the training set
are known in advance, typically by inspecting them individually and hand-labelling
them. We can express the category of a digit using target vector t, which represents
the identity of the corresponding digit. Suitable techniques for representing cate-
gories in terms of vectors will be discussed later. Note that there is one such target
vector t for each digit image x.

The result of running the machine learning algorithm can be expressed as a
function y(x) which takes a new digit image x as input and that generates an output
vector y, encoded in the same way as the target vectors. The precise form of the
function y(x) is determined during the training phase, also known as the learning
phase, on the basis of the training data. Once the model is trained it can then de-
termine the identity of new digit images, which are said to comprise a test set. The
ability to categorize correctly new examples that differ from those used for train-
ing is known as generalization. In practical applications, the variability of the input
vectors will be such that the training data can comprise only a tiny fraction of all
possible input vectors, and so generalization is a central goal in pattern recognition.
For most practical applications, the original input variables are typically prepro-
cessed to transform them into some new space of variables where, it is hoped, the
pattern recognition problem will be easier to solve. For instance, in the digit recogni-
tion problem, the images of the digits are typically translated and scaled so that each
digit is contained within a box of a ﬁxed size. This greatly reduces the variability
within each digit class, because the location and scale of all the digits are now the
same, which makes it much easier for a subsequent pattern recognition algorithm
to distinguish between the different classes. This pre-processing stage is sometimes
also called feature extraction. Note that new test data must be pre-processed using
the same steps as the training data.

Pre-processing might also be performed in order to speed up computation. For
example, if the goal is real-time face detection in a high-resolution video stream,
the computer must handle huge numbers of pixels per second, and presenting these
directly to a complex pattern recognition algorithm may be computationally infeasi-
ble. Instead, the aim is to ﬁnd useful features that are fast to compute, and yet that

1. INTRODUCTION

3

also preserve useful discriminatory information enabling faces to be distinguished
from non-faces. These features are then used as the inputs to the pattern recognition
algorithm. For instance, the average value of the image intensity over a rectangular
subregion can be evaluated extremely efﬁciently (Viola and Jones, 2004), and a set of
such features can prove very effective in fast face detection. Because the number of
such features is smaller than the number of pixels, this kind of pre-processing repre-
sents a form of dimensionality reduction. Care must be taken during pre-processing
because often information is discarded, and if this information is important to the
solution of the problem then the overall accuracy of the system can suffer.

Applications in which the training data comprises examples of the input vectors
along with their corresponding target vectors are known as supervised learning prob-
lems. Cases such as the digit recognition example, in which the aim is to assign each
input vector to one of a ﬁnite number of discrete categories, are called classiﬁcation
problems. If the desired output consists of one or more continuous variables, then
the task is called regression. An example of a regression problem would be the pre-
diction of the yield in a chemical manufacturing process in which the inputs consist
of the concentrations of reactants, the temperature, and the pressure.

In other pattern recognition problems, the training data consists of a set of input
vectors x without any corresponding target values. The goal in such unsupervised
learning problems may be to discover groups of similar examples within the data,
where it is called clustering, or to determine the distribution of data within the input
space, known as density estimation, or to project the data from a high-dimensional
space down to two or three dimensions for the purpose of visualization.

Finally, the technique of reinforcement learning (Sutton and Barto, 1998) is con-
cerned with the problem of ﬁnding suitable actions to take in a given situation in
order to maximize a reward. Here the learning algorithm is not given examples of
optimal outputs, in contrast to supervised learning, but must instead discover them
by a process of trial and error. Typically there is a sequence of states and actions in
which the learning algorithm is interacting with its environment. In many cases, the
current action not only affects the immediate reward but also has an impact on the re-
ward at all subsequent time steps. For example, by using appropriate reinforcement
learning techniques a neural network can learn to play the game of backgammon to a
high standard (Tesauro, 1994). Here the network must learn to take a board position
as input, along with the result of a dice throw, and produce a strong move as the
output. This is done by having the network play against a copy of itself for perhaps a
million games. A major challenge is that a game of backgammon can involve dozens
of moves, and yet it is only at the end of the game that the reward, in the form of
victory, is achieved. The reward must then be attributed appropriately to all of the
moves that led to it, even though some moves will have been good ones and others
less so. This is an example of a credit assignment problem. A general feature of re-
inforcement learning is the trade-off between exploration, in which the system tries
out new kinds of actions to see how effective they are, and exploitation, in which
the system makes use of actions that are known to yield a high reward. Too strong
a focus on either exploration or exploitation will yield poor results. Reinforcement
learning continues to be an active area of machine learning research. However, a

4

1. INTRODUCTION

Figure 1.2 Plot of a training data set of N =
10 points, shown as blue circles,
each comprising an observation
of the input variable x along with
the corresponding target variable
t. The green curve shows the
function sin(2πx) used to gener-
ate the data. Our goal is to pre-
dict the value of t for some new
value of x, without knowledge of
the green curve.

t

1

0

−1

0

1

x

detailed treatment lies beyond the scope of this book.

Although each of these tasks needs its own tools and techniques, many of the
key ideas that underpin them are common to all such problems. One of the main
goals of this chapter is to introduce, in a relatively informal way, several of the most
important of these concepts and to illustrate them using simple examples. Later in
the book we shall see these same ideas re-emerge in the context of more sophisti-
cated models that are applicable to real-world pattern recognition applications. This
chapter also provides a self-contained introduction to three important tools that will
be used throughout the book, namely probability theory, decision theory, and infor-
mation theory. Although these might sound like daunting topics, they are in fact
straightforward, and a clear understanding of them is essential if machine learning
techniques are to be used to best effect in practical applications.

1.1. Example: Polynomial Curve Fitting

We begin by introducing a simple regression problem, which we shall use as a run-
ning example throughout this chapter to motivate a number of key concepts. Sup-
pose we observe a real-valued input variable x and we wish to use this observation to
predict the value of a real-valued target variable t. For the present purposes, it is in-
structive to consider an artiﬁcial example using synthetically generated data because
we then know the precise process that generated the data for comparison against any
learned model. The data for this example is generated from the function sin(2πx)
with random noise included in the target values, as described in detail in Appendix A.
Now suppose that we are given a training set comprising N observations of x,
written x ≡ (x1, . . . , xN )T, together with corresponding observations of the values
of t, denoted t ≡ (t1, . . . , tN )T. Figure 1.2 shows a plot of a training set comprising
N = 10 data points. The input data set x in Figure 1.2 was generated by choos-
ing values of xn, for n = 1, . . . , N, spaced uniformly in range [0, 1], and the target
data set t was obtained by ﬁrst computing the corresponding values of the function





5



M







N

1.1. Example: Polynomial Curve Fitting

sin(2πx) and then adding a small level of random noise having a Gaussian distri-
bution (the Gaussian distribution is discussed in Section 1.2.4) to each such point in
order to obtain the corresponding value tn. By generating data in this way, we are
capturing a property of many real data sets, namely that they possess an underlying
regularity, which we wish to learn, but that individual observations are corrupted by
random noise. This noise might arise from intrinsically stochastic (i.e. random) pro-
cesses such as radioactive decay but more typically is due to there being sources of
variability that are themselves unobserved.

Our goal is to exploit this training set in order to make predictions of the value
t of the target variable for some new value
x of the input variable. As we shall see
later, this involves implicitly trying to discover the underlying function sin(2πx).
This is intrinsically a difﬁcult problem as we have to generalize from a ﬁnite data
set. Furthermore the observed data are corrupted with noise, and so for a given
x
t. Probability theory, discussed
there is uncertainty as to the appropriate value for
in Section 1.2, provides a framework for expressing such uncertainty in a precise
and quantitative manner, and decision theory, discussed in Section 1.5, allows us to
exploit this probabilistic representation in order to make predictions that are optimal
according to appropriate criteria.

For the moment, however, we shall proceed rather informally and consider a
simple approach based on curve ﬁtting. In particular, we shall ﬁt the data using a
polynomial function of the form

y(x, w) = w0 + w1x + w2x2 + . . . + wM xM =

wjxj

(1.1)

j=0

where M is the order of the polynomial, and xj denotes x raised to the power of j.
The polynomial coefﬁcients w0, . . . , wM are collectively denoted by the vector w.
Note that, although the polynomial function y(x, w) is a nonlinear function of x, it
is a linear function of the coefﬁcients w. Functions, such as the polynomial, which
are linear in the unknown parameters have important properties and are called linear
models and will be discussed extensively in Chapters 3 and 4.

The values of the coefﬁcients will be determined by ﬁtting the polynomial to the
training data. This can be done by minimizing an error function that measures the
misﬁt between the function y(x, w), for any given value of w, and the training set
data points. One simple choice of error function, which is widely used, is given by
the sum of the squares of the errors between the predictions y(xn, w) for each data
point xn and the corresponding target values tn, so that we minimize

E(w) =

1
2

{y(xn, w) − tn}2

n=1

(1.2)

where the factor of 1/2 is included for later convenience. We shall discuss the mo-
tivation for this choice of error function later in this chapter. For the moment we
simply note that it is a nonnegative quantity that would be zero if, and only if, the

6

1. INTRODUCTION

Figure 1.3 The error

function (1.2) corre-
sponds to (one half of) the sum of
the squares of the displacements
(shown by the vertical green bars)
of each data point from the function
y(x, w).

t

tn

y(xn, w)

xn

x

function y(x, w) were to pass exactly through each training data point. The geomet-
rical interpretation of the sum-of-squares error function is illustrated in Figure 1.3.

We can solve the curve ﬁtting problem by choosing the value of w for which
E(w) is as small as possible. Because the error function is a quadratic function of
the coefﬁcients w, its derivatives with respect to the coefﬁcients will be linear in the
elements of w, and so the minimization of the error function has a unique solution,
denoted by w, which can be found in closed form. The resulting polynomial is
given by the function y(x, w).

Exercise 1.1

There remains the problem of choosing the order M of the polynomial, and as
we shall see this will turn out to be an example of an important concept called model
comparison or model selection. In Figure 1.4, we show four examples of the results
of ﬁtting polynomials having orders M = 0, 1, 3, and 9 to the data set shown in
Figure 1.2.

We notice that the constant (M = 0) and ﬁrst order (M = 1) polynomials
give rather poor ﬁts to the data and consequently rather poor representations of the
function sin(2πx). The third order (M = 3) polynomial seems to give the best ﬁt
to the function sin(2πx) of the examples shown in Figure 1.4. When we go to a
much higher order polynomial (M = 9), we obtain an excellent ﬁt to the training
data. In fact, the polynomial passes exactly through each data point and E(w) = 0.
However, the ﬁtted curve oscillates wildly and gives a very poor representation of
the function sin(2πx). This latter behaviour is known as over-ﬁtting.

As we have noted earlier, the goal is to achieve good generalization by making
accurate predictions for new data. We can obtain some quantitative insight into the
dependence of the generalization performance on M by considering a separate test
set comprising 100 data points generated using exactly the same procedure used
to generate the training set points but with new choices for the random noise values
included in the target values. For each choice of M, we can then evaluate the residual
value of E(w) given by (1.2) for the training data, and we can also evaluate E(w)
for the test data set. It is sometimes more convenient to use the root-mean-square



Figure 1.4 Plots of polynomials having various orders M, shown as red curves, ﬁtted to the data set shown in
Figure 1.2.

(RMS) error deﬁned by

ERMS =

2E(w)/N

(1.3)

in which the division by N allows us to compare different sizes of data sets on
an equal footing, and the square root ensures that ERMS is measured on the same
scale (and in the same units) as the target variable t. Graphs of the training and
test set RMS errors are shown, for various values of M, in Figure 1.5. The test
set error is a measure of how well we are doing in predicting the values of t for
new data observations of x. We note from Figure 1.5 that small values of M give
relatively large values of the test set error, and this can be attributed to the fact that
the corresponding polynomials are rather inﬂexible and are incapable of capturing
the oscillations in the function sin(2πx). Values of M in the range 3 � M � 8
give small values for the test set error, and these also give reasonable representations
of the generating function sin(2πx), as can be seen, for the case of M = 3, from
Figure 1.4.

1.1. Example: Polynomial Curve Fitting

7

t

1

0

−1

t

1

0

−1

0

0

M = 0

t

1

0

−1

1

x

0

M = 3

t

1

0

−1

1

x

0

M = 1

1

x

M = 9

1

x

8

1. INTRODUCTION

Figure 1.5 Graphs of

the root-mean-square
error, deﬁned by (1.3), evaluated
on the training set and on an inde-
pendent test set for various values
of M.

Training
Test

1

S
M
R
E

0.5

0

0

3

M

6

9

For M = 9, the training set error goes to zero, as we might expect because
this polynomial contains 10 degrees of freedom corresponding to the 10 coefﬁcients
w0, . . . , w9, and so can be tuned exactly to the 10 data points in the training set.
However, the test set error has become very large and, as we saw in Figure 1.4, the
corresponding function y(x, w) exhibits wild oscillations.

This may seem paradoxical because a polynomial of given order contains all
lower order polynomials as special cases. The M = 9 polynomial is therefore capa-
ble of generating results at least as good as the M = 3 polynomial. Furthermore, we
might suppose that the best predictor of new data would be the function sin(2πx)
from which the data was generated (and we shall see later that this is indeed the
case). We know that a power series expansion of the function sin(2πx) contains
terms of all orders, so we might expect that results should improve monotonically as
we increase M.

We can gain some insight into the problem by examining the values of the co-
efﬁcients w obtained from polynomials of various order, as shown in Table 1.1.
We see that, as M increases, the magnitude of the coefﬁcients typically gets larger.
In particular for the M = 9 polynomial, the coefﬁcients have become ﬁnely tuned
to the data by developing large positive and negative values so that the correspond-

Table 1.1 Table of the coefﬁcients w for
polynomials of various order.
Observe how the typical mag-
nitude of
the coefﬁcients in-
creases dramatically as the or-
der of the polynomial increases.

w
0
w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9

0.19

M = 0 M = 1 M = 6
0.31
7.99
-25.43
17.37

0.82
-1.27

M = 9
0.35
232.37
-5321.83
48568.31
-231639.30
640042.26
-1061800.52
1042400.18
-557682.99
125201.43

t

1

0

−1

0

1.1. Example: Polynomial Curve Fitting

9

N = 100

N = 15

t

1

0

−1

1

x

0

1

x

Figure 1.6 Plots of the solutions obtained by minimizing the sum-of-squares error function using the M = 9
polynomial for N = 15 data points (left plot) and N = 100 data points (right plot). We see that increasing the
size of the data set reduces the over-ﬁtting problem.

ing polynomial function matches each of the data points exactly, but between data
points (particularly near the ends of the range) the function exhibits the large oscilla-
tions observed in Figure 1.4. Intuitively, what is happening is that the more ﬂexible
polynomials with larger values of M are becoming increasingly tuned to the random
noise on the target values.

It is also interesting to examine the behaviour of a given model as the size of the
data set is varied, as shown in Figure 1.6. We see that, for a given model complexity,
the over-ﬁtting problem become less severe as the size of the data set increases.
Another way to say this is that the larger the data set, the more complex (in other
words more ﬂexible) the model that we can afford to ﬁt to the data. One rough
heuristic that is sometimes advocated is that the number of data points should be
no less than some multiple (say 5 or 10) of the number of adaptive parameters in
the model. However, as we shall see in Chapter 3, the number of parameters is not
necessarily the most appropriate measure of model complexity.

Also, there is something rather unsatisfying about having to limit the number of
parameters in a model according to the size of the available training set. It would
seem more reasonable to choose the complexity of the model according to the com-
plexity of the problem being solved. We shall see that the least squares approach
to ﬁnding the model parameters represents a speciﬁc case of maximum likelihood
(discussed in Section 1.2.5), and that the over-ﬁtting problem can be understood as
a general property of maximum likelihood. By adopting a Bayesian approach, the
over-ﬁtting problem can be avoided. We shall see that there is no difﬁculty from
a Bayesian perspective in employing models for which the number of parameters
greatly exceeds the number of data points. Indeed, in a Bayesian model the effective
number of parameters adapts automatically to the size of the data set.

For the moment, however, it is instructive to continue with the current approach
and to consider how in practice we can apply it to data sets of limited size where we

Section 3.4





t

N

1

0

−1

may wish to use relatively complex and ﬂexible models. One technique that is often
used to control the over-ﬁtting phenomenon in such cases is that of regularization,
which involves adding a penalty term to the error function (1.2) in order to discourage
the coefﬁcients from reaching large values. The simplest such penalty term takes the
form of a sum of squares of all of the coefﬁcients, leading to a modiﬁed error function
of the form

E(w) =

1
2

{y(xn, w) − tn}2 + λ

2w2

(1.4)

n=1
1 + . . . + w2
0 + w2

where w2 ≡ wTw = w2
M , and the coefﬁcient λ governs the rel-
ative importance of the regularization term compared with the sum-of-squares error
term. Note that often the coefﬁcient w0 is omitted from the regularizer because its
inclusion causes the results to depend on the choice of origin for the target variable
(Hastie et al., 2001), or it may be included but with its own regularization coefﬁcient
(we shall discuss this topic in more detail in Section 5.5.1). Again, the error function
in (1.4) can be minimized exactly in closed form. Techniques such as this are known
in the statistics literature as shrinkage methods because they reduce the value of the
coefﬁcients. The particular case of a quadratic regularizer is called ridge regres-
sion (Hoerl and Kennard, 1970). In the context of neural networks, this approach is
known as weight decay.

Figure 1.7 shows the results of ﬁtting the polynomial of order M = 9 to the
same data set as before but now using the regularized error function given by (1.4).
We see that, for a value of ln λ = −18, the over-ﬁtting has been suppressed and we
now obtain a much closer representation of the underlying function sin(2πx). If,
however, we use too large a value for λ then we again obtain a poor ﬁt, as shown in
Figure 1.7 for ln λ = 0. The corresponding coefﬁcients from the ﬁtted polynomials
are given in Table 1.2, showing that regularization has the desired effect of reducing

Exercise 1.2

10

1. INTRODUCTION

t

1

0

−1

0

ln λ = −18

1

x

ln λ = 0

0

1

x

Figure 1.7 Plots of M = 9 polynomials ﬁtted to the data set shown in Figure 1.2 using the regularized error
function (1.4) for two values of the regularization parameter λ corresponding to ln λ = −18 and ln λ = 0. The
case of no regularizer, i.e., λ = 0, corresponding to ln λ = −∞, is shown at the bottom right of Figure 1.4.

1.1. Example: Polynomial Curve Fitting

11

Table 1.2 Table of the coefﬁcients w for M =
9 polynomials with various values for
the regularization parameter λ. Note
that ln λ = −∞ corresponds to a
model with no regularization, i.e., to
the graph at the bottom right in Fig-
ure 1.4. We see that, as the value of
λ increases, the typical magnitude of
the coefﬁcients gets smaller.

ln λ = −∞ ln λ = −18
0.35
4.74
-0.77
-31.97
-3.89
55.28
41.32
-45.95
-91.53
72.68

0.35
232.37
-5321.83
48568.31
-231639.30
640042.26
-1061800.52
1042400.18
-557682.99
125201.43

ln λ = 0
0.13
-0.05
-0.06
-0.05
-0.03
-0.02
-0.01
-0.00
0.00
0.01

w
0
w
1
w
2
w
3
w
4
w
5
w
6
w
7
w
8
w
9

the magnitude of the coefﬁcients.

The impact of the regularization term on the generalization error can be seen by
plotting the value of the RMS error (1.3) for both training and test sets against ln λ,
as shown in Figure 1.8. We see that in effect λ now controls the effective complexity
of the model and hence determines the degree of over-ﬁtting.

The issue of model complexity is an important one and will be discussed at
length in Section 1.3. Here we simply note that, if we were trying to solve a practical
application using this approach of minimizing an error function, we would have to
ﬁnd a way to determine a suitable value for the model complexity. The results above
suggest a simple way of achieving this, namely by taking the available data and
partitioning it into a training set, used to determine the coefﬁcients w, and a separate
validation set, also called a hold-out set, used to optimize the model complexity
(either M or λ).
In many cases, however, this will prove to be too wasteful of
valuable training data, and we have to seek more sophisticated approaches.

So far our discussion of polynomial curve ﬁtting has appealed largely to in-
tuition. We now seek a more principled approach to solving problems in pattern
recognition by turning to a discussion of probability theory. As well as providing the
foundation for nearly all of the subsequent developments in this book, it will also

Section 1.3

Figure 1.8 Graph of the root-mean-square er-
ror (1.3) versus ln λ for the M = 9
polynomial.

1

Training
Test

S
M
R
E

0.5

0

−35

−30

−25

ln λ

−20

12

1. INTRODUCTION

give us some important insights into the concepts we have introduced in the con-
text of polynomial curve ﬁtting and will allow us to extend these to more complex
situations.

1.2. Probability Theory

A key concept in the ﬁeld of pattern recognition is that of uncertainty. It arises both
through noise on measurements, as well as through the ﬁnite size of data sets. Prob-
ability theory provides a consistent framework for the quantiﬁcation and manipula-
tion of uncertainty and forms one of the central foundations for pattern recognition.
When combined with decision theory, discussed in Section 1.5, it allows us to make
optimal predictions given all the information available to us, even though that infor-
mation may be incomplete or ambiguous.

We will introduce the basic concepts of probability theory by considering a sim-
ple example. Imagine we have two boxes, one red and one blue, and in the red box
we have 2 apples and 6 oranges, and in the blue box we have 3 apples and 1 orange.
This is illustrated in Figure 1.9. Now suppose we randomly pick one of the boxes
and from that box we randomly select an item of fruit, and having observed which
sort of fruit it is we replace it in the box from which it came. We could imagine
repeating this process many times. Let us suppose that in so doing we pick the red
box 40% of the time and we pick the blue box 60% of the time, and that when we
remove an item of fruit from a box we are equally likely to select any of the pieces
of fruit in the box.

In this example, the identity of the box that will be chosen is a random variable,
which we shall denote by B. This random variable can take one of two possible
values, namely r (corresponding to the red box) or b (corresponding to the blue
box). Similarly, the identity of the fruit is also a random variable and will be denoted
by F . It can take either of the values a (for apple) or o (for orange).

To begin with, we shall deﬁne the probability of an event to be the fraction
of times that event occurs out of the total number of trials, in the limit that the total
number of trials goes to inﬁnity. Thus the probability of selecting the red box is 4/10

Figure 1.9 We use a simple example of two
coloured boxes each containing fruit
(apples shown in green and or-
anges shown in orange) to intro-
duce the basic ideas of probability.

Figure 1.10 We can derive the sum and product rules of probability by
considering two random variables, X, which takes the values {xi} where
i = 1, . . . , M, and Y , which takes the values {yj} where j = 1, . . . , L.
In this illustration we have M = 5 and L = 3.
If we consider a total
number N of instances of these variables, then we denote the number
of instances where X = xi and Y = yj by nij, which is the number of
points in the corresponding cell of the array. The number of points in
column i, corresponding to X = xi, is denoted by ci, and the number of
points in row j, corresponding to Y = yj, is denoted by rj.

yj

1.2. Probability Theory

13

rj

}

ci

}

nij

xi

and the probability of selecting the blue box is 6/10. We write these probabilities
as p(B = r) = 4/10 and p(B = b) = 6/10. Note that, by deﬁnition, probabilities
must lie in the interval [0, 1]. Also, if the events are mutually exclusive and if they
include all possible outcomes (for instance, in this example the box must be either
red or blue), then we see that the probabilities for those events must sum to one.

We can now ask questions such as: “what is the overall probability that the se-
lection procedure will pick an apple?”, or “given that we have chosen an orange,
what is the probability that the box we chose was the blue one?”. We can answer
questions such as these, and indeed much more complex questions associated with
problems in pattern recognition, once we have equipped ourselves with the two el-
ementary rules of probability, known as the sum rule and the product rule. Having
obtained these rules, we shall then return to our boxes of fruit example.

In order to derive the rules of probability, consider the slightly more general ex-
ample shown in Figure 1.10 involving two random variables X and Y (which could
for instance be the Box and Fruit variables considered above). We shall suppose that
X can take any of the values xi where i = 1, . . . , M, and Y can take the values yj
where j = 1, . . . , L. Consider a total of N trials in which we sample both of the
variables X and Y , and let the number of such trials in which X = xi and Y = yj
be nij. Also, let the number of trials in which X takes the value xi (irrespective
of the value that Y takes) be denoted by ci, and similarly let the number of trials in
which Y takes the value yj be denoted by rj.

The probability that X will take the value xi and Y will take the value yj is
written p(X = xi, Y = yj) and is called the joint probability of X = xi and
Y = yj. It is given by the number of points falling in the cell i,j as a fraction of the
total number of points, and hence



Here we are implicitly considering the limit N → ∞. Similarly, the probability that
X takes the value xi irrespective of the value of Y is written as p(X = xi) and is
given by the fraction of the total number of points that fall in column i, so that

p(X = xi, Y = yj) = nij
N

.

p(X = xi) = ci
N

.

(1.5)

(1.6)

Because the number of instances in column i in Figure 1.10 is just the sum of the
number of instances in each cell of that column, we have ci =
j nij and therefore,



L

j=1

14

1. INTRODUCTION

from (1.5) and (1.6), we have

p(X = xi) =

p(X = xi, Y = yj)

(1.7)

which is the sum rule of probability. Note that p(X = xi) is sometimes called the
marginal probability, because it is obtained by marginalizing, or summing out, the
other variables (in this case Y ).

If we consider only those instances for which X = xi, then the fraction of
such instances for which Y = yj is written p(Y = yj|X = xi) and is called the
conditional probability of Y = yj given X = xi.
It is obtained by ﬁnding the
fraction of those points in column i that fall in cell i,j and hence is given by

p(Y = yj|X = xi) = nij

.

(1.8)

From (1.5), (1.6), and (1.8), we can then derive the following relationship



ci

p(X = xi, Y = yj) = nij
N

= nij
ci

ci
N

·

= p(Y = yj|X = xi)p(X = xi)

(1.9)

which is the product rule of probability.

So far we have been quite careful to make a distinction between a random vari-
able, such as the box B in the fruit example, and the values that the random variable
can take, for example r if the box were the red one. Thus the probability that B takes
the value r is denoted p(B = r). Although this helps to avoid ambiguity, it leads
to a rather cumbersome notation, and in many cases there will be no need for such
pedantry. Instead, we may simply write p(B) to denote a distribution over the ran-
dom variable B, or p(r) to denote the distribution evaluated for the particular value
r, provided that the interpretation is clear from the context.

With this more compact notation, we can write the two fundamental rules of

probability theory in the following form.

The Rules of Probability

sum rule

p(X) =

p(X, Y )

Y

product rule

p(X, Y ) = p(Y |X)p(X).

(1.10)

(1.11)

Here p(X, Y ) is a joint probability and is verbalized as “the probability of X and
Y ”. Similarly, the quantity p(Y |X) is a conditional probability and is verbalized as
“the probability of Y given X”, whereas the quantity p(X) is a marginal probability



and is simply “the probability of X”. These two simple rules form the basis for all
of the probabilistic machinery that we use throughout this book.

From the product rule, together with the symmetry property p(X, Y ) = p(Y, X),
we immediately obtain the following relationship between conditional probabilities

p(Y |X) = p(X|Y )p(Y )

p(X)

(1.12)

1.2. Probability Theory

15

which is called Bayes’ theorem and which plays a central role in pattern recognition
and machine learning. Using the sum rule, the denominator in Bayes’ theorem can
be expressed in terms of the quantities appearing in the numerator

p(X) =

p(X|Y )p(Y ).

Y

(1.13)

We can view the denominator in Bayes’ theorem as being the normalization constant
required to ensure that the sum of the conditional probability on the left-hand side of
(1.12) over all values of Y equals one.

In Figure 1.11, we show a simple example involving a joint distribution over two
variables to illustrate the concept of marginal and conditional distributions. Here
a ﬁnite sample of N = 60 data points has been drawn from the joint distribution
and is shown in the top left. In the top right is a histogram of the fractions of data
points having each of the two values of Y . From the deﬁnition of probability, these
fractions would equal the corresponding probabilities p(Y ) in the limit N → ∞. We
can view the histogram as a simple way to model a probability distribution given only
a ﬁnite number of points drawn from that distribution. Modelling distributions from
data lies at the heart of statistical pattern recognition and will be explored in great
detail in this book. The remaining two plots in Figure 1.11 show the corresponding
histogram estimates of p(X) and p(X|Y = 1).
Let us now return to our example involving boxes of fruit. For the moment, we
shall once again be explicit about distinguishing between the random variables and
their instantiations. We have seen that the probabilities of selecting either the red or
the blue boxes are given by

p(B = r) = 4/10
p(B = b) = 6/10

(1.14)
(1.15)

respectively. Note that these satisfy p(B = r) + p(B = b) = 1.

Now suppose that we pick a box at random, and it turns out to be the blue box.
Then the probability of selecting an apple is just the fraction of apples in the blue
box which is 3/4, and so p(F = a|B = b) = 3/4. In fact, we can write out all four
conditional probabilities for the type of fruit, given the selected box

p(F = a|B = r) = 1/4
p(F = o|B = r) = 3/4
p(F = a|B = b) = 3/4
p(F = o|B = b) = 1/4.

(1.16)
(1.17)
(1.18)
(1.19)

16

1. INTRODUCTION

p(X, Y )

p(Y )

Y = 2

Y = 1

X

p(X)

p(X|Y = 1)

X

X

Figure 1.11 An illustration of a distribution over two variables, X, which takes 9 possible values, and Y , which
takes two possible values. The top left ﬁgure shows a sample of 60 points drawn from a joint probability distri-
bution over these variables. The remaining ﬁgures show histogram estimates of the marginal distributions p(X)
and p(Y ), as well as the conditional distribution p(X|Y = 1) corresponding to the bottom row in the top left
ﬁgure.

Again, note that these probabilities are normalized so that

p(F = a|B = r) + p(F = o|B = r) = 1

and similarly

p(F = a|B = b) + p(F = o|B = b) = 1.

We can now use the sum and product rules of probability to evaluate the overall

probability of choosing an apple

(1.20)

(1.21)

(1.22)

p(F = a) = p(F = a|B = r)p(B = r) + p(F = a|B = b)p(B = b)

=

1
4 ×

4
10

+

3
4 ×

6
10

=

11
20

from which it follows, using the sum rule, that p(F = o) = 1 − 11/20 = 9/20.

1.2. Probability Theory

17



Suppose instead we are told that a piece of fruit has been selected and it is an
orange, and we would like to know which box it came from. This requires that
we evaluate the probability distribution over boxes conditioned on the identity of
the fruit, whereas the probabilities in (1.16)–(1.19) give the probability distribution
over the fruit conditioned on the identity of the box. We can solve the problem of
reversing the conditional probability by using Bayes’ theorem to give

3
4 ×

4
10 ×

20
9

=

=

(1.23)

p(F = o)

p(B = r|F = o) = p(F = o|B = r)p(B = r)

2
3 .
From the sum rule, it then follows that p(B = b|F = o) = 1 − 2/3 = 1/3.
We can provide an important interpretation of Bayes’ theorem as follows. If
we had been asked which box had been chosen before being told the identity of
the selected item of fruit, then the most complete information we have available is
provided by the probability p(B). We call this the prior probability because it is the
probability available before we observe the identity of the fruit. Once we are told that
the fruit is an orange, we can then use Bayes’ theorem to compute the probability
p(B|F ), which we shall call the posterior probability because it is the probability
obtained after we have observed F . Note that in this example, the prior probability
of selecting the red box was 4/10, so that we were more likely to select the blue box
than the red one. However, once we have observed that the piece of selected fruit is
an orange, we ﬁnd that the posterior probability of the red box is now 2/3, so that
it is now more likely that the box we selected was in fact the red one. This result
accords with our intuition, as the proportion of oranges is much higher in the red box
than it is in the blue box, and so the observation that the fruit was an orange provides
signiﬁcant evidence favouring the red box. In fact, the evidence is sufﬁciently strong
that it outweighs the prior and makes it more likely that the red box was chosen
rather than the blue one.

Finally, we note that if the joint distribution of two variables factorizes into the
product of the marginals, so that p(X, Y ) = p(X)p(Y ), then X and Y are said to
be independent. From the product rule, we see that p(Y |X) = p(Y ), and so the
conditional distribution of Y given X is indeed independent of the value of X. For
instance, in our boxes of fruit example, if each box contained the same fraction of
apples and oranges, then p(F|B) = P (F ), so that the probability of selecting, say,
an apple is independent of which box is chosen.

1.2.1 Probability densities
As well as considering probabilities deﬁned over discrete sets of events, we
also wish to consider probabilities with respect to continuous variables. We shall
limit ourselves to a relatively informal discussion. If the probability of a real-valued
variable x falling in the interval (x, x + δx) is given by p(x)δx for δx → 0, then
p(x) is called the probability density over x. This is illustrated in Figure 1.12. The
probability that x will lie in an interval (a, b) is then given by

p(x ∈ (a, b)) =

b

a

p(x) dx.

(1.24)









 dx

dy

18

1. INTRODUCTION

Figure 1.12 The concept of probability for
discrete variables can be ex-
tended to that of a probability
density p(x) over a continuous
variable x and is such that the
probability of x lying in the inter-
val (x, x + δx) is given by p(x)δx
for δx → 0. The probability
density can be expressed as the
derivative of a cumulative distri-
bution function P (x).

p(x)

P (x)

x

(1.25)

(1.26)

δx

p(x) � 0
p(x) dx = 1.

∞

−∞

Because probabilities are nonnegative, and because the value of x must lie some-
where on the real axis, the probability density p(x) must satisfy the two conditions

Under a nonlinear change of variable, a probability density transforms differently
from a simple function, due to the Jacobian factor. For instance, if we consider
f(y) = f(g(y)).
a change of variables x = g(y), then a function f(x) becomes
Now consider a probability density px(x) that corresponds to a density py(y) with
respect to the new variable y, where the sufﬁces denote the fact that px(x) and py(y)
are different densities. Observations falling in the range (x, x + δx) will, for small
values of δx, be transformed into the range (y, y + δy) where px(x)δx  py(y)δy,
and hence

py(y) = px(x)

= px(g(y))|g(y)| .

(1.27)

One consequence of this property is that the concept of the maximum of a probability
density is dependent on the choice of variable.

The probability that x lies in the interval (−∞, z) is given by the cumulative

distribution function deﬁned by

P (z) =

p(x) dx

z

−∞

(1.28)

which satisﬁes P (x) = p(x), as shown in Figure 1.12.

If we have several continuous variables x1, . . . , xD, denoted collectively by the
vector x, then we can deﬁne a joint probability density p(x) = p(x1, . . . , xD) such

Exercise 1.4







x

(1.29)

(1.30)

(1.31)

(1.32)

1.2. Probability Theory

19

that the probability of x falling in an inﬁnitesimal volume δx containing the point x
is given by p(x)δx. This multivariate probability density must satisfy

p(x) � 0
p(x) dx = 1

in which the integral is taken over the whole of x space. We can also consider joint
probability distributions over a combination of discrete and continuous variables.

Note that if x is a discrete variable, then p(x) is sometimes called a probability
mass function because it can be regarded as a set of ‘probability masses’ concentrated
at the allowed values of x.

The sum and product rules of probability, as well as Bayes’ theorem, apply
equally to the case of probability densities, or to combinations of discrete and con-
tinuous variables. For instance, if x and y are two real variables, then the sum and
product rules take the form

p(x) =

p(x, y) dy
p(x, y) = p(y|x)p(x).

A formal justiﬁcation of the sum and product rules for continuous variables (Feller,
1966) requires a branch of mathematics called measure theory and lies outside the
scope of this book. Its validity can be seen informally, however, by dividing each
real variable into intervals of width ∆ and considering the discrete probability dis-
tribution over these intervals. Taking the limit ∆ → 0 then turns sums into integrals
and gives the desired result.

1.2.2 Expectations and covariances
One of the most important operations involving probabilities is that of ﬁnding
weighted averages of functions. The average value of some function f(x) under a
probability distribution p(x) is called the expectation of f(x) and will be denoted by
E[f]. For a discrete distribution, it is given by

E[f] =

p(x)f(x)

(1.33)

so that the average is weighted by the relative probabilities of the different values
of x. In the case of continuous variables, expectations are expressed in terms of an
integration with respect to the corresponding probability density

E[f] =

p(x)f(x) dx.

(1.34)

In either case, if we are given a ﬁnite number N of points drawn from the probability
distribution or probability density, then the expectation can be approximated as a




n=1

N




	

	

20

1. INTRODUCTION

ﬁnite sum over these points

E[f] 

1
N

f(xn).

(1.35)

We shall make extensive use of this result when we discuss sampling methods in
Chapter 11. The approximation in (1.35) becomes exact in the limit N → ∞.
Sometimes we will be considering expectations of functions of several variables,
in which case we can use a subscript to indicate which variable is being averaged
over, so that for instance

(1.36)
denotes the average of the function f(x, y) with respect to the distribution of x. Note
that Ex[f(x, y)] will be a function of y.

Ex[f(x, y)]

We can also consider a conditional expectation with respect to a conditional

distribution, so that

p(x|y)f(x)
with an analogous deﬁnition for continuous variables.

Ex[f|y] =

x

The variance of f(x) is deﬁned by

var[f] = E

(f(x) − E[f(x)])2

and provides a measure of how much variability there is in f(x) around its mean
value E[f(x)]. Expanding out the square, we see that the variance can also be written
in terms of the expectations of f(x) and f(x)2

In particular, we can consider the variance of the variable x itself, which is given by

var[f] = E[f(x)2] − E[f(x)]2.

var[x] = E[x2] − E[x]2.

For two random variables x and y, the covariance is deﬁned by

cov[x, y] = Ex,y [{x − E[x]}{y − E[y]}]

= Ex,y[xy] − E[x]E[y]

which expresses the extent to which x and y vary together. If x and y are indepen-
dent, then their covariance vanishes.

In the case of two vectors of random variables x and y, the covariance is a matrix

cov[x, y] = Ex,y

{x − E[x]}{yT − E[yT]}

= Ex,y[xyT] − E[x]E[yT].

(1.42)

If we consider the covariance of the components of a vector x with each other, then
we use a slightly simpler notation cov[x] ≡ cov[x, x].

(1.37)

(1.38)

(1.39)

(1.40)

(1.41)

Exercise 1.5

Exercise 1.6

1.2. Probability Theory

21

1.2.3 Bayesian probabilities
So far in this chapter, we have viewed probabilities in terms of the frequencies
of random, repeatable events. We shall refer to this as the classical or frequentist
interpretation of probability. Now we turn to the more general Bayesian view, in
which probabilities provide a quantiﬁcation of uncertainty.

Consider an uncertain event, for example whether the moon was once in its own
orbit around the sun, or whether the Arctic ice cap will have disappeared by the end
of the century. These are not events that can be repeated numerous times in order
to deﬁne a notion of probability as we did earlier in the context of boxes of fruit.
Nevertheless, we will generally have some idea, for example, of how quickly we
think the polar ice is melting. If we now obtain fresh evidence, for instance from a
new Earth observation satellite gathering novel forms of diagnostic information, we
may revise our opinion on the rate of ice loss. Our assessment of such matters will
affect the actions we take, for instance the extent to which we endeavour to reduce
the emission of greenhouse gasses. In such circumstances, we would like to be able
to quantify our expression of uncertainty and make precise revisions of uncertainty in
the light of new evidence, as well as subsequently to be able to take optimal actions
or decisions as a consequence. This can all be achieved through the elegant, and very
general, Bayesian interpretation of probability.

The use of probability to represent uncertainty, however, is not an ad-hoc choice,
but is inevitable if we are to respect common sense while making rational coherent
inferences. For instance, Cox (1946) showed that if numerical values are used to
represent degrees of belief, then a simple set of axioms encoding common sense
properties of such beliefs leads uniquely to a set of rules for manipulating degrees of
belief that are equivalent to the sum and product rules of probability. This provided
the ﬁrst rigorous proof that probability theory could be regarded as an extension of
Boolean logic to situations involving uncertainty (Jaynes, 2003). Numerous other
authors have proposed different sets of properties or axioms that such measures of
uncertainty should satisfy (Ramsey, 1931; Good, 1950; Savage, 1961; deFinetti,
1970; Lindley, 1982). In each case, the resulting numerical quantities behave pre-
cisely according to the rules of probability. It is therefore natural to refer to these
quantities as (Bayesian) probabilities.

In the ﬁeld of pattern recognition, too, it is helpful to have a more general no-

Thomas Bayes
1701–1761

Thomas Bayes was born in Tun-
bridge Wells and was a clergyman
as well as an amateur scientist and
a mathematician. He studied logic
and theology at Edinburgh Univer-
sity and was elected Fellow of the
Royal Society in 1742. During the 18th century, is-
sues regarding probability arose in connection with

gambling and with the new concept of insurance. One
particularly important problem concerned so-called in-
verse probability. A solution was proposed by Thomas
Bayes in his paper ‘Essay towards solving a problem
in the doctrine of chances’, which was published in
1764, some three years after his death, in the Philo-
sophical Transactions of the Royal Society.
In fact,
Bayes only formulated his theory for the case of a uni-
form prior, and it was Pierre-Simon Laplace who inde-
pendently rediscovered the theory in general form and
who demonstrated its broad applicability.

22

1. INTRODUCTION



tion of probability. Consider the example of polynomial curve ﬁtting discussed in
Section 1.1. It seems reasonable to apply the frequentist notion of probability to the
random values of the observed variables tn. However, we would like to address and
quantify the uncertainty that surrounds the appropriate choice for the model param-
eters w. We shall see that, from a Bayesian perspective, we can use the machinery
of probability theory to describe the uncertainty in model parameters such as w, or
indeed in the choice of model itself.

Bayes’ theorem now acquires a new signiﬁcance. Recall that in the boxes of fruit
example, the observation of the identity of the fruit provided relevant information
that altered the probability that the chosen box was the red one. In that example,
Bayes’ theorem was used to convert a prior probability into a posterior probability
by incorporating the evidence provided by the observed data. As we shall see in
detail later, we can adopt a similar approach when making inferences about quantities
such as the parameters w in the polynomial curve ﬁtting example. We capture our
assumptions about w, before observing the data, in the form of a prior probability
distribution p(w). The effect of the observed data D = {t1, . . . , tN} is expressed
through the conditional probability p(D|w), and we shall see later, in Section 1.2.5,
how this can be represented explicitly. Bayes’ theorem, which takes the form

p(w|D) = p(D|w)p(w)

p(D)

(1.43)

then allows us to evaluate the uncertainty in w after we have observed D in the form
of the posterior probability p(w|D).
The quantity p(D|w) on the right-hand side of Bayes’ theorem is evaluated for
the observed data set D and can be viewed as a function of the parameter vector
w, in which case it is called the likelihood function. It expresses how probable the
observed data set is for different settings of the parameter vector w. Note that the
likelihood is not a probability distribution over w, and its integral with respect to w
does not (necessarily) equal one.

Given this deﬁnition of likelihood, we can state Bayes’ theorem in words

posterior ∝ likelihood × prior

(1.44)

where all of these quantities are viewed as functions of w. The denominator in
(1.43) is the normalization constant, which ensures that the posterior distribution
on the left-hand side is a valid probability density and integrates to one. Indeed,
integrating both sides of (1.43) with respect to w, we can express the denominator
in Bayes’ theorem in terms of the prior distribution and the likelihood function

p(D) =

p(D|w)p(w) dw.

(1.45)

In both the Bayesian and frequentist paradigms, the likelihood function p(D|w)
plays a central role. However, the manner in which it is used is fundamentally dif-
ferent in the two approaches. In a frequentist setting, w is considered to be a ﬁxed
parameter, whose value is determined by some form of ‘estimator’, and error bars

1.2. Probability Theory

23

on this estimate are obtained by considering the distribution of possible data sets D.
By contrast, from the Bayesian viewpoint there is only a single data set D (namely
the one that is actually observed), and the uncertainty in the parameters is expressed
through a probability distribution over w.

A widely used frequentist estimator is maximum likelihood, in which w is set
to the value that maximizes the likelihood function p(D|w). This corresponds to
choosing the value of w for which the probability of the observed data set is maxi-
mized. In the machine learning literature, the negative log of the likelihood function
is called an error function. Because the negative logarithm is a monotonically de-
creasing function, maximizing the likelihood is equivalent to minimizing the error.
One approach to determining frequentist error bars is the bootstrap (Efron, 1979;
Hastie et al., 2001), in which multiple data sets are created as follows. Suppose our
original data set consists of N data points X = {x1, . . . , xN}. We can create a new
data set XB by drawing N points at random from X, with replacement, so that some
points in X may be replicated in XB, whereas other points in X may be absent from
XB. This process can be repeated L times to generate L data sets each of size N and
each obtained by sampling from the original data set X. The statistical accuracy of
parameter estimates can then be evaluated by looking at the variability of predictions
between the different bootstrap data sets.

One advantage of the Bayesian viewpoint is that the inclusion of prior knowl-
edge arises naturally. Suppose, for instance, that a fair-looking coin is tossed three
times and lands heads each time. A classical maximum likelihood estimate of the
probability of landing heads would give 1, implying that all future tosses will land
heads! By contrast, a Bayesian approach with any reasonable prior will lead to a
much less extreme conclusion.

There has been much controversy and debate associated with the relative mer-
its of the frequentist and Bayesian paradigms, which have not been helped by the
fact that there is no unique frequentist, or even Bayesian, viewpoint. For instance,
one common criticism of the Bayesian approach is that the prior distribution is of-
ten selected on the basis of mathematical convenience rather than as a reﬂection of
any prior beliefs. Even the subjective nature of the conclusions through their de-
pendence on the choice of prior is seen by some as a source of difﬁculty. Reducing
the dependence on the prior is one motivation for so-called noninformative priors.
However, these lead to difﬁculties when comparing different models, and indeed
Bayesian methods based on poor choices of prior can give poor results with high
conﬁdence. Frequentist evaluation methods offer some protection from such prob-
lems, and techniques such as cross-validation remain useful in areas such as model
comparison.

This book places a strong emphasis on the Bayesian viewpoint, reﬂecting the
huge growth in the practical importance of Bayesian methods in the past few years,
while also discussing useful frequentist concepts as required.

Although the Bayesian framework has its origins in the 18th century, the prac-
tical application of Bayesian methods was for a long time severely limited by the
difﬁculties in carrying through the full Bayesian procedure, particularly the need to
marginalize (sum or integrate) over the whole of parameter space, which, as we shall

Section 2.1

Section 2.4.3

Section 1.3










24

1. INTRODUCTION

see, is required in order to make predictions or to compare different models. The
development of sampling methods, such as Markov chain Monte Carlo (discussed in
Chapter 11) along with dramatic improvements in the speed and memory capacity
of computers, opened the door to the practical use of Bayesian techniques in an im-
pressive range of problem domains. Monte Carlo methods are very ﬂexible and can
be applied to a wide range of models. However, they are computationally intensive
and have mainly been used for small-scale problems.

More recently, highly efﬁcient deterministic approximation schemes such as
variational Bayes and expectation propagation (discussed in Chapter 10) have been
developed. These offer a complementary alternative to sampling methods and have
allowed Bayesian techniques to be used in large-scale applications (Blei et al., 2003).

1.2.4 The Gaussian distribution
We shall devote the whole of Chapter 2 to a study of various probability dis-
tributions and their key properties. It is convenient, however, to introduce here one
of the most important probability distributions for continuous variables, called the
normal or Gaussian distribution. We shall make extensive use of this distribution in
the remainder of this chapter and indeed throughout much of the book.

For the case of a single real-valued variable x, the Gaussian distribution is de-

ﬁned by

N

x|µ, σ2

=

1

(2πσ2)1/2

exp

1
2σ2 (x − µ)2

−

(1.46)

which is governed by two parameters: µ, called the mean, and σ2, called the vari-
ance. The square root of the variance, given by σ, is called the standard deviation,
and the reciprocal of the variance, written as β = 1/σ2, is called the precision. We
shall see the motivation for these terms shortly. Figure 1.13 shows a plot of the
Gaussian distribution.

From the form of (1.46) we see that the Gaussian distribution satisﬁes

N (x|µ, σ2) > 0.

(1.47)

Exercise 1.7

Also it is straightforward to show that the Gaussian is normalized, so that

Pierre-Simon Laplace
1749–1827

It
is said that Laplace was seri-
ously lacking in modesty and at one
point declared himself
to be the
best mathematician in France at the
time, a claim that was arguably true.
As well as being proliﬁc in mathe-
matics, he also made numerous contributions to as-
tronomy, including the nebular hypothesis by which the

earth is thought to have formed from the condensa-
tion and cooling of a large rotating disk of gas and
dust. In 1812 he published the ﬁrst edition of Th´eorie
Analytique des Probabilit´es, in which Laplace states
that “probability theory is nothing but common sense
reduced to calculation”. This work included a discus-
sion of the inverse probability calculation (later termed
Bayes’ theorem by Poincar´e), which he used to solve
problems in life expectancy, jurisprudence, planetary
masses, triangulation, and error estimation.





















2σ

µ



25

x

(1.48)

(1.49)

(1.50)

(1.51)

Figure 1.13 Plot of the univariate Gaussian
showing the mean µ and the
standard deviation σ.

N (x|µ, σ2)

1.2. Probability Theory

Exercise 1.8

Exercise 1.9

∞

−∞

N

x|µ, σ2

dx = 1.

Thus (1.46) satisﬁes the two requirements for a valid probability density.

We can readily ﬁnd expectations of functions of x under the Gaussian distribu-

tion. In particular, the average value of x is given by

E[x] =

N

x|µ, σ2

x dx = µ.

∞

−∞

Because the parameter µ represents the average value of x under the distribution, it
is referred to as the mean. Similarly, for the second order moment

E[x2] =

∞

−∞

N

x|µ, σ2

x2 dx = µ2 + σ2.

From (1.49) and (1.50), it follows that the variance of x is given by

var[x] = E[x2] − E[x]2 = σ2

and hence σ2 is referred to as the variance parameter. The maximum of a distribution
is known as its mode. For a Gaussian, the mode coincides with the mean.

We are also interested in the Gaussian distribution deﬁned over a D-dimensional

vector x of continuous variables, which is given by

N (x|µ, Σ) =

1

1

(2π)D/2

|Σ|1/2

exp

1
2

−

(x − µ)TΣ−1(x − µ)

(1.52)

where the D-dimensional vector µ is called the mean, the D × D matrix Σ is called
the covariance, and |Σ| denotes the determinant of Σ. We shall make use of the
multivariate Gaussian distribution brieﬂy in this chapter, although its properties will
be studied in detail in Section 2.3.

26

1. INTRODUCTION

Figure 1.14 Illustration of the likelihood function for
a Gaussian distribution, shown by the
red curve. Here the black points de-
note a data set of values {xn}, and
the likelihood function given by (1.53)
corresponds to the product of the blue
values. Maximizing the likelihood in-
volves adjusting the mean and vari-
ance of the Gaussian so as to maxi-
mize this product.

p(x)

N (xn|µ, σ2)

xn

x



N






Now suppose that we have a data set of observations x = (x1, . . . , xN )T, rep-
resenting N observations of the scalar variable x. Note that we are using the type-
face x to distinguish this from a single observation of the vector-valued variable
(x1, . . . , xD)T, which we denote by x. We shall suppose that the observations are
drawn independently from a Gaussian distribution whose mean µ and variance σ2
are unknown, and we would like to determine these parameters from the data set.
Data points that are drawn independently from the same distribution are said to be
independent and identically distributed, which is often abbreviated to i.i.d. We have
seen that the joint probability of two independent events is given by the product of
the marginal probabilities for each event separately. Because our data set x is i.i.d.,
we can therefore write the probability of the data set, given µ and σ2, in the form

Section 1.2.5

p(x|µ, σ2) =

N

xn|µ, σ2

.

n=1

(1.53)

When viewed as a function of µ and σ2, this is the likelihood function for the Gaus-
sian and is interpreted diagrammatically in Figure 1.14.

One common criterion for determining the parameters in a probability distribu-
tion using an observed data set is to ﬁnd the parameter values that maximize the
likelihood function. This might seem like a strange criterion because, from our fore-
going discussion of probability theory, it would seem more natural to maximize the
probability of the parameters given the data, not the probability of the data given the
parameters. In fact, these two criteria are related, as we shall discuss in the context
of curve ﬁtting.

For the moment, however, we shall determine values for the unknown parame-
ters µ and σ2 in the Gaussian by maximizing the likelihood function (1.53). In prac-
tice, it is more convenient to maximize the log of the likelihood function. Because
the logarithm is a monotonically increasing function of its argument, maximization
of the log of a function is equivalent to maximization of the function itself. Taking
the log not only simpliﬁes the subsequent mathematical analysis, but it also helps
numerically because the product of a large number of small probabilities can easily
underﬂow the numerical precision of the computer, and this is resolved by computing
instead the sum of the log probabilities. From (1.46) and (1.53), the log likelihood









N

n=1




1
N

n=1

N




N − 1
N

N
2

xn

n=1

N

N



Exercise 1.11

Section 1.1

Exercise 1.12

1.2. Probability Theory

27

function can be written in the form

ln p

x|µ, σ2

1
2σ2

= −

(xn − µ)2 −

ln σ2 −

N
2

ln(2π).

(1.54)

Maximizing (1.54) with respect to µ, we obtain the maximum likelihood solution
given by

µML =

(1.55)

(1.56)

which is the sample mean, i.e., the mean of the observed values {xn}. Similarly,
maximizing (1.54) with respect to σ2, we obtain the maximum likelihood solution
for the variance in the form

ML =
σ2

1
N

(xn − µML)2

which is the sample variance measured with respect to the sample mean µML. Note
that we are performing a joint maximization of (1.54) with respect to µ and σ2, but
in the case of the Gaussian distribution the solution for µ decouples from that for σ2
so that we can ﬁrst evaluate (1.55) and then subsequently use this result to evaluate
(1.56).

Later in this chapter, and also in subsequent chapters, we shall highlight the sig-
niﬁcant limitations of the maximum likelihood approach. Here we give an indication
of the problem in the context of our solutions for the maximum likelihood param-
eter settings for the univariate Gaussian distribution. In particular, we shall show
that the maximum likelihood approach systematically underestimates the variance
of the distribution. This is an example of a phenomenon called bias and is related
to the problem of over-ﬁtting encountered in the context of polynomial curve ﬁtting.
We ﬁrst note that the maximum likelihood solutions µML and σ2
ML are functions of
the data set values x1, . . . , xN . Consider the expectations of these quantities with
respect to the data set values, which themselves come from a Gaussian distribution
with parameters µ and σ2. It is straightforward to show that

E[µML] = µ

E[σ2

ML] =

σ2

(1.57)

(1.58)

so that on average the maximum likelihood estimate will obtain the correct mean but
will underestimate the true variance by a factor (N − 1)/N. The intuition behind
this result is given by Figure 1.15.
From (1.58) it follows that the following estimate for the variance parameter is

unbiased

σ2 = N

ML =

N − 1 σ2

1

N − 1

n=1

(xn − µML)2.

(1.59)

28

1. INTRODUCTION

Figure 1.15 Illustration of how bias arises in using max-
imum likelihood to determine the variance
of a Gaussian. The green curve shows
the true Gaussian distribution from which
data is generated, and the three red curves
show the Gaussian distributions obtained
by ﬁtting to three data sets, each consist-
ing of two data points shown in blue, us-
ing the maximum likelihood results (1.55)
and (1.56). Averaged across the three data
sets, the mean is correct, but the variance
is systematically under-estimated because
it is measured relative to the sample mean
and not relative to the true mean.

(a)

(b)

(c)






Section 1.1

In Section 10.1.3, we shall see how this result arises automatically when we adopt a
Bayesian approach.

Note that the bias of the maximum likelihood solution becomes less signiﬁcant
as the number N of data points increases, and in the limit N → ∞ the maximum
likelihood solution for the variance equals the true variance of the distribution that
generated the data. In practice, for anything other than small N, this bias will not
prove to be a serious problem. However, throughout this book we shall be interested
in more complex models with many parameters, for which the bias problems asso-
ciated with maximum likelihood will be much more severe. In fact, as we shall see,
the issue of bias in maximum likelihood lies at the root of the over-ﬁtting problem
that we encountered earlier in the context of polynomial curve ﬁtting.

1.2.5 Curve ﬁtting re-visited
We have seen how the problem of polynomial curve ﬁtting can be expressed in
terms of error minimization. Here we return to the curve ﬁtting example and view it
from a probabilistic perspective, thereby gaining some insights into error functions
and regularization, as well as taking us towards a full Bayesian treatment.

The goal in the curve ﬁtting problem is to be able to make predictions for the
target variable t given some new value of the input variable x on the basis of a set of
training data comprising N input values x = (x1, . . . , xN )T and their corresponding
target values t = (t1, . . . , tN )T. We can express our uncertainty over the value of
the target variable using a probability distribution. For this purpose, we shall assume
that, given the value of x, the corresponding value of t has a Gaussian distribution
with a mean equal to the value y(x, w) of the polynomial curve given by (1.1). Thus
we have

where, for consistency with the notation in later chapters, we have deﬁned a preci-
sion parameter β corresponding to the inverse variance of the distribution. This is
illustrated schematically in Figure 1.16.

p(t|x, w, β) = N

t|y(x, w), β−1

(1.60)





N

n=1




t




N

N

n=1

Figure 1.16 Schematic illustration of a Gaus-
sian conditional distribution for t given x given by
(1.60), in which the mean is given by the polyno-
mial function y(x, w), and the precision is given
by the parameter β, which is related to the vari-
ance by β−1 = σ2.

y(x0, w)

1.2. Probability Theory

29

y(x, w)

p(t|x0, w, β)

2σ

x

x0

We now use the training data {x, t} to determine the values of the unknown
parameters w and β by maximum likelihood. If the data are assumed to be drawn
independently from the distribution (1.60), then the likelihood function is given by

p(t|x, w, β) =

N

tn|y(xn, w), β−1

.

(1.61)

As we did in the case of the simple Gaussian distribution earlier, it is convenient to
maximize the logarithm of the likelihood function. Substituting for the form of the
Gaussian distribution, given by (1.46), we obtain the log likelihood function in the
form

ln p(t|x, w, β) = −

β
2

{y(xn, w) − tn}2 + N
2

ln β −

N
2

ln(2π).

(1.62)

Consider ﬁrst the determination of the maximum likelihood solution for the polyno-
mial coefﬁcients, which will be denoted by wML. These are determined by maxi-
mizing (1.62) with respect to w. For this purpose, we can omit the last two terms
on the right-hand side of (1.62) because they do not depend on w. Also, we note
that scaling the log likelihood by a positive constant coefﬁcient does not alter the
location of the maximum with respect to w, and so we can replace the coefﬁcient
β/2 with 1/2. Finally, instead of maximizing the log likelihood, we can equivalently
minimize the negative log likelihood. We therefore see that maximizing likelihood is
equivalent, so far as determining w is concerned, to minimizing the sum-of-squares
error function deﬁned by (1.2). Thus the sum-of-squares error function has arisen as
a consequence of maximizing likelihood under the assumption of a Gaussian noise
distribution.

We can also use maximum likelihood to determine the precision parameter β of

the Gaussian conditional distribution. Maximizing (1.62) with respect to β gives

1
βML

=

1
N

{y(xn, wML) − tn}2 .

n=1

(1.63)















N

n=1

30

1. INTRODUCTION

Section 1.2.4

Again we can ﬁrst determine the parameter vector wML governing the mean and sub-
sequently use this to ﬁnd the precision βML as was the case for the simple Gaussian
distribution.

Having determined the parameters w and β, we can now make predictions for
new values of x. Because we now have a probabilistic model, these are expressed
in terms of the predictive distribution that gives the probability distribution over t,
rather than simply a point estimate, and is obtained by substituting the maximum
likelihood parameters into (1.60) to give
p(t|x, wML, βML) = N

t|y(x, wML), β−1

(1.64)

.

ML

Now let us take a step towards a more Bayesian approach and introduce a prior
distribution over the polynomial coefﬁcients w. For simplicity, let us consider a
Gaussian distribution of the form

p(w|α) = N (w|0, α−1I) =

α
2π

(M +1)/2

exp

α
2

−

wTw

(1.65)

where α is the precision of the distribution, and M +1 is the total number of elements
in the vector w for an M th order polynomial. Variables such as α, which control
the distribution of model parameters, are called hyperparameters. Using Bayes’
theorem, the posterior distribution for w is proportional to the product of the prior
distribution and the likelihood function

p(w|x, t, α, β) ∝ p(t|x, w, β)p(w|α).

(1.66)

We can now determine w by ﬁnding the most probable value of w given the data,
in other words by maximizing the posterior distribution. This technique is called
maximum posterior, or simply MAP. Taking the negative logarithm of (1.66) and
combining with (1.62) and (1.65), we ﬁnd that the maximum of the posterior is
given by the minimum of

β
2

{y(xn, w) − tn}2 + α
2

wTw.

(1.67)

Thus we see that maximizing the posterior distribution is equivalent to minimizing
the regularized sum-of-squares error function encountered earlier in the form (1.4),
with a regularization parameter given by λ = α/β.

1.2.6 Bayesian curve ﬁtting
Although we have included a prior distribution p(w|α), we are so far still mak-
ing a point estimate of w and so this does not yet amount to a Bayesian treatment. In
a fully Bayesian approach, we should consistently apply the sum and product rules
of probability, which requires, as we shall see shortly, that we integrate over all val-
ues of w. Such marginalizations lie at the heart of Bayesian methods for pattern
recognition.





N





N



1.2. Probability Theory

31

In the curve ﬁtting problem, we are given the training data x and t, along with
a new test point x, and our goal is to predict the value of t. We therefore wish
to evaluate the predictive distribution p(t|x, x, t). Here we shall assume that the
parameters α and β are ﬁxed and known in advance (in later chapters we shall discuss
how such parameters can be inferred from data in a Bayesian setting).

A Bayesian treatment simply corresponds to a consistent application of the sum
and product rules of probability, which allow the predictive distribution to be written
in the form

p(t|x, x, t) =

p(t|x, w)p(w|x, t) dw.

(1.68)

Here p(t|x, w) is given by (1.60), and we have omitted the dependence on α and
β to simplify the notation. Here p(w|x, t) is the posterior distribution over param-
eters, and can be found by normalizing the right-hand side of (1.66). We shall see
in Section 3.3 that, for problems such as the curve-ﬁtting example, this posterior
distribution is a Gaussian and can be evaluated analytically. Similarly, the integra-
tion in (1.68) can also be performed analytically with the result that the predictive
distribution is given by a Gaussian of the form

p(t|x, x, t) = N
where the mean and variance are given by

t|m(x), s2(x)

Here the matrix S is given by

m(x) = βφ(x)TS

φ(xn)tn

n=1

s2(x) = β−1 + φ(x)TSφ(x).

S−1 = αI + β

φ(xn)φ(x)T

n=1

where I is the unit matrix, and we have deﬁned the vector φ(x) with elements
φi(x) = xi for i = 0, . . . , M.

We see that the variance, as well as the mean, of the predictive distribution in
(1.69) is dependent on x. The ﬁrst term in (1.71) represents the uncertainty in the
predicted value of t due to the noise on the target variables and was expressed already
in the maximum likelihood predictive distribution (1.64) through β−1
ML. However, the
second term arises from the uncertainty in the parameters w and is a consequence
of the Bayesian treatment. The predictive distribution for the synthetic sinusoidal
regression problem is illustrated in Figure 1.17.

(1.69)

(1.70)

(1.71)

(1.72)

32

1. INTRODUCTION

Figure 1.17 The predictive distribution result-
ing from a Bayesian treatment of
polynomial curve ﬁtting using an
M = 9 polynomial, with the ﬁxed
parameters α = 5 × 10−3 and β =
11.1 (corresponding to the known
noise variance), in which the red
curve denotes the mean of
the
predictive distribution and the red
region corresponds to ±1 stan-
dard deviation around the mean.

t

1

0

−1

0

1.3. Model Selection

1

x

In our example of polynomial curve ﬁtting using least squares, we saw that there was
an optimal order of polynomial that gave the best generalization. The order of the
polynomial controls the number of free parameters in the model and thereby governs
the model complexity. With regularized least squares, the regularization coefﬁcient
λ also controls the effective complexity of the model, whereas for more complex
models, such as mixture distributions or neural networks there may be multiple pa-
rameters governing complexity.
In a practical application, we need to determine
the values of such parameters, and the principal objective in doing so is usually to
achieve the best predictive performance on new data. Furthermore, as well as ﬁnd-
ing the appropriate values for complexity parameters within a given model, we may
wish to consider a range of different types of model in order to ﬁnd the best one for
our particular application.

We have already seen that, in the maximum likelihood approach, the perfor-
mance on the training set is not a good indicator of predictive performance on un-
seen data due to the problem of over-ﬁtting. If data is plentiful, then one approach is
simply to use some of the available data to train a range of models, or a given model
with a range of values for its complexity parameters, and then to compare them on
independent data, sometimes called a validation set, and select the one having the
best predictive performance. If the model design is iterated many times using a lim-
ited size data set, then some over-ﬁtting to the validation data can occur and so it may
be necessary to keep aside a third test set on which the performance of the selected
model is ﬁnally evaluated.

In many applications, however, the supply of data for training and testing will be
limited, and in order to build good models, we wish to use as much of the available
data as possible for training. However, if the validation set is small, it will give a
relatively noisy estimate of predictive performance. One solution to this dilemma is
to use cross-validation, which is illustrated in Figure 1.18. This allows a proportion
(S − 1)/S of the available data to be used for training while making use of all of the

1.4. The Curse of Dimensionality

33

Figure 1.18 The technique of S-fold cross-validation,

illus-
trated here for the case of S = 4, involves tak-
ing the available data and partitioning it into S
groups (in the simplest case these are of equal
size). Then S − 1 of the groups are used to train
a set of models that are then evaluated on the re-
maining group. This procedure is then repeated
for all S possible choices for the held-out group,
indicated here by the red blocks, and the perfor-
mance scores from the S runs are then averaged.

run 1

run 2

run 3

run 4

data to assess performance. When data is particularly scarce, it may be appropriate
to consider the case S = N, where N is the total number of data points, which gives
the leave-one-out technique.

One major drawback of cross-validation is that the number of training runs that
must be performed is increased by a factor of S, and this can prove problematic for
models in which the training is itself computationally expensive. A further problem
with techniques such as cross-validation that use separate data to assess performance
is that we might have multiple complexity parameters for a single model (for in-
stance, there might be several regularization parameters). Exploring combinations
of settings for such parameters could, in the worst case, require a number of training
runs that is exponential in the number of parameters. Clearly, we need a better ap-
proach. Ideally, this should rely only on the training data and should allow multiple
hyperparameters and model types to be compared in a single training run. We there-
fore need to ﬁnd a measure of performance which depends only on the training data
and which does not suffer from bias due to over-ﬁtting.

Historically various ‘information criteria’ have been proposed that attempt to
correct for the bias of maximum likelihood by the addition of a penalty term to
compensate for the over-ﬁtting of more complex models. For example, the Akaike
information criterion, or AIC (Akaike, 1974), chooses the model for which the quan-
tity

ln p(D|wML) − M

(1.73)
is largest. Here p(D|wML) is the best-ﬁt log likelihood, and M is the number of
adjustable parameters in the model. A variant of this quantity, called the Bayesian
information criterion, or BIC, will be discussed in Section 4.4.1. Such criteria do
not take account of the uncertainty in the model parameters, however, and in practice
they tend to favour overly simple models. We therefore turn in Section 3.4 to a fully
Bayesian approach where we shall see how complexity penalties arise in a natural
and principled way.

1.4. The Curse of Dimensionality

In the polynomial curve ﬁtting example we had just one input variable x. For prac-
tical applications of pattern recognition, however, we will have to deal with spaces

34

1. INTRODUCTION

Figure 1.19 Scatter plot of the oil ﬂow data
for input variables x6 and x7, in
which red denotes the ‘homoge-
nous’ class, green denotes the
‘annular’ class, and blue denotes
the ‘laminar’ class. Our goal
is
to classify the new test point de-
noted by ‘×’.

2

1.5

x7

1

0.5

0

0

0.25

0.5
x6

0.75

1

of high dimensionality comprising many input variables. As we now discuss, this
poses some serious challenges and is an important factor inﬂuencing the design of
pattern recognition techniques.

In order to illustrate the problem we consider a synthetically generated data set
representing measurements taken from a pipeline containing a mixture of oil, wa-
ter, and gas (Bishop and James, 1993). These three materials can be present in one
of three different geometrical conﬁgurations known as ‘homogenous’, ‘annular’, and
‘laminar’, and the fractions of the three materials can also vary. Each data point com-
prises a 12-dimensional input vector consisting of measurements taken with gamma
ray densitometers that measure the attenuation of gamma rays passing along nar-
row beams through the pipe. This data set is described in detail in Appendix A.
Figure 1.19 shows 100 points from this data set on a plot showing two of the mea-
surements x6 and x7 (the remaining ten input values are ignored for the purposes of
this illustration). Each data point is labelled according to which of the three geomet-
rical classes it belongs to, and our goal is to use this data as a training set in order to
be able to classify a new observation (x6, x7), such as the one denoted by the cross
in Figure 1.19. We observe that the cross is surrounded by numerous red points, and
so we might suppose that it belongs to the red class. However, there are also plenty
of green points nearby, so we might think that it could instead belong to the green
class. It seems unlikely that it belongs to the blue class. The intuition here is that the
identity of the cross should be determined more strongly by nearby points from the
training set and less strongly by more distant points. In fact, this intuition turns out
to be reasonable and will be discussed more fully in later chapters.

How can we turn this intuition into a learning algorithm? One very simple ap-
proach would be to divide the input space into regular cells, as indicated in Fig-
ure 1.20. When we are given a test point and we wish to predict its class, we ﬁrst
decide which cell it belongs to, and we then ﬁnd all of the training data points that

1.4. The Curse of Dimensionality

35

Figure 1.20 Illustration of a simple approach
to the solution of a classiﬁcation
problem in which the input space
is divided into cells and any new
test point is assigned to the class
that has a majority number of rep-
resentatives in the same cell as
the test point. As we shall see
shortly,
this simplistic approach
has some severe shortcomings.

2

1.5

x7

1

0.5

0

0

0.25

0.5
x6

0.75

1

fall in the same cell. The identity of the test point is predicted as being the same
as the class having the largest number of training points in the same cell as the test
point (with ties being broken at random).

There are numerous problems with this naive approach, but one of the most se-
vere becomes apparent when we consider its extension to problems having larger
numbers of input variables, corresponding to input spaces of higher dimensionality.
The origin of the problem is illustrated in Figure 1.21, which shows that, if we divide
a region of a space into regular cells, then the number of such cells grows exponen-
tially with the dimensionality of the space. The problem with an exponentially large
number of cells is that we would need an exponentially large quantity of training data
in order to ensure that the cells are not empty. Clearly, we have no hope of applying
such a technique in a space of more than a few variables, and so we need to ﬁnd a
more sophisticated approach.

We can gain further insight into the problems of high-dimensional spaces by
returning to the example of polynomial curve ﬁtting and considering how we would

Section 1.1

of

Figure 1.21 Illustration
the
curse of dimensionality, showing
how the number of regions of a
regular grid grows exponentially
with the dimensionality D of
the
space. For clarity, only a subset of
the cubical regions are shown for
D = 3.

x2

x2

x1

D = 1

x1

x3

D = 2

D = 3

x1



D



D



D



D



D



D

36

1. INTRODUCTION

extend this approach to deal with input spaces having several variables. If we have
D input variables, then a general polynomial with coefﬁcients up to order 3 would
take the form

Exercise 1.16

y(x, w) = w0 +

wixi +

wijxixj +

wijkxixjxk.

(1.74)

i=1

i=1

j=1

i=1

j=1

k=1

As D increases, so the number of independent coefﬁcients (not all of the coefﬁcients
are independent due to interchange symmetries amongst the x variables) grows pro-
portionally to D3. In practice, to capture complex dependencies in the data, we may
need to use a higher-order polynomial. For a polynomial of order M, the growth in
the number of coefﬁcients is like DM . Although this is now a power law growth,
rather than an exponential growth, it still points to the method becoming rapidly
unwieldy and of limited practical utility.

Our geometrical intuitions, formed through a life spent in a space of three di-
mensions, can fail badly when we consider spaces of higher dimensionality. As a
simple example, consider a sphere of radius r = 1 in a space of D dimensions, and
ask what is the fraction of the volume of the sphere that lies between radius r = 1−	
and r = 1. We can evaluate this fraction by noting that the volume of a sphere of
radius r in D dimensions must scale as rD, and so we write

VD(r) = KDrD

(1.75)

Exercise 1.18

where the constant KD depends only on D. Thus the required fraction is given by

Exercise 1.20

VD(1) − VD(1 − 	)

VD(1)

= 1 − (1 − 	)D

(1.76)

which is plotted as a function of 	 for various values of D in Figure 1.22. We see
that, for large D, this fraction tends to 1 even for small values of 	. Thus, in spaces
of high dimensionality, most of the volume of a sphere is concentrated in a thin shell
near the surface!

As a further example, of direct relevance to pattern recognition, consider the
behaviour of a Gaussian distribution in a high-dimensional space. If we transform
from Cartesian to polar coordinates, and then integrate out the directional variables,
we obtain an expression for the density p(r) as a function of radius r from the origin.
Thus p(r)δr is the probability mass inside a thin shell of thickness δr located at
radius r. This distribution is plotted, for various values of D, in Figure 1.23, and we
see that for large D the probability mass of the Gaussian is concentrated in a thin
shell.

The severe difﬁculty that can arise in spaces of many dimensions is sometimes
called the curse of dimensionality (Bellman, 1961). In this book, we shall make ex-
tensive use of illustrative examples involving input spaces of one or two dimensions,
because this makes it particularly easy to illustrate the techniques graphically. The
reader should be warned, however, that not all intuitions developed in spaces of low
dimensionality will generalize to spaces of many dimensions.

1.4. The Curse of Dimensionality

37

Figure 1.22 Plot of the fraction of the volume of
a sphere lying in the range r = 1−
to r = 1 for various values of the
dimensionality D.

n
o

i
t
c
a
r
f
 

e
m
u
o
v

l

1

0.8

0.6

0.4

0.2

0

0

D = 20

D = 5

D = 2

D = 1

0.2

0.4

0.6

0.8

1

	

Although the curse of dimensionality certainly raises important issues for pat-
tern recognition applications, it does not prevent us from ﬁnding effective techniques
applicable to high-dimensional spaces. The reasons for this are twofold. First, real
data will often be conﬁned to a region of the space having lower effective dimension-
ality, and in particular the directions over which important variations in the target
variables occur may be so conﬁned. Second, real data will typically exhibit some
smoothness properties (at least locally) so that for the most part small changes in the
input variables will produce small changes in the target variables, and so we can ex-
ploit local interpolation-like techniques to allow us to make predictions of the target
variables for new values of the input variables. Successful pattern recognition tech-
niques exploit one or both of these properties. Consider, for example, an application
in manufacturing in which images are captured of identical planar objects on a con-
veyor belt, in which the goal is to determine their orientation. Each image is a point

Figure 1.23 Plot of the probability density with
respect
to radius r of a Gaus-
sian distribution for various values
of
In a
high-dimensional space, most of the
probability mass of a Gaussian is lo-
cated within a thin shell at a speciﬁc
radius.

the dimensionality D.

2

)
r
(
p

1

D = 1

D = 2

D = 20

0

0

2
r

4

38

1. INTRODUCTION

in a high-dimensional space whose dimensionality is determined by the number of
pixels. Because the objects can occur at different positions within the image and
in different orientations, there are three degrees of freedom of variability between
images, and a set of images will live on a three dimensional manifold embedded
within the high-dimensional space. Due to the complex relationships between the
object position or orientation and the pixel intensities, this manifold will be highly
nonlinear. If the goal is to learn a model that can take an input image and output the
orientation of the object irrespective of its position, then there is only one degree of
freedom of variability within the manifold that is signiﬁcant.

1.5. Decision Theory

We have seen in Section 1.2 how probability theory provides us with a consistent
mathematical framework for quantifying and manipulating uncertainty. Here we
turn to a discussion of decision theory that, when combined with probability theory,
allows us to make optimal decisions in situations involving uncertainty such as those
encountered in pattern recognition.

Suppose we have an input vector x together with a corresponding vector t of
target variables, and our goal is to predict t given a new value for x. For regression
problems, t will comprise continuous variables, whereas for classiﬁcation problems
t will represent class labels. The joint probability distribution p(x, t) provides a
complete summary of the uncertainty associated with these variables. Determination
of p(x, t) from a set of training data is an example of inference and is typically a
very difﬁcult problem whose solution forms the subject of much of this book. In
a practical application, however, we must often make a speciﬁc prediction for the
value of t, or more generally take a speciﬁc action based on our understanding of the
values t is likely to take, and this aspect is the subject of decision theory.

Consider, for example, a medical diagnosis problem in which we have taken an
X-ray image of a patient, and we wish to determine whether the patient has cancer
or not. In this case, the input vector x is the set of pixel intensities in the image,
and output variable t will represent the presence of cancer, which we denote by the
class C1, or the absence of cancer, which we denote by the class C2. We might, for
instance, choose t to be a binary variable such that t = 0 corresponds to class C1 and
t = 1 corresponds to class C2. We shall see later that this choice of label values is
particularly convenient for probabilistic models. The general inference problem then
involves determining the joint distribution p(x,Ck), or equivalently p(x, t), which
gives us the most complete probabilistic description of the situation. Although this
can be a very useful and informative quantity, in the end we must decide either to
give treatment to the patient or not, and we would like this choice to be optimal
in some appropriate sense (Duda and Hart, 1973). This is the decision step, and
it is the subject of decision theory to tell us how to make optimal decisions given
the appropriate probabilities. We shall see that the decision stage is generally very
simple, even trivial, once we have solved the inference problem.

Here we give an introduction to the key ideas of decision theory as required for





1.5. Decision Theory

39

the rest of the book. Further background, as well as more detailed accounts, can be
found in Berger (1985) and Bather (2000).

Before giving a more detailed analysis, let us ﬁrst consider informally how we
might expect probabilities to play a role in making decisions. When we obtain the
X-ray image x for a new patient, our goal is to decide which of the two classes to
assign to the image. We are interested in the probabilities of the two classes given
the image, which are given by p(Ck|x). Using Bayes’ theorem, these probabilities
can be expressed in the form

p(Ck|x) = p(x|Ck)p(Ck)

p(x)

.

(1.77)

Note that any of the quantities appearing in Bayes’ theorem can be obtained from
the joint distribution p(x,Ck) by either marginalizing or conditioning with respect to
the appropriate variables. We can now interpret p(Ck) as the prior probability for the
class Ck, and p(Ck|x) as the corresponding posterior probability. Thus p(C1) repre-
sents the probability that a person has cancer, before we take the X-ray measurement.
Similarly, p(C1|x) is the corresponding probability, revised using Bayes’ theorem in
light of the information contained in the X-ray. If our aim is to minimize the chance
of assigning x to the wrong class, then intuitively we would choose the class having
the higher posterior probability. We now show that this intuition is correct, and we
also discuss more general criteria for making decisions.

1.5.1 Minimizing the misclassiﬁcation rate
Suppose that our goal is simply to make as few misclassiﬁcations as possible.
We need a rule that assigns each value of x to one of the available classes. Such a
rule will divide the input space into regions Rk called decision regions, one for each
class, such that all points in Rk are assigned to class Ck. The boundaries between
decision regions are called decision boundaries or decision surfaces. Note that each
decision region need not be contiguous but could comprise some number of disjoint
regions. We shall encounter examples of decision boundaries and decision regions in
later chapters. In order to ﬁnd the optimal decision rule, consider ﬁrst of all the case
of two classes, as in the cancer problem for instance. A mistake occurs when an input
vector belonging to class C1 is assigned to class C2 or vice versa. The probability of
this occurring is given by

p(mistake) = p(x ∈ R1,C2) + p(x ∈ R2,C1)

=

p(x,C2) dx +

R1

p(x,C1) dx.

R2

(1.78)

We are free to choose the decision rule that assigns each point x to one of the two
classes. Clearly to minimize p(mistake) we should arrange that each x is assigned to
whichever class has the smaller value of the integrand in (1.78). Thus, if p(x,C1) >
p(x,C2) for a given value of x, then we should assign that x to class C1. From the
product rule of probability we have p(x,Ck) = p(Ck|x)p(x). Because the factor
p(x) is common to both terms, we can restate this result as saying that the minimum



x

b

x0




k=1

K

K



b

b

b

b

b

b

b

40

1. INTRODUCTION

p(x,C1)

R1

p(x,C2)

R2

x

x. Values of x �

Figure 1.24 Schematic illustration of the joint probabilities p(x,Ck) for each of two classes plotted
against x, together with the decision boundary x =
x are classiﬁed as
class C2 and hence belong to decision region R2, whereas points x <
x are classiﬁed
as C1 and belong to R1. Errors arise from the blue, green, and red regions, so that for
x the errors are due to points from class C2 being misclassiﬁed as C1 (represented by
x <
the sum of the red and green regions), and conversely for points in the region x �
x the
errors are due to points from class C1 being misclassiﬁed as C2 (represented by the blue
region). As we vary the location
x of the decision boundary, the combined areas of the
blue and green regions remains constant, whereas the size of the red region varies. The
optimal choice for
x is where the curves for p(x,C1) and p(x, C2) cross, corresponding to
x = x0, because in this case the red region disappears. This is equivalent to the minimum
misclassiﬁcation rate decision rule, which assigns each value of x to the class having the
higher posterior probability p(Ck|x).

probability of making a mistake is obtained if each value of x is assigned to the class
for which the posterior probability p(Ck|x) is largest. This result is illustrated for
two classes, and a single input variable x, in Figure 1.24.
For the more general case of K classes, it is slightly easier to maximize the

probability of being correct, which is given by

p(correct) =

p(x ∈ Rk,Ck)

=

k=1

Rk

p(x,Ck) dx

(1.79)

which is maximized when the regions Rk are chosen such that each x is assigned
to the class for which p(x,Ck) is largest. Again, using the product rule p(x,Ck) =
p(Ck|x)p(x), and noting that the factor of p(x) is common to all terms, we see
that each x should be assigned to the class having the largest posterior probability
p(Ck|x).

 cancer











Figure 1.25 An example of a loss matrix with ele-
ments Lkj for the cancer treatment problem. The rows
correspond to the true class, whereas the columns cor-
respond to the assignment of class made by our deci-
sion criterion.

cancer
normal

0
1

normal
1000

0

1.5. Decision Theory

41

1.5.2 Minimizing the expected loss
For many applications, our objective will be more complex than simply mini-
mizing the number of misclassiﬁcations. Let us consider again the medical diagnosis
problem. We note that, if a patient who does not have cancer is incorrectly diagnosed
as having cancer, the consequences may be some patient distress plus the need for
further investigations. Conversely, if a patient with cancer is diagnosed as healthy,
the result may be premature death due to lack of treatment. Thus the consequences
of these two types of mistake can be dramatically different. It would clearly be better
to make fewer mistakes of the second kind, even if this was at the expense of making
more mistakes of the ﬁrst kind.

We can formalize such issues through the introduction of a loss function, also
called a cost function, which is a single, overall measure of loss incurred in taking
any of the available decisions or actions. Our goal is then to minimize the total loss
incurred. Note that some authors consider instead a utility function, whose value
they aim to maximize. These are equivalent concepts if we take the utility to be
simply the negative of the loss, and throughout this text we shall use the loss function
convention. Suppose that, for a new value of x, the true class is Ck and that we assign
x to class Cj (where j may or may not be equal to k). In so doing, we incur some
level of loss that we denote by Lkj, which we can view as the k, j element of a loss
matrix. For instance, in our cancer example, we might have a loss matrix of the form
shown in Figure 1.25. This particular loss matrix says that there is no loss incurred
if the correct decision is made, there is a loss of 1 if a healthy patient is diagnosed as
having cancer, whereas there is a loss of 1000 if a patient having cancer is diagnosed
as healthy.

The optimal solution is the one which minimizes the loss function. However,
the loss function depends on the true class, which is unknown. For a given input
vector x, our uncertainty in the true class is expressed through the joint probability
distribution p(x,Ck) and so we seek instead to minimize the average loss, where the
average is computed with respect to this distribution, which is given by

E[L] =

k

j

Rj

Lkjp(x,Ck) dx.

(1.80)

Each x can be assigned independently to one of the decision regions Rj. Our goal
is to choose the regions Rj in order to minimize the expected loss (1.80), which
k Lkjp(x,Ck). As before, we can use
implies that for each x we should minimize
the product rule p(x,Ck) = p(Ck|x)p(x) to eliminate the common factor of p(x).
Thus the decision rule that minimizes the expected loss is the one that assigns each

new x to the class j for which the quantity

Inputs
x such that the larger of the two poste-
rior probabilities is less than or equal to
some threshold θ will be rejected.

1.0
θ

42

1. INTRODUCTION

Figure 1.26 Illustration of the reject option.

p(C1|x)

p(C2|x)

reject region

x

0.0

Lkjp(Ck|x)

k

(1.81)

is a minimum. This is clearly trivial to do, once we know the posterior class proba-
bilities p(Ck|x).

1.5.3 The reject option
We have seen that classiﬁcation errors arise from the regions of input space
where the largest of the posterior probabilities p(Ck|x) is signiﬁcantly less than unity,
or equivalently where the joint distributions p(x,Ck) have comparable values. These
are the regions where we are relatively uncertain about class membership. In some
applications, it will be appropriate to avoid making decisions on the difﬁcult cases
in anticipation of a lower error rate on those examples for which a classiﬁcation de-
cision is made. This is known as the reject option. For example, in our hypothetical
medical illustration, it may be appropriate to use an automatic system to classify
those X-ray images for which there is little doubt as to the correct class, while leav-
ing a human expert to classify the more ambiguous cases. We can achieve this by
introducing a threshold θ and rejecting those inputs x for which the largest of the
posterior probabilities p(Ck|x) is less than or equal to θ. This is illustrated for the
case of two classes, and a single continuous input variable x, in Figure 1.26. Note
that setting θ = 1 will ensure that all examples are rejected, whereas if there are K
classes then setting θ < 1/K will ensure that no examples are rejected. Thus the
fraction of examples that get rejected is controlled by the value of θ.

We can easily extend the reject criterion to minimize the expected loss, when
a loss matrix is given, taking account of the loss incurred when a reject decision is
made.

1.5.4 Inference and decision
We have broken the classiﬁcation problem down into two separate stages, the
inference stage in which we use training data to learn a model for p(Ck|x), and the

Exercise 1.24



p(Ck|x) = p(x|Ck)p(Ck)

p(x)

(1.82)

to ﬁnd the posterior class probabilities p(Ck|x). As usual, the denominator
in Bayes’ theorem can be found in terms of the quantities appearing in the
numerator, because

p(x) =

p(x|Ck)p(Ck).

k

(1.83)

Equivalently, we can model the joint distribution p(x,Ck) directly and then
normalize to obtain the posterior probabilities. Having found the posterior
probabilities, we use decision theory to determine class membership for each
new input x. Approaches that explicitly or implicitly model the distribution of
inputs as well as outputs are known as generative models, because by sampling
from them it is possible to generate synthetic data points in the input space.

(b) First solve the inference problem of determining the posterior class probabilities
p(Ck|x), and then subsequently use decision theory to assign each new x to
one of the classes. Approaches that model the posterior probabilities directly
are called discriminative models.

(c) Find a function f(x), called a discriminant function, which maps each input x
directly onto a class label. For instance, in the case of two-class problems,
f(·) might be binary valued and such that f = 0 represents class C1 and f = 1
represents class C2. In this case, probabilities play no role.

Let us consider the relative merits of these three alternatives. Approach (a) is the
most demanding because it involves ﬁnding the joint distribution over both x and
Ck. For many applications, x will have high dimensionality, and consequently we
may need a large training set in order to be able to determine the class-conditional
densities to reasonable accuracy. Note that the class priors p(Ck) can often be esti-
mated simply from the fractions of the training set data points in each of the classes.
One advantage of approach (a), however, is that it also allows the marginal density
of data p(x) to be determined from (1.83). This can be useful for detecting new data
points that have low probability under the model and for which the predictions may

1.5. Decision Theory

43

subsequent decision stage in which we use these posterior probabilities to make op-
timal class assignments. An alternative possibility would be to solve both problems
together and simply learn a function that maps inputs x directly into decisions. Such
a function is called a discriminant function.

In fact, we can identify three distinct approaches to solving decision problems,
all of which have been used in practical applications. These are given, in decreasing
order of complexity, by:

(a) First solve the inference problem of determining the class-conditional densities
p(x|Ck) for each class Ck individually. Also separately infer the prior class
probabilities p(Ck). Then use Bayes’ theorem in the form

44

1. INTRODUCTION

s
e

i
t
i
s
n
e
d

 
s
s
a
c

l

5

4

3

2

1

0

0

p(x|C2)

p(x|C1)

0.2

0.4

0.6

0.8

1

x

1.2

1

0.8

0.6

0.4

0.2

0

0

p(C1|x)

p(C2|x)

0.2

0.4

x

0.6

0.8

1

Figure 1.27 Example of the class-conditional densities for two classes having a single input variable x (left
plot) together with the corresponding posterior probabilities (right plot). Note that the left-hand mode of the
class-conditional density p(x|C1), shown in blue on the left plot, has no effect on the posterior probabilities. The
vertical green line in the right plot shows the decision boundary in x that gives the minimum misclassiﬁcation
rate.

be of low accuracy, which is known as outlier detection or novelty detection (Bishop,
1994; Tarassenko, 1995).

However, if we only wish to make classiﬁcation decisions, then it can be waste-
ful of computational resources, and excessively demanding of data, to ﬁnd the joint
distribution p(x,Ck) when in fact we only really need the posterior probabilities
p(Ck|x), which can be obtained directly through approach (b). Indeed, the class-
conditional densities may contain a lot of structure that has little effect on the pos-
terior probabilities, as illustrated in Figure 1.27. There has been much interest in
exploring the relative merits of generative and discriminative approaches to machine
learning, and in ﬁnding ways to combine them (Jebara, 2004; Lasserre et al., 2006).
An even simpler approach is (c) in which we use the training data to ﬁnd a
discriminant function f(x) that maps each x directly onto a class label, thereby
combining the inference and decision stages into a single learning problem. In the
example of Figure 1.27, this would correspond to ﬁnding the value of x shown by
the vertical green line, because this is the decision boundary giving the minimum
probability of misclassiﬁcation.

With option (c), however, we no longer have access to the posterior probabilities
p(Ck|x). There are many powerful reasons for wanting to compute the posterior
probabilities, even if we subsequently use them to make decisions. These include:

Minimizing risk. Consider a problem in which the elements of the loss matrix are
subjected to revision from time to time (such as might occur in a ﬁnancial

1.5. Decision Theory

45

application). If we know the posterior probabilities, we can trivially revise the
minimum risk decision criterion by modifying (1.81) appropriately. If we have
only a discriminant function, then any change to the loss matrix would require
that we return to the training data and solve the classiﬁcation problem afresh.

Reject option. Posterior probabilities allow us to determine a rejection criterion that
will minimize the misclassiﬁcation rate, or more generally the expected loss,
for a given fraction of rejected data points.

Compensating for class priors. Consider our medical X-ray problem again, and
suppose that we have collected a large number of X-ray images from the gen-
eral population for use as training data in order to build an automated screening
system. Because cancer is rare amongst the general population, we might ﬁnd
that, say, only 1 in every 1,000 examples corresponds to the presence of can-
cer. If we used such a data set to train an adaptive model, we could run into
severe difﬁculties due to the small proportion of the cancer class. For instance,
a classiﬁer that assigned every point to the normal class would already achieve
99.9% accuracy and it would be difﬁcult to avoid this trivial solution. Also,
even a large data set will contain very few examples of X-ray images corre-
sponding to cancer, and so the learning algorithm will not be exposed to a
broad range of examples of such images and hence is not likely to generalize
well. A balanced data set in which we have selected equal numbers of exam-
ples from each of the classes would allow us to ﬁnd a more accurate model.
However, we then have to compensate for the effects of our modiﬁcations to
the training data. Suppose we have used such a modiﬁed data set and found
models for the posterior probabilities. From Bayes’ theorem (1.82), we see that
the posterior probabilities are proportional to the prior probabilities, which we
can interpret as the fractions of points in each class. We can therefore simply
take the posterior probabilities obtained from our artiﬁcially balanced data set
and ﬁrst divide by the class fractions in that data set and then multiply by the
class fractions in the population to which we wish to apply the model. Finally,
we need to normalize to ensure that the new posterior probabilities sum to one.
Note that this procedure cannot be applied if we have learned a discriminant
function directly instead of determining posterior probabilities.

Combining models. For complex applications, we may wish to break the problem
into a number of smaller subproblems each of which can be tackled by a sep-
arate module. For example, in our hypothetical medical diagnosis problem,
we may have information available from, say, blood tests as well as X-ray im-
ages. Rather than combine all of this heterogeneous information into one huge
input space, it may be more effective to build one system to interpret the X-
ray images and a different one to interpret the blood data. As long as each of
the two models gives posterior probabilities for the classes, we can combine
the outputs systematically using the rules of probability. One simple way to
do this is to assume that, for each class separately, the distributions of inputs
for the X-ray images, denoted by xI, and the blood data, denoted by xB, are









independent, so that

p(xI, xB|Ck) = p(xI|Ck)p(xB|Ck).

(1.84)

This is an example of conditional independence property, because the indepen-
dence holds when the distribution is conditioned on the class Ck. The posterior
probability, given both the X-ray and blood data, is then given by

p(Ck|xI, xB) ∝ p(xI, xB|Ck)p(Ck)
p(Ck|xI)p(Ck|xB)

∝ p(xI|Ck)p(xB|Ck)p(Ck)
∝

p(Ck)

(1.85)

Thus we need the class prior probabilities p(Ck), which we can easily estimate
from the fractions of data points in each class, and then we need to normalize
the resulting posterior probabilities so they sum to one. The particular condi-
tional independence assumption (1.84) is an example of the naive Bayes model.
Note that the joint marginal distribution p(xI, xB) will typically not factorize
under this model. We shall see in later chapters how to construct models for
combining data that do not require the conditional independence assumption
(1.84).

1.5.5 Loss functions for regression
So far, we have discussed decision theory in the context of classiﬁcation prob-
lems. We now turn to the case of regression problems, such as the curve ﬁtting
example discussed earlier. The decision stage consists of choosing a speciﬁc esti-
mate y(x) of the value of t for each input x. Suppose that in doing so, we incur a
loss L(t, y(x)). The average, or expected, loss is then given by

E[L] =

L(t, y(x))p(x, t) dx dt.

(1.86)

A common choice of loss function in regression problems is the squared loss given
by L(t, y(x)) = {y(x) − t}2. In this case, the expected loss can be written

E[L] =

{y(x) − t}2p(x, t) dx dt.

Our goal is to choose y(x) so as to minimize E[L].
If we assume a completely
ﬂexible function y(x), we can do this formally using the calculus of variations to
give

δE[L]
δy(x)

= 2

{y(x) − t}p(x, t) dt = 0.

Solving for y(x), and using the sum and product rules of probability, we obtain

(1.87)

(1.88)

tp(x, t) dt

y(x) =

p(x)

=

tp(t|x) dt = Et[t|x]

(1.89)

46

1. INTRODUCTION

Section 8.2

Section 8.2.2

Section 1.1

Appendix D





Figure 1.28 The regression function y(x),
which minimizes the expected
squared loss, is given by the
mean of the conditional distri-
bution p(t|x).

t

y(x0)

1.5. Decision Theory

47

y(x)

p(t|x0)

x0

x

Exercise 1.25

which is the conditional average of t conditioned on x and is known as the regression
function. This result is illustrated in Figure 1.28. It can readily be extended to mul-
tiple target variables represented by the vector t, in which case the optimal solution
is the conditional average y(x) = Et[t|x].
We can also derive this result in a slightly different way, which will also shed
light on the nature of the regression problem. Armed with the knowledge that the
optimal solution is the conditional expectation, we can expand the square term as
follows

{y(x) − t}2 = {y(x) − E[t|x] + E[t|x] − t}2
= {y(x) − E[t|x]}2 + 2{y(x) − E[t|x]}{E[t|x] − t} + {E[t|x] − t}2

where, to keep the notation uncluttered, we use E[t|x] to denote Et[t|x]. Substituting
into the loss function and performing the integral over t, we see that the cross-term
vanishes and we obtain an expression for the loss function in the form

E[L] =

{y(x) − E[t|x]}2 p(x) dx +

{E[t|x] − t}2p(x) dx.

(1.90)

The function y(x) we seek to determine enters only in the ﬁrst term, which will be
minimized when y(x) is equal to E[t|x], in which case this term will vanish. This
is simply the result that we derived previously and that shows that the optimal least
squares predictor is given by the conditional mean. The second term is the variance
of the distribution of t, averaged over x.
It represents the intrinsic variability of
the target data and can be regarded as noise. Because it is independent of y(x), it
represents the irreducible minimum value of the loss function.

As with the classiﬁcation problem, we can either determine the appropriate prob-
abilities and then use these to make optimal decisions, or we can build models that
make decisions directly. Indeed, we can identify three distinct approaches to solving
regression problems given, in order of decreasing complexity, by:
(a) First solve the inference problem of determining the joint density p(x, t). Then
normalize to ﬁnd the conditional density p(t|x), and ﬁnally marginalize to ﬁnd
the conditional mean given by (1.89).



Section 5.6

Exercise 1.27

E[Lq] =

|y(x) − t|qp(x, t) dx dt

(1.91)
which reduces to the expected squared loss for q = 2. The function |y − t|q is
plotted against y − t for various values of q in Figure 1.29. The minimum of E[Lq]
is given by the conditional mean for q = 2, the conditional median for q = 1, and
the conditional mode for q → 0.

1.6.

Information Theory

In this chapter, we have discussed a variety of concepts from probability theory and
decision theory that will form the foundations for much of the subsequent discussion
in this book. We close this chapter by introducing some additional concepts from
the ﬁeld of information theory, which will also prove useful in our development of
pattern recognition and machine learning techniques. Again, we shall focus only on
the key concepts, and we refer the reader elsewhere for more detailed discussions
(Viterbi and Omura, 1979; Cover and Thomas, 1991; MacKay, 2003) .

We begin by considering a discrete random variable x and we ask how much
information is received when we observe a speciﬁc value for this variable. The
amount of information can be viewed as the ‘degree of surprise’ on learning the
value of x. If we are told that a highly improbable event has just occurred, we will
have received more information than if we were told that some very likely event
has just occurred, and if we knew that the event was certain to happen we would
receive no information. Our measure of information content will therefore depend
on the probability distribution p(x), and we therefore look for a quantity h(x) that
is a monotonic function of the probability p(x) and that expresses the information
content. The form of h(·) can be found by noting that if we have two events x
and y that are unrelated, then the information gain from observing both of them
should be the sum of the information gained from each of them separately, so that
h(x, y) = h(x) + h(y). Two unrelated events will be statistically independent and
so p(x, y) = p(x)p(y). From these two relationships, it is easily shown that h(x)
must be given by the logarithm of p(x) and so we have

Exercise 1.28

48

1. INTRODUCTION

(b) First solve the inference problem of determining the conditional density p(t|x),
and then subsequently marginalize to ﬁnd the conditional mean given by (1.89).

(c) Find a regression function y(x) directly from the training data.
The relative merits of these three approaches follow the same lines as for classiﬁca-
tion problems above.

The squared loss is not the only possible choice of loss function for regression.
Indeed, there are situations in which squared loss can lead to very poor results and
where we need to develop more sophisticated approaches. An important example
concerns situations in which the conditional distribution p(t|x) is multimodal, as
often arises in the solution of inverse problems. Here we consider brieﬂy one simple
generalization of the squared loss, called the Minkowski loss, whose expectation is
given by

2

q
|
t

1
−
y
|

0
−2

2

q
|
t

1
−
y
|

q = 0.3

−1

0
y − t

1

2

q = 2

0
−2

−1

1

2

0
y − t

1.6. Information Theory

49

q = 1

−1

0
y − t

1

2

q = 10

−1

0
y − t

1

2

2



0
−2

0
−2

2

q
|
t

1
−
y
|

q
|
t

1
−
y
|

Figure 1.29 Plots of the quantity Lq = |y − t|q for various values of q.

h(x) = − log2 p(x)

(1.92)

where the negative sign ensures that information is positive or zero. Note that low
probability events x correspond to high information content. The choice of basis
for the logarithm is arbitrary, and for the moment we shall adopt the convention
prevalent in information theory of using logarithms to the base of 2. In this case, as
we shall see shortly, the units of h(x) are bits (‘binary digits’).

Now suppose that a sender wishes to transmit the value of a random variable to
a receiver. The average amount of information that they transmit in the process is
obtained by taking the expectation of (1.92) with respect to the distribution p(x) and
is given by

H[x] = −

x

p(x) log2 p(x).

(1.93)

This important quantity is called the entropy of the random variable x. Note that
limp→0 p ln p = 0 and so we shall take p(x) ln p(x) = 0 whenever we encounter a
value for x such that p(x) = 0.
So far we have given a rather heuristic motivation for the deﬁnition of informa-

50

1. INTRODUCTION

tion (1.92) and the corresponding entropy (1.93). We now show that these deﬁnitions
indeed possess useful properties. Consider a random variable x having 8 possible
states, each of which is equally likely. In order to communicate the value of x to
a receiver, we would need to transmit a message of length 3 bits. Notice that the
entropy of this variable is given by

H[x] = −8 ×

1
8

log2

1
8

= 3 bits.

1
8

1
64

Now consider an example (Cover and Thomas, 1991) of a variable having 8 pos-
sible states {a, b, c, d, e, f, g, h} for which the respective probabilities are given by
( 1
2 , 1

8 , 1

4 , 1

16 , 1
64 , 1
1
2

64). The entropy in this case is given by
64 , 1
64 , 1
1
4
1
2 −
4
64

1
16 −

1
4 −

1
8 −

1
16

log2

log2

log2

log2

log2

= 2 bits.

H[x] = −
We see that the nonuniform distribution has a smaller entropy than the uniform one,
and we shall gain some insight into this shortly when we discuss the interpretation of
entropy in terms of disorder. For the moment, let us consider how we would transmit
the identity of the variable’s state to a receiver. We could do this, as before, using
a 3-bit number. However, we can take advantage of the nonuniform distribution by
using shorter codes for the more probable events, at the expense of longer codes for
the less probable events, in the hope of getting a shorter average code length. This
can be done by representing the states {a, b, c, d, e, f, g, h} using, for instance, the
following set of code strings: 0, 10, 110, 1110, 111100, 111101, 111110, 111111.
The average length of the code that has to be transmitted is then

1
2 × 1 +

1
4 × 2 +

average code length =

1
64 × 6 = 2 bits
which again is the same as the entropy of the random variable. Note that shorter code
strings cannot be used because it must be possible to disambiguate a concatenation
of such strings into its component parts. For instance, 11001110 decodes uniquely
into the state sequence c, a, d.

1
16 × 4 + 4 ×

1
8 × 3 +

This relation between entropy and shortest coding length is a general one. The
noiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound
on the number of bits needed to transmit the state of a random variable.

From now on, we shall switch to the use of natural logarithms in deﬁning en-
tropy, as this will provide a more convenient link with ideas elsewhere in this book.
In this case, the entropy is measured in units of ‘nats’ instead of bits, which differ
simply by a factor of ln 2.

We have introduced the concept of entropy in terms of the average amount of
information needed to specify the state of a random variable. In fact, the concept of
entropy has much earlier origins in physics where it was introduced in the context
of equilibrium thermodynamics and later given a deeper interpretation as a measure
of disorder through developments in statistical mechanics. We can understand this
alternative view of entropy by considering a set of N identical objects that are to be
divided amongst a set of bins, such that there are ni objects in the ith bin. Consider

W = N!





ni
N

1
N








= −

1
N

i

i



1
N







1.6. Information Theory

51

the number of different ways of allocating the objects to the bins. There are N
ways to choose the ﬁrst object, (N − 1) ways to choose the second object, and
so on, leading to a total of N! ways to allocate all N objects to the bins, where N!
(pronounced ‘factorial N’) denotes the product N ×(N −1)×···×2×1. However,
we don’t wish to distinguish between rearrangements of objects within each bin. In
the ith bin there are ni! ways of reordering the objects, and so the total number of
ways of allocating the N objects to the bins is given by

which is called the multiplicity. The entropy is then deﬁned as the logarithm of the
multiplicity scaled by an appropriate constant

i ni!

(1.94)

H =

ln W =

ln N! −

ln ni!.

(1.95)

We now consider the limit N → ∞, in which the fractions ni/N are held ﬁxed, and
apply Stirling’s approximation

ln N!  N ln N − N

(1.96)

which gives

ni
N

i

ln

H = − lim
N→∞
i ni = N. Here pi = limN→∞(ni/N) is the probability
where we have used
of an object being assigned to the ith bin. In physics terminology, the speciﬁc ar-
rangements of objects in the bins is called a microstate, and the overall distribution
of occupation numbers, expressed through the ratios ni/N, is called a macrostate.
The multiplicity W is also known as the weight of the macrostate.

pi ln pi

(1.97)

We can interpret the bins as the states xi of a discrete random variable X, where

p(X = xi) = pi. The entropy of the random variable X is then

H[p] = −

i

p(xi) ln p(xi).

(1.98)

Distributions p(xi) that are sharply peaked around a few values will have a relatively
low entropy, whereas those that are spread more evenly across many values will
have higher entropy, as illustrated in Figure 1.30. Because 0 � pi � 1, the entropy
is nonnegative, and it will equal its minimum value of 0 when one of the pi =
1 and all other pj=i = 0. The maximum entropy conﬁguration can be found by
maximizing H using a Lagrange multiplier to enforce the normalization constraint
on the probabilities. Thus we maximize

H = −

i

p(xi) ln p(xi) + λ

p(xi) − 1

i

(1.99)

Appendix E

H = 1.77

H = 3.09

52

1. INTRODUCTION

0.5

s
e

i
t
i
l
i

b
a
b
o
r
p

0.25

0



s
e

i
t
i
l
i

b
a
b
o
r
p

0.25

0.5

0









Figure 1.30 Histograms of two probability distributions over 30 bins illustrating the higher value of the entropy
H for the broader distribution. The largest entropy would arise from a uniform distribution that would give H =
− ln(1/30) = 3.40.

Exercise 1.29

from which we ﬁnd that all of the p(xi) are equal and are given by p(xi) = 1/M
where M is the total number of states xi. The corresponding value of the entropy
is then H = ln M. This result can also be derived from Jensen’s inequality (to be
discussed shortly). To verify that the stationary point is indeed a maximum, we can
evaluate the second derivative of the entropy, which gives

H

∂

∂p(xi)∂p(xj)

= −Iij

1
pi

(1.100)

where Iij are the elements of the identity matrix.

We can extend the deﬁnition of entropy to include distributions p(x) over con-
tinuous variables x as follows. First divide x into bins of width ∆. Then, assuming
p(x) is continuous, the mean value theorem (Weisstein, 1999) tells us that, for each
such bin, there must exist a value xi such that

(i+1)∆

i∆

p(x) dx = p(xi)∆.

(1.101)

We can now quantize the continuous variable x by assigning any value x to the value
xi whenever x falls in the ith bin. The probability of observing the value xi is then
p(xi)∆. This gives a discrete distribution for which the entropy takes the form

H∆ = −

i

p(xi)∆ ln (p(xi)∆) = −

p(xi)∆ ln p(xi) − ln ∆

(1.102)

i

i p(xi)∆ = 1, which follows from (1.101). We now omit
where we have used
the second term − ln ∆ on the right-hand side of (1.102) and then consider the limit









1.6. Information Theory

53

∆ → 0. The ﬁrst term on the right-hand side of (1.102) will approach the integral of
p(x) ln p(x) in this limit so that

lim
∆→0

i

p(xi)∆ ln p(xi)

= −

p(x) ln p(x) dx

(1.103)

where the quantity on the right-hand side is called the differential entropy. We see
that the discrete and continuous forms of the entropy differ by a quantity ln ∆, which
diverges in the limit ∆ → 0. This reﬂects the fact that to specify a continuous
variable very precisely requires a large number of bits. For a density deﬁned over
multiple continuous variables, denoted collectively by the vector x, the differential
entropy is given by

H[x] = −

p(x) ln p(x) dx.

(1.104)

In the case of discrete distributions, we saw that the maximum entropy con-
ﬁguration corresponded to an equal distribution of probabilities across the possible
states of the variable. Let us now consider the maximum entropy conﬁguration for
a continuous variable. In order for this maximum to be well deﬁned, it will be nec-
essary to constrain the ﬁrst and second moments of p(x) as well as preserving the
normalization constraint. We therefore maximize the differential entropy with the

Ludwig Boltzmann
1844–1906

Ludwig Eduard Boltzmann was an
Austrian physicist who created the
ﬁeld of statistical mechanics. Prior
to Boltzmann,
the concept of en-
tropy was already known from
classical thermodynamics where it
quantiﬁes the fact that when we take energy from a
system, not all of that energy is typically available
to do useful work. Boltzmann showed that the ther-
modynamic entropy S, a macroscopic quantity, could
be related to the statistical properties at the micro-
scopic level. This is expressed through the famous
equation S = k ln W in which W represents the
number of possible microstates in a macrostate, and
k  1.38 × 10−23 (in units of Joules per Kelvin) is
known as Boltzmann’s constant. Boltzmann’s ideas
were disputed by many scientists of they day. One dif-
ﬁculty they saw arose from the second law of thermo-

dynamics, which states that the entropy of a closed
system tends to increase with time. By contrast, at
the microscopic level the classical Newtonian equa-
tions of physics are reversible, and so they found it
difﬁcult to see how the latter could explain the for-
mer. They didn’t fully appreciate Boltzmann’s argu-
ments, which were statistical in nature and which con-
cluded not that entropy could never decrease over
time but simply that with overwhelming probability it
would generally increase. Boltzmann even had a long-
running dispute with the editor of the leading German
physics journal who refused to let him refer to atoms
and molecules as anything other than convenient the-
oretical constructs. The continued attacks on his work
lead to bouts of depression, and eventually he com-
mitted suicide. Shortly after Boltzmann’s death, new
experiments by Perrin on colloidal suspensions veri-
ﬁed his theories and conﬁrmed the value of the Boltz-
mann constant. The equation S = k ln W is carved on
Boltzmann’s tombstone.





∞








∞







1

1
2




∞










54

1. INTRODUCTION

three constraints

Appendix E

Appendix D

Exercise 1.34

Exercise 1.35

p(x) dx = 1

xp(x) dx = µ

−∞
∞

−∞

(x − µ)2p(x) dx = σ2.

−∞

(1.105)

(1.106)

(1.107)

The constrained maximization can be performed using Lagrange multipliers so that
we maximize the following functional with respect to p(x)

p(x) ln p(x) dx + λ1

−

+λ2

−∞

∞

−∞

∞

−∞

p(x) dx − 1
∞

−∞

xp(x) dx − µ

+ λ3

(x − µ)2p(x) dx − σ2

.

Using the calculus of variations, we set the derivative of this functional to zero giving

p(x) = exp

−1 + λ1 + λ2x + λ3(x − µ)2

.

(1.108)

The Lagrange multipliers can be found by back substitution of this result into the
three constraint equations, leading ﬁnally to the result

p(x) =

(2πσ2)1/2

exp

−

(x − µ)2

2σ2

(1.109)

and so the distribution that maximizes the differential entropy is the Gaussian. Note
that we did not constrain the distribution to be nonnegative when we maximized the
entropy. However, because the resulting distribution is indeed nonnegative, we see
with hindsight that such a constraint is not necessary.

If we evaluate the differential entropy of the Gaussian, we obtain

H[x] =

1 + ln(2πσ2)

.

(1.110)

Thus we see again that the entropy increases as the distribution becomes broader,
i.e., as σ2 increases. This result also shows that the differential entropy, unlike the
discrete entropy, can be negative, because H(x) < 0 in (1.110) for σ2 < 1/(2πe).

Suppose we have a joint distribution p(x, y) from which we draw pairs of values
of x and y. If a value of x is already known, then the additional information needed
to specify the corresponding value of y is given by − ln p(y|x). Thus the average
additional information needed to specify y can be written as

H[y|x] = −

p(y, x) ln p(y|x) dy dx

(1.111)














Exercise 1.37

which is called the conditional entropy of y given x. It is easily seen, using the
product rule, that the conditional entropy satisﬁes the relation

1.6. Information Theory

55

H[x, y] = H[y|x] + H[x]

(1.112)

where H[x, y] is the differential entropy of p(x, y) and H[x] is the differential en-
tropy of the marginal distribution p(x). Thus the information needed to describe x
and y is given by the sum of the information needed to describe x alone plus the
additional information required to specify y given x.

1.6.1 Relative entropy and mutual information
So far in this section, we have introduced a number of concepts from information
theory, including the key notion of entropy. We now start to relate these ideas to
pattern recognition. Consider some unknown distribution p(x), and suppose that
we have modelled this using an approximating distribution q(x). If we use q(x) to
construct a coding scheme for the purpose of transmitting values of x to a receiver,
then the average additional amount of information (in nats) required to specify the
value of x (assuming we choose an efﬁcient coding scheme) as a result of using q(x)
instead of the true distribution p(x) is given by

KL(pq) = −
= −

p(x) ln q(x) dx −
p(x) ln

q(x)
p(x)

dx.

p(x) ln p(x) dx

−

(1.113)

This is known as the relative entropy or Kullback-Leibler divergence, or KL diver-
gence (Kullback and Leibler, 1951), between the distributions p(x) and q(x). Note
that it is not a symmetrical quantity, that is to say KL(pq) ≡ KL(qp).
We now show that the Kullback-Leibler divergence satisﬁes KL(pq) � 0 with
equality if, and only if, p(x) = q(x). To do this we ﬁrst introduce the concept of
convex functions. A function f(x) is said to be convex if it has the property that
every chord lies on or above the function, as shown in Figure 1.31. Any value of x
in the interval from x = a to x = b can be written in the form λa + (1 − λ)b where
0 � λ � 1. The corresponding point on the chord is given by λf(a) + (1 − λ)f(b),

Claude Shannon
1916–2001

After graduating from Michigan and
MIT, Shannon joined the AT&T Bell
Telephone laboratories in 1941. His
paper ‘A Mathematical Theory of
Communication’ published in the
Bell System Technical Journal in
1948 laid the foundations for modern information the-

ory. This paper introduced the word ‘bit’, and his con-
cept that information could be sent as a stream of 1s
and 0s paved the way for the communications revo-
lution.
It is said that von Neumann recommended to
Shannon that he use the term entropy, not only be-
cause of its similarity to the quantity used in physics,
but also because “nobody knows what entropy really
is, so in any discussion you will always have an advan-
tage”.






i=1

M



f







i=1

M







56

1. INTRODUCTION

Figure 1.31 A convex function f (x) is one for which ev-
ery chord (shown in blue) lies on or above
the function (shown in red).

f(x)

xλ
xλ

b

x

chord

a

and the corresponding value of the function is f (λa + (1 − λ)b). Convexity then
implies
(1.114)

f(λa + (1 − λ)b) � λf(a) + (1 − λ)f(b).

This is equivalent to the requirement that the second derivative of the function be
everywhere positive. Examples of convex functions are x ln x (for x > 0) and x2. A
function is called strictly convex if the equality is satisﬁed only for λ = 0 and λ = 1.
If a function has the opposite property, namely that every chord lies on or below the
function, it is called concave, with a corresponding deﬁnition for strictly concave. If
a function f(x) is convex, then −f(x) will be concave.
convex function f(x) satisﬁes

Using the technique of proof by induction, we can show from (1.114) that a

Exercise 1.36

Exercise 1.38

λixi

�

λif(xi)

(1.115)

where λi � 0 and
i λi = 1, for any set of points {xi}. The result (1.115) is
known as Jensen’s inequality. If we interpret the λi as the probability distribution
over a discrete variable x taking the values {xi}, then (1.115) can be written

f (E[x]) � E[f(x)]

(1.116)
where E[·] denotes the expectation. For continuous variables, Jensen’s inequality
takes the form
(1.117)

f(x)p(x) dx.

xp(x) dx

f

�

We can apply Jensen’s inequality in the form (1.117) to the Kullback-Leibler

divergence (1.113) to give

KL(pq) = −

p(x) ln

q(x)
p(x)

dx � − ln

q(x) dx = 0

(1.118)






n=1

N





1.6. Information Theory

57

where we have used the fact that − ln x is a convex function, together with the nor-
q(x) dx = 1. In fact, − ln x is a strictly convex function,
malization condition
so the equality will hold if, and only if, q(x) = p(x) for all x. Thus we can in-
terpret the Kullback-Leibler divergence as a measure of the dissimilarity of the two
distributions p(x) and q(x).

We see that there is an intimate relationship between data compression and den-
sity estimation (i.e., the problem of modelling an unknown probability distribution)
because the most efﬁcient compression is achieved when we know the true distri-
bution. If we use a distribution that is different from the true one, then we must
necessarily have a less efﬁcient coding, and on average the additional information
that must be transmitted is (at least) equal to the Kullback-Leibler divergence be-
tween the two distributions.

Suppose that data is being generated from an unknown distribution p(x) that we
wish to model. We can try to approximate this distribution using some parametric
distribution q(x|θ), governed by a set of adjustable parameters θ, for example a
multivariate Gaussian. One way to determine θ is to minimize the Kullback-Leibler
divergence between p(x) and q(x|θ) with respect to θ. We cannot do this directly
because we don’t know p(x). Suppose, however, that we have observed a ﬁnite set
of training points xn, for n = 1, . . . , N, drawn from p(x). Then the expectation
with respect to p(x) can be approximated by a ﬁnite sum over these points, using
(1.35), so that

KL(pq) 

{− ln q(xn|θ) + ln p(xn)} .

(1.119)

The second term on the right-hand side of (1.119) is independent of θ, and the ﬁrst
term is the negative log likelihood function for θ under the distribution q(x|θ) eval-
uated using the training set. Thus we see that minimizing this Kullback-Leibler
divergence is equivalent to maximizing the likelihood function.

Now consider the joint distribution between two sets of variables x and y given
by p(x, y). If the sets of variables are independent, then their joint distribution will
factorize into the product of their marginals p(x, y) = p(x)p(y). If the variables are
not independent, we can gain some idea of whether they are ‘close’ to being indepen-
dent by considering the Kullback-Leibler divergence between the joint distribution
and the product of the marginals, given by

I[x, y] ≡ KL(p(x, y)p(x)p(y))

= −

p(x, y) ln

p(x)p(y)
p(x, y)

dx dy

(1.120)

Exercise 1.41

which is called the mutual information between the variables x and y. From the
properties of the Kullback-Leibler divergence, we see that I(x, y) � 0 with equal-
ity if, and only if, x and y are independent. Using the sum and product rules of
probability, we see that the mutual information is related to the conditional entropy
through

I[x, y] = H[x] − H[x|y] = H[y] − H[y|x].

(1.121)



M



N



N








58

1. INTRODUCTION

Thus we can view the mutual information as the reduction in the uncertainty about x
by virtue of being told the value of y (or vice versa). From a Bayesian perspective,
we can view p(x) as the prior distribution for x and p(x|y) as the posterior distribu-
tion after we have observed new data y. The mutual information therefore represents
the reduction in uncertainty about x as a consequence of the new observation y.

Exercises

1.1 () www Consider the sum-of-squares error function given by (1.2) in which
the function y(x, w) is given by the polynomial (1.1). Show that the coefﬁcients
w = {wi} that minimize this error function are given by the solution to the following
set of linear equations

Aijwj = Ti

j=0

(1.122)

where

Aij =

(xn)i+j,

n=1

Ti =

(xn)itn.

(1.123)

n=1

Here a sufﬁx i or j denotes the index of a component, whereas (x)i denotes x raised
to the power of i.

1.2 () Write down the set of coupled linear equations, analogous to (1.122), satisﬁed
by the coefﬁcients wi which minimize the regularized sum-of-squares error function
given by (1.4).

1.3 ( ) Suppose that we have three coloured boxes r (red), b (blue), and g (green).
Box r contains 3 apples, 4 oranges, and 3 limes, box b contains 1 apple, 1 orange,
and 0 limes, and box g contains 3 apples, 3 oranges, and 4 limes. If a box is chosen
at random with probabilities p(r) = 0.2, p(b) = 0.2, p(g) = 0.6, and a piece of
fruit is removed from the box (with equal probability of selecting any of the items in
the box), then what is the probability of selecting an apple? If we observe that the
selected fruit is in fact an orange, what is the probability that it came from the green
box?

1.4 ( ) www Consider a probability density px(x) deﬁned over a continuous vari-
able x, and suppose that we make a nonlinear change of variable using x = g(y),
so that the density transforms according to (1.27). By differentiating (1.27), show
that the location
y of the maximum of the density in y is not in general related to the
location
x of the maximum of the density over x by the simple functional relation
x = g(
y) as a consequence of the Jacobian factor. This shows that the maximum
of a probability density (in contrast to a simple function) is dependent on the choice
of variable. Verify that, in the case of a linear transformation, the location of the
maximum transforms in the same way as the variable itself.

1.5 () Using the deﬁnition (1.38) show that var[f(x)] satisﬁes (1.39).























Exercises

59

1.6 () Show that if two variables x and y are independent, then their covariance is

zero.

1.7 ( ) www In this exercise, we prove the normalization condition (1.48) for the

univariate Gaussian. To do this consider, the integral

I =

exp

∞

−∞

1
2σ2 x2

−

dx

(1.124)

which we can evaluate by ﬁrst writing its square in the form

I 2 =

∞

−∞

∞

exp

−∞

1
2σ2 x2 −

−

1
2σ2 y2

dx dy.

(1.125)

Now make the transformation from Cartesian coordinates (x, y) to polar coordinates
(r, θ) and then substitute u = r2. Show that, by performing the integrals over θ and
u, and then taking the square root of both sides, we obtain

I =

2πσ2

1/2

.

(1.126)

Finally, use this result to show that the Gaussian distribution N (x|µ, σ2) is normal-
ized.

1.8 ( ) www By using a change of variables, verify that the univariate Gaussian
distribution given by (1.46) satisﬁes (1.49). Next, by differentiating both sides of the
normalization condition

∞

−∞

N

x|µ, σ2

dx = 1

(1.127)

with respect to σ2, verify that the Gaussian satisﬁes (1.50). Finally, show that (1.51)
holds.

1.9 () www Show that the mode (i.e. the maximum) of the Gaussian distribution
(1.46) is given by µ. Similarly, show that the mode of the multivariate Gaussian
(1.52) is given by µ.

1.10 () www Suppose that the two variables x and z are statistically independent.

Show that the mean and variance of their sum satisﬁes

E[x + z] = E[x] + E[z]

var[x + z] = var[x] + var[z].

(1.128)
(1.129)

1.11 () By setting the derivatives of the log likelihood function (1.54) with respect to µ

and σ2 equal to zero, verify the results (1.55) and (1.56).

60

1. INTRODUCTION



D



D







iM−1

j=1

i=1

D

D

D








i2=1

j=1

i=1

D

D

D

D

D

i1

1.12 ( ) www Using the results (1.49) and (1.50), show that

E[xnxm] = µ2 + Inmσ2

(1.130)

where xn and xm denote data points sampled from a Gaussian distribution with mean
µ and variance σ2, and Inm satisﬁes Inm = 1 if n = m and Inm = 0 otherwise.
Hence prove the results (1.57) and (1.58).

1.13 () Suppose that the variance of a Gaussian is estimated using the result (1.56) but
with the maximum likelihood estimate µML replaced with the true value µ of the
mean. Show that this estimator has the property that its expectation is given by the
true variance σ2.

1.14 ( )

Show that an arbitrary square matrix with elements wij can be written in
the form wij = wS
ij are symmetric and anti-symmetric
matrices, respectively, satisfying wS
ji for all i and j. Now
consider the second order term in a higher order polynomial in D dimensions, given
by

ij = −wA

ij where wS

ij and wA

ji and wA

ij = wS

ij + wA

Show that

wijxixj.

wijxixj =

wS

ijxixj

i=1

j=1

(1.131)

(1.132)

so that the contribution from the anti-symmetric matrix vanishes. We therefore see
that, without loss of generality, the matrix of coefﬁcients wij can be chosen to be
symmetric, and so not all of the D2 elements of this matrix can be chosen indepen-
ij is given
dently. Show that the number of independent parameters in the matrix wS
by D(D + 1)/2.

1.15 (  ) www In this exercise and the next, we explore how the number of indepen-
dent parameters in a polynomial grows with the order M of the polynomial and with
the dimensionality D of the input space. We start by writing down the M th order
term for a polynomial in D dimensions in the form

···

wi1i2···iM xi1xi2 ··· xiM .

i1=1

iM =1

(1.133)

The coefﬁcients wi1i2···iM comprise DM elements, but the number of independent
parameters is signiﬁcantly fewer due to the many interchange symmetries of the
factor xi1xi2 ··· xiM . Begin by showing that the redundancy in the coefﬁcients can
be removed by rewriting this M th order term in the form

···

iM =1

wi1i2···iM xi1xi2 ··· xiM .

i1=1

i2=1

(1.134)



D

i=1






i=1

D

M

Exercises

61

w coefﬁcients and w coefﬁcients need
Note that the precise relationship between the
not be made explicit. Use this result to show that the number of independent param-
eters n(D, M), which appear at order M, satisﬁes the following recursion relation

(1.135)

(1.136)

(1.137)

Next use proof by induction to show that the following result holds

n(D, M) =

n(i, M − 1).

(i + M − 2)!
(i − 1)! (M − 1)!

=

(D + M − 1)!
(D − 1)! M!

which can be done by ﬁrst proving the result for D = 1 and arbitrary M by making
use of the result 0! = 1, then assuming it is correct for dimension D and verifying
that it is correct for dimension D + 1. Finally, use the two previous results, together
with proof by induction, to show

n(D, M) =

(D + M − 1)!
(D − 1)! M! .

To do this, ﬁrst show that the result is true for M = 2, and any value of D � 1,
by comparison with the result of Exercise 1.14. Then make use of (1.135), together
with (1.136), to show that, if the result holds at order M − 1, then it will also hold at
order M

1.16 (  ) In Exercise 1.15, we proved the result (1.135) for the number of independent
parameters in the M th order term of a D-dimensional polynomial. We now ﬁnd an
expression for the total number N(D, M) of independent parameters in all of the
terms up to and including the M6th order. First show that N(D, M) satisﬁes

N(D, M) =

n(D, m)

(1.138)

m=0

where n(D, m) is the number of independent parameters in the term of order m.
Now make use of the result (1.137), together with proof by induction, to show that

N(d, M) =

(D + M)!

D! M!

.

(1.139)

This can be done by ﬁrst proving that the result holds for M = 0 and arbitrary
D � 1, then assuming that it holds at order M, and hence showing that it holds at
order M + 1. Finally, make use of Stirling’s approximation in the form

n!  nne−n

(1.140)
for large n to show that, for D 
 M, the quantity N(D, M) grows like DM ,
and for M 
 D it grows like M D. Consider a cubic (M = 3) polynomial in D
dimensions, and evaluate numerically the total number of independent parameters
for (i) D = 10 and (ii) D = 100, which correspond to typical small-scale and
medium-scale machine learning applications.









D

62

1. INTRODUCTION

1.17 ( ) www The gamma function is deﬁned by

Γ(x) ≡

∞

0

ux−1e−u du.

(1.141)

Using integration by parts, prove the relation Γ(x + 1) = xΓ(x). Show also that
Γ(1) = 1 and hence that Γ(x + 1) = x! when x is an integer.

1.18 ( ) www We can use the result (1.126) to derive an expression for the surface
area SD, and the volume VD, of a sphere of unit radius in D dimensions. To do this,
consider the following result, which is obtained by transforming from Cartesian to
polar coordinates

∞

e−x2

i dxi = SD

∞

0

e−r2

rD−1 dr.

(1.142)

i=1

−∞

Using the deﬁnition (1.141) of the Gamma function, together with (1.126), evaluate
both sides of this equation, and hence show that

SD =

2πD/2
Γ(D/2) .

(1.143)

Next, by integrating with respect to radius from 0 to 1, show that the volume of the
unit sphere in D dimensions is given by

(1.144)
Finally, use the results Γ(1) = 1 and Γ(3/2) = √π/2 to show that (1.143) and
(1.144) reduce to the usual expressions for D = 2 and D = 3.

.

VD = SD
D

1.19 ( ) Consider a sphere of radius a in D-dimensions together with the concentric
hypercube of side 2a, so that the sphere touches the hypercube at the centres of each
of its sides. By using the results of Exercise 1.18, show that the ratio of the volume
of the sphere to the volume of the cube is given by

volume of sphere
volume of cube

=

πD/2

D2D−1Γ(D/2) .

(1.145)

Now make use of Stirling’s formula in the form

Γ(x + 1)  (2π)1/2e−xxx+1/2

(1.146)
which is valid for x 
 1, to show that, as D → ∞, the ratio (1.145) goes to zero.
Show also that the ratio of the distance from the centre of the hypercube to one of
the corners, divided by the perpendicular distance to one of the sides, is √D, which
therefore goes to ∞ as D → ∞. From these results we see that, in a space of high
dimensionality, most of the volume of a cube is concentrated in the large number of
corners, which themselves become very long ‘spikes’!



























Exercises

63

1.20 ( ) www In this exercise, we explore the behaviour of the Gaussian distribution
in high-dimensional spaces. Consider a Gaussian distribution in D dimensions given
by

p(x) =

.

(1.147)

1

(2πσ2)D/2

exp

−x2

2σ2

We wish to ﬁnd the density with respect to radius in polar coordinates in which the
direction variables have been integrated out. To do this, show that the integral of
the probability density over a thin shell of radius r and thickness 	, where 	  1, is
given by p(r)	 where

p(r) = SDrD−1
(2πσ2)D/2

exp

r2
2σ2

−

(1.148)

(1.149)

where SD is the surface area of a unit sphere in D dimensions. Show that the function
p(r) has a single stationary point located, for large D, at
r + 	) where 	 
p(

r  √Dσ. By considering

r, show that for large D,

p(

r + 	) = p(

r) exp

3	2
2σ2

−

r is a maximum of the radial probability density and also that p(r)
which shows that
r with length scale σ. We have
decays exponentially away from its maximum at
already seen that σ 
r for large D, and so we see that most of the probability
mass is concentrated in a thin shell at large radius. Finally, show that the probability
r by a factor of exp(D/2).
density p(x) is larger at the origin than at the radius
We therefore see that most of the probability mass in a high-dimensional Gaussian
distribution is located at a different radius from the region of high probability density.
This property of distributions in spaces of high dimensionality will have important
consequences when we consider Bayesian inference of model parameters in later
chapters.

1.21 ( ) Consider two nonnegative numbers a and b, and show that, if a � b, then
a � (ab)1/2. Use this result to show that, if the decision regions of a two-class
classiﬁcation problem are chosen to minimize the probability of misclassiﬁcation,
this probability will satisfy

p(mistake) �

{p(x,C1)p(x,C2)}1/2 dx.

(1.150)

1.22 () www Given a loss matrix with elements Lkj, the expected risk is minimized
if, for each x, we choose the class that minimizes (1.81). Verify that, when the
loss matrix is given by Lkj = 1 − Ikj, where Ikj are the elements of the identity
matrix, this reduces to the criterion of choosing the class having the largest posterior
probability. What is the interpretation of this form of loss matrix?

1.23 () Derive the criterion for minimizing the expected loss when there is a general

loss matrix and general prior probabilities for the classes.



64

1. INTRODUCTION

1.24 ( ) www Consider a classiﬁcation problem in which the loss incurred when
an input vector from class Ck is classiﬁed as belonging to class Cj is given by the
loss matrix Lkj, and for which the loss incurred in selecting the reject option is λ.
Find the decision criterion that will give the minimum expected loss. Verify that this
reduces to the reject criterion discussed in Section 1.5.3 when the loss matrix is given
by Lkj = 1 − Ikj. What is the relationship between λ and the rejection threshold θ?
1.25 () www Consider the generalization of the squared loss function (1.87) for a
single target variable t to the case of multiple target variables described by the vector
t given by

E[L(t, y(x))] =

y(x) − t2p(x, t) dx dt.

(1.151)

Using the calculus of variations, show that the function y(x) for which this expected
loss is minimized is given by y(x) = Et[t|x]. Show that this result reduces to (1.89)
for the case of a single target variable t.

1.26 () By expansion of the square in (1.151), derive a result analogous to (1.90) and
hence show that the function y(x) that minimizes the expected squared loss for the
case of a vector t of target variables is again given by the conditional expectation of
t.

1.27 ( ) www Consider the expected loss for regression problems under the Lq loss
function given by (1.91). Write down the condition that y(x) must satisfy in order
to minimize E[Lq]. Show that, for q = 1, this solution represents the conditional
median, i.e., the function y(x) such that the probability mass for t < y(x) is the
same as for t � y(x). Also show that the minimum expected Lq loss for q → 0 is
given by the conditional mode, i.e., by the function y(x) equal to the value of t that
maximizes p(t|x) for each x.

1.28 () In Section 1.6, we introduced the idea of entropy h(x) as the information gained
on observing the value of a random variable x having distribution p(x). We saw
that, for independent variables x and y for which p(x, y) = p(x)p(y), the entropy
functions are additive, so that h(x, y) = h(x) + h(y). In this exercise, we derive the
relation between h and p in the form of a function h(p). First show that h(p2) =
2h(p), and hence by induction that h(pn) = nh(p) where n is a positive integer.
Hence show that h(pn/m) = (n/m)h(p) where m is also a positive integer. This
implies that h(px) = xh(p) where x is a positive rational number, and hence by
continuity when it is a positive real number. Finally, show that this implies h(p)
must take the form h(p) ∝ ln p.

1.29 () www Consider an M-state discrete random variable x, and use Jensen’s in-
equality in the form (1.115) to show that the entropy of its distribution p(x) satisﬁes
H[x] � ln M.

1.30 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians

p(x) = N (x|µ, σ2) and q(x) = N (x|m, s2).

Table 1.3 The joint distribution p(x, y) for two binary variables

x and y used in Exercise 1.39.

Exercises

65

y

0
1/3
0

1
1/3
1/3

x

0
1

1.31 ( ) www Consider two variables x and y having joint distribution p(x, y). Show

that the differential entropy of this pair of variables satisﬁes

with equality if, and only if, x and y are statistically independent.

H[x, y] � H[x] + H[y]

(1.152)

1.32 () Consider a vector x of continuous variables with distribution p(x) and corre-
sponding entropy H[x]. Suppose that we make a nonsingular linear transformation
of x to obtain a new variable y = Ax. Show that the corresponding entropy is given
by H[y] = H[x] + ln|A| where |A| denotes the determinant of A.

1.33 ( ) Suppose that the conditional entropy H[y|x] between two discrete random
variables x and y is zero. Show that, for all values of x such that p(x) > 0, the
variable y must be a function of x, in other words for each x there is only one value
of y such that p(y|x) = 0.

1.34 ( ) www Use the calculus of variations to show that the stationary point of the
functional (1.108) is given by (1.108). Then use the constraints (1.105), (1.106),
and (1.107) to eliminate the Lagrange multipliers and hence show that the maximum
entropy solution is given by the Gaussian (1.109).

1.35 () www Use the results (1.106) and (1.107) to show that the entropy of the

univariate Gaussian (1.109) is given by (1.110).

1.36 () A strictly convex function is deﬁned as one for which every chord lies above
the function. Show that this is equivalent to the condition that the second derivative
of the function be positive.

1.37 () Using the deﬁnition (1.111) together with the product rule of probability, prove

the result (1.112).

1.38 ( ) www Using proof by induction, show that the inequality (1.114) for convex

functions implies the result (1.115).

1.39 (  ) Consider two binary variables x and y having the joint distribution given in

Table 1.3.
Evaluate the following quantities

(a) H[x]
(b) H[y]

(c) H[y|x]
(d) H[x|y]

(e) H[x, y]
I[x, y].
(f)

Draw a diagram to show the relationship between these various quantities.

66

1. INTRODUCTION

1.40 () By applying Jensen’s inequality (1.115) with f(x) = ln x, show that the arith-

metic mean of a set of real numbers is never less than their geometrical mean.

1.41 () www Using the sum and product rules of probability, show that the mutual

information I(x, y) satisﬁes the relation (1.121).

2

Probability
Distributions

In Chapter 1, we emphasized the central role played by probability theory in the
solution of pattern recognition problems. We turn now to an exploration of some
particular examples of probability distributions and their properties. As well as be-
ing of great interest in their own right, these distributions can form building blocks
for more complex models and will be used extensively throughout the book. The
distributions introduced in this chapter will also serve another important purpose,
namely to provide us with the opportunity to discuss some key statistical concepts,
such as Bayesian inference, in the context of simple models before we encounter
them in more complex situations in later chapters.

One role for the distributions discussed in this chapter is to model the prob-
ability distribution p(x) of a random variable x, given a ﬁnite set x1, . . . , xN of
observations. This problem is known as density estimation. For the purposes of
this chapter, we shall assume that the data points are independent and identically
distributed. It should be emphasized that the problem of density estimation is fun-

67

68

2. PROBABILITY DISTRIBUTIONS

damentally ill-posed, because there are inﬁnitely many probability distributions that
could have given rise to the observed ﬁnite data set. Indeed, any distribution p(x)
that is nonzero at each of the data points x1, . . . , xN is a potential candidate. The
issue of choosing an appropriate distribution relates to the problem of model selec-
tion that has already been encountered in the context of polynomial curve ﬁtting in
Chapter 1 and that is a central issue in pattern recognition.

We begin by considering the binomial and multinomial distributions for discrete
random variables and the Gaussian distribution for continuous random variables.
These are speciﬁc examples of parametric distributions, so-called because they are
governed by a small number of adaptive parameters, such as the mean and variance in
the case of a Gaussian for example. To apply such models to the problem of density
estimation, we need a procedure for determining suitable values for the parameters,
given an observed data set. In a frequentist treatment, we choose speciﬁc values
for the parameters by optimizing some criterion, such as the likelihood function. By
contrast, in a Bayesian treatment we introduce prior distributions over the parameters
and then use Bayes’ theorem to compute the corresponding posterior distribution
given the observed data.

We shall see that an important role is played by conjugate priors, that lead to
posterior distributions having the same functional form as the prior, and that there-
fore lead to a greatly simpliﬁed Bayesian analysis. For example, the conjugate prior
for the parameters of the multinomial distribution is called the Dirichlet distribution,
while the conjugate prior for the mean of a Gaussian is another Gaussian. All of these
distributions are examples of the exponential family of distributions, which possess
a number of important properties, and which will be discussed in some detail.

One limitation of the parametric approach is that it assumes a speciﬁc functional
form for the distribution, which may turn out to be inappropriate for a particular
application. An alternative approach is given by nonparametric density estimation
methods in which the form of the distribution typically depends on the size of the data
set. Such models still contain parameters, but these control the model complexity
rather than the form of the distribution. We end this chapter by considering three
nonparametric methods based respectively on histograms, nearest-neighbours, and
kernels.

2.1. Binary Variables

We begin by considering a single binary random variable x ∈ {0, 1}. For example,
x might describe the outcome of ﬂipping a coin, with x = 1 representing ‘heads’,
and x = 0 representing ‘tails’. We can imagine that this is a damaged coin so that
the probability of landing heads is not necessarily the same as that of landing tails.
The probability of x = 1 will be denoted by the parameter µ so that

p(x = 1|µ) = µ

(2.1)



N



N




N

N




n=1

N

2.1. Binary Variables

69

where 0 � µ � 1, from which it follows that p(x = 0|µ) = 1 − µ. The probability
distribution over x can therefore be written in the form
Bern(x|µ) = µx(1 − µ)1−x

(2.2)

which is known as the Bernoulli distribution. It is easily veriﬁed that this distribution
is normalized and that it has mean and variance given by

E[x] = µ
var[x] = µ(1 − µ).

(2.3)
(2.4)
Now suppose we have a data set D = {x1, . . . , xN} of observed values of x.
We can construct the likelihood function, which is a function of µ, on the assumption
that the observations are drawn independently from p(x|µ), so that

p(D|µ) =

p(xn|µ) =

n=1

n=1

µxn(1 − µ)1−xn.

(2.5)

In a frequentist setting, we can estimate a value for µ by maximizing the likelihood
function, or equivalently by maximizing the logarithm of the likelihood. In the case
of the Bernoulli distribution, the log likelihood function is given by

ln p(D|µ) =

ln p(xn|µ) =

n=1

{xn ln µ + (1 − xn) ln(1 − µ)} .

(2.6)

At this point, it is worth noting that the log likelihood function depends on the N
observations xn only through their sum
n xn. This sum provides an example of a
sufﬁcient statistic for the data under this distribution, and we shall study the impor-
tant role of sufﬁcient statistics in some detail. If we set the derivative of ln p(D|µ)
with respect to µ equal to zero, we obtain the maximum likelihood estimator

µML =

1
N

xn

n=1

(2.7)

Exercise 2.1

Section 2.4

Jacob Bernoulli
1654–1705

Jacob Bernoulli, also known as
Jacques or James Bernoulli, was a
Swiss mathematician and was the
ﬁrst of many in the Bernoulli family
to pursue a career in science and
mathematics. Although compelled
to study philosophy and theology against his will by
his parents, he travelled extensively after graduating
in order to meet with many of the leading scientists of

his time, including Boyle and Hooke in England. When
he returned to Switzerland, he taught mechanics and
became Professor of Mathematics at Basel in 1687.
Unfortunately, rivalry between Jacob and his younger
brother Johann turned an initially productive collabora-
tion into a bitter and public dispute. Jacob’s most sig-
niﬁcant contributions to mathematics appeared in The
ArtofConjecture published in 1713, eight years after
his death, which deals with topics in probability the-
ory including what has become known as the Bernoulli
distribution.



1



0

0.2

0.1

0





2

3

4

5
m

6

7

8

9

10

which is also known as the sample mean. If we denote the number of observations
of x = 1 (heads) within this data set by m, then we can write (2.7) in the form

µML = m
N

(2.8)

so that the probability of landing heads is given, in this maximum likelihood frame-
work, by the fraction of observations of heads in the data set.

Now suppose we ﬂip a coin, say, 3 times and happen to observe 3 heads. Then
N = m = 3 and µML = 1. In this case, the maximum likelihood result would
predict that all future observations should give heads. Common sense tells us that
this is unreasonable, and in fact this is an extreme example of the over-ﬁtting associ-
ated with maximum likelihood. We shall see shortly how to arrive at more sensible
conclusions through the introduction of a prior distribution over µ.

We can also work out the distribution of the number m of observations of x = 1,
given that the data set has size N. This is called the binomial distribution, and
from (2.5) we see that it is proportional to µm(1 − µ)N−m. In order to obtain the
normalization coefﬁcient we note that out of N coin ﬂips, we have to add up all
of the possible ways of obtaining m heads, so that the binomial distribution can be
written

Bin(m|N, µ) =

N
m

µm(1 − µ)N−m

N!

N
m

≡

(N − m)!m!

(2.9)

(2.10)

is the number of ways of choosing m objects out of a total of N identical objects.
Figure 2.1 shows a plot of the binomial distribution for N = 10 and µ = 0.25.

The mean and variance of the binomial distribution can be found by using the
result of Exercise 1.10, which shows that for independent events the mean of the
sum is the sum of the means, and the variance of the sum is the sum of the variances.
Because m = x1 + . . . + xN , and for each observation the mean and variance are

70

2. PROBABILITY DISTRIBUTIONS

Figure 2.1 Histogram plot of the binomial dis-
tribution (2.9) as a function of m for
N = 10 and µ = 0.25.

0.3

where

Exercise 2.3



N



N



given by (2.3) and (2.4), respectively, we have

2.1. Binary Variables

71

E[m] ≡

mBin(m|N, µ) = N µ

m=0

(2.11)

(2.12)

var[m] ≡

m=0

(m − E[m])2 Bin(m|N, µ) = N µ(1 − µ).

These results can also be proved directly using calculus.

2.1.1 The beta distribution
We have seen in (2.8) that the maximum likelihood setting for the parameter µ
in the Bernoulli distribution, and hence in the binomial distribution, is given by the
fraction of the observations in the data set having x = 1. As we have already noted,
this can give severely over-ﬁtted results for small data sets. In order to develop a
Bayesian treatment for this problem, we need to introduce a prior distribution p(µ)
over the parameter µ. Here we consider a form of prior distribution that has a simple
interpretation as well as some useful analytical properties. To motivate this prior,
we note that the likelihood function takes the form of the product of factors of the
form µx(1 − µ)1−x. If we choose a prior to be proportional to powers of µ and
(1 − µ), then the posterior distribution, which is proportional to the product of the
prior and the likelihood function, will have the same functional form as the prior.
This property is called conjugacy and we will see several examples of it later in this
chapter. We therefore choose a prior, called the beta distribution, given by

Beta(µ|a, b) =

Γ(a + b)
Γ(a)Γ(b) µa−1(1 − µ)b−1

(2.13)

where Γ(x) is the gamma function deﬁned by (1.141), and the coefﬁcient in (2.13)
ensures that the beta distribution is normalized, so that

Exercise 2.4

Exercise 2.5

Exercise 2.6

The mean and variance of the beta distribution are given by

1

0

Beta(µ|a, b) dµ = 1.

E[µ] =

var[µ] =

a

a + b
(a + b)2(a + b + 1) .

ab

(2.14)

(2.15)

(2.16)

The parameters a and b are often called hyperparameters because they control the
distribution of the parameter µ. Figure 2.2 shows plots of the beta distribution for
various values of the hyperparameters.

The posterior distribution of µ is now obtained by multiplying the beta prior
(2.13) by the binomial likelihood function (2.9) and normalizing. Keeping only the
factors that depend on µ, we see that this posterior distribution has the form

p(µ|m, l, a, b) ∝ µm+a−1(1 − µ)l+b−1

(2.17)

72

2. PROBABILITY DISTRIBUTIONS

a = 0.1
b = 0.1

a = 2

b = 3

3

2

1

0

0

3

2

1

0

0

0.5

µ

1

a = 1
b = 1

0

0.5

µ

1

a = 8
b = 4

3

2

1

0

3

2

1

0.5

µ

1

0

0

0.5

µ

1

Figure 2.2 Plots of the beta distribution Beta(µ|a, b) given by (2.13) as a function of µ for various values of the
hyperparameters a and b.

where l = N − m, and therefore corresponds to the number of ‘tails’ in the coin
example. We see that (2.17) has the same functional dependence on µ as the prior
distribution, reﬂecting the conjugacy properties of the prior with respect to the like-
lihood function. Indeed, it is simply another beta distribution, and its normalization
coefﬁcient can therefore be obtained by comparison with (2.13) to give

p(µ|m, l, a, b) =

Γ(m + a + l + b)
Γ(m + a)Γ(l + b) µm+a−1(1 − µ)l+b−1.

(2.18)

We see that the effect of observing a data set of m observations of x = 1 and
l observations of x = 0 has been to increase the value of a by m, and the value of
b by l, in going from the prior distribution to the posterior distribution. This allows
us to provide a simple interpretation of the hyperparameters a and b in the prior as
an effective number of observations of x = 1 and x = 0, respectively. Note that
a and b need not be integers. Furthermore, the posterior distribution can act as the
prior if we subsequently observe additional data. To see this, we can imagine taking
observations one at a time and after each observation updating the current posterior





likelihood function

prior

2

1

0

0

2

1

0

0.5
µ

1

0

0.5
µ

1

2.1. Binary Variables

73

posterior

2

1

0

0

0.5
µ

1

Figure 2.3 Illustration of one step of sequential Bayesian inference. The prior is given by a beta distribution
with parameters a = 2, b = 2, and the likelihood function, given by (2.9) with N = m = 1, corresponds to a
single observation of x = 1, so that the posterior is given by a beta distribution with parameters a = 3, b = 2.

Section 2.3.5

distribution by multiplying by the likelihood function for the new observation and
then normalizing to obtain the new, revised posterior distribution. At each stage, the
posterior is a beta distribution with some total number of (prior and actual) observed
values for x = 1 and x = 0 given by the parameters a and b. Incorporation of an
additional observation of x = 1 simply corresponds to incrementing the value of a
by 1, whereas for an observation of x = 0 we increment b by 1. Figure 2.3 illustrates
one step in this process.

We see that this sequential approach to learning arises naturally when we adopt
a Bayesian viewpoint. It is independent of the choice of prior and of the likelihood
function and depends only on the assumption of i.i.d. data. Sequential methods make
use of observations one at a time, or in small batches, and then discard them before
the next observations are used. They can be used, for example, in real-time learning
scenarios where a steady stream of data is arriving, and predictions must be made
before all of the data is seen. Because they do not require the whole data set to be
stored or loaded into memory, sequential methods are also useful for large data sets.
Maximum likelihood methods can also be cast into a sequential framework.

If our goal is to predict, as best we can, the outcome of the next trial, then we
must evaluate the predictive distribution of x, given the observed data set D. From
the sum and product rules of probability, this takes the form

p(x = 1|D) =

1

0

p(x = 1|µ)p(µ|D) dµ =

1

0

µp(µ|D) dµ = E[µ|D].

(2.19)

Using the result (2.18) for the posterior distribution p(µ|D), together with the result
(2.15) for the mean of the beta distribution, we obtain
m + a

(2.20)

p(x = 1|D) =

m + a + l + b

which has a simple interpretation as the total fraction of observations (both real ob-
servations and ﬁctitious prior observations) that correspond to x = 1. Note that in
the limit of an inﬁnitely large data set m, l → ∞ the result (2.20) reduces to the
maximum likelihood result (2.8). As we shall see, it is a very general property that
the Bayesian and maximum likelihood results will agree in the limit of an inﬁnitely






74

2. PROBABILITY DISTRIBUTIONS

Exercise 2.7

Exercise 2.8

large data set. For a ﬁnite data set, the posterior mean for µ always lies between the
prior mean and the maximum likelihood estimate for µ corresponding to the relative
frequencies of events given by (2.7).

From Figure 2.2, we see that as the number of observations increases, so the
posterior distribution becomes more sharply peaked. This can also be seen from
the result (2.16) for the variance of the beta distribution, in which we see that the
variance goes to zero for a → ∞ or b → ∞. In fact, we might wonder whether it is
a general property of Bayesian learning that, as we observe more and more data, the
uncertainty represented by the posterior distribution will steadily decrease.

To address this, we can take a frequentist view of Bayesian learning and show
that, on average, such a property does indeed hold. Consider a general Bayesian
inference problem for a parameter θ for which we have observed a data set D, de-
scribed by the joint distribution p(θ,D). The following result

Eθ[θ] = ED [Eθ[θ|D]]

p(θ)θ dθ

where

Eθ[θ] ≡
ED[Eθ[θ|D]] ≡

θp(θ|D) dθ

p(D) dD

(2.21)

(2.22)

(2.23)

says that the posterior mean of θ, averaged over the distribution generating the data,
is equal to the prior mean of θ. Similarly, we can show that

varθ[θ] = ED [varθ[θ|D]] + varD [Eθ[θ|D]] .

(2.24)

The term on the left-hand side of (2.24) is the prior variance of θ. On the right-
hand side, the ﬁrst term is the average posterior variance of θ, and the second term
measures the variance in the posterior mean of θ. Because this variance is a positive
quantity, this result shows that, on average, the posterior variance of θ is smaller than
the prior variance. The reduction in variance is greater if the variance in the posterior
mean is greater. Note, however, that this result only holds on average, and that for a
particular observed data set it is possible for the posterior variance to be larger than
the prior variance.

2.2. Multinomial Variables

Binary variables can be used to describe quantities that can take one of two possible
values. Often, however, we encounter discrete variables that can take on one of K
possible mutually exclusive states. Although there are various alternative ways to
express such variables, we shall see shortly that a particularly convenient represen-
tation is the 1-of-K scheme in which the variable is represented by a K-dimensional
vector x in which one of the elements xk equals 1, and all remaining elements equal










n=1

K

N

x

x

K

k=1







P

xnk

k=1

k=1

k=1

(
k

µ

K

K

K

n

K



K



2.2. Multinomial Variables

75

0. So, for instance if we have a variable that can take K = 6 states and a particular
observation of the variable happens to correspond to the state where x3 = 1, then x
will be represented by

Note that such vectors satisfy
by the parameter µk, then the distribution of x is given

x = (0, 0, 1, 0, 0, 0)T.

(2.25)
k=1 xk = 1. If we denote the probability of xk = 1

K

(2.26)

(2.27)

(2.28)

where µ = (µ1, . . . , µK)T, and the parameters µk are constrained to satisfy µk � 0
k µk = 1, because they represent probabilities. The distribution (2.26) can be
and
regarded as a generalization of the Bernoulli distribution to more than two outcomes.
It is easily seen that the distribution is normalized

p(x|µ) =

µxk
k

p(x|µ) =

µk = 1

and that

E[x|µ] =

p(x|µ)x = (µ1, . . . , µM )T = µ.

Now consider a data set D of N independent observations x1, . . . , xN . The

corresponding likelihood function takes the form

p(D|µ) =

µxnk
k =

n xnk)

=

µmk
k .

k=1

(2.29)

We see that the likelihood function depends on the N data points only through the
K quantities

mk =

(2.30)

Section 2.4

Appendix E

which represent the number of observations of xk = 1. These are called the sufﬁcient
statistics for this distribution.

In order to ﬁnd the maximum likelihood solution for µ, we need to maximize
ln p(D|µ) with respect to µk taking account of the constraint that the µk must sum
to one. This can be achieved using a Lagrange multiplier λ and maximizing

mk ln µk + λ

k=1

k=1

µk − 1

.

Setting the derivative of (2.31) with respect to µk to zero, we obtain

µk = −mk/λ.

(2.31)

(2.32)











K



K




K

k=1





=

K

K

k=1

76

2. PROBABILITY DISTRIBUTIONS

We can solve for the Lagrange multiplier λ by substituting (2.32) into the constraint
k µk = 1 to give λ = −N. Thus we obtain the maximum likelihood solution in
(2.33)

the form

k = mk
µML
N

which is the fraction of the N observations for which xk = 1.

We can consider the joint distribution of the quantities m1, . . . , mK, conditioned
on the parameters µ and on the total number N of observations. From (2.29) this
takes the form

Mult(m1, m2, . . . , mK|µ, N) =

N

m1m2 . . . mK

µmk
k

k=1

(2.34)

which is known as the multinomial distribution. The normalization coefﬁcient is the
number of ways of partitioning N objects into K groups of size m1, . . . , mK and is
given by

N

m1m2 . . . mK

N!

m1!m2! . . . mK! .

Note that the variables mk are subject to the constraint

2.2.1 The Dirichlet distribution
We now introduce a family of prior distributions for the parameters {µk} of
the multinomial distribution (2.34). By inspection of the form of the multinomial
distribution, we see that the conjugate prior is given by

mk = N.

p(µ|α) ∝

µαk−1
k

(2.35)

(2.36)

(2.37)

Exercise 2.9

where 0 � µk � 1 and
k µk = 1. Here α1, . . . , αK are the parameters of the
distribution, and α denotes (α1, . . . , αK)T. Note that, because of the summation
constraint, the distribution over the space of the {µk} is conﬁned to a simplex of
dimensionality K − 1, as illustrated for K = 3 in Figure 2.4.

The normalized form for this distribution is by

Dir(µ|α) =

Γ(α0)

Γ(α1)··· Γ(αK)

µαk−1
k

k=1

(2.38)

which is called the Dirichlet distribution. Here Γ(x) is the gamma function deﬁned
by (1.141) while

α0 =

αk.

k=1

(2.39)

P



K



K

µ3

2.2. Multinomial Variables

Figure 2.4 The Dirichlet distribution over three variables µ1, µ2, µ3
is conﬁned to a simplex (a bounded linear manifold) of
the form shown, as a consequence of the constraints
0 � µk � 1 and

k µk = 1.

µ2

77

µ1

Plots of the Dirichlet distribution over the simplex, for various settings of the param-
eters αk, are shown in Figure 2.5.

Multiplying the prior (2.38) by the likelihood function (2.34), we obtain the

posterior distribution for the parameters {µk} in the form

p(µ|D, α) ∝ p(D|µ)p(µ|α) ∝

k=1

µαk+mk−1
k

.

(2.40)

We see that the posterior distribution again takes the form of a Dirichlet distribution,
conﬁrming that the Dirichlet is indeed a conjugate prior for the multinomial. This
allows us to determine the normalization coefﬁcient by comparison with (2.38) so
that

p(µ|D, α) = Dir(µ|α + m)

=

Γ(α0 + N)

Γ(α1 + m1)··· Γ(αK + mK)

k=1

µαk+mk−1
k

(2.41)

where we have denoted m = (m1, . . . , mK)T. As for the case of the binomial
distribution with its beta prior, we can interpret the parameters αk of the Dirichlet
prior as an effective number of observations of xk = 1.

Note that two-state quantities can either be represented as binary variables and

Lejeune Dirichlet
1805–1859

Lejeune
Johann Peter Gustav
Dirichlet was a modest and re-
served mathematician who made
contributions in number theory, me-
chanics, and astronomy, and who
gave the ﬁrst rigorous analysis of
Fourier series. His family originated from Richelet
in Belgium, and the name Lejeune Dirichlet comes

from ‘le jeune de Richelet’ (the young person from
Richelet). Dirichlet’s ﬁrst paper, which was published
in 1825, brought him instant fame. It concerned Fer-
mat’s last theorem, which claims that there are no
positive integer solutions to xn + yn = zn for n > 2.
Dirichlet gave a partial proof for the case n = 5, which
was sent to Legendre for review and who in turn com-
pleted the proof. Later, Dirichlet gave a complete proof
for n = 14, although a full proof of Fermat’s last theo-
rem for arbitrary n had to wait until the work of Andrew
Wiles in the closing years of the 20th century.

�

�

�

�

78

2. PROBABILITY DISTRIBUTIONS

Figure 2.5 Plots of the Dirichlet distribution over three variables, where the two horizontal axes are coordinates
in the plane of the simplex and the vertical axis corresponds to the value of the density. Here {αk} = 0.1 on the
left plot, {αk} = 1 in the centre plot, and {αk} = 10 in the right plot.

modelled using the binomial distribution (2.9) or as 1-of-2 variables and modelled
using the multinomial distribution (2.34) with K = 2.

2.3. The Gaussian Distribution

The Gaussian, also known as the normal distribution, is a widely used model for the
distribution of continuous variables. In the case of a single variable x, the Gaussian
distribution can be written in the form
1

(2.42)

N (x|µ, σ2) =

(2πσ2)1/2

exp

1
2σ2 (x − µ)2

−

where µ is the mean and σ2 is the variance. For a D-dimensional vector x, the
multivariate Gaussian distribution takes the form

Section 1.6

Exercise 2.14

N (x|µ, Σ) =

1

1

(2π)D/2

|Σ|1/2

exp

1
2

−

(x − µ)TΣ−1(x − µ)

(2.43)

where µ is a D-dimensional mean vector, Σ is a D × D covariance matrix, and |Σ|
denotes the determinant of Σ.
The Gaussian distribution arises in many different contexts and can be motivated
from a variety of different perspectives. For example, we have already seen that for
a single real variable, the distribution that maximizes the entropy is the Gaussian.
This property applies also to the multivariate Gaussian.

Another situation in which the Gaussian distribution arises is when we consider
the sum of multiple random variables. The central limit theorem (due to Laplace)
tells us that, subject to certain mild conditions, the sum of a set of random variables,
which is of course itself a random variable, has a distribution that becomes increas-
ingly Gaussian as the number of terms in the sum increases (Walker, 1969). We can

2.3. The Gaussian Distribution

79

N = 1

3

2

1

0

0

N = 2

3

2

1

0

0

N = 10

3

2

1

0

0

0.5

1

0.5

1

0.5

1

Figure 2.6 Histogram plots of the mean of N uniformly distributed numbers for various values of N. We
observe that as N increases, the distribution tends towards a Gaussian.

illustrate this by considering N variables x1, . . . , xN each of which has a uniform
distribution over the interval [0, 1] and then considering the distribution of the mean
(x1 + ··· + xN )/N. For large N, this distribution tends to a Gaussian, as illustrated
in Figure 2.6.
In practice, the convergence to a Gaussian as N increases can be
very rapid. One consequence of this result is that the binomial distribution (2.9),
which is a distribution over m deﬁned by the sum of N observations of the random
binary variable x, will tend to a Gaussian as N → ∞ (see Figure 2.1 for the case of
N = 10).
The Gaussian distribution has many important analytical properties, and we shall
consider several of these in detail. As a result, this section will be rather more tech-
nically involved than some of the earlier sections, and will require familiarity with
various matrix identities. However, we strongly encourage the reader to become pro-
ﬁcient in manipulating Gaussian distributions using the techniques presented here as
this will prove invaluable in understanding the more complex models presented in
later chapters.

We begin by considering the geometrical form of the Gaussian distribution. The

Appendix C

Carl Friedrich Gauss
1777–1855

It
is said that when Gauss went
to elementary school at age 7, his
teacher B¨uttner, trying to keep the
class occupied, asked the pupils to
sum the integers from 1 to 100. To
the teacher’s amazement, Gauss
arrived at the answer in a matter of moments by noting
that the sum can be represented as 50 pairs (1 + 100,
2+99, etc.) each of which added to 101, giving the an-
swer 5,050. It is now believed that the problem which
was actually set was of the same form but somewhat
harder in that the sequence had a larger starting value
and a larger increment. Gauss was a German math-

ematician and scientist with a reputation for being a
hard-working perfectionist. One of his many contribu-
tions was to show that least squares can be derived
under the assumption of normally distributed errors.
He also created an early formulation of non-Euclidean
geometry (a self-consistent geometrical theory that vi-
olates the axioms of Euclid) but was reluctant to dis-
cuss it openly for fear that his reputation might suffer
if it were seen that he believed in such a geometry.
At one point, Gauss was asked to conduct a geodetic
survey of the state of Hanover, which led to his for-
mulation of the normal distribution, now also known
as the Gaussian. After his death, a study of his di-
aries revealed that he had discovered several impor-
tant mathematical results years or even decades be-
fore they were published by others.







i=1

i=1

D

D

Σui = λiui

i uj = Iij
uT

Iij =

if i = j

1,
0, otherwise.

Σ =

λiuiuT
i

Σ−1 =

1
λi

uiuT
i .

D

∆2 =

i=1

y2
i
λi

80

2. PROBABILITY DISTRIBUTIONS

Exercise 2.17

Exercise 2.18

Exercise 2.19

functional dependence of the Gaussian on x is through the quadratic form

∆2 = (x − µ)TΣ−1(x − µ)

(2.44)

which appears in the exponent. The quantity ∆ is called the Mahalanobis distance
from µ to x and reduces to the Euclidean distance when Σ is the identity matrix. The
Gaussian distribution will be constant on surfaces in x-space for which this quadratic
form is constant.

First of all, we note that the matrix Σ can be taken to be symmetric, without
loss of generality, because any antisymmetric component would disappear from the
exponent. Now consider the eigenvector equation for the covariance matrix

where i = 1, . . . , D. Because Σ is a real, symmetric matrix its eigenvalues will be
real, and its eigenvectors can be chosen to form an orthonormal set, so that

where Iij is the i, j element of the identity matrix and satisﬁes

The covariance matrix Σ can be expressed as an expansion in terms of its eigenvec-
tors in the form

and similarly the inverse covariance matrix Σ−1 can be expressed as

Substituting (2.49) into (2.44), the quadratic form becomes

(2.45)

(2.46)

(2.47)

(2.48)

(2.49)

(2.50)

where we have deﬁned

(2.51)
We can interpret {yi} as a new coordinate system deﬁned by the orthonormal vectors
ui that are shifted and rotated with respect to the original xi coordinates. Forming
the vector y = (y1, . . . , yD)T, we have

i (x − µ).

yi = uT

y = U(x − µ)

(2.52)













Appendix C

x2

Figure 2.7 The red curve shows the ellip-
tical surface of constant proba-
bility density for a Gaussian in
a two-dimensional space x =
(x1, x2) on which the density
its value at
is exp(−1/2) of
x = µ. The major axes of
the ellipse are deﬁned by the
eigenvectors ui of the covari-
ance matrix, with correspond-
ing eigenvalues λi.

λ1/2
2

2.3. The Gaussian Distribution

y2

u2

µ

y1

u1

λ1/2
1

81

x1

where U is a matrix whose rows are given by uT
i . From (2.46) it follows that U is
an orthogonal matrix, i.e., it satisﬁes UUT = I, and hence also UTU = I, where I
is the identity matrix.

The quadratic form, and hence the Gaussian density, will be constant on surfaces
If all of the eigenvalues λi are positive, then these
for which (2.51) is constant.
surfaces represent ellipsoids, with their centres at µ and their axes oriented along ui,
and with scaling factors in the directions of the axes given by λ1/2
, as illustrated in
Figure 2.7.

i

For the Gaussian distribution to be well deﬁned, it is necessary for all of the
eigenvalues λi of the covariance matrix to be strictly positive, otherwise the dis-
tribution cannot be properly normalized. A matrix whose eigenvalues are strictly
positive is said to be positive deﬁnite. In Chapter 12, we will encounter Gaussian
distributions for which one or more of the eigenvalues are zero, in which case the
distribution is singular and is conﬁned to a subspace of lower dimensionality. If all
of the eigenvalues are nonnegative, then the covariance matrix is said to be positive
semideﬁnite.

Now consider the form of the Gaussian distribution in the new coordinate system
deﬁned by the yi. In going from the x to the y coordinate system, we have a Jacobian
matrix J with elements given by

Jij = ∂xi
∂yj

= Uji

(2.53)

where Uji are the elements of the matrix UT. Using the orthonormality property of
the matrix U, we see that the square of the determinant of the Jacobian matrix is

(2.54)
and hence |J| = 1. Also, the determinant |Σ| of the covariance matrix can be written

= |I| = 1

|J|2 =

|U| =

UTU

UT

UT

2 =








−

−

j=1

D







j=1

D



D

















82

2. PROBABILITY DISTRIBUTIONS

as the product of its eigenvalues, and hence

|Σ|1/2 =

λ1/2
j

.

(2.55)

(2.56)

Thus in the yj coordinate system, the Gaussian distribution takes the form

p(y) = p(x)|J| =

1

exp

(2πλj)1/2

y2
j
2λj

−

which is the product of D independent univariate Gaussian distributions. The eigen-
vectors therefore deﬁne a new set of shifted and rotated coordinates with respect
to which the joint probability distribution factorizes into a product of independent
distributions. The integral of the distribution in the y coordinate system is then

p(y) dy =

j=1

∞

−∞

1

exp

(2πλj)1/2

y2
j
2λj

−

dyj = 1

(2.57)

where we have used the result (1.48) for the normalization of the univariate Gaussian.
This conﬁrms that the multivariate Gaussian (2.43) is indeed normalized.

We now look at the moments of the Gaussian distribution and thereby provide an
interpretation of the parameters µ and Σ. The expectation of x under the Gaussian
distribution is given by

E[x] =

=

1

(2π)D/2

1

(2π)D/2

1

1

|Σ|1/2

|Σ|1/2

exp

exp

(x − µ)TΣ−1(x − µ)
zTΣ−1z
(z + µ) dz

x dx

(2.58)

1
2
1
2

where we have changed variables using z = x − µ. We now note that the exponent
is an even function of the components of z and, because the integrals over these are
taken over the range (−∞,∞), the term in z in the factor (z + µ) will vanish by
symmetry. Thus
(2.59)

E[x] = µ

and so we refer to µ as the mean of the Gaussian distribution.

We now consider second order moments of the Gaussian. In the univariate case,
we considered the second order moment given by E[x2]. For the multivariate Gaus-
sian, there are D2 second order moments given by E[xixj], which we can group
together to form the matrix E[xxT]. This matrix can be written as

E[xxT] =

1

(2π)D/2
1
1

1

|Σ|1/2

=

(2π)D/2

|Σ|1/2

exp

1
2

−

(x − µ)TΣ−1(x − µ)

xxT dx

exp

1
2

−

zTΣ−1z

(z + µ)(z + µ)T dz





D

i=1






yjuj



D

j=1






1
2
D

z =

−

j=1

i=1

D




	

k=1

D

2.3. The Gaussian Distribution

83

where again we have changed variables using z = x − µ. Note that the cross-terms
involving µzT and µTz will again vanish by symmetry. The term µµT is constant
and can be taken outside the integral, which itself is unity because the Gaussian
distribution is normalized. Consider the term involving zzT. Again, we can make
use of the eigenvector expansion of the covariance matrix given by (2.45), together
with the completeness of the set of eigenvectors, to write

(2.60)

(2.61)

where yj = uT

j z, which gives

1

(2π)D/2

1

|Σ|1/2
1

(2π)D/2

1

|Σ|1/2

=

=

uiuT

i λi = Σ

exp

zTΣ−1z

zzT dz

uiuT
j

exp

−

y2
k
2λk

yiyj dy

where we have made use of the eigenvector equation (2.45), together with the fact
that the integral on the right-hand side of the middle line vanishes by symmetry
unless i = j, and in the ﬁnal line we have made use of the results (1.50) and (2.55),
together with (2.48). Thus we have

E[xxT] = µµT + Σ.

(2.62)

For single random variables, we subtracted the mean before taking second mo-
ments in order to deﬁne a variance. Similarly, in the multivariate case it is again
convenient to subtract off the mean, giving rise to the covariance of a random vector
x deﬁned by

(2.63)
For the speciﬁc case of a Gaussian distribution, we can make use of E[x] = µ,
together with the result (2.62), to give

(x − E[x])(x − E[x])T

cov[x] = E

.

cov[x] = Σ.

(2.64)

Because the parameter matrix Σ governs the covariance of x under the Gaussian
distribution, it is called the covariance matrix.

Although the Gaussian distribution (2.43) is widely used as a density model, it
suffers from some signiﬁcant limitations. Consider the number of free parameters in
the distribution. A general symmetric covariance matrix Σ will have D(D + 1)/2
independent parameters, and there are another D independent parameters in µ, giv-
ing D(D + 3)/2 parameters in total. For large D, the total number of parameters

Exercise 2.21

84

2. PROBABILITY DISTRIBUTIONS

Figure 2.8 Contours of constant
probability density for a Gaussian
distribution in two dimensions in
which the covariance matrix is (a) of
general form, (b) diagonal, in which
the elliptical contours are aligned
with the coordinate axes, and (c)
proportional to the identity matrix, in
which the contours are concentric
circles.

x2

x2

x2

x1

x1

x1

(a)

(b)

(c)

therefore grows quadratically with D, and the computational task of manipulating
and inverting large matrices can become prohibitive. One way to address this prob-
lem is to use restricted forms of the covariance matrix. If we consider covariance
matrices that are diagonal, so that Σ = diag(σ2
i ), we then have a total of 2D inde-
pendent parameters in the density model. The corresponding contours of constant
density are given by axis-aligned ellipsoids. We could further restrict the covariance
matrix to be proportional to the identity matrix, Σ = σ2I, known as an isotropic co-
variance, giving D + 1 independent parameters in the model and spherical surfaces
of constant density. The three possibilities of general, diagonal, and isotropic covari-
ance matrices are illustrated in Figure 2.8. Unfortunately, whereas such approaches
limit the number of degrees of freedom in the distribution and make inversion of the
covariance matrix a much faster operation, they also greatly restrict the form of the
probability density and limit its ability to capture interesting correlations in the data.
A further limitation of the Gaussian distribution is that it is intrinsically uni-
modal (i.e., has a single maximum) and so is unable to provide a good approximation
to multimodal distributions. Thus the Gaussian distribution can be both too ﬂexible,
in the sense of having too many parameters, while also being too limited in the range
of distributions that it can adequately represent. We will see later that the introduc-
tion of latent variables, also called hidden variables or unobserved variables, allows
both of these problems to be addressed. In particular, a rich family of multimodal
distributions is obtained by introducing discrete latent variables leading to mixtures
of Gaussians, as discussed in Section 2.3.9. Similarly, the introduction of continuous
latent variables, as described in Chapter 12, leads to models in which the number of
free parameters can be controlled independently of the dimensionality D of the data
space while still allowing the model to capture the dominant correlations in the data
set. Indeed, these two approaches can be combined and further extended to derive
a very rich set of hierarchical models that can be adapted to a broad range of prac-
tical applications. For instance, the Gaussian version of the Markov random ﬁeld,
which is widely used as a probabilistic model of images, is a Gaussian distribution
over the joint space of pixel intensities but rendered tractable through the imposition
of considerable structure reﬂecting the spatial organization of the pixels. Similarly,
the linear dynamical system, used to model time series data for applications such
as tracking, is also a joint Gaussian distribution over a potentially large number of
observed and latent variables and again is tractable due to the structure imposed on
the distribution. A powerful framework for expressing the form and properties of

Section 8.3

Section 13.3













and of the covariance matrix Σ given by

µ =

µa
µb

Σ =

Σaa Σab
Σba Σbb

.

Note that the symmetry ΣT = Σ of the covariance matrix implies that Σaa and Σbb
are symmetric, while Σba = ΣT
ab.

In many situations, it will be convenient to work with the inverse of the covari-

ance matrix

Λ ≡ Σ−1

which is known as the precision matrix. In fact, we shall see that some properties
of Gaussian distributions are most naturally expressed in terms of the covariance,
whereas others take a simpler form when viewed in terms of the precision. We
therefore also introduce the partitioned form of the precision matrix

(2.66)

(2.67)

(2.68)

2.3. The Gaussian Distribution

85

such complex distributions is that of probabilistic graphical models, which will form
the subject of Chapter 8.

2.3.1 Conditional Gaussian distributions
An important property of the multivariate Gaussian distribution is that if two
sets of variables are jointly Gaussian, then the conditional distribution of one set
conditioned on the other is again Gaussian. Similarly, the marginal distribution of
either set is also Gaussian.

Consider ﬁrst the case of conditional distributions. Suppose x is a D-dimensional
vector with Gaussian distribution N (x|µ, Σ) and that we partition x into two dis-
joint subsets xa and xb. Without loss of generality, we can take xa to form the ﬁrst
M components of x, with xb comprising the remaining D − M components, so that
(2.65)

x =

xa
xb

.

We also deﬁne corresponding partitions of the mean vector µ given by

Exercise 2.22

Λ =

Λaa Λab
Λba Λbb

(2.69)

corresponding to the partitioning (2.65) of the vector x. Because the inverse of a
symmetric matrix is also symmetric, we see that Λaa and Λbb are symmetric, while
ab = Λba. It should be stressed at this point that, for instance, Λaa is not simply
ΛT
given by the inverse of Σaa. In fact, we shall shortly examine the relation between
the inverse of a partitioned matrix and the inverses of its partitions.

Let us begin by ﬁnding an expression for the conditional distribution p(xa|xb).
From the product rule of probability, we see that this conditional distribution can be

86

2. PROBABILITY DISTRIBUTIONS

evaluated from the joint distribution p(x) = p(xa, xb) simply by ﬁxing xb to the
observed value and normalizing the resulting expression to obtain a valid probability
distribution over xa.
Instead of performing this normalization explicitly, we can
obtain the solution more efﬁciently by considering the quadratic form in the exponent
of the Gaussian distribution given by (2.44) and then reinstating the normalization
coefﬁcient at the end of the calculation. If we make use of the partitioning (2.65),
(2.66), and (2.69), we obtain

1
2

−

(x − µ)TΣ−1(x − µ) =
−
−

(xa − µa)TΛaa(xa − µa) −
(xb − µb)TΛba(xa − µa) −

1
2
1
2

1
(xa − µa)TΛab(xb − µb)
2
1
(xb − µb)TΛbb(xb − µb).
2

(2.70)

We see that as a function of xa, this is again a quadratic form, and hence the cor-
responding conditional distribution p(xa|xb) will be Gaussian. Because this distri-
bution is completely characterized by its mean and its covariance, our goal will be
to identify expressions for the mean and covariance of p(xa|xb) by inspection of
(2.70).
This is an example of a rather common operation associated with Gaussian
distributions, sometimes called ‘completing the square’, in which we are given a
quadratic form deﬁning the exponent terms in a Gaussian distribution, and we need
to determine the corresponding mean and covariance. Such problems can be solved
straightforwardly by noting that the exponent in a general Gaussian distribution
N (x|µ, Σ) can be written

xTΣ−1x + xTΣ−1µ + const

(2.71)

1
2

−

(x − µ)TΣ−1(x − µ) = −

1
2

where ‘const’ denotes terms which are independent of x, and we have made use of
the symmetry of Σ. Thus if we take our general quadratic form and express it in
the form given by the right-hand side of (2.71), then we can immediately equate the
matrix of coefﬁcients entering the second order term in x to the inverse covariance
matrix Σ−1 and the coefﬁcient of the linear term in x to Σ−1µ, from which we can
obtain µ.

Now let us apply this procedure to the conditional Gaussian distribution p(xa|xb)
for which the quadratic form in the exponent is given by (2.70). We will denote the
mean and covariance of this distribution by µa|b and Σa|b, respectively. Consider
the functional dependence of (2.70) on xa in which xb is regarded as a constant. If
we pick out all terms that are second order in xa, we have

1
2

−

a Λaaxa
xT

(2.72)

from which we can immediately conclude that the covariance (inverse precision) of
p(xa|xb) is given by

(2.73)

Σa|b = Λ−1
aa .

















2.3. The Gaussian Distribution

87

(2.74)

(2.75)

Now consider all of the terms in (2.70) that are linear in xa
a {Λaaµa − Λab(xb − µb)}
xT

where we have used ΛT
the coefﬁcient of xa in this expression must equal Σ−1

ba = Λab. From our discussion of the general form (2.71),

a|bµa|b and hence

µa|b = Σa|b {Λaaµa − Λab(xb − µb)}

= µa − Λ−1

aa Λab(xb − µb)

where we have made use of (2.73).

The results (2.73) and (2.75) are expressed in terms of the partitioned precision
matrix of the original joint distribution p(xa, xb). We can also express these results
in terms of the corresponding partitioned covariance matrix. To do this, we make use
of the following identity for the inverse of a partitioned matrix

A B
C D

where we have deﬁned

−1

=

M

−D−1CM D−1 + D−1CMBD−1

−MBD−1

(2.76)

(2.77)
The quantity M−1 is known as the Schur complement of the matrix on the left-hand
side of (2.76) with respect to the submatrix D. Using the deﬁnition

M = (A − BD−1C)−1.

Σaa Σab
Σba Σbb

−1

=

Λaa Λab
Λba Λbb

and making use of (2.76), we have

Λaa = (Σaa − ΣabΣ−1
Λab = −(Σaa − ΣabΣ−1

bb Σba)−1

bb Σba)−1ΣabΣ−1
bb .

(2.78)

(2.79)
(2.80)

From these we obtain the following expressions for the mean and covariance of the
conditional distribution p(xa|xb)

bb (xb − µb)
bb Σba.

µa|b = µa + ΣabΣ−1
Σa|b = Σaa − ΣabΣ−1

(2.81)
(2.82)
Comparing (2.73) and (2.82), we see that the conditional distribution p(xa|xb) takes
a simpler form when expressed in terms of the partitioned precision matrix than
when it is expressed in terms of the partitioned covariance matrix. Note that the
mean of the conditional distribution p(xa|xb), given by (2.81), is a linear function of
xb and that the covariance, given by (2.82), is independent of xa. This represents an
example of a linear-Gaussian model.

Exercise 2.24

Section 8.1.4









88

2. PROBABILITY DISTRIBUTIONS

2.3.2 Marginal Gaussian distributions
We have seen that if a joint distribution p(xa, xb) is Gaussian, then the condi-
tional distribution p(xa|xb) will again be Gaussian. Now we turn to a discussion of
the marginal distribution given by

p(xa) =

p(xa, xb) dxb

(2.83)

which, as we shall see, is also Gaussian. Once again, our strategy for evaluating this
distribution efﬁciently will be to focus on the quadratic form in the exponent of the
joint distribution and thereby to identify the mean and covariance of the marginal
distribution p(xa).

The quadratic form for the joint distribution can be expressed, using the par-
titioned precision matrix, in the form (2.70). Because our goal is to integrate out
xb, this is most easily achieved by ﬁrst considering the terms involving xb and then
completing the square in order to facilitate integration. Picking out just those terms
that involve xb, we have

1
2

b Λbbxb+xT
xT

−
where we have deﬁned

b m = −

1
2

(xb−Λ−1

bb m)TΛbb(xb−Λ−1

bb m)+

m = Λbbµb − Λba(xa − µa).

1
2

mTΛ−1

bb m (2.84)

(2.85)

We see that the dependence on xb has been cast into the standard quadratic form of a
Gaussian distribution corresponding to the ﬁrst term on the right-hand side of (2.84),
plus a term that does not depend on xb (but that does depend on xa). Thus, when
we take the exponential of this quadratic form, we see that the integration over xb
required by (2.83) will take the form

exp

1
2

−

(xb − Λ−1

bb m)TΛbb(xb − Λ−1

bb m)

dxb.

(2.86)

This integration is easily performed by noting that it is the integral over an unnor-
malized Gaussian, and so the result will be the reciprocal of the normalization co-
efﬁcient. We know from the form of the normalized Gaussian given by (2.43), that
this coefﬁcient is independent of the mean and depends only on the determinant of
the covariance matrix. Thus, by completing the square with respect to xb, we can
integrate out xb and the only term remaining from the contributions on the left-hand
side of (2.84) that depends on xa is the last term on the right-hand side of (2.84) in
which m is given by (2.85). Combining this term with the remaining terms from



















89

(2.87)

(2.88)

(2.89)

(2.90)

(2.91)

(2.92)
(2.93)

2.3. The Gaussian Distribution

(2.70) that depend on xa, we obtain

1
2

[Λbbµb − Λba(xa − µa)]T Λ−1

bb [Λbbµb − Λba(xa − µa)]

a (Λaaµa + Λabµb) + const

−
= −

a Λaaxa + xT
xT

1
2
1
a (Λaa − ΛabΛ−1
xT
2
a (Λaa − ΛabΛ−1
+xT

bb Λba)xa

bb Λba)−1µa + const

where ‘const’ denotes quantities independent of xa. Again, by comparison with
(2.71), we see that the covariance of the marginal distribution of p(xa) is given by

Similarly, the mean is given by

Σa = (Λaa − ΛabΛ−1

bb Λba)−1.

Σa(Λaa − ΛabΛ−1

bb Λba)µa = µa

where we have used (2.88). The covariance in (2.88) is expressed in terms of the
partitioned precision matrix given by (2.69). We can rewrite this in terms of the
corresponding partitioning of the covariance matrix given by (2.67), as we did for
the conditional distribution. These partitioned matrices are related by

Making use of (2.76), we then have

Thus we obtain the intuitively satisfying result that the marginal distribution p(xa)
has mean and covariance given by

Λaa Λab
Λba Λbb

−1

=

Σaa Σab
Σba Σbb

Λaa − ΛabΛ−1

bb Λba

−1 = Σaa.

E[xa] = µa

cov[xa] = Σaa.

We see that for a marginal distribution, the mean and covariance are most simply ex-
pressed in terms of the partitioned covariance matrix, in contrast to the conditional
distribution for which the partitioned precision matrix gives rise to simpler expres-
sions.

Our results for the marginal and conditional distributions of a partitioned Gaus-

sian are summarized below.

Partitioned Gaussians
Given a joint Gaussian distribution N (x|µ, Σ) with Λ ≡ Σ−1 and

x =

xa
xb

,

µ =

µa
µb

(2.94)



1



10

5

0

0





2. PROBABILITY DISTRIBUTIONS

xb = 0.7

p(xa|xb = 0.7)

90

1

xb

0.5

p(xa, xb)

0

0

0.5

xa

p(xa)

0.5

xa

1

Figure 2.9 The plot on the left shows the contours of a Gaussian distribution p(xa, xb) over two variables, and
the plot on the right shows the marginal distribution p(xa) (blue curve) and the conditional distribution p(xa|xb)
for xb = 0.7 (red curve).

Σ =

Σaa Σab
Σba Σbb

, Λ =

Λaa Λab
Λba Λbb

.

Conditional distribution:

p(xa|xb) = N (x|µa|b, Λ−1
aa )

µa|b = µa − Λ−1

aa Λab(xb − µb).

Marginal distribution:

p(xa) = N (xa|µa, Σaa).

(2.95)

(2.96)
(2.97)

(2.98)

We illustrate the idea of conditional and marginal distributions associated with

a multivariate Gaussian using an example involving two variables in Figure 2.9.

2.3.3 Bayes’ theorem for Gaussian variables
In Sections 2.3.1 and 2.3.2, we considered a Gaussian p(x) in which we parti-
tioned the vector x into two subvectors x = (xa, xb) and then found expressions for
the conditional distribution p(xa|xb) and the marginal distribution p(xa). We noted
that the mean of the conditional distribution p(xa|xb) was a linear function of xb.
Here we shall suppose that we are given a Gaussian marginal distribution p(x) and a
Gaussian conditional distribution p(y|x) in which p(y|x) has a mean that is a linear
function of x, and a covariance which is independent of x. This is an example of
























1
2
1
2





2.3. The Gaussian Distribution

91

a linear Gaussian model (Roweis and Ghahramani, 1999), which we shall study in
greater generality in Section 8.1.4. We wish to ﬁnd the marginal distribution p(y)
and the conditional distribution p(x|y). This is a problem that will arise frequently
in subsequent chapters, and it will prove convenient to derive the general results here.

We shall take the marginal and conditional distributions to be

p(x) = N
p(y|x) = N

x|µ, Λ−1
y|Ax + b, L−1

where µ, A, and b are parameters governing the means, and Λ and L are precision
matrices. If x has dimensionality M and y has dimensionality D, then the matrix A
has size D × M.
deﬁne

First we ﬁnd an expression for the joint distribution over x and y. To do this, we

z =

x
y

and then consider the log of the joint distribution

ln p(z) = ln p(x) + ln p(y|x)

= −
−

(x − µ)TΛ(x − µ)
(y − Ax − b)TL(y − Ax − b) + const

where ‘const’ denotes terms independent of x and y. As before, we see that this is a
quadratic function of the components of z, and hence p(z) is Gaussian distribution.
To ﬁnd the precision of this Gaussian, we consider the second order terms in (2.102),
which can be written as

−

1
xT(Λ + ATLA)x −
2
= −

x
y

1
2

T

1
2

yTLy +

1
2

yTLAx +

xTATLy

Λ + ATLA −ATL

L

−LA

x
y

1
2

zTRz

(2.103)

1
2
= −

and so the Gaussian distribution over z has precision (inverse covariance) matrix
given by

R =

Λ + ATLA −ATL

.

L

−LA

(2.104)

(2.99)
(2.100)

(2.101)

(2.102)

Exercise 2.29

The covariance matrix is found by taking the inverse of the precision, which can be
done using the matrix inversion formula (2.76) to give

cov[z] = R−1 =

Λ−1
AΛ−1 L−1 + AΛ−1AT

Λ−1AT

.

(2.105)


















92

2. PROBABILITY DISTRIBUTIONS

Similarly, we can ﬁnd the mean of the Gaussian distribution over z by identify-

ing the linear terms in (2.102), which are given by

xTΛµ − xTATLb + yTLb =

T

x
y

Λµ − ATLb

Lb

.

Using our earlier result (2.71) obtained by completing the square over the quadratic
form of a multivariate Gaussian, we ﬁnd that the mean of z is given by

Exercise 2.30

Making use of (2.105), we then obtain

E[z] = R−1

E[z] =

Λµ − ATLb

Lb

.

µ

Aµ + b

.

(2.106)

(2.107)

(2.108)

Section 2.3

Section 2.3

Next we ﬁnd an expression for the marginal distribution p(y) in which we have
marginalized over x. Recall that the marginal distribution over a subset of the com-
ponents of a Gaussian random vector takes a particularly simple form when ex-
pressed in terms of the partitioned covariance matrix. Speciﬁcally, its mean and
covariance are given by (2.92) and (2.93), respectively. Making use of (2.105) and
(2.108) we see that the mean and covariance of the marginal distribution p(y) are
given by

E[y] = Aµ + b

cov[y] = L−1 + AΛ−1AT.

(2.109)
(2.110)

A special case of this result is when A = I, in which case it reduces to the convolu-
tion of two Gaussians, for which we see that the mean of the convolution is the sum
of the mean of the two Gaussians, and the covariance of the convolution is the sum
of their covariances.

Finally, we seek an expression for the conditional p(x|y). Recall that the results
for the conditional distribution are most easily expressed in terms of the partitioned
precision matrix, using (2.73) and (2.75). Applying these results to (2.105) and
(2.108) we see that the conditional distribution p(x|y) has mean and covariance
given by

E[x|y] = (Λ + ATLA)−1
cov[x|y] = (Λ + ATLA)−1.

ATL(y − b) + Λµ

(2.111)
(2.112)

The evaluation of this conditional can be seen as an example of Bayes’ theorem.
We can interpret the distribution p(x) as a prior distribution over x. If the variable
y is observed, then the conditional distribution p(x|y) represents the corresponding
posterior distribution over x. Having found the marginal and conditional distribu-
tions, we effectively expressed the joint distribution p(z) = p(x)p(y|x) in the form
p(x|y)p(y). These results are summarized below.



N






1
2

n=1

n=1

n=1

N

N

N

2.3. The Gaussian Distribution

93

Marginal and Conditional Gaussians

Given a marginal Gaussian distribution for x and a conditional Gaussian distri-
bution for y given x in the form

p(x) = N (x|µ, Λ−1)
p(y|x) = N (y|Ax + b, L−1)

the marginal distribution of y and the conditional distribution of x given y are
given by

p(y) = N (y|Aµ + b, L−1 + AΛ−1AT)
p(x|y) = N (x|Σ{ATL(y − b) + Λµ}, Σ)

where

Σ = (Λ + ATLA)−1.

(2.113)
(2.114)

(2.115)
(2.116)

(2.117)

2.3.4 Maximum likelihood for the Gaussian
Given a data set X = (x1, . . . , xN )T in which the observations {xn} are as-
sumed to be drawn independently from a multivariate Gaussian distribution, we can
estimate the parameters of the distribution by maximum likelihood. The log likeli-
hood function is given by

ln p(X|µ, Σ) = −

N D
2

ln(2π)−

N
2

ln|Σ|−

(xn−µ)TΣ−1(xn−µ). (2.118)

By simple rearrangement, we see that the likelihood function depends on the data set
only through the two quantities

Appendix C

These are known as the sufﬁcient statistics for the Gaussian distribution. Using
(C.19), the derivative of the log likelihood with respect to µ is given by

xn,

n=1

xnxT
n.

(2.119)

∂
∂µ

ln p(X|µ, Σ) =

Σ−1(xn − µ)

(2.120)

and setting this derivative to zero, we obtain the solution for the maximum likelihood
estimate of the mean given by

N

µML =

1
N

xn

n=1

(2.121)




n=1

N







94

2. PROBABILITY DISTRIBUTIONS

Exercise 2.34

Exercise 2.35

which is the mean of the observed set of data points. The maximization of (2.118)
with respect to Σ is rather more involved. The simplest approach is to ignore the
symmetry constraint and show that the resulting solution is symmetric as required.
Alternative derivations of this result, which impose the symmetry and positive deﬁ-
niteness constraints explicitly, can be found in Magnus and Neudecker (1999). The
result is as expected and takes the form

ΣML =

1
N

(xn − µML)(xn − µML)T

(2.122)

which involves µML because this is the result of a joint maximization with respect
to µ and Σ. Note that the solution (2.121) for µML does not depend on ΣML, and so
we can ﬁrst evaluate µML and then use this to evaluate ΣML.

If we evaluate the expectations of the maximum likelihood solutions under the

true distribution, we obtain the following results

E[µML] = µ
E[ΣML] = N − 1

Σ.

N

(2.123)

(2.124)

We see that the expectation of the maximum likelihood estimate for the mean is equal
to the true mean. However, the maximum likelihood estimate for the covariance has
an expectation that is less than the true value, and hence it is biased. We can correct
this bias by deﬁning a different estimator

Σ given by

Σ =

N

1

N − 1

n=1

(xn − µML)(xn − µML)T.

(2.125)

Clearly from (2.122) and (2.124), the expectation of

Σ is equal to Σ.

2.3.5 Sequential estimation
Our discussion of the maximum likelihood solution for the parameters of a Gaus-
sian distribution provides a convenient opportunity to give a more general discussion
of the topic of sequential estimation for maximum likelihood. Sequential methods
allow data points to be processed one at a time and then discarded and are important
for on-line applications, and also where large data sets are involved so that batch
processing of all data points at once is infeasible.

Consider the result (2.121) for the maximum likelihood estimator of the mean
ML when it is based on N observations. If we

µML, which we will denote by µ(N )



N



z



Figure 2.10 A schematic illustration of two correlated ran-
dom variables z and θ,
together with the
regression function f (θ) given by the con-
The Robbins-
ditional expectation E[z|θ].
Monro algorithm provides a general sequen-
tial procedure for ﬁnding the root θ of such
functions.

2.3. The Gaussian Distribution

95

θ

f(θ)

θ

dissect out the contribution from the ﬁnal data point xN , we obtain

µ(N )

ML =

1
N

xn

n=1

=

=

N−1

n=1

1
N

xN +

1
N
1
N
ML +

xN + N − 1
N
1
N

xn

µ(N−1)

ML

= µ(N−1)

(xN − µ(N−1)

ML

).

(2.126)

This result has a nice interpretation, as follows. After observing N − 1 data points
we have estimated µ by µ(N−1)
. We now observe data point xN , and we obtain our
revised estimate µ(N )
ML by moving the old estimate a small amount, proportional to
1/N, in the direction of the ‘error signal’ (xN − µ(N−1)
). Note that, as N increases,
so the contribution from successive data points gets smaller.

ML

ML

The result (2.126) will clearly give the same answer as the batch result (2.121)
because the two formulae are equivalent. However, we will not always be able to de-
rive a sequential algorithm by this route, and so we seek a more general formulation
of sequential learning, which leads us to the Robbins-Monro algorithm. Consider a
pair of random variables θ and z governed by a joint distribution p(z, θ). The con-
ditional expectation of z given θ deﬁnes a deterministic function f(θ) that is given
by

f(θ) ≡ E[z|θ] =

zp(z|θ) dz

(2.127)

and is illustrated schematically in Figure 2.10. Functions deﬁned in this way are
called regression functions.

Our goal is to ﬁnd the root θ at which f(θ) = 0. If we had a large data set
of observations of z and θ, then we could model the regression function directly and
then obtain an estimate of its root. Suppose, however, that we observe values of
z one at a time and we wish to ﬁnd a corresponding sequential estimation scheme
for θ. The following general procedure for solving such problems was given by

	










N

N =1
∞

N =1




N

n=1



96

2. PROBABILITY DISTRIBUTIONS

Robbins and Monro (1951). We shall assume that the conditional variance of z is
ﬁnite so that

(2.128)
and we shall also, without loss of generality, consider the case where f(θ) > 0 for
θ > θ and f(θ) < 0 for θ < θ, as is the case in Figure 2.10. The Robbins-Monro
procedure then deﬁnes a sequence of successive estimates of the root θ given by

(z − f)2 | θ

< ∞

E

θ(N ) = θ(N−1) + aN−1z(θ(N−1))

where z(θ(N )) is an observed value of z when θ takes the value θ(N ). The coefﬁcients
{aN} represent a sequence of positive numbers that satisfy the conditions

(2.129)

(2.130)

(2.131)

(2.132)

lim
N→∞
∞

aN = 0

aN = ∞

a2
N < ∞.

It can then be shown (Robbins and Monro, 1951; Fukunaga, 1990) that the sequence
of estimates given by (2.129) does indeed converge to the root with probability one.
Note that the ﬁrst condition (2.130) ensures that the successive corrections decrease
in magnitude so that the process can converge to a limiting value. The second con-
dition (2.131) is required to ensure that the algorithm does not converge short of the
root, and the third condition (2.132) is needed to ensure that the accumulated noise
has ﬁnite variance and hence does not spoil convergence.

Now let us consider how a general maximum likelihood problem can be solved
sequentially using the Robbins-Monro algorithm. By deﬁnition, the maximum like-
lihood solution θML is a stationary point of the log likelihood function and hence
satisﬁes

∂
∂θ

1
N

ln p(xn|θ)

n=1

= 0.

θML

(2.133)

Exchanging the derivative and the summation, and taking the limit N → ∞ we have

1
N

lim
N→∞

∂
∂θ

ln p(xn|θ) = Ex

∂
∂θ

ln p(x|θ)

(2.134)

and so we see that ﬁnding the maximum likelihood solution corresponds to ﬁnd-
ing the root of a regression function. We can therefore apply the Robbins-Monro
procedure, which now takes the form

θ(N ) = θ(N−1) + aN−1

∂

∂θ(N−1) ln p(xN|θ(N−1)).

(2.135)



N

z





N



2.3. The Gaussian Distribution

Figure 2.11 In the case of a Gaussian distribution, with θ
corresponding to the mean µ, the regression
function illustrated in Figure 2.10 takes the form
of a straight line, as shown in red.
In this
case, the random variable z corresponds to the
derivative of the log likelihood function and is
given by (x − µML)/σ2, and its expectation that
deﬁnes the regression function is a straight line
given by (µ − µML)/σ2. The root of the regres-
sion function corresponds to the maximum like-
lihood estimator µML.

µML

97

p(z|µ)

µ

As a speciﬁc example, we consider once again the sequential estimation of the
mean of a Gaussian distribution, in which case the parameter θ(N ) is the estimate
µ(N )
ML of the mean of the Gaussian, and the random variable z is given by

z = ∂

∂µML

ln p(x|µML, σ2) =

1
σ2 (x − µML).

(2.136)

Thus the distribution of z is Gaussian with mean µ − µML, as illustrated in Fig-
ure 2.11. Substituting (2.136) into (2.135), we obtain the univariate form of (2.126),
provided we choose the coefﬁcients aN to have the form aN = σ2/N. Note that
although we have focussed on the case of a single variable, the same technique,
together with the same restrictions (2.130)–(2.132) on the coefﬁcients aN , apply
equally to the multivariate case (Blum, 1965).

2.3.6 Bayesian inference for the Gaussian
The maximum likelihood framework gave point estimates for the parameters µ
and Σ. Now we develop a Bayesian treatment by introducing prior distributions
over these parameters. Let us begin with a simple example in which we consider a
single Gaussian random variable x. We shall suppose that the variance σ2 is known,
and we consider the task of inferring the mean µ given a set of N observations
X = {x1, . . . , xN}. The likelihood function, that is the probability of the observed
data given µ, viewed as a function of µ, is given by

p(X|µ) =

p(xn|µ) =

n=1

1

(2πσ2)N/2

exp

1
2σ2

−

(xn − µ)2

n=1

.

(2.137)

Again we emphasize that the likelihood function p(X|µ) is not a probability distri-
bution over µ and is not normalized.
We see that the likelihood function takes the form of the exponential of a quad-
ratic form in µ. Thus if we choose a prior p(µ) given by a Gaussian, it will be a











N

p(µ) = N
and the posterior distribution is given by

µ|µ0, σ2

0

p(µ|X) ∝ p(X|µ)p(µ).

where

p(µ|X) = N

µ|µN , σ2

N

µN =

1
σ2
N

=

0

σ2

0 + σ2 µ0 + N σ2
+ N
σ2

N σ2

N σ2
1
σ2
0

0 + σ2 µML

(2.138)

(2.139)

(2.140)

(2.141)

(2.142)

in which µML is the maximum likelihood solution for µ given by the sample mean

µML =

1
N

xn.

n=1

(2.143)

It is worth spending a moment studying the form of the posterior mean and
variance. First of all, we note that the mean of the posterior distribution given by
(2.141) is a compromise between the prior mean µ0 and the maximum likelihood
solution µML. If the number of observed data points N = 0, then (2.141) reduces
to the prior mean as expected. For N → ∞, the posterior mean is given by the
maximum likelihood solution. Similarly, consider the result (2.142) for the variance
of the posterior distribution. We see that this is most naturally expressed in terms
of the inverse variance, which is called the precision. Furthermore, the precisions
are additive, so that the precision of the posterior is given by the precision of the
prior plus one contribution of the data precision from each of the observed data
points. As we increase the number of observed data points, the precision steadily
increases, corresponding to a posterior distribution with steadily decreasing variance.
With no observed data points, we have the prior variance, whereas if the number of
N goes to zero and the posterior distribution
data points N → ∞, the variance σ2
becomes inﬁnitely peaked around the maximum likelihood solution. We therefore
see that the maximum likelihood result of a point estimate for µ given by (2.143) is
recovered precisely from the Bayesian formalism in the limit of an inﬁnite number
of observations. Note also that for ﬁnite N, if we take the limit σ2
0 → ∞ in which the
prior has inﬁnite variance then the posterior mean (2.141) reduces to the maximum
N = σ2/N.
likelihood result, while from (2.142) the posterior variance is given by σ2

98

2. PROBABILITY DISTRIBUTIONS

conjugate distribution for this likelihood function because the corresponding poste-
rior will be a product of two exponentials of quadratic functions of µ and hence will
also be Gaussian. We therefore take our prior distribution to be

Exercise 2.38

Simple manipulation involving completing the square in the exponent shows that the
posterior distribution is given by





N−1

n=1

 



N







N

2.3. The Gaussian Distribution

Figure 2.12 Illustration of Bayesian inference for
the mean µ of a Gaussian distri-
bution, in which the variance is as-
sumed to be known. The curves
show the prior distribution over µ
(the curve labelled N = 0), which
in this case is itself Gaussian, along
with the posterior distribution given
by (2.140) for increasing numbers N
of data points. The data points are
generated from a Gaussian of mean
0.8 and variance 0.1, and the prior is
chosen to have mean 0. In both the
prior and the likelihood function, the
variance is set to the true value.

5

0
−1

N = 0

N = 10

N = 2

N = 1

0

99

1

Exercise 2.40

Section 2.3.5

We illustrate our analysis of Bayesian inference for the mean of a Gaussian
distribution in Figure 2.12. The generalization of this result to the case of a D-
dimensional Gaussian random variable x with known covariance and unknown mean
is straightforward.

We have already seen how the maximum likelihood expression for the mean of
a Gaussian can be re-cast as a sequential update formula in which the mean after
observing N data points was expressed in terms of the mean after observing N − 1
data points together with the contribution from data point xN . In fact, the Bayesian
paradigm leads very naturally to a sequential view of the inference problem. To see
this in the context of the inference of the mean of a Gaussian, we write the posterior
distribution with the contribution from the ﬁnal data point xN separated out so that

p(µ|D) ∝

p(µ)

p(xn|µ)

p(xN|µ).

(2.144)

The term in square brackets is (up to a normalization coefﬁcient) just the posterior
distribution after observing N − 1 data points. We see that this can be viewed as
a prior distribution, which is combined using Bayes’ theorem with the likelihood
function associated with data point xN to arrive at the posterior distribution after
observing N data points. This sequential view of Bayesian inference is very general
and applies to any problem in which the observed data are assumed to be independent
and identically distributed.

So far, we have assumed that the variance of the Gaussian distribution over the
data is known and our goal is to infer the mean. Now let us suppose that the mean
is known and we wish to infer the variance. Again, our calculations will be greatly
simpliﬁed if we choose a conjugate form for the prior distribution. It turns out to be
most convenient to work with the precision λ ≡ 1/σ2. The likelihood function for λ
takes the form

p(X|λ) =

n=1

N (xn|µ, λ−1) ∝ λN/2 exp

(xn − µ)2

.

n=1

(2.145)

λ
2

−



2



1

0

N




N

n=1

100

2. PROBABILITY DISTRIBUTIONS

2

1

0

0

a = 0.1
b = 0.1

2

1

λ

1

0

0

2

λ

1

a = 1
b = 1

a = 4
b = 6

2

2

0

λ

1

Figure 2.13 Plot of the gamma distribution Gam(λ|a, b) deﬁned by (2.146) for various values of the parameters
a and b.

The corresponding conjugate prior should therefore be proportional to the product
of a power of λ and the exponential of a linear function of λ. This corresponds to
the gamma distribution which is deﬁned by

Gam(λ|a, b) =

1
Γ(a) baλa−1 exp(−bλ).

(2.146)

Exercise 2.41

Exercise 2.42

Here Γ(a) is the gamma function that is deﬁned by (1.141) and that ensures that
(2.146) is correctly normalized. The gamma distribution has a ﬁnite integral if a > 0,
and the distribution itself is ﬁnite if a � 1. It is plotted, for various values of a and
b, in Figure 2.13. The mean and variance of the gamma distribution are given by

E[λ] = a
b
var[λ] = a
b2 .

(2.147)

(2.148)

Consider a prior distribution Gam(λ|a0, b0). If we multiply by the likelihood

function (2.145), then we obtain a posterior distribution

p(λ|X) ∝ λa0−1λN/2 exp

−b0λ −

λ
2

(xn − µ)2

n=1

(2.149)

which we recognize as a gamma distribution of the form Gam(λ|aN , bN ) where

aN = a0 + N
2
1
2

bN = b0 +

(xn − µ)2 = b0 + N

2 σ2

ML

(2.150)

(2.151)

ML is the maximum likelihood estimator of the variance. Note that in (2.149)
where σ2
there is no need to keep track of the normalization constants in the prior and the
likelihood function because, if required, the correct coefﬁcient can be found at the
end using the normalized form (2.146) for the gamma distribution.










N

n=1


















n=1

N




n=1

N




Section 2.2

2.3. The Gaussian Distribution

101

From (2.150), we see that the effect of observing N data points is to increase
the value of the coefﬁcient a by N/2. Thus we can interpret the parameter a0 in
the prior in terms of 2a0 ‘effective’ prior observations. Similarly, from (2.151) we
see that the N data points contribute N σ2
ML is
the variance, and so we can interpret the parameter b0 in the prior as arising from
the 2a0 ‘effective’ prior observations having variance 2b0/(2a0) = b0/a0. Recall
that we made an analogous interpretation for the Dirichlet prior. These distributions
are examples of the exponential family, and we shall see that the interpretation of
a conjugate prior in terms of effective ﬁctitious data points is a general one for the
exponential family of distributions.

ML/2 to the parameter b, where σ2

Instead of working with the precision, we can consider the variance itself. The
conjugate prior in this case is called the inverse gamma distribution, although we
shall not discuss this further because we will ﬁnd it more convenient to work with
the precision.

Now suppose that both the mean and the precision are unknown. To ﬁnd a

conjugate prior, we consider the dependence of the likelihood function on µ and λ

p(X|µ, λ) =

1/2

λ
2π

exp

N

λ
2

−

(xn − µ)2

λ1/2 exp

∝

λµ2
2

−

exp

λµ

xn −

λ
2

x2
n

.

(2.152)

We now wish to identify a prior distribution p(µ, λ) that has the same functional
dependence on µ and λ as the likelihood function and that should therefore take the
form

p(µ, λ) ∝
= exp

λ1/2 exp

β

λµ2
2

−

exp{cλµ − dλ}

βλ
2

−

(µ − c/β)2

λβ/2 exp

−

d −

c2
2β

λ

(2.153)

where c, d, and β are constants. Since we can always write p(µ, λ) = p(µ|λ)p(λ),
we can ﬁnd p(µ|λ) and p(λ) by inspection. In particular, we see that p(µ|λ) is a
Gaussian whose precision is a linear function of λ and that p(λ) is a gamma distri-
bution, so that the normalized prior takes the form

p(µ, λ) = N (µ|µ0, (βλ)−1)Gam(λ|a, b)

(2.154)

where we have deﬁned new constants given by µ0 = c/β, a = 1 + β/2, b =
d−c2/2β. The distribution (2.154) is called the normal-gamma or Gaussian-gamma
distribution and is plotted in Figure 2.14. Note that this is not simply the product
of an independent Gaussian prior over µ and a gamma prior over λ, because the
precision of µ is a linear function of λ. Even if we chose a prior in which µ and λ
were independent, the posterior distribution would exhibit a coupling between the
precision of µ and the value of λ.



2




0
µ







D





102

2. PROBABILITY DISTRIBUTIONS

Figure 2.14 Contour plot of the normal-gamma
distribution (2.154) for parameter
values µ0 = 0, β = 2, a = 5 and
b = 6.

2

λ

1

0
−2

In the case of the multivariate Gaussian distribution N

for a D-
dimensional variable x, the conjugate prior distribution for the mean µ, assuming
the precision is known, is again a Gaussian. For known mean and unknown precision
matrix Λ, the conjugate prior is the Wishart distribution given by

x|µ, Λ−1

Exercise 2.45

W(Λ|W, ν) = B|Λ|(ν−D−1)/2 exp

1
2

−

Tr(W−1Λ)

(2.155)

where ν is called the number of degrees of freedom of the distribution, W is a D×D
scale matrix, and Tr(·) denotes the trace. The normalization constant B is given by

B(W, ν) = |W|−ν/2

2νD/2 πD(D−1)/4

Γ

i=1

ν + 1 − i

2

−1

.

(2.156)

Again, it is also possible to deﬁne a conjugate prior over the covariance matrix itself,
rather than over the precision matrix, which leads to the inverse Wishart distribu-
tion, although we shall not discuss this further. If both the mean and the precision
are unknown, then, following a similar line of reasoning to the univariate case, the
conjugate prior is given by

p(µ, Λ|µ0, β, W, ν) = N (µ|µ0, (βΛ)−1)W(Λ|W, ν)

(2.157)

which is known as the normal-Wishart or Gaussian-Wishart distribution.

2.3.7 Student’s t-distribution
We have seen that the conjugate prior for the precision of a Gaussian is given
by a gamma distribution. If we have a univariate Gaussian N (x|µ, τ−1) together
with a Gamma prior Gam(τ|a, b) and we integrate out the precision, we obtain the
marginal distribution of x in the form

Section 2.3.6

Exercise 2.46















0.5

0.4

0.3

0.2

0.1

0
−5










0



ν → ∞
ν = 1.0
ν = 0.1

5

(2.158)

dτ

2.3. The Gaussian Distribution

103

Figure 2.15 Plot of Student’s t-distribution (2.159)
for µ = 0 and λ = 1 for various values
of ν. The limit ν → ∞ corresponds
to a Gaussian distribution with mean
µ and precision λ.

p(x|µ, a, b) =
=

∞
0 N (x|µ, τ−1)Gam(τ|a, b) dτ
∞
exp

bae(−bτ )τ a−1

1/2

0
ba
Γ(a)

=

Γ(a)
1/2
1
2π

τ
2π
(x − µ)2

2

b +

(x − µ)2

−

τ
2
−a−1/2

Γ(a + 1/2)

where we have made the change of variable z = τ[b + (x − µ)2/2]. By convention
we deﬁne new parameters given by ν = 2a and λ = a/b, in terms of which the
distribution p(x|µ, a, b) takes the form
λ
πν

1 + λ(x − µ)2

St(x|µ, λ, ν) =

Γ(ν/2 + 1/2)

−ν/2−1/2

Γ(ν/2)

(2.159)

1/2

ν

which is known as Student’s t-distribution. The parameter λ is sometimes called the
precision of the t-distribution, even though it is not in general equal to the inverse
of the variance. The parameter ν is called the degrees of freedom, and its effect is
illustrated in Figure 2.15. For the particular case of ν = 1, the t-distribution reduces
to the Cauchy distribution, while in the limit ν → ∞ the t-distribution St(x|µ, λ, ν)
becomes a Gaussian N (x|µ, λ−1) with mean µ and precision λ.
From (2.158), we see that Student’s t-distribution is obtained by adding up an
inﬁnite number of Gaussian distributions having the same mean but different preci-
sions. This can be interpreted as an inﬁnite mixture of Gaussians (Gaussian mixtures
will be discussed in detail in Section 2.3.9. The result is a distribution that in gen-
eral has longer ‘tails’ than a Gaussian, as was seen in Figure 2.15. This gives the t-
distribution an important property called robustness, which means that it is much less
sensitive than the Gaussian to the presence of a few data points which are outliers.
The robustness of the t-distribution is illustrated in Figure 2.16, which compares the
maximum likelihood solutions for a Gaussian and a t-distribution. Note that the max-
imum likelihood solution for the t-distribution can be found using the expectation-
maximization (EM) algorithm. Here we see that the effect of a small number of

Exercise 2.47

Exercise 12.24

2. PROBABILITY DISTRIBUTIONS

104

0.5

0.4

0.3

0.2

0.1

0
−5






0.5

0.4

0.3

0.2

0.1

0
−5




0

5

(a)

10

0

5

10

(b)

Figure 2.16 Illustration of the robustness of Student’s t-distribution compared to a Gaussian. (a) Histogram
distribution of 30 data points drawn from a Gaussian distribution, together with the maximum likelihood ﬁt ob-
tained from a t-distribution (red curve) and a Gaussian (green curve, largely hidden by the red curve). Because
the t-distribution contains the Gaussian as a special case it gives almost the same solution as the Gaussian.
(b) The same data set but with three additional outlying data points showing how the Gaussian (green curve) is
strongly distorted by the outliers, whereas the t-distribution (red curve) is relatively unaffected.

outliers is much less signiﬁcant for the t-distribution than for the Gaussian. Outliers
can arise in practical applications either because the process that generates the data
corresponds to a distribution having a heavy tail or simply through mislabelled data.
Robustness is also an important property for regression problems. Unsurprisingly,
the least squares approach to regression does not exhibit robustness, because it cor-
responds to maximum likelihood under a (conditional) Gaussian distribution. By
basing a regression model on a heavy-tailed distribution such as a t-distribution, we
obtain a more robust model.

If we go back to (2.158) and substitute the alternative parameters ν = 2a, λ =

a/b, and η = τ b/a, we see that the t-distribution can be written in the form

St(x|µ, λ, ν) =

x|µ, (ηλ)−1

Gam(η|ν/2, ν/2) dη.

(2.160)

∞
0 N

We can then generalize this to a multivariate Gaussian N (x|µ, Λ) to obtain the cor-
responding multivariate Student’s t-distribution in the form

St(x|µ, Λ, ν) =

∞
0 N (x|µ, (ηΛ)−1)Gam(η|ν/2, ν/2) dη.

(2.161)

Exercise 2.48

Using the same technique as for the univariate case, we can evaluate this integral to
give





2.3. The Gaussian Distribution

St(x|µ, Λ, ν) =

Γ(D/2 + ν/2)

Γ(ν/2)

|Λ|1/2
(πν)D/2

1 +

∆2
ν

−D/2−ν/2

105

(2.162)

(2.163)

where D is the dimensionality of x, and ∆2 is the squared Mahalanobis distance
deﬁned by

∆2 = (x − µ)TΛ(x − µ).

Exercise 2.49

This is the multivariate form of Student’s t-distribution and satisﬁes the following
properties

E[x] = µ,

cov[x] =

mode[x] = µ

ν

(ν − 2)

if
if

ν > 1
ν > 2

Λ−1,

(2.164)

(2.165)

(2.166)

with corresponding results for the univariate case.

2.3.8 Periodic variables
Although Gaussian distributions are of great practical signiﬁcance, both in their
own right and as building blocks for more complex probabilistic models, there are
situations in which they are inappropriate as density models for continuous vari-
ables. One important case, which arises in practical applications, is that of periodic
variables.

An example of a periodic variable would be the wind direction at a particular
geographical location. We might, for instance, measure values of wind direction on a
number of days and wish to summarize this using a parametric distribution. Another
example is calendar time, where we may be interested in modelling quantities that
are believed to be periodic over 24 hours or over an annual cycle. Such quantities
can conveniently be represented using an angular (polar) coordinate 0 � θ < 2π.

We might be tempted to treat periodic variables by choosing some direction
as the origin and then applying a conventional distribution such as the Gaussian.
Such an approach, however, would give results that were strongly dependent on the
arbitrary choice of origin. Suppose, for instance, that we have two observations at
θ1 = 1◦ and θ2 = 359◦, and we model them using a standard univariate Gaussian
distribution. If we choose the origin at 0◦, then the sample mean of this data set
will be 180◦ with standard deviation 179◦, whereas if we choose the origin at 180◦,
then the mean will be 0◦ and the standard deviation will be 1◦. We clearly need to
develop a special approach for the treatment of periodic variables.

Let us consider the problem of evaluating the mean of a set of observations
D = {θ1, . . . , θN} of a periodic variable. From now on, we shall assume that θ is
measured in radians. We have already seen that the simple average (θ1+···+θN )/N
will be strongly coordinate dependent. To ﬁnd an invariant measure of the mean, we
note that the observations can be viewed as points on the unit circle and can therefore
be described instead by two-dimensional unit vectors x1, . . . , xN where xn = 1
for n = 1, . . . , N, as illustrated in Figure 2.17. We can average the vectors {xn}





1
N

n=1

N



N

x2





N

106

2. PROBABILITY DISTRIBUTIONS

Figure 2.17 Illustration of the representation of val-
ues θn of a periodic variable as two-
dimensional vectors xn living on the unit
circle. Also shown is the average x of
those vectors.

x4

xn

x3

¯x

¯r

¯θ

x2

x1

x1

(2.167)

instead to give

x =

and then ﬁnd the corresponding angle θ of this average. Clearly, this deﬁnition will
ensure that the location of the mean is independent of the origin of the angular coor-
dinate. Note that x will typically lie inside the unit circle. The Cartesian coordinates
of the observations are given by xn = (cos θn, sin θn), and we can write the Carte-
sian coordinates of the sample mean in the form x = (r cos θ, r sin θ). Substituting
into (2.167) and equating the x1 and x2 components then gives

r cos θ =

1
N

cos θn,

n=1

r sin θ =

1
N

sin θn.

(2.168)

n=1

Taking the ratio, and using the identity tan θ = sin θ/ cos θ, we can solve for θ to
give

θ = tan−1

.

(2.169)

n sin θn
n cos θn

Shortly, we shall see how this result arises naturally as the maximum likelihood
estimator for an appropriately deﬁned distribution over a periodic variable.

We now consider a periodic generalization of the Gaussian called the von Mises
distribution. Here we shall limit our attention to univariate distributions, although
periodic distributions can also be found over hyperspheres of arbitrary dimension.
For an extensive discussion of periodic distributions, see Mardia and Jupp (2000).

By convention, we will consider distributions p(θ) that have period 2π. Any
probability density p(θ) deﬁned over θ must not only be nonnegative and integrate











x2



.



107

p(x)

x1

(2.170)

(2.171)

(2.172)

(2.174)

(2.175)

(2.176)

2.3. The Gaussian Distribution

Figure 2.18 The von Mises distribution can be derived by considering
a two-dimensional Gaussian of the form (2.173), whose
density contours are shown in blue and conditioning on
the unit circle shown in red.

to one, but it must also be periodic. Thus p(θ) must satisfy the three conditions

r = 1

p(θ) � 0

p(θ) dθ = 1

2π

0

p(θ + 2π) = p(θ).

From (2.172), it follows that p(θ + M2π) = p(θ) for any integer M.

We can easily obtain a Gaussian-like distribution that satisﬁes these three prop-
erties as follows. Consider a Gaussian distribution over two variables x = (x1, x2)
having mean µ = (µ1, µ2) and a covariance matrix Σ = σ2I where I is the 2 × 2
identity matrix, so that

p(x1, x2) =

1
2πσ2 exp

(x1 − µ1)2 + (x2 − µ2)2

2σ2

−

(2.173)

The contours of constant p(x) are circles, as illustrated in Figure 2.18. Now suppose
we consider the value of this distribution along a circle of ﬁxed radius. Then by con-
struction this distribution will be periodic, although it will not be normalized. We can
determine the form of this distribution by transforming from Cartesian coordinates
(x1, x2) to polar coordinates (r, θ) so that

x2 = r sin θ.
We also map the mean µ into polar coordinates by writing

x1 = r cos θ,

µ1 = r0 cos θ0,

µ2 = r0 sin θ0.

Next we substitute these transformations into the two-dimensional Gaussian distribu-
tion (2.173), and then condition on the unit circle r = 1, noting that we are interested
only in the dependence on θ. Focussing on the exponent in the Gaussian distribution
we have

−

1
2σ2
= −
= r0

(r cos θ − r0 cos θ0)2 + (r sin θ − r0 sin θ0)2
1
2σ2
σ2 cos(θ − θ0) + const

0 − 2r0 cos θ cos θ0 − 2r0 sin θ sin θ0

1 + r2





N

108

2. PROBABILITY DISTRIBUTIONS

m = 5, θ0 = π/4
m = 1, θ0 = 3π/4

3π/4

π/4

0

2π

m = 5, θ0 = π/4
m = 1, θ0 = 3π/4

Figure 2.19 The von Mises distribution plotted for two different parameter values, shown as a Cartesian plot
on the left and as the corresponding polar plot on the right.

Exercise 2.51

Exercise 2.52

where ‘const’ denotes terms independent of θ, and we have made use of the following
trigonometrical identities

cos2 A + sin2 A = 1

(2.177)
(2.178)
If we now deﬁne m = r0/σ2, we obtain our ﬁnal expression for the distribution of
p(θ) along the unit circle r = 1 in the form

cos A cos B + sin A sin B = cos(A − B).

p(θ|θ0, m) =

2πI0(m)

exp{m cos(θ − θ0)}

1

(2.179)

which is called the von Mises distribution, or the circular normal. Here the param-
eter θ0 corresponds to the mean of the distribution, while m, which is known as
the concentration parameter, is analogous to the inverse variance (precision) for the
Gaussian. The normalization coefﬁcient in (2.179) is expressed in terms of I0(m),
which is the zeroth-order Bessel function of the ﬁrst kind (Abramowitz and Stegun,
1965) and is deﬁned by

I0(m) =

1
2π

2π

0

exp{m cos θ} dθ.

(2.180)

For large m, the distribution becomes approximately Gaussian. The von Mises dis-
tribution is plotted in Figure 2.19, and the function I0(m) is plotted in Figure 2.20.

Now consider the maximum likelihood estimators for the parameters θ0 and m

for the von Mises distribution. The log likelihood function is given by

ln p(D|θ0, m) = −N ln(2π) − N ln I0(m) + m

cos(θn − θ0).

n=1

(2.181)

2.3. The Gaussian Distribution




n=1

N

1






0.5

n=1

N

0

0




0

N

5
m







N

I0(m)

3000

2000

1000

0

0

5
m

A(m)

10

Figure 2.20 Plot of the Bessel function I0(m) deﬁned by (2.180), together with the function A(m) deﬁned by
(2.186).

Setting the derivative with respect to θ0 equal to zero gives

sin(θn − θ0) = 0.

To solve for θ0, we make use of the trigonometric identity

Exercise 2.53

from which we obtain

sin(A − B) = cos B sin A − cos A sin B

0 = tan−1
θML

n sin θn
n cos θn

which we recognize as the result (2.169) obtained earlier for the mean of the obser-
vations viewed in a two-dimensional Cartesian space.

Similarly, maximizing (2.181) with respect to m, and making use of I0(m) =

I1(m) (Abramowitz and Stegun, 1965), we have

where we have substituted for the maximum likelihood solution for θML
that we are performing a joint optimization over θ and m), and we have deﬁned

0

(recalling

A(m) =

1
N

cos(θn − θML

)

A(m) = I1(m)
I0(m) .

The function A(m) is plotted in Figure 2.20. Making use of the trigonometric iden-
tity (2.178), we can write (2.185) in the form

A(mML) =

1
N

cos θn

cos θML

0 −

n=1

sin θn

sin θML

0

.

(2.187)

n=1

1
N

109

10

(2.182)

(2.183)

(2.184)

(2.185)

(2.186)

110

2. PROBABILITY DISTRIBUTIONS

100

80

60

40

1

On the left

Figure 2.21 Plots of the ‘old faith-
ful’ data in which the blue curves
show contours of constant proba-
bility density.
is a
single Gaussian distribution which
has been ﬁtted to the data us-
ing maximum likelihood. Note that
this distribution fails to capture the
two clumps in the data and indeed
places much of its probability mass
in the central region between the
clumps where the data are relatively
sparse. On the right the distribution
is given by a linear combination of
two Gaussians which has been ﬁtted
to the data by maximum likelihood
using techniques discussed Chap-
ter 9, and which gives a better rep-
resentation of the data.

100

80

60

40

1

2

3

4

5

6

2

3

4

5

6

The right-hand side of (2.187) is easily evaluated, and the function A(m) can be
inverted numerically.

For completeness, we mention brieﬂy some alternative techniques for the con-
struction of periodic distributions. The simplest approach is to use a histogram of
observations in which the angular coordinate is divided into ﬁxed bins. This has the
virtue of simplicity and ﬂexibility but also suffers from signiﬁcant limitations, as we
shall see when we discuss histogram methods in more detail in Section 2.5. Another
approach starts, like the von Mises distribution, from a Gaussian distribution over a
Euclidean space but now marginalizes onto the unit circle rather than conditioning
(Mardia and Jupp, 2000). However, this leads to more complex forms of distribution
and will not be discussed further. Finally, any valid distribution over the real axis
(such as a Gaussian) can be turned into a periodic distribution by mapping succes-
sive intervals of width 2π onto the periodic variable (0, 2π), which corresponds to
‘wrapping’ the real axis around unit circle. Again, the resulting distribution is more
complex to handle than the von Mises distribution.

One limitation of the von Mises distribution is that it is unimodal. By forming
mixtures of von Mises distributions, we obtain a ﬂexible framework for modelling
periodic variables that can handle multimodality. For an example of a machine learn-
ing application that makes use of von Mises distributions, see Lawrence et al. (2002),
and for extensions to modelling conditional densities for regression problems, see
Bishop and Nabney (1996).

2.3.9 Mixtures of Gaussians
While the Gaussian distribution has some important analytical properties, it suf-
fers from signiﬁcant limitations when it comes to modelling real data sets. Consider
the example shown in Figure 2.21. This is known as the ‘Old Faithful’ data set,
and comprises 272 measurements of the eruption of the Old Faithful geyser at Yel-
lowstone National Park in the USA. Each measurement comprises the duration of

Appendix A




k=1

K

K

the eruption in minutes (horizontal axis) and the time in minutes to the next erup-
tion (vertical axis). We see that the data set forms two dominant clumps, and that
a simple Gaussian distribution is unable to capture this structure, whereas a linear
superposition of two Gaussians gives a better characterization of the data set.

Such superpositions, formed by taking linear combinations of more basic dis-
tributions such as Gaussians, can be formulated as probabilistic models known as
mixture distributions (McLachlan and Basford, 1988; McLachlan and Peel, 2000).
In Figure 2.22 we see that a linear combination of Gaussians can give rise to very
complex densities. By using a sufﬁcient number of Gaussians, and by adjusting their
means and covariances as well as the coefﬁcients in the linear combination, almost
any continuous density can be approximated to arbitrary accuracy.

We therefore consider a superposition of K Gaussian densities of the form

p(x) =

πkN (x|µk, Σk)

(2.188)

which is called a mixture of Gaussians. Each Gaussian density N (x|µk, Σk) is
called a component of the mixture and has its own mean µk and covariance Σk.
Contour and surface plots for a Gaussian mixture having 3 components are shown in
Figure 2.23.

In this section we shall consider Gaussian components to illustrate the frame-
work of mixture models. More generally, mixture models can comprise linear com-
binations of other distributions. For instance, in Section 9.3.3 we shall consider
mixtures of Bernoulli distributions as an example of a mixture model for discrete
variables.

The parameters πk in (2.188) are called mixing coefﬁcients. If we integrate both
sides of (2.188) with respect to x, and note that both p(x) and the individual Gaussian
components are normalized, we obtain

πk = 1.

k=1

(2.189)

Also, the requirement that p(x) � 0, together with N (x|µk, Σk) � 0, implies
πk � 0 for all k. Combining this with the condition (2.189) we obtain

0 � πk � 1.

(2.190)

2.3. The Gaussian Distribution

Figure 2.22 Example of a Gaussian mixture distribution
in one dimension showing three Gaussians
(each scaled by a coefﬁcient) in blue and
their sum in red.

p(x)

111

x

Section 9.3.3






k=1

K

N

1





K



112

2. PROBABILITY DISTRIBUTIONS

1

(a)

1

(b)

0.5

0

0.5

0.3

0.2

0.5

0

0

0.5

1

0

0.5

Figure 2.23 Illustration of a mixture of 3 Gaussians in a two-dimensional space.
(a) Contours of constant
density for each of the mixture components, in which the 3 components are denoted red, blue and green, and
the values of the mixing coefﬁcients are shown below each component. (b) Contours of the marginal probability
density p(x) of the mixture distribution. (c) A surface plot of the distribution p(x).

We therefore see that the mixing coefﬁcients satisfy the requirements to be probabil-
ities.

From the sum and product rules, the marginal density is given by

p(x) =

p(k)p(x|k)

(2.191)

which is equivalent to (2.188) in which we can view πk = p(k) as the prior prob-
ability of picking the kth component, and the density N (x|µk, Σk) = p(x|k) as
the probability of x conditioned on k. As we shall see in later chapters, an impor-
tant role is played by the posterior probabilities p(k|x), which are also known as
responsibilities. From Bayes’ theorem these are given by

γk(x) ≡ p(k|x)

=

=

p(k)p(x|k)
l p(l)p(x|l)
πkN (x|µk, Σk)
l πlN (x|µl, Σl) .

(2.192)

We shall discuss the probabilistic interpretation of the mixture distribution in greater
detail in Chapter 9.

The form of the Gaussian mixture distribution is governed by the parameters π,
µ and Σ, where we have used the notation π ≡ {π1, . . . , πK}, µ ≡ {µ1, . . . , µK}
and Σ ≡ {Σ1, . . . ΣK}. One way to set the values of these parameters is to use
maximum likelihood. From (2.188) the log of the likelihood function is given by

ln p(X|π, µ, Σ) =

ln

n=1

k=1

πkN (xn|µk, Σk)

(2.193)



















2.4. The Exponential Family

113

where X = {x1, . . . , xN}. We immediately see that the situation is now much
more complex than with a single Gaussian, due to the presence of the summation
over k inside the logarithm. As a result, the maximum likelihood solution for the
parameters no longer has a closed-form analytical solution. One approach to maxi-
mizing the likelihood function is to use iterative numerical optimization techniques
(Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008). Alterna-
tively we can employ a powerful framework called expectation maximization, which
will be discussed at length in Chapter 9.

2.4. The Exponential Family

The probability distributions that we have studied so far in this chapter (with the
exception of the Gaussian mixture) are speciﬁc examples of a broad class of distri-
butions called the exponential family (Duda and Hart, 1973; Bernardo and Smith,
1994). Members of the exponential family have many important properties in com-
mon, and it is illuminating to discuss these properties in some generality.

The exponential family of distributions over x, given parameters η, is deﬁned to

be the set of distributions of the form

p(x|η) = h(x)g(η) exp

ηTu(x)

(2.194)

where x may be scalar or vector, and may be discrete or continuous. Here η are
called the natural parameters of the distribution, and u(x) is some function of x.
The function g(η) can be interpreted as the coefﬁcient that ensures that the distribu-
tion is normalized and therefore satisﬁes

g(η)

h(x) exp

ηTu(x)

dx = 1

(2.195)

where the integration is replaced by summation if x is a discrete variable.

We begin by taking some examples of the distributions introduced earlier in
the chapter and showing that they are indeed members of the exponential family.
Consider ﬁrst the Bernoulli distribution

p(x|µ) = Bern(x|µ) = µx(1 − µ)1−x.

Expressing the right-hand side as the exponential of the logarithm, we have

p(x|µ) = exp{x ln µ + (1 − x) ln(1 − µ)}
.

ln

x

= (1 − µ) exp

µ
1 − µ

Comparison with (2.194) allows us to identify

(2.196)

(2.197)

(2.198)

η = ln

µ
1 − µ

114

2. PROBABILITY DISTRIBUTIONS





M



M

k=1

u(x) = x
h(x) = 1
g(η) = σ(−η).

u(x) = x
h(x) = 1
g(η) = 1.

µk = 1





k=1

M

M−1

k=1

which we can solve for µ to give µ = σ(η), where

σ(η) =

1

1 + exp(−η)

is called the logistic sigmoid function. Thus we can write the Bernoulli distribution
using the standard representation (2.194) in the form
p(x|η) = σ(−η) exp(ηx)

(2.200)
where we have used 1 − σ(η) = σ(−η), which is easily proved from (2.199). Com-
parison with (2.194) shows that

Next consider the multinomial distribution that, for a single observation x, takes

the form

p(x|µ) =

k=1

µxk
k = exp

xk ln µk

(2.204)

where x = (x1, . . . , xN )T. Again, we can write this in the standard representation
(2.194) so that

(2.205)
where ηk = ln µk, and we have deﬁned η = (η1, . . . , ηM )T. Again, comparing with
(2.194) we have

p(x|η) = exp(ηTx)

Note that the parameters ηk are not independent because the parameters µk are sub-
ject to the constraint

so that, given any M − 1 of the parameters µk, the value of the remaining parameter
is ﬁxed. In some circumstances, it will be convenient to remove this constraint by
expressing the distribution in terms of only M − 1 parameters. This can be achieved
by using the relationship (2.209) to eliminate µM by expressing it in terms of the
remaining {µk} where k = 1, . . . , M − 1, thereby leaving M − 1 parameters. Note
that these remaining parameters are still subject to the constraints

0 � µk � 1,

µk � 1.

(2.210)

(2.199)

(2.201)
(2.202)
(2.203)

(2.206)
(2.207)
(2.208)

(2.209)







M
















ln
















µk
M−1
j=1 µj

exp(ηk)

exp(ηk)

1 −

M−1

M−1

M−1

j µj

exp

1 +

µk

k=1

k=1

k=1

1 −





k=1





2.4. The Exponential Family

115

Making use of the constraint (2.209), the multinomial distribution in this representa-
tion then becomes

exp

xk ln µk

k=1

= exp

= exp

M−1

k=1
M−1

k=1

We now identify

xk ln

1 −

xk ln µk +

xk

ln

M−1

k=1
M−1

µk

µk

1 −

1 −

+ ln

= ηk

.

(2.211)

(2.212)

which we can solve for µk by ﬁrst summing both sides over k and then rearranging
and back-substituting to give

µk =

j exp(ηj) .

(2.213)

This is called the softmax function, or the normalized exponential. In this represen-
tation, the multinomial distribution therefore takes the form

p(x|η) =

1 +

−1

exp(ηTx).

(2.214)

This is the standard form of the exponential family, with parameter vector η =
(η1, . . . , ηM−1)T in which

u(x) = x
h(x) = 1

g(η) =

1 +

exp(ηk)

−1

.

(2.215)
(2.216)

(2.217)

Finally, let us consider the Gaussian distribution. For the univariate Gaussian,

we have

p(x|µ, σ2) =
=

1

(2πσ2)1/2

1

(2πσ2)1/2

exp

−

−

1
2σ2 (x − µ)2
1
2σ2 x2 + µ

σ2 x −

1
2σ2 µ2

(2.218)

(2.219)





















dx

ηT

N



η2
1
4η2

N







N





116

2. PROBABILITY DISTRIBUTIONS

which, after some simple rearrangement, can be cast in the standard exponential
family form (2.194) with

Exercise 2.57

Exercise 2.58

(2.220)

(2.221)

(2.222)

(2.223)

(2.224)

(2.225)

(2.226)

η =

u(x) =

µ/σ2
−1/2σ2
x
x2

h(x) = (2π)−1/2
g(η) = (−2η2)1/2 exp

.

2.4.1 Maximum likelihood and sufﬁcient statistics
Let us now consider the problem of estimating the parameter vector η in the gen-
eral exponential family distribution (2.194) using the technique of maximum likeli-
hood. Taking the gradient of both sides of (2.195) with respect to η, we have

h(x) exp

ηTu(x)

∇g(η)
+ g(η)

h(x) exp

ηTu(x)

u(x) dx = 0.

Rearranging, and making use again of (2.195) then gives

1
g(η)∇g(η) = g(η)

−

h(x) exp

ηTu(x)

u(x) dx = E[u(x)]

where we have used (2.194). We therefore obtain the result

−∇ ln g(η) = E[u(x)].

Note that the covariance of u(x) can be expressed in terms of the second derivatives
of g(η), and similarly for higher order moments. Thus, provided we can normalize a
distribution from the exponential family, we can always ﬁnd its moments by simple
differentiation.

Now consider a set of independent identically distributed data denoted by X =

{x1, . . . , xn}, for which the likelihood function is given by

p(X|η) =

h(xn)

n=1

g(η)N exp

u(xn)

.

(2.227)

n=1

Setting the gradient of ln p(X|η) with respect to η to zero, we get the following
condition to be satisﬁed by the maximum likelihood estimator ηML

−∇ ln g(ηML) =

1
N

u(xn)

n=1

(2.228)














N

2.4. The Exponential Family

117

which can in principle be solved to obtain ηML. We see that the solution for the
n u(xn), which
maximum likelihood estimator depends on the data only through
is therefore called the sufﬁcient statistic of the distribution (2.194). We do not need
to store the entire data set itself but only the value of the sufﬁcient statistic. For
the Bernoulli distribution, for example, the function u(x) is given just by x and
so we need only keep the sum of the data points {xn}, whereas for the Gaussian
u(x) = (x, x2)T, and so we should keep both the sum of {xn} and the sum of {x2
n}.
If we consider the limit N → ∞, then the right-hand side of (2.228) becomes
E[u(x)], and so by comparing with (2.226) we see that in this limit ηML will equal
the true value η.

In fact, this sufﬁciency property holds also for Bayesian inference, although
we shall defer discussion of this until Chapter 8 when we have equipped ourselves
with the tools of graphical models and can thereby gain a deeper insight into these
important concepts.

2.4.2 Conjugate priors
We have already encountered the concept of a conjugate prior several times, for
example in the context of the Bernoulli distribution (for which the conjugate prior
is the beta distribution) or the Gaussian (where the conjugate prior for the mean is
a Gaussian, and the conjugate prior for the precision is the Wishart distribution). In
general, for a given probability distribution p(x|η), we can seek a prior p(η) that is
conjugate to the likelihood function, so that the posterior distribution has the same
functional form as the prior. For any member of the exponential family (2.194), there
exists a conjugate prior that can be written in the form
p(η|χ, ν) = f(χ, ν)g(η)ν exp

(2.229)

νηTχ

where f(χ, ν) is a normalization coefﬁcient, and g(η) is the same function as ap-
pears in (2.194). To see that this is indeed conjugate, let us multiply the prior (2.229)
by the likelihood function (2.227) to obtain the posterior distribution, up to a nor-
malization coefﬁcient, in the form

p(η|X, χ, ν) ∝ g(η)ν+N exp

ηT

n=1

u(xn) + νχ

.

(2.230)

This again takes the same functional form as the prior (2.229), conﬁrming conjugacy.
Furthermore, we see that the parameter ν can be interpreted as a effective number of
pseudo-observations in the prior, each of which has a value for the sufﬁcient statistic
u(x) given by χ.

2.4.3 Noninformative priors
In some applications of probabilistic inference, we may have prior knowledge
that can be conveniently expressed through the prior distribution. For example, if
the prior assigns zero probability to some value of variable, then the posterior dis-
tribution will necessarily also assign zero probability to that value, irrespective of





 dλ

dη













118

2. PROBABILITY DISTRIBUTIONS

any subsequent observations of data. In many cases, however, we may have little
idea of what form the distribution should take. We may then seek a form of prior
distribution, called a noninformative prior, which is intended to have as little inﬂu-
ence on the posterior distribution as possible (Jeffries, 1946; Box and Tao, 1973;
Bernardo and Smith, 1994). This is sometimes referred to as ‘letting the data speak
for themselves’.

If we have a distribution p(x|λ) governed by a parameter λ, we might be tempted
to propose a prior distribution p(λ) = const as a suitable prior. If λ is a discrete
variable with K states, this simply amounts to setting the prior probability of each
state to 1/K. In the case of continuous parameters, however, there are two potential
difﬁculties with this approach. The ﬁrst is that, if the domain of λ is unbounded,
this prior distribution cannot be correctly normalized because the integral over λ
diverges. Such priors are called improper. In practice, improper priors can often
be used provided the corresponding posterior distribution is proper, i.e., that it can
be correctly normalized. For instance, if we put a uniform prior distribution over
the mean of a Gaussian, then the posterior distribution for the mean, once we have
observed at least one data point, will be proper.

A second difﬁculty arises from the transformation behaviour of a probability
density under a nonlinear change of variables, given by (1.27). If a function h(λ)
is constant, and we change variables to λ = η2, then
h(η) = h(η2) will also be
constant. However, if we choose the density pλ(λ) to be constant, then the density
of η will be given, from (1.27), by

pη(η) = pλ(λ)

= pλ(η2)2η ∝ η

(2.231)

and so the density over η will not be constant. This issue does not arise when we use
maximum likelihood, because the likelihood function p(x|λ) is a simple function of
λ and so we are free to use any convenient parameterization. If, however, we are to
choose a prior distribution that is constant, we must take care to use an appropriate
representation for the parameters.

Here we consider two simple examples of noninformative priors (Berger, 1985).

First of all, if a density takes the form

p(x|µ) = f(x − µ)

(2.232)

then the parameter µ is known as a location parameter. This family of densities
x = x + c,
exhibits translation invariance because if we shift x by a constant to give
then

µ) = f(

(2.233)
µ = µ + c. Thus the density takes the same form in the
where we have deﬁned
new variable as in the original one, and so the density is independent of the choice
of origin. We would like to choose a prior distribution that reﬂects this translation
invariance property, and so we choose a prior that assigns equal probability mass to

x −

p(

x|

µ)
















x



x
σ

σ

1


σ








2.4. The Exponential Family

119

an interval A � µ � B as to the shifted interval A − c � µ � B − c. This implies

B

A

p(µ) dµ =

p(µ) dµ =

B−c

A−c

B

A

p(µ − c) dµ

and because this must hold for all choices of A and B, we have

p(µ − c) = p(µ)

which implies that p(µ) is constant. An example of a location parameter would be
the mean µ of a Gaussian distribution. As we have seen, the conjugate prior distri-
bution for µ in this case is a Gaussian p(µ|µ0, σ2
0), and we obtain a
noninformative prior by taking the limit σ2
0 → ∞. Indeed, from (2.141) and (2.142)
we see that this gives a posterior distribution over µ in which the contributions from
the prior vanish.

0) = N (µ|µ0, σ2

As a second example, consider a density of the form

Exercise 2.59

where σ > 0. Note that this will be a normalized density provided f(x) is correctly
normalized. The parameter σ is known as a scale parameter, and the density exhibits
scale invariance because if we scale x by a constant to give

x = cx, then

p(x|σ) =

1
σ

f

σ) =

p(

x|

f

(2.234)

(2.235)

(2.236)

(2.237)

σ = cσ. This transformation corresponds to a change of
where we have deﬁned
scale, for example from meters to kilometers if x is a length, and we would like
to choose a prior distribution that reﬂects this scale invariance. If we consider an
interval A � σ � B, and a scaled interval A/c � σ � B/c, then the prior should
assign equal probability mass to these two intervals. Thus we have

B

A

p(σ) dσ =

B/c

A/c

p(σ) dσ =

B

A

p

1
c

σ

1
c

dσ

(2.238)

and because this must hold for choices of A and B, we have

p(σ) = p

1
c

σ

1
c

(2.239)

and hence p(σ) ∝ 1/σ. Note that again this is an improper prior because the integral
of the distribution over 0 � σ � ∞ is divergent. It is sometimes also convenient
to think of the prior distribution for a scale parameter in terms of the density of the
log of the parameter. Using the transformation rule (1.27) for densities we see that
p(ln σ) = const. Thus, for this prior there is the same probability mass in the range
1 � σ � 10 as in the range 10 � σ � 100 and in 100 � σ � 1000.











120

Section 2.3

2. PROBABILITY DISTRIBUTIONS

An example of a scale parameter would be the standard deviation σ of a Gaussian

distribution, after we have taken account of the location parameter µ, because

−(

x/σ)2

N (x|µ, σ2) ∝ σ−1 exp

(2.240)
x = x − µ. As discussed earlier, it is often more convenient to work in terms
where
of the precision λ = 1/σ2 rather than σ itself. Using the transformation rule for
densities, we see that a distribution p(σ) ∝ 1/σ corresponds to a distribution over λ
of the form p(λ) ∝ 1/λ. We have seen that the conjugate prior for λ was the gamma
distribution Gam(λ|a0, b0) given by (2.146). The noninformative prior is obtained
as the special case a0 = b0 = 0. Again, if we examine the results (2.150) and (2.151)
for the posterior distribution of λ, we see that for a0 = b0 = 0, the posterior depends
only on terms arising from the data and not from the prior.

2.5. Nonparametric Methods

Throughout this chapter, we have focussed on the use of probability distributions
having speciﬁc functional forms governed by a small number of parameters whose
values are to be determined from a data set. This is called the parametric approach
to density modelling. An important limitation of this approach is that the chosen
density might be a poor model of the distribution that generates the data, which can
result in poor predictive performance. For instance, if the process that generates the
data is multimodal, then this aspect of the distribution can never be captured by a
Gaussian, which is necessarily unimodal.

In this ﬁnal section, we consider some nonparametric approaches to density es-
timation that make few assumptions about the form of the distribution. Here we shall
focus mainly on simple frequentist methods. The reader should be aware, however,
that nonparametric Bayesian methods are attracting increasing interest (Walker et al.,
1999; Neal, 2000; M¨uller and Quintana, 2004; Teh et al., 2006).

Let us start with a discussion of histogram methods for density estimation, which
we have already encountered in the context of marginal and conditional distributions
in Figure 1.11 and in the context of the central limit theorem in Figure 2.6. Here we
explore the properties of histogram density models in more detail, focussing on the
case of a single continuous variable x. Standard histograms simply partition x into
distinct bins of width ∆i and then count the number ni of observations of x falling
in bin i. In order to turn this count into a normalized probability density, we simply
divide by the total number N of observations and by the width ∆i of the bins to
obtain probability values for each bin given by
pi = ni
N∆i

(2.241)

p(x) dx = 1. This gives a model for the density
for which it is easily seen that
p(x) that is constant over the width of each bin, and often the bins are chosen to have
the same width ∆i = ∆.

Figure 2.24 An illustration of the histogram approach
to density estimation, in which a data set
of 50 data points is generated from the
distribution shown by the green curve.
Histogram density estimates, based on
(2.241), with a common bin width ∆ are
shown for various values of ∆.

2.5. Nonparametric Methods

121

∆ = 0.04

∆ = 0.08

∆ = 0.25

5

0

5

0

0

5

0

0

0

0.5

0.5

0.5

1

1

1

In Figure 2.24, we show an example of histogram density estimation. Here
the data is drawn from the distribution, corresponding to the green curve, which is
formed from a mixture of two Gaussians. Also shown are three examples of his-
togram density estimates corresponding to three different choices for the bin width
∆. We see that when ∆ is very small (top ﬁgure), the resulting density model is very
spiky, with a lot of structure that is not present in the underlying distribution that
generated the data set. Conversely, if ∆ is too large (bottom ﬁgure) then the result is
a model that is too smooth and that consequently fails to capture the bimodal prop-
erty of the green curve. The best results are obtained for some intermediate value
of ∆ (middle ﬁgure). In principle, a histogram density model is also dependent on
the choice of edge location for the bins, though this is typically much less signiﬁcant
than the value of ∆.

Note that the histogram method has the property (unlike the methods to be dis-
cussed shortly) that, once the histogram has been computed, the data set itself can
be discarded, which can be advantageous if the data set is large. Also, the histogram
approach is easily applied if the data points are arriving sequentially.

In practice, the histogram technique can be useful for obtaining a quick visual-
ization of data in one or two dimensions but is unsuited to most density estimation
applications. One obvious problem is that the estimated density has discontinuities
that are due to the bin edges rather than any property of the underlying distribution
that generated the data. Another major limitation of the histogram approach is its
scaling with dimensionality. If we divide each variable in a D-dimensional space
into M bins, then the total number of bins will be M D. This exponential scaling
with D is an example of the curse of dimensionality. In a space of high dimensional-
ity, the quantity of data needed to provide meaningful estimates of local probability
density would be prohibitive.

The histogram approach to density estimation does, however, teach us two im-
portant lessons. First, to estimate the probability density at a particular location,
we should consider the data points that lie within some local neighbourhood of that
point. Note that the concept of locality requires that we assume some form of dis-
tance measure, and here we have been assuming Euclidean distance. For histograms,

Section 1.4



122

2. PROBABILITY DISTRIBUTIONS

this neighbourhood property was deﬁned by the bins, and there is a natural ‘smooth-
ing’ parameter describing the spatial extent of the local region, in this case the bin
width. Second, the value of the smoothing parameter should be neither too large nor
too small in order to obtain good results. This is reminiscent of the choice of model
complexity in polynomial curve ﬁtting discussed in Chapter 1 where the degree M
of the polynomial, or alternatively the value α of the regularization parameter, was
optimal for some intermediate value, neither too large nor too small. Armed with
these insights, we turn now to a discussion of two widely used nonparametric tech-
niques for density estimation, kernel estimators and nearest neighbours, which have
better scaling with dimensionality than the simple histogram model.

2.5.1 Kernel density estimators
Let us suppose that observations are being drawn from some unknown probabil-
ity density p(x) in some D-dimensional space, which we shall take to be Euclidean,
and we wish to estimate the value of p(x). From our earlier discussion of locality,
let us consider some small region R containing x. The probability mass associated
with this region is given by

Section 2.1

P =

p(x) dx.

(2.242)

R

Now suppose that we have collected a data set comprising N observations drawn
from p(x). Because each data point has a probability P of falling within R, the total
number K of points that lie inside R will be distributed according to the binomial
distribution
(2.243)

N!

Bin(K|N, P ) =

K!(N − K)! P K(1 − P )1−K.

Using (2.11), we see that the mean fraction of points falling inside the region is
E[K/N] = P , and similarly using (2.12) we see that the variance around this mean
is var[K/N] = P (1 − P )/N. For large N, this distribution will be sharply peaked
around the mean and so
(2.244)
If, however, we also assume that the region R is sufﬁciently small that the probability
density p(x) is roughly constant over the region, then we have

K  N P.

P  p(x)V

(2.245)

where V is the volume of R. Combining (2.244) and (2.245), we obtain our density
estimate in the form
(2.246)

.

p(x) = K
N V

Note that the validity of (2.246) depends on two contradictory assumptions, namely
that the region R be sufﬁciently small that the density is approximately constant over
the region and yet sufﬁciently large (in relation to the value of that density) that the
number K of points falling inside the region is sufﬁcient for the binomial distribution
to be sharply peaked.






n=1

N

N

n=1





N

2.5. Nonparametric Methods

123

We can exploit the result (2.246) in two different ways. Either we can ﬁx K and
determine the value of V from the data, which gives rise to the K-nearest-neighbour
technique discussed shortly, or we can ﬁx V and determine K from the data, giv-
ing rise to the kernel approach. It can be shown that both the K-nearest-neighbour
density estimator and the kernel density estimator converge to the true probability
density in the limit N → ∞ provided V shrinks suitably with N, and K grows with
N (Duda and Hart, 1973).
We begin by discussing the kernel method in detail, and to start with we take
the region R to be a small hypercube centred on the point x at which we wish to
determine the probability density. In order to count the number K of points falling
within this region, it is convenient to deﬁne the following function

k(u) =

|ui| � 1/2,
1,
0, otherwise

i = 1, . . . , D,

(2.247)

which represents a unit cube centred on the origin. The function k(u) is an example
of a kernel function, and in this context is also called a Parzen window. From (2.247),
the quantity k((x − xn)/h) will be one if the data point xn lies inside a cube of side
h centred on x, and zero otherwise. The total number of data points lying inside this
cube will therefore be

Substituting this expression into (2.246) then gives the following result for the esti-
mated density at x

K =

k

x − xn

h

.

p(x) =

1
N

1
hD k

x − xn

h

(2.248)

(2.249)

where we have used V = hD for the volume of a hypercube of side h in D di-
mensions. Using the symmetry of the function k(u), we can now re-interpret this
equation, not as a single cube centred on x but as the sum over N cubes centred on
the N data points xn.

As it stands, the kernel density estimator (2.249) will suffer from one of the same
problems that the histogram method suffered from, namely the presence of artiﬁcial
discontinuities, in this case at the boundaries of the cubes. We can obtain a smoother
density model if we choose a smoother kernel function, and a common choice is the
Gaussian, which gives rise to the following kernel density model

p(x) =

1
N

1

exp

−x − xn2

2h2

(2.250)

(2πh2)1/2

n=1

where h represents the standard deviation of the Gaussian components. Thus our
density model is obtained by placing a Gaussian over each data point and then adding
up the contributions over the whole data set, and then dividing by N so that the den-
sity is correctly normalized. In Figure 2.25, we apply the model (2.250) to the data













124

2. PROBABILITY DISTRIBUTIONS

Figure 2.25 Illustration of

the kernel density model
(2.250) applied to the same data set used
to demonstrate the histogram approach in
Figure 2.24. We see that h acts as a
smoothing parameter and that if it is set
too small (top panel), the result is a very
noisy density model, whereas if it is set
too large (bottom panel), then the bimodal
nature of the underlying distribution from
which the data is generated (shown by the
green curve) is washed out. The best den-
sity model is obtained for some intermedi-
ate value of h (middle panel).

h = 0.005

h = 0.07

h = 0.2

5

0

5

0

0

5

0

0

0

0.5

0.5

0.5

1

1

1

set used earlier to demonstrate the histogram technique. We see that, as expected,
the parameter h plays the role of a smoothing parameter, and there is a trade-off
between sensitivity to noise at small h and over-smoothing at large h. Again, the
optimization of h is a problem in model complexity, analogous to the choice of bin
width in histogram density estimation, or the degree of the polynomial used in curve
ﬁtting.

We can choose any other kernel function k(u) in (2.249) subject to the condi-

tions

k(u) � 0,
k(u) du = 1

(2.251)

(2.252)

which ensure that the resulting probability distribution is nonnegative everywhere
and integrates to one. The class of density model given by (2.249) is called a kernel
density estimator, or Parzen estimator. It has a great merit that there is no compu-
tation involved in the ‘training’ phase because this simply requires storage of the
training set. However, this is also one of its great weaknesses because the computa-
tional cost of evaluating the density grows linearly with the size of the data set.

2.5.2 Nearest-neighbour methods
One of the difﬁculties with the kernel approach to density estimation is that the
parameter h governing the kernel width is ﬁxed for all kernels. In regions of high
data density, a large value of h may lead to over-smoothing and a washing out of
structure that might otherwise be extracted from the data. However, reducing h may
lead to noisy estimates elsewhere in data space where the density is smaller. Thus
the optimal choice for h may be dependent on location within the data space. This
issue is addressed by nearest-neighbour methods for density estimation.

We therefore return to our general result (2.246) for local density estimation,
and instead of ﬁxing V and determining the value of K from the data, we consider
a ﬁxed value of K and use the data to ﬁnd an appropriate value for V . To do this,
we consider a small sphere centred on the point x at which we wish to estimate the

2.5. Nonparametric Methods

Figure 2.26 Illustration of K-nearest-neighbour den-
sity estimation using the same data set
as in Figures 2.25 and 2.24. We see
that the parameter K governs the degree
of smoothing, so that a small value of
K leads to a very noisy density model
(top panel), whereas a large value (bot-
tom panel) smoothes out the bimodal na-
ture of the true distribution (shown by the
green curve) from which the data set was
generated.

K = 1

K = 5

K = 30

5

0

5

0

0

5

0

0

0

125

1

1

1



0.5

0.5

0.5

Exercise 2.61

density p(x), and we allow the radius of the sphere to grow until it contains precisely
K data points. The estimate of the density p(x) is then given by (2.246) with V set to
the volume of the resulting sphere. This technique is known as K nearest neighbours
and is illustrated in Figure 2.26, for various choices of the parameter K, using the
same data set as used in Figure 2.24 and Figure 2.25. We see that the value of K
now governs the degree of smoothing and that again there is an optimum choice for
K that is neither too large nor too small. Note that the model produced by K nearest
neighbours is not a true density model because the integral over all space diverges.
We close this chapter by showing how the K-nearest-neighbour technique for
density estimation can be extended to the problem of classiﬁcation. To do this, we
apply the K-nearest-neighbour density estimation technique to each class separately
and then make use of Bayes’ theorem. Let us suppose that we have a data set com-
k Nk = N. If we
prising Nk points in class Ck with N points in total, so that
wish to classify a new point x, we draw a sphere centred on x containing precisely
K points irrespective of their class. Suppose this sphere has volume V and contains
Kk points from class Ck. Then (2.246) provides an estimate of the density associated
with each class
(2.253)

.

p(x|Ck) = Kk

NkV

Similarly, the unconditional density is given by
p(x) = K
N V

while the class priors are given by

p(Ck) = Nk

N

.

(2.254)

(2.255)

We can now combine (2.253), (2.254), and (2.255) using Bayes’ theorem to obtain
the posterior probability of class membership

p(Ck|x) = p(x|Ck)p(Ck)

p(x)

= Kk
K

.

(2.256)

126

2. PROBABILITY DISTRIBUTIONS

x2

x2

Figure 2.27 (a) In the K-nearest-
neighbour classiﬁer, a new point,
shown by the black diamond, is clas-
siﬁed according to the majority class
membership of the K closest train-
ing data points, in this case K =
3.
In the nearest-neighbour
(K = 1) approach to classiﬁcation,
the resulting decision boundary is
composed of hyperplanes that form
perpendicular bisectors of pairs of
points from different classes.

(b)

x1

x1

(a)

(b)

If we wish to minimize the probability of misclassiﬁcation, this is done by assigning
the test point x to the class having the largest posterior probability, corresponding to
the largest value of Kk/K. Thus to classify a new point, we identify the K nearest
points from the training data set and then assign the new point to the class having the
largest number of representatives amongst this set. Ties can be broken at random.
The particular case of K = 1 is called the nearest-neighbour rule, because a test
point is simply assigned to the same class as the nearest point from the training set.
These concepts are illustrated in Figure 2.27.

In Figure 2.28, we show the results of applying the K-nearest-neighbour algo-
rithm to the oil ﬂow data, introduced in Chapter 1, for various values of K. As
expected, we see that K controls the degree of smoothing, so that small K produces
many small regions of each class, whereas large K leads to fewer larger regions.

K = 1

x7

2

1

K = 3

x7

2

1

x7

2

1

K = 3 1

0

0

1

x6

2

0

0

1

x6

2

0

0

1

x6

2

Figure 2.28 Plot of 200 data points from the oil data set showing values of x6 plotted against x7, where the
red, green, and blue points correspond to the ‘laminar’, ‘annular’, and ‘homogeneous’ classes, respectively. Also
shown are the classiﬁcations of the input space given by the K-nearest-neighbour algorithm for various values
of K.





x=0

1















Exercises

127

An interesting property of the nearest-neighbour (K = 1) classiﬁer is that, in the
limit N → ∞, the error rate is never more than twice the minimum achievable error
rate of an optimal classiﬁer, i.e., one that uses the true class distributions (Cover and
Hart, 1967) .

As discussed so far, both the K-nearest-neighbour method, and the kernel den-
sity estimator, require the entire training data set to be stored, leading to expensive
computation if the data set is large. This effect can be offset, at the expense of some
additional one-off computation, by constructing tree-based search structures to allow
(approximate) near neighbours to be found efﬁciently without doing an exhaustive
search of the data set. Nevertheless, these nonparametric methods are still severely
limited. On the other hand, we have seen that simple parametric models are very
restricted in terms of the forms of distribution that they can represent. We therefore
need to ﬁnd density models that are very ﬂexible and yet for which the complexity
of the models can be controlled independently of the size of the training set, and we
shall see in subsequent chapters how to achieve this.

Exercises

erties

2.1 () www Verify that the Bernoulli distribution (2.2) satisﬁes the following prop-

(2.257)

(2.258)
(2.259)

(2.260)

(2.261)

p(x|µ) = 1
E[x] = µ
var[x] = µ(1 − µ).

Show that the entropy H[x] of a Bernoulli distributed random binary variable x is
given by

H[x] = −µ ln µ − (1 − µ) ln(1 − µ).

2.2 ( ) The form of the Bernoulli distribution given by (2.2) is not symmetric be-
tween the two values of x. In some situations, it will be more convenient to use an
equivalent formulation for which x ∈ {−1, 1}, in which case the distribution can be
written

p(x|µ) =

1 − µ
2

(1−x)/2

1 + µ

(1+x)/2

2

where µ ∈ [−1, 1]. Show that the distribution (2.261) is normalized, and evaluate its
mean, variance, and entropy.

2.3 ( ) www In this exercise, we prove that the binomial distribution (2.9) is nor-
malized. First use the deﬁnition (2.10) of the number of combinations of m identical
objects chosen from a total of N to show that

N
m

+

N
m − 1

=

N + 1

m

.

(2.262)







N







N




128

2. PROBABILITY DISTRIBUTIONS

Use this result to prove by induction the following result

(1 + x)N =

xm

(2.263)

N
m

m=0

which is known as the binomial theorem, and which is valid for all real values of x.
Finally, show that the binomial distribution is normalized, so that

N
m

µm(1 − µ)N−m = 1

m=0

(2.264)

which can be done by ﬁrst pulling out a factor (1 − µ)N out of the summation and
then making use of the binomial theorem.

2.4 ( ) Show that the mean of the binomial distribution is given by (2.11). To do this,
differentiate both sides of the normalization condition (2.264) with respect to µ and
then rearrange to obtain an expression for the mean of n. Similarly, by differentiating
(2.264) twice with respect to µ and making use of the result (2.11) for the mean of
the binomial distribution prove the result (2.12) for the variance of the binomial.

2.5 ( ) www In this exercise, we prove that the beta distribution, given by (2.13), is

correctly normalized, so that (2.14) holds. This is equivalent to showing that

1

0

µa−1(1 − µ)b−1 dµ =

Γ(a)Γ(b)
Γ(a + b) .

From the deﬁnition (1.141) of the gamma function, we have

Γ(a)Γ(b) =

∞

0

exp(−x)xa−1 dx

∞

0

exp(−y)yb−1 dy.

(2.265)

(2.266)

Use this expression to prove (2.265) as follows. First bring the integral over y inside
the integrand of the integral over x, next make the change of variable t = y + x
where x is ﬁxed, then interchange the order of the x and t integrations, and ﬁnally
make the change of variable x = tµ where t is ﬁxed.

2.6 () Make use of the result (2.265) to show that the mean, variance, and mode of the

beta distribution (2.13) are given respectively by

E[µ] =

var[µ] =

mode[µ] =

a

a + b

ab

(a + b)2(a + b + 1)
a − 1
a + b − 2 .

(2.267)

(2.268)

(2.269)











Exercises

129

2.7 ( ) Consider a binomial random variable x given by (2.9), with prior distribution
for µ given by the beta distribution (2.13), and suppose we have observed m occur-
rences of x = 1 and l occurrences of x = 0. Show that the posterior mean value of x
lies between the prior mean and the maximum likelihood estimate for µ. To do this,
show that the posterior mean can be written as λ times the prior mean plus (1 − λ)
times the maximum likelihood estimate, where 0 � λ � 1. This illustrates the con-
cept of the posterior distribution being a compromise between the prior distribution
and the maximum likelihood solution.

2.8 () Consider two variables x and y with joint distribution p(x, y). Prove the follow-

ing two results

E[x] = Ey [Ex[x|y]]
var[x] = Ey [varx[x|y]] + vary [Ex[x|y]] .

(2.270)
(2.271)
Here Ex[x|y] denotes the expectation of x under the conditional distribution p(x|y),
with a similar notation for the conditional variance.

2.9 (  ) www . In this exercise, we prove the normalization of the Dirichlet dis-
tribution (2.38) using induction. We have already shown in Exercise 2.5 that the
beta distribution, which is a special case of the Dirichlet for M = 2, is normalized.
We now assume that the Dirichlet distribution is normalized for M − 1 variables
and prove that it is normalized for M variables. To do this, consider the Dirichlet
k=1 µk = 1 by
distribution over M variables, and take account of the constraint
eliminating µM , so that the Dirichlet is written

M

pM (µ1, . . . , µM−1) = CM

µαk−1
k

1 −

M−1

k=1

αM−1

µj

(2.272)

M−1

j=1

and our goal is to ﬁnd an expression for CM . To do this, integrate over µM−1, taking
care over the limits of integration, and then make a change of variable so that this
integral has limits 0 and 1. By assuming the correct result for CM−1 and making use
of (2.265), derive the expression for CM .

2.10 ( ) Using the property Γ(x + 1) = xΓ(x) of the gamma function, derive the
following results for the mean, variance, and covariance of the Dirichlet distribution
given by (2.38)

E[µj] = αj
α0
var[µj] = αj(α0 − αj)
0(α0 + 1)
α2
αjαl
cov[µjµl] = −
0(α0 + 1) ,
α2

j = l

(2.273)

(2.274)

(2.275)

where α0 is deﬁned by (2.39).

130

2. PROBABILITY DISTRIBUTIONS







2.11 () www By expressing the expectation of ln µj under the Dirichlet distribution

(2.38) as a derivative with respect to αj, show that

where α0 is given by (2.39) and

E[ln µj] = ψ(αj) − ψ(α0)

ψ(a) ≡

d
da

ln Γ(a)

(2.276)

(2.277)

is the digamma function.

2.12 () The uniform distribution for a continuous variable x is deﬁned by

U(x|a, b) =

1
b − a

,

a � x � b.

(2.278)

Verify that this distribution is normalized, and ﬁnd expressions for its mean and
variance.

2.13 ( ) Evaluate the Kullback-Leibler divergence (1.113) between two Gaussians

p(x) = N (x|µ, Σ) and q(x) = N (x|m, L).

2.14 ( ) www This exercise demonstrates that the multivariate distribution with max-
imum entropy, for a given covariance, is a Gaussian. The entropy of a distribution
p(x) is given by

H[x] = −

p(x) ln p(x) dx.

(2.279)

We wish to maximize H[x] over all distributions p(x) subject to the constraints that
p(x) be normalized and that it have a speciﬁc mean and covariance, so that

p(x) dx = 1

p(x)x dx = µ

p(x)(x − µ)(x − µ)T dx = Σ.

(2.280)

(2.281)

(2.282)

By performing a variational maximization of (2.279) and using Lagrange multipliers
to enforce the constraints (2.280), (2.281), and (2.282), show that the maximum
likelihood distribution is given by the Gaussian (2.43).

2.15 ( ) Show that the entropy of the multivariate Gaussian N (x|µ, Σ) is given by

H[x] =

1
2

ln|Σ| + D
2

(1 + ln(2π))

(2.283)

where D is the dimensionality of x.



Exercises

131

2.16 (  ) www Consider two random variables x1 and x2 having Gaussian distri-
butions with means µ1, µ2 and precisions τ1, τ2 respectively. Derive an expression
for the differential entropy of the variable x = x1 + x2. To do this, ﬁrst ﬁnd the
distribution of x by using the relation

p(x) =

∞

−∞

p(x|x2)p(x2) dx2

(2.284)

and completing the square in the exponent. Then observe that this represents the
convolution of two Gaussian distributions, which itself will be Gaussian, and ﬁnally
make use of the result (1.110) for the entropy of the univariate Gaussian.

2.17 () www Consider the multivariate Gaussian distribution given by (2.43). By
writing the precision matrix (inverse covariance matrix) Σ−1 as the sum of a sym-
metric and an anti-symmetric matrix, show that the anti-symmetric term does not
appear in the exponent of the Gaussian, and hence that the precision matrix may be
taken to be symmetric without loss of generality. Because the inverse of a symmetric
matrix is also symmetric (see Exercise 2.22), it follows that the covariance matrix
may also be chosen to be symmetric without loss of generality.

2.18 (  ) Consider a real, symmetric matrix Σ whose eigenvalue equation is given
by (2.45). By taking the complex conjugate of this equation and subtracting the
original equation, and then forming the inner product with eigenvector ui, show that
the eigenvalues λi are real. Similarly, use the symmetry property of Σ to show that
two eigenvectors ui and uj will be orthogonal provided λj = λi. Finally, show that
without loss of generality, the set of eigenvectors can be chosen to be orthonormal,
so that they satisfy (2.46), even if some of the eigenvalues are zero.

2.19 ( ) Show that a real, symmetric matrix Σ having the eigenvector equation (2.45)
can be expressed as an expansion in the eigenvectors, with coefﬁcients given by the
eigenvalues, of the form (2.48). Similarly, show that the inverse matrix Σ−1 has a
representation of the form (2.49).

2.20 ( ) www A positive deﬁnite matrix Σ can be deﬁned as one for which the

quadratic form

(2.285)
is positive for any real value of the vector a. Show that a necessary and sufﬁcient
condition for Σ to be positive deﬁnite is that all of the eigenvalues λi of Σ, deﬁned
by (2.45), are positive.

aTΣa

2.21 () Show that a real, symmetric matrix of size D× D has D(D + 1)/2 independent

parameters.

2.22 () www Show that the inverse of a symmetric matrix is itself symmetric.

2.23 ( ) By diagonalizing the coordinate system using the eigenvector expansion (2.45),
show that the volume contained within the hyperellipsoid corresponding to a constant















132

2. PROBABILITY DISTRIBUTIONS

Mahalanobis distance ∆ is given by

where VD is the volume of the unit sphere in D dimensions, and the Mahalanobis
distance is deﬁned by (2.44).

2.24 ( ) www Prove the identity (2.76) by multiplying both sides by the matrix

VD|Σ|1/2∆D

A B
C D

(2.286)

(2.287)

and making use of the deﬁnition (2.77).

2.25 ( )

In Sections 2.3.1 and 2.3.2, we considered the conditional and marginal distri-
butions for a multivariate Gaussian. More generally, we can consider a partitioning
of the components of x into three groups xa, xb, and xc, with a corresponding par-
titioning of the mean vector µ and of the covariance matrix Σ in the form

µ =

µa
µb
µc

,

Σ =

Σaa Σab Σac
Σba Σbb Σbc
Σca Σcb Σcc

.

(2.288)

By making use of the results of Section 2.3, ﬁnd an expression for the conditional
distribution p(xa|xb) in which xc has been marginalized out.

2.26 ( ) A very useful result from linear algebra is the Woodbury matrix inversion

formula given by

(A + BCD)−1 = A−1 − A−1B(C−1 + DA−1B)−1DA−1.

(2.289)

By multiplying both sides by (A + BCD) prove the correctness of this result.

2.27 () Let x and z be two independent random vectors, so that p(x, z) = p(x)p(z).
Show that the mean of their sum y = x + z is given by the sum of the means of each
of the variable separately. Similarly, show that the covariance matrix of y is given by
the sum of the covariance matrices of x and z. Conﬁrm that this result agrees with
that of Exercise 1.10.

2.28 (  ) www Consider a joint distribution over the variable

z =

x
y

(2.290)

whose mean and covariance are given by (2.108) and (2.105) respectively. By mak-
ing use of the results (2.92) and (2.93) show that the marginal distribution p(x) is
given (2.99). Similarly, by making use of the results (2.81) and (2.82) show that the
conditional distribution p(y|x) is given by (2.100).

Exercises

133

2.29 ( ) Using the partitioned matrix inversion formula (2.76), show that the inverse of

the precision matrix (2.104) is given by the covariance matrix (2.105).

2.30 () By starting from (2.107) and making use of the result (2.105), verify the result

(2.108).

2.31 ( ) Consider two multidimensional random vectors x and z having Gaussian
distributions p(x) = N (x|µx, Σx) and p(z) = N (z|µz, Σz) respectively, together
with their sum y = x+z. Use the results (2.109) and (2.110) to ﬁnd an expression for
the marginal distribution p(y) by considering the linear-Gaussian model comprising
the product of the marginal distribution p(x) and the conditional distribution p(y|x).
2.32 (  ) www This exercise and the next provide practice at manipulating the
quadratic forms that arise in linear-Gaussian models, as well as giving an indepen-
dent check of results derived in the main text. Consider a joint distribution p(x, y)
deﬁned by the marginal and conditional distributions given by (2.99) and (2.100).
By examining the quadratic form in the exponent of the joint distribution, and using
the technique of ‘completing the square’ discussed in Section 2.3, ﬁnd expressions
for the mean and covariance of the marginal distribution p(y) in which the variable
x has been integrated out. To do this, make use of the Woodbury matrix inversion
formula (2.289). Verify that these results agree with (2.109) and (2.110) obtained
using the results of Chapter 2.

2.33 (  ) Consider the same joint distribution as in Exercise 2.32, but now use the
technique of completing the square to ﬁnd expressions for the mean and covariance
of the conditional distribution p(x|y). Again, verify that these agree with the corre-
sponding expressions (2.111) and (2.112).

2.34 ( ) www To ﬁnd the maximum likelihood solution for the covariance matrix
of a multivariate Gaussian, we need to maximize the log likelihood function (2.118)
with respect to Σ, noting that the covariance matrix must be symmetric and positive
deﬁnite. Here we proceed by ignoring these constraints and doing a straightforward
maximization. Using the results (C.21), (C.26), and (C.28) from Appendix C, show
that the covariance matrix Σ that maximizes the log likelihood function (2.118) is
given by the sample covariance (2.122). We note that the ﬁnal result is necessarily
symmetric and positive deﬁnite (provided the sample covariance is nonsingular).

2.35 ( ) Use the result (2.59) to prove (2.62). Now, using the results (2.59), and (2.62),

show that

E[xnxm] = µµT + InmΣ

(2.291)
where xn denotes a data point sampled from a Gaussian distribution with mean µ
and covariance Σ, and Inm denotes the (n, m) element of the identity matrix. Hence
prove the result (2.124).

2.36 ( ) www Using an analogous procedure to that used to obtain (2.126), derive
an expression for the sequential estimation of the variance of a univariate Gaussian



N

n=1







134

2. PROBABILITY DISTRIBUTIONS

distribution, by starting with the maximum likelihood expression

ML =
σ2

1
N

(xn − µ)2.

(2.292)

Verify that substituting the expression for a Gaussian distribution into the Robbins-
Monro sequential estimation formula (2.135) gives a result of the same form, and
hence obtain an expression for the corresponding coefﬁcients aN .

2.37 ( ) Using an analogous procedure to that used to obtain (2.126), derive an ex-
pression for the sequential estimation of the covariance of a multivariate Gaussian
distribution, by starting with the maximum likelihood expression (2.122). Verify that
substituting the expression for a Gaussian distribution into the Robbins-Monro se-
quential estimation formula (2.135) gives a result of the same form, and hence obtain
an expression for the corresponding coefﬁcients aN .

2.38 () Use the technique of completing the square for the quadratic form in the expo-

nent to derive the results (2.141) and (2.142).

2.39 ( )

Starting from the results (2.141) and (2.142) for the posterior distribution
of the mean of a Gaussian random variable, dissect out the contributions from the
ﬁrst N − 1 data points and hence obtain expressions for the sequential update of
µN and σ2
N . Now derive the same results starting from the posterior distribution
p(µ|x1, . . . , xN−1) = N (µ|µN−1, σ2
N−1) and multiplying by the likelihood func-
tion p(xN|µ) = N (xN|µ, σ2) and then completing the square and normalizing to
obtain the posterior distribution after N observations.

2.40 ( ) www Consider a D-dimensional Gaussian random variable x with distribu-
tion N (x|µ, Σ) in which the covariance Σ is known and for which we wish to infer
the mean µ from a set of observations X = {x1, . . . , xN}. Given a prior distribution
p(µ) = N (µ|µ0, Σ0), ﬁnd the corresponding posterior distribution p(µ|X).

2.41 () Use the deﬁnition of the gamma function (1.141) to show that the gamma dis-

tribution (2.146) is normalized.

2.42 ( ) Evaluate the mean, variance, and mode of the gamma distribution (2.146).

2.43 () The following distribution

p(x|σ2, q) =

q

2(2σ2)1/qΓ(1/q)

exp

−|x|q

2σ2

(2.293)

is a generalization of the univariate Gaussian distribution. Show that this distribution
is normalized so that

∞

−∞

p(x|σ2, q) dx = 1

(2.294)

and that it reduces to the Gaussian when q = 2. Consider a regression model in
which the target variable is given by t = y(x, w) + 	 and 	 is a random noise



N

ln p(t|X, w, σ2) = −

1
2σ2

n=1

where ‘const’ denotes terms independent of both w and σ2. Note that, as a function
of w, this is the Lq error function considered in Section 1.5.5.

2.44 ( ) Consider a univariate Gaussian distribution N (x|µ, τ−1) having conjugate
Gaussian-gamma prior given by (2.154), and a data set x = {x1, . . . , xN} of i.i.d.
observations. Show that the posterior distribution is also a Gaussian-gamma distri-
bution of the same functional form as the prior, and write down expressions for the
parameters of this posterior distribution.

2.45 () Verify that the Wishart distribution deﬁned by (2.155) is indeed a conjugate

prior for the precision matrix of a multivariate Gaussian.

2.46 () www Verify that evaluating the integral in (2.158) leads to the result (2.159).
2.47 () www Show that in the limit ν → ∞, the t-distribution (2.159) becomes a
Gaussian. Hint: ignore the normalization coefﬁcient, and simply look at the depen-
dence on x.

2.48 () By following analogous steps to those used to derive the univariate Student’s
t-distribution (2.159), verify the result (2.162) for the multivariate form of the Stu-
dent’s t-distribution, by marginalizing over the variable η in (2.161). Using the
deﬁnition (2.161), show by exchanging integration variables that the multivariate
t-distribution is correctly normalized.

2.49 ( ) By using the deﬁnition (2.161) of the multivariate Student’s t-distribution as a
convolution of a Gaussian with a gamma distribution, verify the properties (2.164),
(2.165), and (2.166) for the multivariate t-distribution deﬁned by (2.162).

2.50 () Show that in the limit ν → ∞, the multivariate Student’s t-distribution (2.162)

reduces to a Gaussian with mean µ and precision Λ.

2.51 () www The various trigonometric identities used in the discussion of periodic

variables in this chapter can be proven easily from the relation

exp(iA) = cos A + i sin A

in which i is the square root of minus one. By considering the identity

exp(iA) exp(−iA) = 1
prove the result (2.177). Similarly, using the identity

cos(A − B) = 
 exp{i(A − B)}

(2.296)

(2.297)

(2.298)

Exercises

135

variable drawn from the distribution (2.293). Show that the log likelihood function
over w and σ2, for an observed data set of input vectors X = {x1, . . . , xN} and
corresponding target variables t = (t1, . . . , tN )T, is given by

|y(xn, w) − tn|q −

N
q

ln(2σ2) + const

(2.295)

136

2. PROBABILITY DISTRIBUTIONS

2.52 ( )

where 
 denotes the real part, prove (2.178). Finally, by using sin(A − B) =
 exp{i(A − B)}, where  denotes the imaginary part, prove the result (2.183).

For large m, the von Mises distribution (2.179) becomes sharply peaked
around the mode θ0. By deﬁning ξ = m1/2(θ − θ0) and making the Taylor ex-
pansion of the cosine function given by

cos α = 1 −

α2
2

+ O(α4)

(2.299)

show that as m → ∞, the von Mises distribution tends to a Gaussian.

2.53 () Using the trigonometric identity (2.183), show that solution of (2.182) for θ0 is

given by (2.184).

2.54 () By computing ﬁrst and second derivatives of the von Mises distribution (2.179),
and using I0(m) > 0 for m > 0, show that the maximum of the distribution occurs
when θ = θ0 and that the minimum occurs when θ = θ0 + π (mod 2π).

2.55 () By making use of the result (2.168), together with (2.184) and the trigonometric
identity (2.178), show that the maximum likelihood solution mML for the concentra-
tion of the von Mises distribution satisﬁes A(mML) = r where r is the radius of the
mean of the observations viewed as unit vectors in the two-dimensional Euclidean
plane, as illustrated in Figure 2.17.

2.56 ( ) www Express the beta distribution (2.13), the gamma distribution (2.146),
and the von Mises distribution (2.179) as members of the exponential family (2.194)
and thereby identify their natural parameters.

2.57 () Verify that the multivariate Gaussian distribution can be cast in exponential
family form (2.194) and derive expressions for η, u(x), h(x) and g(η) analogous to
(2.220)–(2.223).

2.58 () The result (2.226) showed that the negative gradient of ln g(η) for the exponen-
tial family is given by the expectation of u(x). By taking the second derivatives of
(2.195), show that

−∇∇ ln g(η) = E[u(x)u(x)T] − E[u(x)]E[u(x)T] = cov[u(x)].

(2.300)

2.59 () By changing variables using y = x/σ, show that the density (2.236) will be

correctly normalized, provided f(x) is correctly normalized.

2.60 ( ) www Consider a histogram-like density model in which the space x is di-
vided into ﬁxed regions for which the density p(x) takes the constant value hi over
the ith region, and that the volume of region i is denoted ∆i. Suppose we have a set
of N observations of x such that ni of these observations fall in region i. Using a
Lagrange multiplier to enforce the normalization constraint on the density, derive an
expression for the maximum likelihood estimator for the {hi}.

2.61 () Show that the K-nearest-neighbour density model deﬁnes an improper distribu-

tion whose integral over all space is divergent.

3

Linear

Models for
Regression

The focus so far in this book has been on unsupervised learning, including topics
such as density estimation and data clustering. We turn now to a discussion of super-
vised learning, starting with regression. The goal of regression is to predict the value
of one or more continuous target variables t given the value of a D-dimensional vec-
tor x of input variables. We have already encountered an example of a regression
problem when we considered polynomial curve ﬁtting in Chapter 1. The polynomial
is a speciﬁc example of a broad class of functions called linear regression models,
which share the property of being linear functions of the adjustable parameters, and
which will form the focus of this chapter. The simplest form of linear regression
models are also linear functions of the input variables. However, we can obtain a
much more useful class of functions by taking linear combinations of a ﬁxed set of
nonlinear functions of the input variables, known as basis functions. Such models
are linear functions of the parameters, which gives them simple analytical properties,
and yet can be nonlinear with respect to the input variables.

137

138

3. LINEAR MODELS FOR REGRESSION

Given a training data set comprising N observations {xn}, where n = 1, . . . , N,
together with corresponding target values {tn}, the goal is to predict the value of t
for a new value of x. In the simplest approach, this can be done by directly con-
structing an appropriate function y(x) whose values for new inputs x constitute the
predictions for the corresponding values of t. More generally, from a probabilistic
perspective, we aim to model the predictive distribution p(t|x) because this expresses
our uncertainty about the value of t for each value of x. From this conditional dis-
tribution we can make predictions of t, for any new value of x, in such a way as to
minimize the expected value of a suitably chosen loss function. As discussed in Sec-
tion 1.5.5, a common choice of loss function for real-valued variables is the squared
loss, for which the optimal solution is given by the conditional expectation of t.

Although linear models have signiﬁcant limitations as practical techniques for
pattern recognition, particularly for problems involving input spaces of high dimen-
sionality, they have nice analytical properties and form the foundation for more so-
phisticated models to be discussed in later chapters.



M−1

j=1



3.1. Linear Basis Function Models

The simplest linear model for regression is one that involves a linear combination of
the input variables

y(x, w) = w0 + w1x1 + . . . + wDxD

(3.1)

where x = (x1, . . . , xD)T. This is often simply known as linear regression. The key
property of this model is that it is a linear function of the parameters w0, . . . , wD. It is
also, however, a linear function of the input variables xi, and this imposes signiﬁcant
limitations on the model. We therefore extend the class of models by considering
linear combinations of ﬁxed nonlinear functions of the input variables, of the form

y(x, w) = w0 +

wjφj(x)

(3.2)

where φj(x) are known as basis functions. By denoting the maximum value of the
index j by M − 1, the total number of parameters in this model will be M.
The parameter w0 allows for any ﬁxed offset in the data and is sometimes called
a bias parameter (not to be confused with ‘bias’ in a statistical sense). It is often
convenient to deﬁne an additional dummy ‘basis function’ φ0(x) = 1 so that

M−1

y(x, w) =

wjφj(x) = wTφ(x)

(3.3)

j=0

where w = (w0, . . . , wM−1)T and φ = (φ0, . . . , φM−1)T. In many practical ap-
plications of pattern recognition, we will apply some form of ﬁxed pre-processing,








3.1. Linear Basis Function Models

139

or feature extraction, to the original data variables. If the original variables com-
prise the vector x, then the features can be expressed in terms of the basis functions
{φj(x)}.
By using nonlinear basis functions, we allow the function y(x, w) to be a non-
linear function of the input vector x. Functions of the form (3.2) are called linear
models, however, because this function is linear in w. It is this linearity in the pa-
rameters that will greatly simplify the analysis of this class of models. However, it
also leads to some signiﬁcant limitations, as we discuss in Section 3.6.

The example of polynomial regression considered in Chapter 1 is a particular
example of this model in which there is a single input variable x, and the basis func-
tions take the form of powers of x so that φj(x) = xj. One limitation of polynomial
basis functions is that they are global functions of the input variable, so that changes
in one region of input space affect all other regions. This can be resolved by dividing
the input space up into regions and ﬁt a different polynomial in each region, leading
to spline functions (Hastie et al., 2001).

There are many other possible choices for the basis functions, for example

φj(x) = exp

(x − µj)2

2s2

−

(3.4)

where the µj govern the locations of the basis functions in input space, and the pa-
rameter s governs their spatial scale. These are usually referred to as ‘Gaussian’
basis functions, although it should be noted that they are not required to have a prob-
abilistic interpretation, and in particular the normalization coefﬁcient is unimportant
because these basis functions will be multiplied by adaptive parameters wj.

Another possibility is the sigmoidal basis function of the form

φj(x) = σ

x − µj

s

where σ(a) is the logistic sigmoid function deﬁned by

σ(a) =

1

1 + exp(−a) .

(3.5)

(3.6)

Equivalently, we can use the ‘tanh’ function because this is related to the logistic
sigmoid by tanh(a) = 2σ(a) − 1, and so a general linear combination of logistic
sigmoid functions is equivalent to a general linear combination of ‘tanh’ functions.
These various choices of basis function are illustrated in Figure 3.1.

Yet another possible choice of basis function is the Fourier basis, which leads to
an expansion in sinusoidal functions. Each basis function represents a speciﬁc fre-
quency and has inﬁnite spatial extent. By contrast, basis functions that are localized
to ﬁnite regions of input space necessarily comprise a spectrum of different spatial
frequencies. In many signal processing applications, it is of interest to consider ba-
sis functions that are localized in both space and frequency, leading to a class of
functions known as wavelets. These are also deﬁned to be mutually orthogonal, to
simplify their application. Wavelets are most applicable when the input values live

140

3. LINEAR MODELS FOR REGRESSION

1

0.5

0

−0.5

−1

−1

1

0.75

0.5

0.25

0

−1

0

1

1

0.75

0.5

0.25

0
−1

0

1

0

1

Figure 3.1 Examples of basis functions, showing polynomials on the left, Gaussians of the form (3.4) in the
centre, and sigmoidal of the form (3.5) on the right.

on a regular lattice, such as the successive time points in a temporal sequence, or the
pixels in an image. Useful texts on wavelets include Ogden (1997), Mallat (1999),
and Vidakovic (1999).

Most of the discussion in this chapter, however, is independent of the particular
choice of basis function set, and so for most of our discussion we shall not specify
the particular form of the basis functions, except for the purposes of numerical il-
lustration. Indeed, much of our discussion will be equally applicable to the situation
in which the vector φ(x) of basis functions is simply the identity φ(x) = x. Fur-
thermore, in order to keep the notation simple, we shall focus on the case of a single
target variable t. However, in Section 3.1.5, we consider brieﬂy the modiﬁcations
needed to deal with multiple target variables.

3.1.1 Maximum likelihood and least squares
In Chapter 1, we ﬁtted polynomial functions to data sets by minimizing a sum-
of-squares error function. We also showed that this error function could be motivated
as the maximum likelihood solution under an assumed Gaussian noise model. Let
us return to this discussion and consider the least squares approach, and its relation
to maximum likelihood, in more detail.

As before, we assume that the target variable t is given by a deterministic func-

tion y(x, w) with additive Gaussian noise so that

t = y(x, w) + 	

(3.7)

where 	 is a zero mean Gaussian random variable with precision (inverse variance)
β. Thus we can write

Section 1.5.5

Recall that, if we assume a squared loss function, then the optimal prediction, for a
new value of x, will be given by the conditional mean of the target variable. In the
case of a Gaussian conditional distribution of the form (3.8), the conditional mean

p(t|x, w, β) = N (t|y(x, w), β−1).

(3.8)








1
2

n=1

n=1

N

N

n=1

N

N



3.1. Linear Basis Function Models

will be simply

E[t|x] =

tp(t|x) dt = y(x, w).

Note that the Gaussian noise assumption implies that the conditional distribution of
t given x is unimodal, which may be inappropriate for some applications. An ex-
tension to mixtures of conditional Gaussian distributions, which permit multimodal
conditional distributions, will be discussed in Section 14.5.1.

Now consider a data set of inputs X = {x1, . . . , xN} with corresponding target
values t1, . . . , tN . We group the target variables {tn} into a column vector that we
denote by t where the typeface is chosen to distinguish it from a single observation
of a multivariate target, which would be denoted t. Making the assumption that
these data points are drawn independently from the distribution (3.8), we obtain the
following expression for the likelihood function, which is a function of the adjustable
parameters w and β, in the form

p(t|X, w, β) =

N (tn|wTφ(xn), β−1)

(3.10)

where we have used (3.3). Note that in supervised learning problems such as regres-
sion (and classiﬁcation), we are not seeking to model the distribution of the input
variables. Thus x will always appear in the set of conditioning variables, and so
from now on we will drop the explicit x from expressions such as p(t|x, w, β) in or-
der to keep the notation uncluttered. Taking the logarithm of the likelihood function,
and making use of the standard form (1.46) for the univariate Gaussian, we have

141

(3.9)

(3.11)

(3.12)

ln p(t|w, β) =

lnN (tn|wTφ(xn), β−1)

= N
2

ln β −

N
2

ln(2π) − βED(w)

where the sum-of-squares error function is deﬁned by

ED(w) =

{tn − wTφ(xn)}2.

Having written down the likelihood function, we can use maximum likelihood to
determine w and β. Consider ﬁrst the maximization with respect to w. As observed
already in Section 1.2.5, we see that maximization of the likelihood function under a
conditional Gaussian noise distribution for a linear model is equivalent to minimizing
a sum-of-squares error function given by ED(w). The gradient of the log likelihood
function (3.11) takes the form

∇ ln p(t|w, β) =

n=1

tn − wTφ(xn)

φ(xn)T.

(3.13)


⎞⎟⎟⎠








n=1

j=1

N

M−1

M−1

wjφj

j=1

φj =

1
N



N


⎛⎜⎜⎝ φ0(x1)



φ0(x2)

1
2

n=1

...

N

N







N

...

142

3. LINEAR MODELS FOR REGRESSION

Setting this gradient to zero gives

n=1
Solving for w we obtain

0 =

tnφ(xn)T − wT

φ(xn)φ(xn)T

.

(3.14)

(3.15)
which are known as the normal equations for the least squares problem. Here Φ is an
N×M matrix, called the design matrix, whose elements are given by Φnj = φj(xn),
so that

wML =

ΦTΦ

−1 ΦTt

Φ =

The quantity

φ1(x1)
φ1(x2)

φ0(xN ) φ1(xN )

...

φM−1(x1)
φM−1(x2)

···
···
...
··· φM−1(xN )
−1 ΦT

ΦTΦ

Φ† ≡

(3.17)
is known as the Moore-Penrose pseudo-inverse of the matrix Φ (Rao and Mitra,
1971; Golub and Van Loan, 1996). It can be regarded as a generalization of the
notion of matrix inverse to nonsquare matrices. Indeed, if Φ is square and invertible,
then using the property (AB)−1 = B−1A−1 we see that Φ† ≡ Φ−1.
we make the bias parameter explicit, then the error function (3.12) becomes

At this point, we can gain some insight into the role of the bias parameter w0. If

.

(3.16)

ED(w) =

{tn − w0 −

wjφj(xn)}2.

Setting the derivative with respect to w0 equal to zero, and solving for w0, we obtain

w0 = t −

where we have deﬁned

t =

1
N

N

tn,

n=1

φj(xn).

n=1

Thus the bias w0 compensates for the difference between the averages (over the
training set) of the target values and the weighted sum of the averages of the basis
function values.

We can also maximize the log likelihood function (3.11) with respect to the noise

precision parameter β, giving

1
βML

=

1
N

{tn − wT

MLφ(xn)}2

n=1

(3.21)

(3.18)

(3.19)

(3.20)

3.1. Linear Basis Function Models

143

Figure 3.2 Geometrical interpretation of the least-squares
solution, in an N-dimensional space whose axes
are the values of t1, . . . , tN . The least-squares
regression function is obtained by ﬁnding the or-
thogonal projection of the data vector t onto the
subspace spanned by the basis functions φj(x)
in which each basis function is viewed as a vec-
tor ϕj of length N with elements φj(xn).

S

ϕ1

y

ϕ2

t

and so we see that the inverse of the noise precision is given by the residual variance
of the target values around the regression function.

3.1.2 Geometry of least squares
At this point, it is instructive to consider the geometrical interpretation of the
least-squares solution. To do this we consider an N-dimensional space whose axes
are given by the tn, so that t = (t1, . . . , tN )T is a vector in this space. Each basis
function φj(xn), evaluated at the N data points, can also be represented as a vector in
the same space, denoted by ϕj, as illustrated in Figure 3.2. Note that ϕj corresponds
to the jth column of Φ, whereas φ(xn) corresponds to the nth row of Φ. If the
number M of basis functions is smaller than the number N of data points, then the
M vectors φj(xn) will span a linear subspace S of dimensionality M. We deﬁne
y to be an N-dimensional vector whose nth element is given by y(xn, w), where
n = 1, . . . , N. Because y is an arbitrary linear combination of the vectors ϕj, it can
live anywhere in the M-dimensional subspace. The sum-of-squares error (3.12) is
then equal (up to a factor of 1/2) to the squared Euclidean distance between y and
t. Thus the least-squares solution for w corresponds to that choice of y that lies in
subspace S and that is closest to t. Intuitively, from Figure 3.2, we anticipate that
this solution corresponds to the orthogonal projection of t onto the subspace S. This
is indeed the case, as can easily be veriﬁed by noting that the solution for y is given
by ΦwML, and then conﬁrming that this takes the form of an orthogonal projection.
In practice, a direct solution of the normal equations can lead to numerical difﬁ-
culties when ΦTΦ is close to singular. In particular, when two or more of the basis
vectors ϕj are co-linear, or nearly so, the resulting parameter values can have large
magnitudes. Such near degeneracies will not be uncommon when dealing with real
data sets. The resulting numerical difﬁculties can be addressed using the technique
of singular value decomposition, or SVD (Press et al., 1992; Bishop and Nabney,
2008). Note that the addition of a regularization term ensures that the matrix is non-
singular, even in the presence of degeneracies.

3.1.3 Sequential learning
Batch techniques, such as the maximum likelihood solution (3.15), which in-
volve processing the entire training set in one go, can be computationally costly for
large data sets. As we have discussed in Chapter 1, if the data set is sufﬁciently large,
it may be worthwhile to use sequential algorithms, also known as on-line algorithms,

Exercise 3.2

144

3. LINEAR MODELS FOR REGRESSION





N



N

n=1

in which the data points are considered one at a time, and the model parameters up-
dated after each such presentation. Sequential learning is also appropriate for real-
time applications in which the data observations are arriving in a continuous stream,
and predictions must be made before all of the data points are seen.

We can obtain a sequential learning algorithm by applying the technique of
stochastic gradient descent, also known as sequential gradient descent, as follows. If
the error function comprises a sum over data points E =
n En, then after presen-
tation of pattern n, the stochastic gradient descent algorithm updates the parameter
vector w using

(3.22)
where τ denotes the iteration number, and η is a learning rate parameter. We shall
discuss the choice of value for η shortly. The value of w is initialized to some starting
vector w(0). For the case of the sum-of-squares error function (3.12), this gives

w(τ +1) = w(τ ) − η∇En

w(τ +1) = w(τ ) + η(tn − w(τ )Tφn)φn

(3.23)

where φn = φ(xn). This is known as least-mean-squares or the LMS algorithm.
The value of η needs to be chosen with care to ensure that the algorithm converges
(Bishop and Nabney, 2008).

3.1.4 Regularized least squares
In Section 1.1, we introduced the idea of adding a regularization term to an
error function in order to control over-ﬁtting, so that the total error function to be
minimized takes the form

ED(w) + λEW (w)

(3.24)
where λ is the regularization coefﬁcient that controls the relative importance of the
data-dependent error ED(w) and the regularization term EW (w). One of the sim-
plest forms of regularizer is given by the sum-of-squares of the weight vector ele-
ments

If we also consider the sum-of-squares error function given by

EW (w) =

wTw.

1
2

then the total error function becomes

E(w) =

1
2

{tn − wTφ(xn)}2

n=1

1
2

{tn − wTφ(xn)}2 + λ
2

wTw.

This particular choice of regularizer is known in the machine learning literature as
weight decay because in sequential learning algorithms, it encourages weight values
to decay towards zero, unless supported by the data. In statistics, it provides an ex-
ample of a parameter shrinkage method because it shrinks parameter values towards

(3.25)

(3.26)

(3.27)





M



N

n=1





M

3.1. Linear Basis Function Models

145

q = 0.5

q = 1

q = 2

q = 4

Figure 3.3 Contours of the regularization term in (3.29) for various values of the parameter q.

zero. It has the advantage that the error function remains a quadratic function of
w, and so its exact minimizer can be found in closed form. Speciﬁcally, setting the
gradient of (3.27) with respect to w to zero, and solving for w as before, we obtain

w =

λI + ΦTΦ

−1 ΦTt.

(3.28)

This represents a simple extension of the least-squares solution (3.15).

A more general regularizer is sometimes used, for which the regularized error

takes the form

1
2

{tn − wTφ(xn)}2 + λ
2

|wj|q

j=1

(3.29)

where q = 2 corresponds to the quadratic regularizer (3.27). Figure 3.3 shows con-
tours of the regularization function for different values of q.

The case of q = 1 is know as the lasso in the statistics literature (Tibshirani,
1996).
It has the property that if λ is sufﬁciently large, some of the coefﬁcients
wj are driven to zero, leading to a sparse model in which the corresponding basis
functions play no role. To see this, we ﬁrst note that minimizing (3.29) is equivalent
to minimizing the unregularized sum-of-squares error (3.12) subject to the constraint

|wj|q � η

j=1

(3.30)

for an appropriate value of the parameter η, where the two approaches can be related
using Lagrange multipliers. The origin of the sparsity can be seen from Figure 3.4,
which shows that the minimum of the error function, subject to the constraint (3.30).
As λ is increased, so an increasing number of parameters are driven to zero.

Regularization allows complex models to be trained on data sets of limited size
without severe over-ﬁtting, essentially by limiting the effective model complexity.
However, the problem of determining the optimal model complexity is then shifted
from one of ﬁnding the appropriate number of basis functions to one of determining
a suitable value of the regularization coefﬁcient λ. We shall return to the issue of
model complexity later in this chapter.

Exercise 3.5

Appendix E



N







N

''

''

146

3. LINEAR MODELS FOR REGRESSION

w2

Figure 3.4 Plot of
the contours
of the unregularized error function
(blue) along with the constraint re-
gion (3.30) for the quadratic regular-
izer q = 2 on the left and the lasso
regularizer q = 1 on the right,
in
which the optimum value for the pa-
rameter vector w is denoted by w.
The lasso gives a sparse solution in
which w

1 = 0.

w

w2

w

w1

w1

For the remainder of this chapter we shall focus on the quadratic regularizer

(3.27) both for its practical importance and its analytical tractability.

3.1.5 Multiple outputs
So far, we have considered the case of a single target variable t. In some applica-
tions, we may wish to predict K > 1 target variables, which we denote collectively
by the target vector t. This could be done by introducing a different set of basis func-
tions for each component of t, leading to multiple, independent regression problems.
However, a more interesting, and more common, approach is to use the same set of
basis functions to model all of the components of the target vector so that

y(x, w) = WTφ(x)

(3.31)
where y is a K-dimensional column vector, W is an M × K matrix of parameters,
and φ(x) is an M-dimensional column vector with elements φj(x), with φ0(x) = 1
as before. Suppose we take the conditional distribution of the target vector to be an
isotropic Gaussian of the form

p(t|x, W, β) = N (t|WTφ(x), β−1I).

(3.32)

If we have a set of observations t1, . . . , tN , we can combine these into a matrix T
of size N × K such that the nth row is given by tT
n. Similarly, we can combine the
input vectors x1, . . . , xN into a matrix X. The log likelihood function is then given
by

ln p(T|X, W, β) =

n=1

lnN (tn|WTφ(xn), β−1I)

= N K
2

ln

β
2π

β
2

−

tn − WTφ(xn)

n=1

2

. (3.33)











147

(3.34)

(3.35)

3.2. The Bias-Variance Decomposition

As before, we can maximize this function with respect to W, giving

WML =

ΦTΦ

−1 ΦTT.

If we examine this result for each target variable tk, we have

wk =

ΦTΦ

−1 ΦTtk = Φ†tk

Exercise 3.6

where tk is an N-dimensional column vector with components tnk for n = 1, . . . N.
Thus the solution to the regression problem decouples between the different target
variables, and we need only compute a single pseudo-inverse matrix Φ†, which is
shared by all of the vectors wk.

The extension to general Gaussian noise distributions having arbitrary covari-
ance matrices is straightforward. Again, this leads to a decoupling into K inde-
pendent regression problems. This result is unsurprising because the parameters W
deﬁne only the mean of the Gaussian noise distribution, and we know from Sec-
tion 2.3.4 that the maximum likelihood solution for the mean of a multivariate Gaus-
sian is independent of the covariance. From now on, we shall therefore consider a
single target variable t for simplicity.

3.2. The Bias-Variance Decomposition

So far in our discussion of linear models for regression, we have assumed that the
form and number of basis functions are both ﬁxed. As we have seen in Chapter 1,
the use of maximum likelihood, or equivalently least squares, can lead to severe
over-ﬁtting if complex models are trained using data sets of limited size. However,
limiting the number of basis functions in order to avoid over-ﬁtting has the side
effect of limiting the ﬂexibility of the model to capture interesting and important
trends in the data. Although the introduction of regularization terms can control
over-ﬁtting for models with many parameters, this raises the question of how to
determine a suitable value for the regularization coefﬁcient λ. Seeking the solution
that minimizes the regularized error function with respect to both the weight vector
w and the regularization coefﬁcient λ is clearly not the right approach since this
leads to the unregularized solution with λ = 0.

As we have seen in earlier chapters, the phenomenon of over-ﬁtting is really an
unfortunate property of maximum likelihood and does not arise when we marginalize
over parameters in a Bayesian setting. In this chapter, we shall consider the Bayesian
view of model complexity in some depth. Before doing so, however, it is instructive
to consider a frequentist viewpoint of the model complexity issue, known as the bias-
variance trade-off. Although we shall introduce this concept in the context of linear
basis function models, where it is easy to illustrate the ideas using simple examples,
the discussion has more general applicability.

In Section 1.5.5, when we discussed decision theory for regression problems,
we considered various loss functions each of which leads to a corresponding optimal
prediction once we are given the conditional distribution p(t|x). A popular choice is






148

3. LINEAR MODELS FOR REGRESSION

the squared loss function, for which the optimal prediction is given by the conditional
expectation, which we denote by h(x) and which is given by

h(x) = E[t|x] =

tp(t|x) dt.

(3.36)

At this point, it is worth distinguishing between the squared loss function arising
from decision theory and the sum-of-squares error function that arose in the maxi-
mum likelihood estimation of model parameters. We might use more sophisticated
techniques than least squares, for example regularization or a fully Bayesian ap-
proach, to determine the conditional distribution p(t|x). These can all be combined
with the squared loss function for the purpose of making predictions.
We showed in Section 1.5.5 that the expected squared loss can be written in the

form

E[L] =

{y(x) − h(x)}2 p(x) dx +

{h(x) − t}2p(x, t) dx dt.

(3.37)

Recall that the second term, which is independent of y(x), arises from the intrinsic
noise on the data and represents the minimum achievable value of the expected loss.
The ﬁrst term depends on our choice for the function y(x), and we will seek a so-
lution for y(x) which makes this term a minimum. Because it is nonnegative, the
smallest that we can hope to make this term is zero. If we had an unlimited supply of
data (and unlimited computational resources), we could in principle ﬁnd the regres-
sion function h(x) to any desired degree of accuracy, and this would represent the
optimal choice for y(x). However, in practice we have a data set D containing only
a ﬁnite number N of data points, and consequently we do not know the regression
function h(x) exactly.

If we model the h(x) using a parametric function y(x, w) governed by a pa-
rameter vector w, then from a Bayesian perspective the uncertainty in our model is
expressed through a posterior distribution over w. A frequentist treatment, however,
involves making a point estimate of w based on the data set D, and tries instead
to interpret the uncertainty of this estimate through the following thought experi-
ment. Suppose we had a large number of data sets each of size N and each drawn
independently from the distribution p(t, x). For any given data set D, we can run
our learning algorithm and obtain a prediction function y(x;D). Different data sets
from the ensemble will give different functions and consequently different values of
the squared loss. The performance of a particular learning algorithm is then assessed
by taking the average over this ensemble of data sets.

Consider the integrand of the ﬁrst term in (3.37), which for a particular data set

D takes the form

(3.38)
Because this quantity will be dependent on the particular data set D, we take its aver-
age over the ensemble of data sets. If we add and subtract the quantity ED[y(x;D)]

{y(x;D) − h(x)}2.



(



(

+

	



)*




	
+

)*

	

3.2. The Bias-Variance Decomposition

149

(3.39)

inside the braces, and then expand, we obtain

{y(x;D) − ED[y(x;D)] + ED[y(x;D)] − h(x)}2
= {y(x;D) − ED[y(x;D)]}2 + {ED[y(x;D)] − h(x)}2
+2{y(x;D) − ED[y(x;D)]}{ED[y(x;D)] − h(x)}.

We now take the expectation of this expression with respect to D and note that the
ﬁnal term will vanish, giving

{y(x;D) − h(x)}2

ED
= {ED[y(x;D)] − h(x)}2

(bias)2

variance

+ ED

{y(x;D) − ED[y(x;D)]}2

.

(3.40)

We see that the expected squared difference between y(x;D) and the regression
function h(x) can be expressed as the sum of two terms. The ﬁrst term, called the
squared bias, represents the extent to which the average prediction over all data sets
differs from the desired regression function. The second term, called the variance,
measures the extent to which the solutions for individual data sets vary around their
average, and hence this measures the extent to which the function y(x;D) is sensitive
to the particular choice of data set. We shall provide some intuition to support these
deﬁnitions shortly when we consider a simple example.

So far, we have considered a single input value x. If we substitute this expansion
back into (3.37), we obtain the following decomposition of the expected squared loss

expected loss = (bias)2 + variance + noise

where

(bias)2 =

variance =

noise =

{ED[y(x;D)] − h(x)}2p(x) dx
{y(x;D) − ED[y(x;D)]}2
ED
{h(x) − t}2p(x, t) dx dt

(3.41)

(3.42)

p(x) dx

(3.43)

(3.44)

and the bias and variance terms now refer to integrated quantities.

Our goal is to minimize the expected loss, which we have decomposed into the
sum of a (squared) bias, a variance, and a constant noise term. As we shall see, there
is a trade-off between bias and variance, with very ﬂexible models having low bias
and high variance, and relatively rigid models having high bias and low variance.
The model with the optimal predictive capability is the one that leads to the best
balance between bias and variance. This is illustrated by considering the sinusoidal
data set from Chapter 1. Here we generate 100 data sets, each containing N = 25
data points, independently from the sinusoidal curve h(x) = sin(2πx). The data
sets are indexed by l = 1, . . . , L, where L = 100, and for each data set D(l) we

Appendix A

150

3. LINEAR MODELS FOR REGRESSION

t

1

0

−1

t

1

0

−1

t

1

0

−1

0

0

0

ln λ = 2.6

t

1

0

−1

1

x

0

1

x

ln λ = −0.31

t

1

0

−1

1

x

0

1

x

ln λ = −2.4

t

1

0

−1

1

x

0

1

x

Figure 3.5 Illustration of the dependence of bias and variance on model complexity, governed by a regulariza-
tion parameter λ, using the sinusoidal data set from Chapter 1. There are L = 100 data sets, each having N = 25
data points, and there are 24 Gaussian basis functions in the model so that the total number of parameters is
M = 25 including the bias parameter. The left column shows the result of ﬁtting the model to the data sets for
various values of ln λ (for clarity, only 20 of the 100 ﬁts are shown). The right column shows the corresponding
average of the 100 ﬁts (red) along with the sinusoidal function from which the data sets were generated (green).




l=1

L



0.15

0.12

0.09

0.06

0.03




n=1

N

N



0

151

1

2

(3.45)

(3.46)

(3.47)

Figure 3.6 Plot of squared bias and variance,
together with their sum, correspond-
ing to the results shown in Fig-
ure 3.5. Also shown is the average
test set error for a test data set size
of 1000 points. The minimum value
of (bias)2 + variance occurs around
ln λ = −0.31, which is close to the
value that gives the minimum error
on the test data.

3.2. The Bias-Variance Decomposition

(bias)2
variance
(bias)2 + variance
test error

−1

ln λ

0
−3

−2

ﬁt a model with 24 Gaussian basis functions by minimizing the regularized error
function (3.27) to give a prediction function y(l)(x) as shown in Figure 3.5. The
top row corresponds to a large value of the regularization coefﬁcient λ that gives low
variance (because the red curves in the left plot look similar) but high bias (because
the two curves in the right plot are very different). Conversely on the bottom row, for
which λ is small, there is large variance (shown by the high variability between the
red curves in the left plot) but low bias (shown by the good ﬁt between the average
model ﬁt and the original sinusoidal function). Note that the result of averaging many
solutions for the complex model with M = 25 is a very good ﬁt to the regression
function, which suggests that averaging may be a beneﬁcial procedure. Indeed, a
weighted averaging of multiple solutions lies at the heart of a Bayesian approach,
although the averaging is with respect to the posterior distribution of parameters, not
with respect to multiple data sets.

We can also examine the bias-variance trade-off quantitatively for this example.

The average prediction is estimated from

y(x) =

y(l)(x)

1
L

and the integrated squared bias and integrated variance are then given by

(bias)2 =

variance =

1
N

1
N

{y(xn) − h(xn)}2
1
L

L

y(l)(xn) − y(xn)

2

n=1

l=1

where the integral over x weighted by the distribution p(x) is approximated by a
ﬁnite sum over data points drawn from that distribution. These quantities, along
with their sum, are plotted as a function of ln λ in Figure 3.6. We see that small
values of λ allow the model to become ﬁnely tuned to the noise on each individual

152

3. LINEAR MODELS FOR REGRESSION

data set leading to large variance. Conversely, a large value of λ pulls the weight
parameters towards zero leading to large bias.

Although the bias-variance decomposition may provide some interesting in-
sights into the model complexity issue from a frequentist perspective, it is of lim-
ited practical value, because the bias-variance decomposition is based on averages
with respect to ensembles of data sets, whereas in practice we have only the single
observed data set. If we had a large number of independent training sets of a given
size, we would be better off combining them into a single large training set, which
of course would reduce the level of over-ﬁtting for a given model complexity.

Given these limitations, we turn in the next section to a Bayesian treatment of
linear basis function models, which not only provides powerful insights into the
issues of over-ﬁtting but which also leads to practical techniques for addressing the
question model complexity.

3.3. Bayesian Linear Regression

In our discussion of maximum likelihood for setting the parameters of a linear re-
gression model, we have seen that the effective model complexity, governed by the
number of basis functions, needs to be controlled according to the size of the data
set. Adding a regularization term to the log likelihood function means the effective
model complexity can then be controlled by the value of the regularization coefﬁ-
cient, although the choice of the number and form of the basis functions is of course
still important in determining the overall behaviour of the model.

This leaves the issue of deciding the appropriate model complexity for the par-
ticular problem, which cannot be decided simply by maximizing the likelihood func-
tion, because this always leads to excessively complex models and over-ﬁtting. In-
dependent hold-out data can be used to determine model complexity, as discussed
in Section 1.3, but this can be both computationally expensive and wasteful of valu-
able data. We therefore turn to a Bayesian treatment of linear regression, which will
avoid the over-ﬁtting problem of maximum likelihood, and which will also lead to
automatic methods of determining model complexity using the training data alone.
Again, for simplicity we will focus on the case of a single target variable t. Ex-
tension to multiple target variables is straightforward and follows the discussion of
Section 3.1.5.

3.3.1 Parameter distribution
We begin our discussion of the Bayesian treatment of linear regression by in-
troducing a prior probability distribution over the model parameters w. For the mo-
ment, we shall treat the noise precision parameter β as a known constant. First note
that the likelihood function p(t|w) deﬁned by (3.10) is the exponential of a quadratic
function of w. The corresponding conjugate prior is therefore given by a Gaussian
distribution of the form

having mean m0 and covariance S0.

p(w) = N (w|m0, S0)

(3.48)








N

n=1

3.3. Bayesian Linear Regression

153

Next we compute the posterior distribution, which is proportional to the product
of the likelihood function and the prior. Due to the choice of a conjugate Gaus-
sian prior distribution, the posterior will also be Gaussian. We can evaluate this
distribution by the usual procedure of completing the square in the exponential, and
then ﬁnding the normalization coefﬁcient using the standard result for a normalized
Gaussian. However, we have already done the necessary work in deriving the gen-
eral result (2.116), which allows us to write down the posterior distribution directly
in the form

where

p(w|t) = N (w|mN , SN )

mN = SN
S−1
N = S−1

0 + βΦTΦ.

S−1
0 m0 + βΦTt

Note that because the posterior distribution is Gaussian, its mode coincides with its
mean. Thus the maximum posterior weight vector is simply given by wMAP = mN .
If we consider an inﬁnitely broad prior S0 = α−1I with α → 0, the mean mN
of the posterior distribution reduces to the maximum likelihood value wML given
by (3.15). Similarly, if N = 0, then the posterior distribution reverts to the prior.
Furthermore, if data points arrive sequentially, then the posterior distribution at any
stage acts as the prior distribution for the subsequent data point, such that the new
posterior distribution is again given by (3.49).

For the remainder of this chapter, we shall consider a particular form of Gaus-
sian prior in order to simplify the treatment. Speciﬁcally, we consider a zero-mean
isotropic Gaussian governed by a single precision parameter α so that

and the corresponding posterior distribution over w is then given by (3.49) with

p(w|α) = N (w|0, α−1I)

mN = βSN ΦTt
S−1
N = αI + βΦTΦ.

(3.49)

(3.50)
(3.51)

(3.52)

(3.53)
(3.54)

Exercise 3.7

Exercise 3.8

The log of the posterior distribution is given by the sum of the log likelihood and

the log of the prior and, as a function of w, takes the form

ln p(w|t) = −

β
2

{tn − wTφ(xn)}2 −

α
2

wTw + const.

(3.55)

Maximization of this posterior distribution with respect to w is therefore equiva-
lent to the minimization of the sum-of-squares error function with the addition of a
quadratic regularization term, corresponding to (3.27) with λ = α/β.

We can illustrate Bayesian learning in a linear basis function model, as well as
the sequential update of a posterior distribution, using a simple example involving
straight-line ﬁtting. Consider a single input variable x, a single target variable t and

154

3. LINEAR MODELS FOR REGRESSION

a linear model of the form y(x, w) = w0 + w1x. Because this has just two adap-
tive parameters, we can plot the prior and posterior distributions directly in parameter
space. We generate synthetic data from the function f(x, a) = a0 + a1x with param-
eter values a0 = −0.3 and a1 = 0.5 by ﬁrst choosing values of xn from the uniform
distribution U(x|−1, 1), then evaluating f(xn, a), and ﬁnally adding Gaussian noise
with standard deviation of 0.2 to obtain the target values tn. Our goal is to recover
the values of a0 and a1 from such data, and we will explore the dependence on the
size of the data set. We assume here that the noise variance is known and hence we
set the precision parameter to its true value β = (1/0.2)2 = 25. Similarly, we ﬁx
the parameter α to 2.0. We shall shortly discuss strategies for determining α and
β from the training data. Figure 3.7 shows the results of Bayesian learning in this
model as the size of the data set is increased and demonstrates the sequential nature
of Bayesian learning in which the current posterior distribution forms the prior when
a new data point is observed. It is worth taking time to study this ﬁgure in detail as
it illustrates several important aspects of Bayesian inference. The ﬁrst row of this
ﬁgure corresponds to the situation before any data points are observed and shows a
plot of the prior distribution in w space together with six samples of the function
y(x, w) in which the values of w are drawn from the prior. In the second row, we
see the situation after observing a single data point. The location (x, t) of the data
point is shown by a blue circle in the right-hand column. In the left-hand column is a
plot of the likelihood function p(t|x, w) for this data point as a function of w. Note
that the likelihood function provides a soft constraint that the line must pass close to
the data point, where close is determined by the noise precision β. For comparison,
the true parameter values a0 = −0.3 and a1 = 0.5 used to generate the data set
are shown by a white cross in the plots in the left column of Figure 3.7. When we
multiply this likelihood function by the prior from the top row, and normalize, we
obtain the posterior distribution shown in the middle plot on the second row. Sam-
ples of the regression function y(x, w) obtained by drawing samples of w from this
posterior distribution are shown in the right-hand plot. Note that these sample lines
all pass close to the data point. The third row of this ﬁgure shows the effect of ob-
serving a second data point, again shown by a blue circle in the plot in the right-hand
column. The corresponding likelihood function for this second data point alone is
shown in the left plot. When we multiply this likelihood function by the posterior
distribution from the second row, we obtain the posterior distribution shown in the
middle plot of the third row. Note that this is exactly the same posterior distribution
as would be obtained by combining the original prior with the likelihood function
for the two data points. This posterior has now been inﬂuenced by two data points,
and because two points are sufﬁcient to deﬁne a line this already gives a relatively
compact posterior distribution. Samples from this posterior distribution give rise to
the functions shown in red in the third column, and we see that these functions pass
close to both of the data points. The fourth row shows the effect of observing a total
of 20 data points. The left-hand plot shows the likelihood function for the 20th data
point alone, and the middle plot shows the resulting posterior distribution that has
now absorbed information from all 20 observations. Note how the posterior is much
sharper than in the third row. In the limit of an inﬁnite number of data points, the

3.3. Bayesian Linear Regression

155

Figure 3.7 Illustration of sequential Bayesian learning for a simple linear model of the form y(x, w) =

w0 + w1x. A detailed description of this ﬁgure is given in the text.





M













156

3. LINEAR MODELS FOR REGRESSION

posterior distribution would become a delta function centred on the true parameter
values, shown by the white cross.

Other forms of prior over the parameters can be considered. For instance, we

can generalize the Gaussian prior to give

p(w|α) =

q
2

α
2

1/q

1

Γ(1/q)

M

exp

α
2

−

|wj|q

j=1

(3.56)

in which q = 2 corresponds to the Gaussian distribution, and only in this case is the
prior conjugate to the likelihood function (3.10). Finding the maximum of the poste-
rior distribution over w corresponds to minimization of the regularized error function
(3.29). In the case of the Gaussian prior, the mode of the posterior distribution was
equal to the mean, although this will no longer hold if q = 2.

3.3.2 Predictive distribution
In practice, we are not usually interested in the value of w itself but rather in
making predictions of t for new values of x. This requires that we evaluate the
predictive distribution deﬁned by

p(t|t, α, β) =

p(t|w, β)p(w|t, α, β) dw

(3.57)

in which t is the vector of target values from the training set, and we have omitted the
corresponding input vectors from the right-hand side of the conditioning statements
to simplify the notation. The conditional distribution p(t|x, w, β) of the target vari-
able is given by (3.8), and the posterior weight distribution is given by (3.49). We
see that (3.57) involves the convolution of two Gaussian distributions, and so making
use of the result (2.115) from Section 8.1.4, we see that the predictive distribution
takes the form

where the variance σ2

p(t|x, t, α, β) = N (t|mT
N (x) of the predictive distribution is given by

N φ(x), σ2

N (x))

(3.58)

(3.59)

N (x) =
σ2

1
β

+ φ(x)TSN φ(x).

The ﬁrst term in (3.59) represents the noise on the data whereas the second term
reﬂects the uncertainty associated with the parameters w. Because the noise process
and the distribution of w are independent Gaussians, their variances are additive.
Note that, as additional data points are observed, the posterior distribution becomes
N +1(x) �
narrower. As a consequence it can be shown (Qazaz et al., 1997) that σ2
N (x). In the limit N → ∞, the second term in (3.59) goes to zero, and the variance
σ2
of the predictive distribution arises solely from the additive noise governed by the
parameter β.

As an illustration of the predictive distribution for Bayesian linear regression
models, let us return to the synthetic sinusoidal data set of Section 1.1. In Figure 3.8,

Exercise 3.10

Exercise 3.11

t

1

0

−1

t

1

0

−1

0

0

3.3. Bayesian Linear Regression

157

t

1

0

−1

1

x

0

1

x

t

1

0

−1

1

x

0

1

x

Figure 3.8 Examples of the predictive distribution (3.58) for a model consisting of 9 Gaussian basis functions
of the form (3.4) using the synthetic sinusoidal data set of Section 1.1. See the text for a detailed discussion.

we ﬁt a model comprising a linear combination of Gaussian basis functions to data
sets of various sizes and then look at the corresponding posterior distributions. Here
the green curves correspond to the function sin(2πx) from which the data points
were generated (with the addition of Gaussian noise). Data sets of size N = 1,
N = 2, N = 4, and N = 25 are shown in the four plots by the blue circles. For
each plot, the red curve shows the mean of the corresponding Gaussian predictive
distribution, and the red shaded region spans one standard deviation either side of
the mean. Note that the predictive uncertainty depends on x and is smallest in the
neighbourhood of the data points. Also note that the level of uncertainty decreases
as more data points are observed.

The plots in Figure 3.8 only show the point-wise predictive variance as a func-
tion of x. In order to gain insight into the covariance between the predictions at
different values of x, we can draw samples from the posterior distribution over w,
and then plot the corresponding functions y(x, w), as shown in Figure 3.9.

158

3. LINEAR MODELS FOR REGRESSION

t

1

0

−1

t

1

0

−1

0

0

t

1

0

−1

1

x

0

1

x

t

1

0

−1

1

x

0

1

x

Figure 3.9 Plots of the function y(x, w) using samples from the posterior distributions over w corresponding to
the plots in Figure 3.8.

If we used localized basis functions such as Gaussians, then in regions away
from the basis function centres, the contribution from the second term in the predic-
tive variance (3.59) will go to zero, leaving only the noise contribution β−1. Thus,
the model becomes very conﬁdent in its predictions when extrapolating outside the
region occupied by the basis functions, which is generally an undesirable behaviour.
This problem can be avoided by adopting an alternative Bayesian approach to re-
gression known as a Gaussian process.

Note that, if both w and β are treated as unknown, then we can introduce a
conjugate prior distribution p(w, β) that, from the discussion in Section 2.3.6, will
be given by a Gaussian-gamma distribution (Denison et al., 2002). In this case, the
predictive distribution is a Student’s t-distribution.

Section 6.4

Exercise 3.12
Exercise 3.13

3.3. Bayesian Linear Regression

159

Figure 3.10 The equivalent ker-
nel k(x, x) for the Gaussian basis
functions in Figure 3.1, shown as
a plot of x versus x, together with
three slices through this matrix cor-
responding to three different values
of x. The data set used to generate
this kernel comprised 200 values of
x equally spaced over the interval
(−1, 1).



N



N

3.3.3 Equivalent kernel
The posterior mean solution (3.53) for the linear basis function model has an in-
teresting interpretation that will set the stage for kernel methods, including Gaussian
processes. If we substitute (3.53) into the expression (3.3), we see that the predictive
mean can be written in the form

Chapter 6

y(x, mN ) = mT

N φ(x) = βφ(x)TSN ΦTt =

βφ(x)TSN φ(xn)tn

(3.60)

n=1

where SN is deﬁned by (3.51). Thus the mean of the predictive distribution at a point
x is given by a linear combination of the training set target variables tn, so that we
can write

where the function

y(x, mN ) =

k(x, xn)tn

n=1

k(x, x) = βφ(x)TSN φ(x)

(3.61)

(3.62)

is known as the smoother matrix or the equivalent kernel. Regression functions, such
as this, which make predictions by taking linear combinations of the training set
target values are known as linear smoothers. Note that the equivalent kernel depends
on the input values xn from the data set because these appear in the deﬁnition of
SN . The equivalent kernel is illustrated for the case of Gaussian basis functions in
Figure 3.10 in which the kernel functions k(x, x) have been plotted as a function of
x for three different values of x. We see that they are localized around x, and so the
mean of the predictive distribution at x, given by y(x, mN ), is obtained by forming
a weighted combination of the target values in which data points close to x are given
higher weight than points further removed from x. Intuitively, it seems reasonable
that we should weight local evidence more strongly than distant evidence. Note that
this localization property holds not only for the localized Gaussian basis functions
but also for the nonlocal polynomial and sigmoidal basis functions, as illustrated in
Figure 3.11.

160

3. LINEAR MODELS FOR REGRESSION

Figure 3.11 Examples of equiva-
lent kernels k(x, x) for x = 0
plotted as a function of x, corre-
sponding (left) to the polynomial ba-
sis functions and (right) to the sig-
moidal basis functions shown in Fig-
ure 3.1. Note that these are local-
ized functions of x even though the
corresponding basis functions are
nonlocal.

0.04

0.02

0

−1

0



N

1



0.04

0.02

0

−1

1

0

Further insight into the role of the equivalent kernel can be obtained by consid-

ering the covariance between y(x) and y(x), which is given by

cov[y(x), y(x)] = cov[φ(x)Tw, wTφ(x)]

= φ(x)TSN φ(x) = β−1k(x, x)

(3.63)

where we have made use of (3.49) and (3.62). From the form of the equivalent
kernel, we see that the predictive mean at nearby points will be highly correlated,
whereas for more distant pairs of points the correlation will be smaller.

The predictive distribution shown in Figure 3.8 allows us to visualize the point-
wise uncertainty in the predictions, governed by (3.59). However, by drawing sam-
ples from the posterior distribution over w, and plotting the corresponding model
functions y(x, w) as in Figure 3.9, we are visualizing the joint uncertainty in the
posterior distribution between the y values at two (or more) x values, as governed by
the equivalent kernel.

The formulation of linear regression in terms of a kernel function suggests an
alternative approach to regression as follows. Instead of introducing a set of basis
functions, which implicitly determines an equivalent kernel, we can instead deﬁne
a localized kernel directly and use this to make predictions for new input vectors x,
given the observed training set. This leads to a practical framework for regression
(and classiﬁcation) called Gaussian processes, which will be discussed in detail in
Section 6.4.

We have seen that the effective kernel deﬁnes the weights by which the training
set target values are combined in order to make a prediction at a new value of x, and
it can be shown that these weights sum to one, in other words

k(x, xn) = 1

n=1

(3.64)

Exercise 3.14

for all values of x. This intuitively pleasing result can easily be proven informally
y(x)
by noting that the summation is equivalent to considering the predictive mean
for a set of target data in which tn = 1 for all n. Provided the basis functions are
linearly independent, that there are more data points than basis functions, and that
one of the basis functions is constant (corresponding to the bias parameter), then it is
clear that we can ﬁt the training data exactly and hence that the predictive mean will



3.4. Bayesian Model Comparison

161

y(x) = 1, from which we obtain (3.64). Note that the kernel function can
be simply
be negative as well as positive, so although it satisﬁes a summation constraint, the
corresponding predictions are not necessarily convex combinations of the training
set target variables.

Chapter 6

Finally, we note that the equivalent kernel (3.62) satisﬁes an important property
shared by kernel functions in general, namely that it can be expressed in the form an
inner product with respect to a vector ψ(x) of nonlinear functions, so that

k(x, z) = ψ(x)Tψ(z)

(3.65)

where ψ(x) = β1/2S1/2

N φ(x).

3.4. Bayesian Model Comparison

In Chapter 1, we highlighted the problem of over-ﬁtting as well as the use of cross-
validation as a technique for setting the values of regularization parameters or for
choosing between alternative models. Here we consider the problem of model se-
lection from a Bayesian perspective.
In this section, our discussion will be very
general, and then in Section 3.5 we shall see how these ideas can be applied to the
determination of regularization parameters in linear regression.

As we shall see, the over-ﬁtting associated with maximum likelihood can be
avoided by marginalizing (summing or integrating) over the model parameters in-
stead of making point estimates of their values. Models can then be compared di-
rectly on the training data, without the need for a validation set. This allows all
available data to be used for training and avoids the multiple training runs for each
model associated with cross-validation. It also allows multiple complexity parame-
ters to be determined simultaneously as part of the training process. For example,
in Chapter 7 we shall introduce the relevance vector machine, which is a Bayesian
model having one complexity parameter for every training data point.

The Bayesian view of model comparison simply involves the use of probabilities
to represent uncertainty in the choice of model, along with a consistent application
of the sum and product rules of probability. Suppose we wish to compare a set of L
models {Mi} where i = 1, . . . , L. Here a model refers to a probability distribution
over the observed data D. In the case of the polynomial curve-ﬁtting problem, the
distribution is deﬁned over the set of target values t, while the set of input values X
is assumed to be known. Other types of model deﬁne a joint distributions over X
and t. We shall suppose that the data is generated from one of these models but we
are uncertain which one. Our uncertainty is expressed through a prior probability
distribution p(Mi). Given a training set D, we then wish to evaluate the posterior
distribution
(3.66)
The prior allows us to express a preference for different models. Let us simply
assume that all models are given equal prior probability. The interesting term is
the model evidence p(D|Mi) which expresses the preference shown by the data for

p(Mi|D) ∝ p(Mi)p(D|Mi).

Section 1.5.4




i=1

L



3. LINEAR MODELS FOR REGRESSION

different models, and we shall examine this term in more detail shortly. The model
evidence is sometimes also called the marginal likelihood because it can be viewed
as a likelihood function over the space of models, in which the parameters have been
marginalized out. The ratio of model evidences p(D|Mi)/p(D|Mj) for two models
is known as a Bayes factor (Kass and Raftery, 1995).
Once we know the posterior distribution over models, the predictive distribution

is given, from the sum and product rules, by

p(t|x,D) =

p(t|x,Mi,D)p(Mi|D).

(3.67)

This is an example of a mixture distribution in which the overall predictive distribu-
tion is obtained by averaging the predictive distributions p(t|x,Mi,D) of individual
models, weighted by the posterior probabilities p(Mi|D) of those models. For in-
stance, if we have two models that are a-posteriori equally likely and one predicts
a narrow distribution around t = a while the other predicts a narrow distribution
around t = b, the overall predictive distribution will be a bimodal distribution with
modes at t = a and t = b, not a single model at t = (a + b)/2.

A simple approximation to model averaging is to use the single most probable

model alone to make predictions. This is known as model selection.

For a model governed by a set of parameters w, the model evidence is given,

from the sum and product rules of probability, by

p(D|Mi) =

p(D|w,Mi)p(w|Mi) dw.

(3.68)

From a sampling perspective, the marginal likelihood can be viewed as the proba-
bility of generating the data set D from a model whose parameters are sampled at
random from the prior. It is also interesting to note that the evidence is precisely the
normalizing term that appears in the denominator in Bayes’ theorem when evaluating
the posterior distribution over parameters because

p(w|D,Mi) = p(D|w,Mi)p(w|Mi)

p(D|Mi)

.

(3.69)

We can obtain some insight into the model evidence by making a simple approx-
imation to the integral over parameters. Consider ﬁrst the case of a model having a
single parameter w. The posterior distribution over parameters is proportional to
p(D|w)p(w), where we omit the dependence on the model Mi to keep the notation
uncluttered. If we assume that the posterior distribution is sharply peaked around the
most probable value wMAP, with width ∆wposterior, then we can approximate the in-
tegral by the value of the integrand at its maximum times the width of the peak. If we
further assume that the prior is ﬂat with width ∆wprior so that p(w) = 1/∆wprior,
then we have

p(D) =

p(D|w)p(w) dw  p(D|wMAP)

∆wposterior

∆wprior

(3.70)

162

Chapter 11







.

3.4. Bayesian Model Comparison

∆wposterior

163

w

(3.71)

Figure 3.12 We can obtain a rough approximation to
the model evidence if we assume that
the posterior distribution over parame-
ters is sharply peaked around its mode
wMAP.

and so taking logs we obtain

ln p(D)  ln p(D|wMAP) + ln

wMAP

∆wprior

∆wposterior

∆wprior

This approximation is illustrated in Figure 3.12. The ﬁrst term represents the ﬁt to
the data given by the most probable parameter values, and for a ﬂat prior this would
correspond to the log likelihood. The second term penalizes the model according to
its complexity. Because ∆wposterior < ∆wprior this term is negative, and it increases
in magnitude as the ratio ∆wposterior/∆wprior gets smaller. Thus, if parameters are
ﬁnely tuned to the data in the posterior distribution, then the penalty term is large.

For a model having a set of M parameters, we can make a similar approximation
for each parameter in turn. Assuming that all parameters have the same ratio of
∆wposterior/∆wprior, we obtain

ln p(D)  ln p(D|wMAP) + M ln

∆wposterior

∆wprior

.

(3.72)

Thus, in this very simple approximation, the size of the complexity penalty increases
linearly with the number M of adaptive parameters in the model. As we increase
the complexity of the model, the ﬁrst term will typically decrease, because a more
complex model is better able to ﬁt the data, whereas the second term will increase
due to the dependence on M. The optimal model complexity, as determined by
the maximum evidence, will be given by a trade-off between these two competing
terms. We shall later develop a more reﬁned version of this approximation, based on
a Gaussian approximation to the posterior distribution.

We can gain further insight into Bayesian model comparison and understand
how the marginal likelihood can favour models of intermediate complexity by con-
sidering Figure 3.13. Here the horizontal axis is a one-dimensional representation
of the space of possible data sets, so that each point on this axis corresponds to a
speciﬁc data set. We now consider three models M1, M2 and M3 of successively
increasing complexity. Imagine running these models generatively to produce exam-
ple data sets, and then looking at the distribution of data sets that result. Any given

Section 4.4.1

164

3. LINEAR MODELS FOR REGRESSION

Figure 3.13 Schematic illustration of

the
distribution of data sets for
three models of different com-
in which M1 is the
plexity,
simplest and M3 is the most
complex. Note that the dis-
tributions are normalized.
In
this example,
for the partic-
ular observed data set D0,
the model M2 with intermedi-
ate complexity has the largest
evidence.

Bayes factor in the form

p(D)

M1

M2

D0

M3

D

model can generate a variety of different data sets since the parameters are governed
by a prior probability distribution, and for any choice of the parameters there may
be random noise on the target variables. To generate a particular data set from a spe-
ciﬁc model, we ﬁrst choose the values of the parameters from their prior distribution
p(w), and then for these parameter values we sample the data from p(D|w). A sim-
ple model (for example, based on a ﬁrst order polynomial) has little variability and
so will generate data sets that are fairly similar to each other. Its distribution p(D)
is therefore conﬁned to a relatively small region of the horizontal axis. By contrast,
a complex model (such as a ninth order polynomial) can generate a great variety of
different data sets, and so its distribution p(D) is spread over a large region of the
space of data sets. Because the distributions p(D|Mi) are normalized, we see that
the particular data set D0 can have the highest value of the evidence for the model
of intermediate complexity. Essentially, the simpler model cannot ﬁt the data well,
whereas the more complex model spreads its predictive probability over too broad a
range of data sets and so assigns relatively small probability to any one of them.

Implicit in the Bayesian model comparison framework is the assumption that
the true distribution from which the data are generated is contained within the set of
models under consideration. Provided this is so, we can show that Bayesian model
comparison will on average favour the correct model. To see this, consider two
models M1 and M2 in which the truth corresponds to M1. For a given ﬁnite data
set, it is possible for the Bayes factor to be larger for the incorrect model. However, if
we average the Bayes factor over the distribution of data sets, we obtain the expected

Section 1.6.1

p(D|M1) ln p(D|M1)
p(D|M2)

dD

(3.73)

where the average has been taken with respect to the true distribution of the data.
This quantity is an example of the Kullback-Leibler divergence and satisﬁes the prop-
erty of always being positive unless the two distributions are equal in which case it
is zero. Thus on average the Bayes factor will always favour the correct model.

We have seen that the Bayesian framework avoids the problem of over-ﬁtting
and allows models to be compared on the basis of the training data alone. However,




















3.5. The Evidence Approximation

165

a Bayesian approach, like any approach to pattern recognition, needs to make as-
sumptions about the form of the model, and if these are invalid then the results can
be misleading. In particular, we see from Figure 3.12 that the model evidence can
be sensitive to many aspects of the prior, such as the behaviour in the tails. Indeed,
the evidence is not deﬁned if the prior is improper, as can be seen by noting that
an improper prior has an arbitrary scaling factor (in other words, the normalization
coefﬁcient is not deﬁned because the distribution cannot be normalized). If we con-
sider a proper prior and then take a suitable limit in order to obtain an improper prior
(for example, a Gaussian prior in which we take the limit of inﬁnite variance) then
the evidence will go to zero, as can be seen from (3.70) and Figure 3.12. It may,
however, be possible to consider the evidence ratio between two models ﬁrst and
then take a limit to obtain a meaningful answer.

In a practical application, therefore, it will be wise to keep aside an independent

test set of data on which to evaluate the overall performance of the ﬁnal system.

3.5. The Evidence Approximation

In a fully Bayesian treatment of the linear basis function model, we would intro-
duce prior distributions over the hyperparameters α and β and make predictions by
marginalizing with respect to these hyperparameters as well as with respect to the
parameters w. However, although we can integrate analytically over either w or
over the hyperparameters, the complete marginalization over all of these variables
is analytically intractable. Here we discuss an approximation in which we set the
hyperparameters to speciﬁc values determined by maximizing the marginal likeli-
hood function obtained by ﬁrst integrating over the parameters w. This framework
is known in the statistics literature as empirical Bayes (Bernardo and Smith, 1994;
Gelman et al., 2004), or type 2 maximum likelihood (Berger, 1985), or generalized
maximum likelihood (Wahba, 1975), and in the machine learning literature is also
called the evidence approximation (Gull, 1989; MacKay, 1992a).

If we introduce hyperpriors over α and β, the predictive distribution is obtained

by marginalizing over w, α and β so that

p(t|t) =

p(t|w, β)p(w|t, α, β)p(α, β|t) dw dα dβ

(3.74)

where p(t|w, β) is given by (3.8) and p(w|t, α, β) is given by (3.49) with mN and
SN deﬁned by (3.53) and (3.54) respectively. Here we have omitted the dependence
on the input variable x to keep the notation uncluttered. If the posterior distribution
p(α, β|t) is sharply peaked around values
β, then the predictive distribution is
obtained simply by marginalizing over w in which α and β are ﬁxed to the values
α
and

β, so that

α and

p(t|t)  p(t|t,

α,

β) =

p(t|w,

β)p(w|t,

α,

β) dw.

(3.75)



















166

3. LINEAR MODELS FOR REGRESSION

From Bayes’ theorem, the posterior distribution for α and β is given by

p(α, β|t) ∝ p(t|α, β)p(α, β).

(3.76)

α and
If the prior is relatively ﬂat, then in the evidence framework the values of
β are obtained by maximizing the marginal likelihood function p(t|α, β). We shall
proceed by evaluating the marginal likelihood for the linear basis function model and
then ﬁnding its maxima. This will allow us to determine values for these hyperpa-
rameters from the training data alone, without recourse to cross-validation. Recall
that the ratio α/β is analogous to a regularization parameter.

As an aside it is worth noting that, if we deﬁne conjugate (Gamma) prior distri-
butions over α and β, then the marginalization over these hyperparameters in (3.74)
can be performed analytically to give a Student’s t-distribution over w (see Sec-
tion 2.3.7). Although the resulting integral over w is no longer analytically tractable,
it might be thought that approximating this integral, for example using the Laplace
approximation discussed (Section 4.4) which is based on a local Gaussian approxi-
mation centred on the mode of the posterior distribution, might provide a practical
alternative to the evidence framework (Buntine and Weigend, 1991). However, the
integrand as a function of w typically has a strongly skewed mode so that the Laplace
approximation fails to capture the bulk of the probability mass, leading to poorer re-
sults than those obtained by maximizing the evidence (MacKay, 1999).

Returning to the evidence framework, we note that there are two approaches that
we can take to the maximization of the log evidence. We can evaluate the evidence
function analytically and then set its derivative equal to zero to obtain re-estimation
equations for α and β, which we shall do in Section 3.5.2. Alternatively we use a
technique called the expectation maximization (EM) algorithm, which will be dis-
cussed in Section 9.3.4 where we shall also show that these two approaches converge
to the same solution.

3.5.1 Evaluation of the evidence function
The marginal likelihood function p(t|α, β) is obtained by integrating over the

weight parameters w, so that

p(t|α, β) =

p(t|w, β)p(w|α) dw.

(3.77)

Exercise 3.16

Exercise 3.17

One way to evaluate this integral is to make use once again of the result (2.115)
for the conditional distribution in a linear-Gaussian model. Here we shall evaluate
the integral instead by completing the square in the exponent and making use of the
standard form for the normalization coefﬁcient of a Gaussian.

From (3.11), (3.12), and (3.52), we can write the evidence function in the form

p(t|α, β) =

β
2π

N/2

M/2

α
2π

exp{−E(w)} dw

(3.78)







1
2



167

(3.79)

(3.80)

(3.81)

(3.82)

(3.83)

(3.84)

Exercise 3.18

Exercise 3.19

3.5. The Evidence Approximation

where M is the dimensionality of w, and we have deﬁned

E(w) = βED(w) + αEW (w)
2 t − Φw2 + α
2

= β

wTw.

We recognize (3.79) as being equal, up to a constant of proportionality, to the reg-
ularized sum-of-squares error function (3.27). We now complete the square over w
giving

E(w) = E(mN ) +

(w − mN )TA(w − mN )

A = αI + βΦTΦ

where we have introduced

together with

E(mN ) = β

2 t − ΦmN2 + α
2

mT

N mN .

Note that A corresponds to the matrix of second derivatives of the error function

and is known as the Hessian matrix. Here we have also deﬁned mN given by

A = ∇∇E(w)

mN = βA−1ΦTt.

Using (3.54), we see that A = S−1
deﬁnition (3.53), and therefore represents the mean of the posterior distribution.

N , and hence (3.84) is equivalent to the previous

The integral over w can now be evaluated simply by appealing to the standard

result for the normalization coefﬁcient of a multivariate Gaussian, giving

exp{−E(w)} dw
= exp{−E(mN )}
= exp{−E(mN )}(2π)M/2|A|−1/2.

exp

−

1
2

(w − mN )TA(w − mN )

dw

(3.85)

Using (3.78) we can then write the log of the marginal likelihood in the form

ln p(t|α, β) = M
2

ln α + N
2

ln β − E(mN ) −

1
2

ln|A| −

N
2

ln(2π)

(3.86)

which is the required expression for the evidence function.

Returning to the polynomial regression problem, we can plot the model evidence
against the order of the polynomial, as shown in Figure 3.14. Here we have assumed
a prior of the form (1.65) with the parameter α ﬁxed at α = 5 × 10−3. The form
of this plot is very instructive. Referring back to Figure 1.4, we see that the M = 0
polynomial has very poor ﬁt to the data and consequently gives a relatively low value

168

3. LINEAR MODELS FOR REGRESSION

Figure 3.14 Plot of the model evidence versus
the order M, for the polynomial re-
gression model, showing that the
evidence favours the model with
M = 3.

−18

−20

−22

−24

−26










i

2





0

6

8

4

M

for the evidence. Going to the M = 1 polynomial greatly improves the data ﬁt, and
hence the evidence is signiﬁcantly higher. However, in going to M = 2, the data
ﬁt is improved only very marginally, due to the fact that the underlying sinusoidal
function from which the data is generated is an odd function and so has no even terms
in a polynomial expansion. Indeed, Figure 1.5 shows that the residual data error is
reduced only slightly in going from M = 1 to M = 2. Because this richer model
suffers a greater complexity penalty, the evidence actually falls in going from M = 1
to M = 2. When we go to M = 3 we obtain a signiﬁcant further improvement in
data ﬁt, as seen in Figure 1.4, and so the evidence is increased again, giving the
highest overall evidence for any of the polynomials. Further increases in the value
of M produce only small improvements in the ﬁt to the data but suffer increasing
complexity penalty, leading overall to a decrease in the evidence values. Looking
again at Figure 1.5, we see that the generalization error is roughly constant between
M = 3 and M = 8, and it would be difﬁcult to choose between these models on
the basis of this plot alone. The evidence values, however, show a clear preference
for M = 3, since this is the simplest model which gives a good explanation for the
observed data.

3.5.2 Maximizing the evidence function
Let us ﬁrst consider the maximization of p(t|α, β) with respect to α. This can

be done by ﬁrst deﬁning the following eigenvector equation

(3.87)
From (3.81), it then follows that A has eigenvalues α + λi. Now consider the deriva-
tive of the term involving ln|A| in (3.86) with respect to α. We have

ui = λiui.

βΦTΦ

d
dα

ln|A| = d

dα

ln

i

(λi + α) = d
dα

ln(λi + α) =

1

λi + α

i

Thus the stationary points of (3.86) with respect to α satisfy

0 = M

2α −

1
2

mT

N mN −

1
2

1

λi + α

i

.

.

(3.88)

(3.89)










α =

γ =

1
2

n=1

N

i

i

N

1




i



169

(3.90)

(3.91)

(3.92)

(3.94)

(3.95)

3.5. The Evidence Approximation

Multiplying through by 2α and rearranging, we obtain

αmT

N mN = M − α

1

= γ.

λi + α

i

Since there are M terms in the sum over i, the quantity γ can be written

Exercise 3.20

The interpretation of the quantity γ will be discussed shortly. From (3.90) we see
that the value of α that maximizes the marginal likelihood satisﬁes

λi

α + λi

.

γ
mT
N mN

.

Note that this is an implicit solution for α not only because γ depends on α, but also
because the mode mN of the posterior distribution itself depends on the choice of
α. We therefore adopt an iterative procedure in which we make an initial choice for
α and use this to ﬁnd mN , which is given by (3.53), and also to evaluate γ, which
is given by (3.91). These values are then used to re-estimate α using (3.92), and the
process repeated until convergence. Note that because the matrix ΦTΦ is ﬁxed, we
can compute its eigenvalues once at the start and then simply multiply these by β to
obtain the λi.

It should be emphasized that the value of α has been determined purely by look-
ing at the training data. In contrast to maximum likelihood methods, no independent
data set is required in order to optimize the model complexity.

We can similarly maximize the log marginal likelihood (3.86) with respect to β.
To do this, we note that the eigenvalues λi deﬁned by (3.87) are proportional to β,
and hence dλi/dβ = λi/β giving

d
dβ

ln|A| = d

dβ

ln(λi + α) =

1
β

λi

λi + α

= γ
β

.

(3.93)

The stationary point of the marginal likelihood therefore satisﬁes

Exercise 3.22

and rearranging we obtain

0 = N

2β −

tn − mT

N φ(xn)

2

γ
2β

−

1
β

=

N − γ

n=1

tn − mT

N φ(xn)

2

.

Again, this is an implicit solution for β and can be solved by choosing an initial
value for β and then using this to calculate mN and γ and then re-estimate β using
(3.95), repeating until convergence. If both α and β are to be determined from the
data, then their values can be re-estimated together after each update of γ.

170

3. LINEAR MODELS FOR REGRESSION

Figure 3.15 Contours of the likelihood function (red)
and the prior (green) in which the axes in parameter
space have been rotated to align with the eigenvectors
ui of the Hessian. For α = 0, the mode of the poste-
rior is given by the maximum likelihood solution wML,
whereas for nonzero α the mode is at wMAP = mN . In
the direction w1 the eigenvalue λ1, deﬁned by (3.87), is
small compared with α and so the quantity λ1/(λ1 + α)
is close to zero, and the corresponding MAP value of
w1 is also close to zero. By contrast, in the direction w2
the eigenvalue λ2 is large compared with α and so the
quantity λ2/(λ2 +α) is close to unity, and the MAP value
of w2 is close to its maximum likelihood value.

w2

u2

wML

wMAP

u1

w1

3.5.3 Effective number of parameters
The result (3.92) has an elegant interpretation (MacKay, 1992a), which provides
insight into the Bayesian solution for α. To see this, consider the contours of the like-
lihood function and the prior as illustrated in Figure 3.15. Here we have implicitly
transformed to a rotated set of axes in parameter space aligned with the eigenvec-
tors ui deﬁned in (3.87). Contours of the likelihood function are then axis-aligned
ellipses. The eigenvalues λi measure the curvature of the likelihood function, and
so in Figure 3.15 the eigenvalue λ1 is small compared with λ2 (because a smaller
curvature corresponds to a greater elongation of the contours of the likelihood func-
tion). Because βΦTΦ is a positive deﬁnite matrix, it will have positive eigenvalues,
and so the ratio λi/(λi + α) will lie between 0 and 1. Consequently, the quantity γ
deﬁned by (3.91) will lie in the range 0 � γ � M. For directions in which λi 
 α,
the corresponding parameter wi will be close to its maximum likelihood value, and
the ratio λi/(λi + α) will be close to 1. Such parameters are called well determined
because their values are tightly constrained by the data. Conversely, for directions
in which λi  α, the corresponding parameters wi will be close to zero, as will the
ratios λi/(λi + α). These are directions in which the likelihood function is relatively
insensitive to the parameter value and so the parameter has been set to a small value
by the prior. The quantity γ deﬁned by (3.91) therefore measures the effective total
number of well determined parameters.

We can obtain some insight into the result (3.95) for re-estimating β by com-
paring it with the corresponding maximum likelihood result given by (3.21). Both
of these formulae express the variance (the inverse precision) as an average of the
squared differences between the targets and the model predictions. However, they
differ in that the number of data points N in the denominator of the maximum like-
lihood result is replaced by N − γ in the Bayesian result. We recall from (1.56) that
the maximum likelihood estimate of the variance for a Gaussian distribution over a




n=1

N

3.5. The Evidence Approximation

single variable x is given by

ML =
σ2

1
N

(xn − µML)2

and that this estimate is biased because the maximum likelihood solution µML for
the mean has ﬁtted some of the noise on the data. In effect, this has used up one
degree of freedom in the model. The corresponding unbiased estimate is given by
(1.59) and takes the form

171

(3.96)

MAP =
σ2

(xn − µML)2.

(3.97)

N

1

N − 1

n=1

We shall see in Section 10.1.3 that this result can be obtained from a Bayesian treat-
ment in which we marginalize over the unknown mean. The factor of N − 1 in the
denominator of the Bayesian result takes account of the fact that one degree of free-
dom has been used in ﬁtting the mean and removes the bias of maximum likelihood.
Now consider the corresponding results for the linear regression model. The mean
of the target distribution is now given by the function wTφ(x), which contains M
parameters. However, not all of these parameters are tuned to the data. The effective
number of parameters that are determined by the data is γ, with the remaining M −γ
parameters set to small values by the prior. This is reﬂected in the Bayesian result
for the variance that has a factor N − γ in the denominator, thereby correcting for
the bias of the maximum likelihood result.
We can illustrate the evidence framework for setting hyperparameters using the
sinusoidal synthetic data set from Section 1.1, together with the Gaussian basis func-
tion model comprising 9 basis functions, so that the total number of parameters in
the model is given by M = 10 including the bias. Here, for simplicity of illustra-
tion, we have set β to its true value of 11.1 and then used the evidence framework to
determine α, as shown in Figure 3.16.

We can also see how the parameter α controls the magnitude of the parameters
{wi}, by plotting the individual parameters versus the effective number γ of param-
eters, as shown in Figure 3.17.
If we consider the limit N 
 M in which the number of data points is large in
relation to the number of parameters, then from (3.87) all of the parameters will be
well determined by the data because ΦTΦ involves an implicit sum over data points,
and so the eigenvalues λi increase with the size of the data set. In this case, γ = M,
and the re-estimation equations for α and β become

α =

β =

M

2EW (mN )

N

2ED(mN )

(3.98)

(3.99)

where EW and ED are deﬁned by (3.25) and (3.26), respectively. These results
can be used as an easy-to-compute approximation to the full evidence re-estimation

172

3. LINEAR MODELS FOR REGRESSION

−5

0
ln α

5

−5

0
ln α

5

Figure 3.16 The left plot shows γ (red curve) and 2αEW (mN ) (blue curve) versus ln α for the sinusoidal
synthetic data set. It is the intersection of these two curves that deﬁnes the optimum value for α given by the
evidence procedure. The right plot shows the corresponding graph of log evidence ln p(t|α, β) versus ln α (red
curve) showing that the peak coincides with the crossing point of the curves in the left plot. Also shown is the
test set error (blue curve) showing that the evidence maximum occurs close to the point of best generalization.

formulae, because they do not require evaluation of the eigenvalue spectrum of the
Hessian.

Figure 3.17 Plot of

the 10 parameters wi
from the Gaussian basis function
model versus the effective num-
ber of parameters γ, in which the
hyperparameter α is varied in the
range 0 � α � ∞ causing γ to
vary in the range 0 � γ � M.

wi

2

1

0

−1

−2

0

8

4

5

2

6

3

1

7

9

0

2

4

6

8

γ

10

3.6. Limitations of Fixed Basis Functions

Throughout this chapter, we have focussed on models comprising a linear combina-
tion of ﬁxed, nonlinear basis functions. We have seen that the assumption of linearity
in the parameters led to a range of useful properties including closed-form solutions
to the least-squares problem, as well as a tractable Bayesian treatment. Furthermore,
for a suitable choice of basis functions, we can model arbitrary nonlinearities in the




j=1

M

M







Exercises

173

mapping from input variables to targets. In the next chapter, we shall study an anal-
ogous class of models for classiﬁcation.

It might appear, therefore, that such linear models constitute a general purpose
framework for solving problems in pattern recognition. Unfortunately, there are
some signiﬁcant shortcomings with linear models, which will cause us to turn in
later chapters to more complex models such as support vector machines and neural
networks.

The difﬁculty stems from the assumption that the basis functions φj(x) are ﬁxed
before the training data set is observed and is a manifestation of the curse of dimen-
sionality discussed in Section 1.4. As a consequence, the number of basis functions
needs to grow rapidly, often exponentially, with the dimensionality D of the input
space.

Fortunately, there are two properties of real data sets that we can exploit to help
alleviate this problem. First of all, the data vectors {xn} typically lie close to a non-
linear manifold whose intrinsic dimensionality is smaller than that of the input space
as a result of strong correlations between the input variables. We will see an example
of this when we consider images of handwritten digits in Chapter 12. If we are using
localized basis functions, we can arrange that they are scattered in input space only
in regions containing data. This approach is used in radial basis function networks
and also in support vector and relevance vector machines. Neural network models,
which use adaptive basis functions having sigmoidal nonlinearities, can adapt the
parameters so that the regions of input space over which the basis functions vary
corresponds to the data manifold. The second property is that target variables may
have signiﬁcant dependence on only a small number of possible directions within the
data manifold. Neural networks can exploit this property by choosing the directions
in input space to which the basis functions respond.

Exercises

3.1 () www Show that the ‘tanh’ function and the logistic sigmoid function (3.6)

are related by

Hence show that a general linear combination of logistic sigmoid functions of the
form

tanh(a) = 2σ(2a) − 1.

is equivalent to a linear combination of ‘tanh’ functions of the form

y(x, w) = w0 +

wjσ

x − µj

s

y(x, u) = u0 +

uj tanh

j=1

x − µj

s

and ﬁnd expressions to relate the new parameters {u1, . . . , uM} to the original pa-
rameters {w1, . . . , wM}.

(3.100)

(3.101)

(3.102)








n=1

N

N



D

174

3. LINEAR MODELS FOR REGRESSION

3.2 ( ) Show that the matrix

Φ(ΦTΦ)−1ΦT

(3.103)
takes any vector v and projects it onto the space spanned by the columns of Φ. Use
this result to show that the least-squares solution (3.15) corresponds to an orthogonal
projection of the vector t onto the manifold S as shown in Figure 3.2.

3.3 () Consider a data set in which each data point tn is associated with a weighting

factor rn > 0, so that the sum-of-squares error function becomes

ED(w) =

1
2

rn

tn − wTφ(xn)

2

.

(3.104)

Find an expression for the solution w that minimizes this error function. Give two
alternative interpretations of the weighted sum-of-squares error function in terms of
(i) data dependent noise variance and (ii) replicated data points.

3.4 () www Consider a linear model of the form

y(x, w) = w0 +

wixi

i=1

together with a sum-of-squares error function of the form

ED(w) =

1
2

{y(xn, w) − tn}2 .

n=1

(3.105)

(3.106)

Now suppose that Gaussian noise 	i with zero mean and variance σ2 is added in-
dependently to each of the input variables xi. By making use of E[	i] = 0 and
E[	i	j] = δijσ2, show that minimizing ED averaged over the noise distribution is
equivalent to minimizing the sum-of-squares error for noise-free input variables with
the addition of a weight-decay regularization term, in which the bias parameter w0
is omitted from the regularizer.

3.5 () www Using the technique of Lagrange multipliers, discussed in Appendix E,
show that minimization of the regularized error function (3.29) is equivalent to mini-
mizing the unregularized sum-of-squares error (3.12) subject to the constraint (3.30).
Discuss the relationship between the parameters η and λ.

3.6 () www Consider a linear basis function regression model for a multivariate

target variable t having a Gaussian distribution of the form
p(t|W, Σ) = N (t|y(x, W), Σ)

where

y(x, W) = WTφ(x)

(3.107)

(3.108)











n=1

N










Exercises

175

together with a training data set comprising input basis vectors φ(xn) and corre-
sponding target vectors tn, with n = 1, . . . , N. Show that the maximum likelihood
solution WML for the parameter matrix W has the property that each column is
given by an expression of the form (3.15), which was the solution for an isotropic
noise distribution. Note that this is independent of the covariance matrix Σ. Show
that the maximum likelihood solution for Σ is given by

Σ =

1
N

tn − WT

MLφ(xn)

tn − WT

MLφ(xn)

T

.

(3.109)

3.7 () By using the technique of completing the square, verify the result (3.49) for the
posterior distribution of the parameters w in the linear basis function model in which
mN and SN are deﬁned by (3.50) and (3.51) respectively.

3.8 ( ) www Consider the linear basis function model in Section 3.1, and suppose
that we have already observed N data points, so that the posterior distribution over
w is given by (3.49). This posterior can be regarded as the prior for the next obser-
vation. By considering an additional data point (xN +1, tN +1), and by completing
the square in the exponential, show that the resulting posterior distribution is again
given by (3.49) but with SN replaced by SN +1 and mN replaced by mN +1.

3.9 ( ) Repeat the previous exercise but instead of completing the square by hand,

make use of the general result for linear-Gaussian models given by (2.116).

3.10 ( ) www By making use of the result (2.115) to evaluate the integral in (3.57),
verify that the predictive distribution for the Bayesian linear regression model is
given by (3.58) in which the input-dependent variance is given by (3.59).

3.11 ( ) We have seen that, as the size of a data set increases, the uncertainty associated
with the posterior distribution over model parameters decreases. Make use of the
matrix identity (Appendix C)

to show that the uncertainty σ2
given by (3.59) satisﬁes

M + vvT

(M−1v)

vTM−1
1 + vTM−1v

−1 = M−1 −
N (x) associated with the linear regression function

(3.110)

N +1(x) � σ2
σ2

N (x).

(3.111)

3.12 ( ) We saw in Section 2.3.6 that the conjugate prior for a Gaussian distribution
with unknown mean and unknown precision (inverse variance) is a normal-gamma
distribution. This property also holds for the case of the conditional Gaussian dis-
tribution p(t|x, w, β) of the linear regression model. If we consider the likelihood
function (3.10), then the conjugate prior for w and β is given by
p(w, β) = N (w|m0, β−1S0)Gam(β|a0, b0).

(3.112)



N

n=1



N

Show that the corresponding posterior distribution takes the same functional form,
so that

p(w, β|t) = N (w|mN , β−1SN )Gam(β|aN , bN )

(3.113)

and ﬁnd expressions for the posterior parameters mN , SN , aN , and bN .

3.13 ( ) Show that the predictive distribution p(t|x, t) for the model discussed in Ex-

ercise 3.12 is given by a Student’s t-distribution of the form

p(t|x, t) = St(t|µ, λ, ν)

(3.114)

and obtain expressions for µ, λ and ν.

3.14 ( )

In this exercise, we explore in more detail the properties of the equivalent
kernel deﬁned by (3.62), where SN is deﬁned by (3.54). Suppose that the basis
functions φj(x) are linearly independent and that the number N of data points is
greater than the number M of basis functions. Furthermore, let one of the basis
functions be constant, say φ0(x) = 1. By taking suitable linear combinations of
these basis functions, we can construct a new basis set ψj(x) spanning the same
space but that are orthonormal, so that

176

3. LINEAR MODELS FOR REGRESSION

ψj(xn)ψk(xn) = Ijk

(3.115)

where Ijk is deﬁned to be 1 if j = k and 0 otherwise, and we take ψ0(x) = 1. Show
that for α = 0, the equivalent kernel can be written as k(x, x) = ψ(x)Tψ(x)
where ψ = (ψ1, . . . , ψM )T. Use this result to show that the kernel satisﬁes the
summation constraint

k(x, xn) = 1.

n=1

(3.116)

3.15 () www Consider a linear basis function model for regression in which the pa-
rameters α and β are set using the evidence framework. Show that the function
E(mN ) deﬁned by (3.82) satisﬁes the relation 2E(mN ) = N.

3.16 ( ) Derive the result (3.86) for the log evidence function p(t|α, β) of the linear
regression model by making use of (2.115) to evaluate the integral (3.77) directly.

3.17 () Show that the evidence function for the Bayesian linear regression model can

be written in the form (3.78) in which E(w) is deﬁned by (3.79).

3.18 ( ) www By completing the square over w, show that the error function (3.79)

in Bayesian linear regression can be written in the form (3.80).

3.19 ( ) Show that the integration over w in the Bayesian linear regression model gives

the result (3.85). Hence show that the log marginal likelihood is given by (3.86).





3.20 ( ) www Starting from (3.86) verify all of the steps needed to show that maxi-
mization of the log marginal likelihood function (3.86) with respect to α leads to the
re-estimation equation (3.92).

Exercises

177

3.21 ( ) An alternative way to derive the result (3.92) for the optimal value of α in the

evidence framework is to make use of the identity

d
dα

ln|A| = Tr

A−1 d
dα

A

.

(3.117)

Prove this identity by considering the eigenvalue expansion of a real, symmetric
matrix A, and making use of the standard results for the determinant and trace of
A expressed in terms of its eigenvalues (Appendix C). Then make use of (3.117) to
derive (3.92) starting from (3.86).

3.22 ( ) Starting from (3.86) verify all of the steps needed to show that maximiza-
tion of the log marginal likelihood function (3.86) with respect to β leads to the
re-estimation equation (3.95).

3.23 ( ) www Show that the marginal probability of the data, in other words the

model evidence, for the model described in Exercise 3.12 is given by

p(t) =

1

(2π)N/2

ba0
0
baN
N

Γ(aN )
Γ(a0)

|SN|1/2
|S0|1/2

by ﬁrst marginalizing with respect to w and then with respect to β.

3.24 ( ) Repeat the previous exercise but now use Bayes’ theorem in the form

p(t) = p(t|w, β)p(w, β)

p(w, β|t)

(3.118)

(3.119)

and then substitute for the prior and posterior distributions and the likelihood func-
tion in order to derive the result (3.118).

4

Linear

Models for

Classiﬁcation

In the previous chapter, we explored a class of regression models having particularly
simple analytical and computational properties. We now discuss an analogous class
of models for solving classiﬁcation problems. The goal in classiﬁcation is to take an
input vector x and to assign it to one of K discrete classes Ck where k = 1, . . . , K.
In the most common scenario, the classes are taken to be disjoint, so that each input is
assigned to one and only one class. The input space is thereby divided into decision
regions whose boundaries are called decision boundaries or decision surfaces. In
this chapter, we consider linear models for classiﬁcation, by which we mean that the
decision surfaces are linear functions of the input vector x and hence are deﬁned
by (D − 1)-dimensional hyperplanes within the D-dimensional input space. Data
sets whose classes can be separated exactly by linear decision surfaces are said to be
linearly separable.

For regression problems, the target variable t was simply the vector of real num-
bers whose values we wish to predict. In the case of classiﬁcation, there are various

179

180

4. LINEAR MODELS FOR CLASSIFICATION






ways of using target values to represent class labels. For probabilistic models, the
most convenient, in the case of two-class problems, is the binary representation in
which there is a single target variable t ∈ {0, 1} such that t = 1 represents class C1
and t = 0 represents class C2. We can interpret the value of t as the probability that
the class is C1, with the values of probability taking only the extreme values of 0 and
1. For K > 2 classes, it is convenient to use a 1-of-K coding scheme in which t is
a vector of length K such that if the class is Cj, then all elements tk of t are zero
except element tj, which takes the value 1. For instance, if we have K = 5 classes,
then a pattern from class 2 would be given the target vector

t = (0, 1, 0, 0, 0)T.

(4.1)

Again, we can interpret the value of tk as the probability that the class is Ck. For
nonprobabilistic models, alternative choices of target variable representation will
sometimes prove convenient.

In Chapter 1, we identiﬁed three distinct approaches to the classiﬁcation prob-
lem. The simplest involves constructing a discriminant function that directly assigns
each vector x to a speciﬁc class. A more powerful approach, however, models the
conditional probability distribution p(Ck|x) in an inference stage, and then subse-
quently uses this distribution to make optimal decisions. By separating inference
and decision, we gain numerous beneﬁts, as discussed in Section 1.5.4. There are
two different approaches to determining the conditional probabilities p(Ck|x). One
technique is to model them directly, for example by representing them as parametric
models and then optimizing the parameters using a training set. Alternatively, we
can adopt a generative approach in which we model the class-conditional densities
given by p(x|Ck), together with the prior probabilities p(Ck) for the classes, and then
we compute the required posterior probabilities using Bayes’ theorem

p(Ck|x) = p(x|Ck)p(Ck)

p(x)

.

(4.2)

We shall discuss examples of all three approaches in this chapter.

In the linear regression models considered in Chapter 3, the model prediction
y(x, w) was given by a linear function of the parameters w. In the simplest case,
the model is also linear in the input variables and therefore takes the form y(x) =
wTx + w0, so that y is a real number. For classiﬁcation problems, however, we wish
to predict discrete class labels, or more generally posterior probabilities that lie in
the range (0, 1). To achieve this, we consider a generalization of this model in which
we transform the linear function of w using a nonlinear function f(· ) so that

y(x) = f

wTx + w0

(4.3)
In the machine learning literature f(· ) is known as an activation function, whereas
its inverse is called a link function in the statistics literature. The decision surfaces
correspond to y(x) = constant, so that wTx + w0 = constant and hence the deci-
sion surfaces are linear functions of x, even if the function f(·) is nonlinear. For this
reason, the class of models described by (4.3) are called generalized linear models

.

4.1. Discriminant Functions

181

(McCullagh and Nelder, 1989). Note, however, that in contrast to the models used
for regression, they are no longer linear in the parameters due to the presence of the
nonlinear function f(·). This will lead to more complex analytical and computa-
tional properties than for linear regression models. Nevertheless, these models are
still relatively simple compared to the more general nonlinear models that will be
studied in subsequent chapters.

The algorithms discussed in this chapter will be equally applicable if we ﬁrst
make a ﬁxed nonlinear transformation of the input variables using a vector of basis
functions φ(x) as we did for regression models in Chapter 3. We begin by consider-
ing classiﬁcation directly in the original input space x, while in Section 4.3 we shall
ﬁnd it convenient to switch to a notation involving basis functions for consistency
with later chapters.

4.1. Discriminant Functions

A discriminant is a function that takes an input vector x and assigns it to one of K
classes, denoted Ck. In this chapter, we shall restrict attention to linear discriminants,
namely those for which the decision surfaces are hyperplanes. To simplify the dis-
cussion, we consider ﬁrst the case of two classes and then investigate the extension
to K > 2 classes.

4.1.1 Two classes
The simplest representation of a linear discriminant function is obtained by tak-

ing a linear function of the input vector so that

y(x) = wTx + w0

(4.4)

where w is called a weight vector, and w0 is a bias (not to be confused with bias in
the statistical sense). The negative of the bias is sometimes called a threshold. An
input vector x is assigned to class C1 if y(x) � 0 and to class C2 otherwise. The cor-
responding decision boundary is therefore deﬁned by the relation y(x) = 0, which
corresponds to a (D − 1)-dimensional hyperplane within the D-dimensional input
space. Consider two points xA and xB both of which lie on the decision surface.
Because y(xA) = y(xB) = 0, we have wT(xA − xB) = 0 and hence the vector w is
orthogonal to every vector lying within the decision surface, and so w determines the
orientation of the decision surface. Similarly, if x is a point on the decision surface,
then y(x) = 0, and so the normal distance from the origin to the decision surface is
given by

We therefore see that the bias parameter w0 determines the location of the decision
surface. These properties are illustrated for the case of D = 2 in Figure 4.1.

Furthermore, we note that the value of y(x) gives a signed measure of the per-
pendicular distance r of the point x from the decision surface. To see this, consider

wTx
w

.

(4.5)

= −

w0
w

182

4. LINEAR MODELS FOR CLASSIFICATION

Figure 4.1 Illustration of the geometry of a
linear discriminant function in two dimensions.
The decision surface, shown in red, is perpen-
dicular to w, and its displacement from the
origin is controlled by the bias parameter w0.
Also, the signed orthogonal distance of a gen-
eral point x from the decision surface is given
by y(x)/w.

y > 0

y = 0

y < 0

x2

R1

R2

x

y(x)
w

x1

x⊥

−w0
w






w



r = y(x)
w

.

y(x) =

wT

x.

(4.7)

(4.8)

an arbitrary point x and let x⊥ be its orthogonal projection onto the decision surface,
so that
(4.6)

w
w
Multiplying both sides of this result by wT and adding w0, and making use of y(x) =
wTx + w0 and y(x⊥) = wTx⊥ + w0 = 0, we have

x = x⊥ + r

.

This result is illustrated in Figure 4.1.

As with the linear regression models in Chapter 3, it is sometimes convenient
to use a more compact notation in which we introduce an additional dummy ‘input’
value x0 = 1 and then deﬁne

x = (x0, x) so that

w = (w0, w) and

In this case, the decision surfaces are D-dimensional hyperplanes passing through
the origin of the D + 1-dimensional expanded input space.

4.1.2 Multiple classes
Now consider the extension of linear discriminants to K > 2 classes. We might
be tempted be to build a K-class discriminant by combining a number of two-class
discriminant functions. However, this leads to some serious difﬁculties (Duda and
Hart, 1973) as we now show.

Consider the use of K−1 classiﬁers each of which solves a two-class problem of
separating points in a particular class Ck from points not in that class. This is known
as a one-versus-the-rest classiﬁer. The left-hand example in Figure 4.2 shows an





4.1. Discriminant Functions

183

C1

R1

C3

R3

?

R2

C3

C2

C1

C2

?

R1

not C1

R3

not C2

R2

C2

C1

Figure 4.2 Attempting to construct a K class discriminant from a set of two class discriminants leads to am-
biguous regions, shown in green. On the left is an example involving the use of two discriminants designed to
distinguish points in class Ck from points not in class Ck. On the right is an example involving three discriminant
functions each of which is used to separate a pair of classes Ck and Cj.

example involving three classes where this approach leads to regions of input space
that are ambiguously classiﬁed.

An alternative is to introduce K(K − 1)/2 binary discriminant functions, one
for every possible pair of classes. This is known as a one-versus-one classiﬁer. Each
point is then classiﬁed according to a majority vote amongst the discriminant func-
tions. However, this too runs into the problem of ambiguous regions, as illustrated
in the right-hand diagram of Figure 4.2.

We can avoid these difﬁculties by considering a single K-class discriminant

comprising K linear functions of the form

yk(x) = wT

(4.9)
and then assigning a point x to class Ck if yk(x) > yj(x) for all j = k. The decision
boundary between class Ck and class Cj is therefore given by yk(x) = yj(x) and
hence corresponds to a (D − 1)-dimensional hyperplane deﬁned by

k x + wk0

(wk − wj)Tx + (wk0 − wj0) = 0.

(4.10)

This has the same form as the decision boundary for the two-class case discussed in
Section 4.1.1, and so analogous geometrical properties apply.

The decision regions of such a discriminant are always singly connected and
convex. To see this, consider two points xA and xB both of which lie inside decision
x that lies on the line connecting
region Rk, as illustrated in Figure 4.3. Any point
xA and xB can be expressed in the form

x = λxA + (1 − λ)xB

(4.11)

b



184

4. LINEAR MODELS FOR CLASSIFICATION

Figure 4.3 Illustration of the decision regions for a mul-
ticlass linear discriminant, with the decision
boundaries shown in red.
If two points xA
and xB both lie inside the same decision re-
gion Rk, then any point
x that lies on the line
connecting these two points must also lie in
Rk, and hence the decision region must be
singly connected and convex.

Ri

xA

Rj

Rk

ˆx

xB

where 0 � λ � 1. From the linearity of the discriminant functions, it follows that

yk(

x) = λyk(xA) + (1 − λ)yk(xB).

(4.12)
Because both xA and xB lie inside Rk, it follows that yk(xA) > yj(xA), and
yk(xB) > yj(xB), for all j = k, and hence yk(
x also lies
inside Rk. Thus Rk is singly connected and convex.
Note that for two classes, we can either employ the formalism discussed here,
based on two discriminant functions y1(x) and y2(x), or else use the simpler but
equivalent formulation described in Section 4.1.1 based on a single discriminant
function y(x).

x), and so

x) > yj(

We now explore three approaches to learning the parameters of linear discrimi-
nant functions, based on least squares, Fisher’s linear discriminant, and the percep-
tron algorithm.

4.1.3 Least squares for classiﬁcation
In Chapter 3, we considered models that were linear functions of the parame-
ters, and we saw that the minimization of a sum-of-squares error function led to a
simple closed-form solution for the parameter values. It is therefore tempting to see
if we can apply the same formalism to classiﬁcation problems. Consider a general
classiﬁcation problem with K classes, with a 1-of-K binary coding scheme for the
target vector t. One justiﬁcation for using least squares in such a context is that it
approximates the conditional expectation E[t|x] of the target values given the input
vector. For the binary coding scheme, this conditional expectation is given by the
vector of posterior class probabilities. Unfortunately, however, these probabilities
are typically approximated rather poorly, indeed the approximations can have values
outside the range (0, 1), due to the limited ﬂexibility of a linear model as we shall
see shortly.

Each class Ck is described by its own linear model so that

where k = 1, . . . , K. We can conveniently group these together using vector nota-
tion so that

yk(x) = wT

k x + wk0

y(x) =

WT

x

(4.13)

(4.14)







,







,
,
W in the form,

ED(

,











,




,


T



,


,
,



X

4.1. Discriminant Functions

185

k )T and

W is a matrix whose kth column comprises the D + 1-dimensional vector
where
x is the corresponding augmented input vector (1, xT)T with
wk = (wk0, wT
a dummy input x0 = 1. This representation was discussed in detail in Section 3.1. A
new input x is then assigned to the class for which the output yk =
x is largest.
W by minimizing a sum-of-squares
error function, as we did for regression in Chapter 3. Consider a training data set
{xn, tn} where n = 1, . . . , N, and deﬁne a matrix T whose nth row is the vector tT
n,
xT
n. The sum-of-squares error function
together with a matrix
can then be written as

We now determine the parameter matrix

X whose nth row is

wT
k

W) =

Tr

(

W − T)T(

X

W − T)

.

(4.15)

1
2

Setting the derivative with respect to
solution for

W to zero, and rearranging, we then obtain the

W = (

XT

X)−1

XTT =

X†T

(4.16)

X† is the pseudo-inverse of the matrix

where
then obtain the discriminant function in the form

X, as discussed in Section 3.1.1. We

y(x) =

WT

x = TT

X†

x.

(4.17)

Exercise 4.2

Section 2.3.7

An interesting property of least-squares solutions with multiple target variables

is that if every target vector in the training set satisﬁes some linear constraint

aTtn + b = 0

(4.18)

for some constants a and b, then the model prediction for any value of x will satisfy
the same constraint so that

aTy(x) + b = 0.

(4.19)
Thus if we use a 1-of-K coding scheme for K classes, then the predictions made
by the model will have the property that the elements of y(x) will sum to 1 for any
value of x. However, this summation constraint alone is not sufﬁcient to allow the
model outputs to be interpreted as probabilities because they are not constrained to
lie within the interval (0, 1).

The least-squares approach gives an exact closed-form solution for the discrimi-
nant function parameters. However, even as a discriminant function (where we use it
to make decisions directly and dispense with any probabilistic interpretation) it suf-
fers from some severe problems. We have already seen that least-squares solutions
lack robustness to outliers, and this applies equally to the classiﬁcation application,
as illustrated in Figure 4.4. Here we see that the additional data points in the right-
hand ﬁgure produce a signiﬁcant change in the location of the decision boundary,
even though these point would be correctly classiﬁed by the original decision bound-
ary in the left-hand ﬁgure. The sum-of-squares error function penalizes predictions
that are ‘too correct’ in that they lie a long way on the correct side of the decision

186

4. LINEAR MODELS FOR CLASSIFICATION

4

2

0

−2

−4

−6

−8

4

2

0

−2

−4

−6

−8

−4

−2

0

2

4

6

8

−4

−2

0

2

4

6

8

Figure 4.4 The left plot shows data from two classes, denoted by red crosses and blue circles, together with
the decision boundary found by least squares (magenta curve) and also by the logistic regression model (green
curve), which is discussed later in Section 4.3.2. The right-hand plot shows the corresponding results obtained
when extra data points are added at the bottom left of the diagram, showing that least squares is highly sensitive
to outliers, unlike logistic regression.

boundary. In Section 7.1.2, we shall consider several alternative error functions for
classiﬁcation and we shall see that they do not suffer from this difﬁculty.

However, problems with least squares can be more severe than simply lack of
robustness, as illustrated in Figure 4.5. This shows a synthetic data set drawn from
three classes in a two-dimensional input space (x1, x2), having the property that lin-
ear decision boundaries can give excellent separation between the classes. Indeed,
the technique of logistic regression, described later in this chapter, gives a satisfac-
tory solution as seen in the right-hand plot. However, the least-squares solution gives
poor results, with only a small region of the input space assigned to the green class.
The failure of least squares should not surprise us when we recall that it cor-
responds to maximum likelihood under the assumption of a Gaussian conditional
distribution, whereas binary target vectors clearly have a distribution that is far from
Gaussian. By adopting more appropriate probabilistic models, we shall obtain clas-
siﬁcation techniques with much better properties than least squares. For the moment,
however, we continue to explore alternative nonprobabilistic methods for setting the
parameters in the linear classiﬁcation models.

4.1.4 Fisher’s linear discriminant
One way to view a linear classiﬁcation model is in terms of dimensionality
reduction. Consider ﬁrst the case of two classes, and suppose we take the D-





0

4.1. Discriminant Functions

187

6

4

2

0

−2

−4

−6

−6

−4

−2

0

2

4

6

6

4

2

0

−2

−4

−6

−6

−4

−2

Figure 4.5 Example of a synthetic data set comprising three classes, with training data points denoted in red
(×), green (+), and blue (◦). Lines denote the decision boundaries, and the background colours denote the
respective classes of the decision regions. On the left is the result of using a least-squares discriminant. We see
that the region of input space assigned to the green class is too small and so most of the points from this class
are misclassiﬁed. On the right is the result of using logistic regressions as described in Section 4.3.2 showing
correct classiﬁcation of the training data.

2

4

6

dimensional input vector x and project it down to one dimension using

y = wTx.

(4.20)
If we place a threshold on y and classify y � −w0 as class C1, and otherwise class
C2, then we obtain our standard linear classiﬁer discussed in the previous section.
In general, the projection onto one dimension leads to a considerable loss of infor-
mation, and classes that are well separated in the original D-dimensional space may
become strongly overlapping in one dimension. However, by adjusting the com-
ponents of the weight vector w, we can select a projection that maximizes the class
separation. To begin with, consider a two-class problem in which there are N1 points
of class C1 and N2 points of class C2, so that the mean vectors of the two classes are
given by
(4.21)

m1 =

xn.

xn,

m2 =

1
N1

n ∈ C1

1
N2

n ∈ C2

The simplest measure of the separation of the classes, when projected onto w, is the
separation of the projected class means. This suggests that we might choose w so as
to maximize

where

m2 − m1 = wT(m2 − m1)

mk = wTmk

(4.22)

(4.23)





n∈Ck

4. LINEAR MODELS FOR CLASSIFICATION

4

2

0

−2

−2

2

6

188

4

2

0

−2

Appendix E
Exercise 4.4

Figure 4.6 The left plot shows samples from two classes (depicted in red and blue) along with the histograms
resulting from projection onto the line joining the class means. Note that there is considerable class overlap in
the projected space. The right plot shows the corresponding projection based on the Fisher linear discriminant,
showing the greatly improved class separation.

−2

2

6

i w2

is the mean of the projected data from class Ck. However, this expression can be
made arbitrarily large simply by increasing the magnitude of w. To solve this
i = 1. Using
problem, we could constrain w to have unit length, so that
a Lagrange multiplier to perform the constrained maximization, we then ﬁnd that
w ∝ (m2 − m1). There is still a problem with this approach, however, as illustrated
in Figure 4.6. This shows two classes that are well separated in the original two-
dimensional space (x1, x2) but that have considerable overlap when projected onto
the line joining their means. This difﬁculty arises from the strongly nondiagonal
covariances of the class distributions. The idea proposed by Fisher is to maximize
a function that will give a large separation between the projected class means while
also giving a small variance within each class, thereby minimizing the class overlap.
The projection formula (4.20) transforms the set of labelled data points in x
into a labelled set in the one-dimensional space y. The within-class variance of the
transformed data from class Ck is therefore given by
(yn − mk)2

k =
s2

(4.24)

where yn = wTxn. We can deﬁne the total within-class variance for the whole
2. The Fisher criterion is deﬁned to be the ratio of the
data set to be simply s2
between-class variance to the within-class variance and is given by

1 + s2

J(w) =

(m2 − m1)2

1 + s2
s2

2

.

(4.25)

Exercise 4.5

We can make the dependence on w explicit by using (4.20), (4.23), and (4.24) to
rewrite the Fisher criterion in the form





4.1. Discriminant Functions

J(w) =

wTSBw
wTSWw

189

(4.26)

(4.27)

where SB is the between-class covariance matrix and is given by

SB = (m2 − m1)(m2 − m1)T
and SW is the total within-class covariance matrix, given by

SW =

n∈C1

(xn − m1)(xn − m1)T +

n∈C2

(xn − m2)(xn − m2)T.

(4.28)

Differentiating (4.26) with respect to w, we ﬁnd that J(w) is maximized when

(wTSBw)SWw = (wTSWw)SBw.

(4.29)
From (4.27), we see that SBw is always in the direction of (m2−m1). Furthermore,
we do not care about the magnitude of w, only its direction, and so we can drop the
scalar factors (wTSBw) and (wTSWw). Multiplying both sides of (4.29) by S−1
W
we then obtain

(4.30)
Note that if the within-class covariance is isotropic, so that SW is proportional to the
unit matrix, we ﬁnd that w is proportional to the difference of the class means, as
discussed above.

W (m2 − m1).

w ∝ S−1

The result (4.30) is known as Fisher’s linear discriminant, although strictly it
is not a discriminant but rather a speciﬁc choice of direction for projection of the
data down to one dimension. However, the projected data can subsequently be used
to construct a discriminant, by choosing a threshold y0 so that we classify a new
point as belonging to C1 if y(x) � y0 and classify it as belonging to C2 otherwise.
For example, we can model the class-conditional densities p(y|Ck) using Gaussian
distributions and then use the techniques of Section 1.2.4 to ﬁnd the parameters
of the Gaussian distributions by maximum likelihood. Having found Gaussian ap-
proximations to the projected classes, the formalism of Section 1.5.1 then gives an
expression for the optimal threshold. Some justiﬁcation for the Gaussian assumption
comes from the central limit theorem by noting that y = wTx is the sum of a set of
random variables.

4.1.5 Relation to least squares
The least-squares approach to the determination of a linear discriminant was
based on the goal of making the model predictions as close as possible to a set of
target values. By contrast, the Fisher criterion was derived by requiring maximum
class separation in the output space. It is interesting to see the relationship between
these two approaches. In particular, we shall show that, for the two-class problem,
the Fisher criterion can be obtained as a special case of least squares.

So far we have considered 1-of-K coding for the target values. If, however, we
adopt a slightly different target coding scheme, then the least-squares solution for




















E =

1
2

n=1

n=1

N

n=1

n=1

N

N

N





190

4. LINEAR MODELS FOR CLASSIFICATION

the weights becomes equivalent to the Fisher solution (Duda and Hart, 1973). In
particular, we shall take the targets for class C1 to be N/N1, where N1 is the number
of patterns in class C1, and N is the total number of patterns. This target value
approximates the reciprocal of the prior probability for class C1. For class C2, we
shall take the targets to be −N/N2, where N2 is the number of patterns in class C2.

The sum-of-squares error function can be written

wTxn + w0 − tn

2

.

(4.31)

Setting the derivatives of E with respect to w0 and w to zero, we obtain respectively

wTxn + w0 − tn

= 0

wTxn + w0 − tn

xn = 0.

(4.32)

(4.33)

From (4.32), and making use of our choice of target coding scheme for the tn, we
obtain an expression for the bias in the form

where we have used

w0 = −wTm

tn = N1

N
N1 − N2

(4.34)

(4.35)

N
N2

= 0

and where m is the mean of the total data set and is given by

m =

1
N

N

xn =

n=1

1
N

(N1m1 + N2m2).

(4.36)

Exercise 4.6

After some straightforward algebra, and again making use of the choice of tn, the
second equation (4.33) becomes

SW + N1N2
N

SB

w = N(m1 − m2)

(4.37)

where SW is deﬁned by (4.28), SB is deﬁned by (4.27), and we have substituted for
the bias using (4.34). Using (4.27), we note that SBw is always in the direction of
(m2 − m1). Thus we can write

w ∝ S−1

W (m2 − m1)

(4.38)

where we have ignored irrelevant scale factors. Thus the weight vector coincides
with that found from the Fisher criterion. In addition, we have also found an expres-
sion for the bias value w0 given by (4.34). This tells us that a new vector x should be
classiﬁed as belonging to class C1 if y(x) = wT(x−m) > 0 and class C2 otherwise.



K

k=1







n∈Ck
1
Nk

1
N

n=1

n=1

N

N

K



K



4.1. Discriminant Functions

191

4.1.6 Fisher’s discriminant for multiple classes
We now consider the generalization of the Fisher discriminant to K > 2 classes,
and we shall assume that the dimensionality D of the input space is greater than the
number K of classes. Next, we introduce D > 1 linear ‘features’ yk = wT
k x, where
k = 1, . . . , D. These feature values can conveniently be grouped together to form
a vector y. Similarly, the weight vectors {wk} can be considered to be the columns
of a matrix W, so that
(4.39)
Note that again we are not including any bias parameters in the deﬁnition of y. The
generalization of the within-class covariance matrix to the case of K classes follows
from (4.28) to give

y = WTx.

SW =

Sk

where

(xn − mk)(xn − mk)T

Sk =

mk =

xn

n∈Ck

and Nk is the number of patterns in class Ck. In order to ﬁnd a generalization of the
between-class covariance matrix, we follow Duda and Hart (1973) and consider ﬁrst
the total covariance matrix

where m is the mean of the total data set

ST =

(xn − m)(xn − m)T

m =

xn =

1
N

Nkmk

k=1

and N =
k Nk is the total number of data points. The total covariance matrix can
be decomposed into the sum of the within-class covariance matrix, given by (4.40)
and (4.41), plus an additional matrix SB, which we identify as a measure of the
between-class covariance

ST = SW + SB

where

SB =

k=1

Nk(mk − m)(mk − m)T.

(4.40)

(4.41)

(4.42)

(4.43)

(4.44)

(4.45)

(4.46)







n∈Ck

k=1

k=1

K

K

n∈Ck










K

k=1




192

4. LINEAR MODELS FOR CLASSIFICATION

These covariance matrices have been deﬁned in the original x-space. We can now
deﬁne similar matrices in the projected D-dimensional y-space

and

where

sW =

(yn − µk)(yn − µk)T

sB =

Nk(µk − µ)(µk − µ)T

µk =

1
Nk

yn,

µ =

1
N

Nkµk.

(4.47)

(4.48)

(4.49)

Again we wish to construct a scalar that is large when the between-class covariance
is large and when the within-class covariance is small. There are now many possible
choices of criterion (Fukunaga, 1990). One example is given by

J(W) = Tr

s−1
W sB

.

(4.50)

This criterion can then be rewritten as an explicit function of the projection matrix
W in the form

J(w) = Tr

(WSWWT)−1(WSBWT)

.

(4.51)

Maximization of such criteria is straightforward, though somewhat involved, and is
discussed at length in Fukunaga (1990). The weight values are determined by those
eigenvectors of S−1

W SB that correspond to the D largest eigenvalues.

There is one important result that is common to all such criteria, which is worth
emphasizing. We ﬁrst note from (4.46) that SB is composed of the sum of K ma-
trices, each of which is an outer product of two vectors and therefore of rank 1. In
addition, only (K − 1) of these matrices are independent as a result of the constraint
(4.44). Thus, SB has rank at most equal to (K − 1) and so there are at most (K − 1)
nonzero eigenvalues. This shows that the projection onto the (K − 1)-dimensional
subspace spanned by the eigenvectors of SB does not alter the value of J(w), and
so we are therefore unable to ﬁnd more than (K − 1) linear ‘features’ by this means
(Fukunaga, 1990).

4.1.7 The perceptron algorithm
Another example of a linear discriminant model is the perceptron of Rosenblatt
(1962), which occupies an important place in the history of pattern recognition al-
gorithms. It corresponds to a two-class model in which the input vector x is ﬁrst
transformed using a ﬁxed nonlinear transformation to give a feature vector φ(x),
and this is then used to construct a generalized linear model of the form

y(x) = f

wTφ(x)

(4.52)





4.1. Discriminant Functions

193

where the nonlinear activation function f(·) is given by a step function of the form
(4.53)

f(a) =

+1, a � 0
−1, a < 0.

The vector φ(x) will typically include a bias component φ0(x) = 1.
In earlier
discussions of two-class classiﬁcation problems, we have focussed on a target coding
scheme in which t ∈ {0, 1}, which is appropriate in the context of probabilistic
models. For the perceptron, however, it is more convenient to use target values
t = +1 for class C1 and t = −1 for class C2, which matches the choice of activation
function.
The algorithm used to determine the parameters w of the perceptron can most
easily be motivated by error function minimization. A natural choice of error func-
tion would be the total number of misclassiﬁed patterns. However, this does not lead
to a simple learning algorithm because the error is a piecewise constant function
of w, with discontinuities wherever a change in w causes the decision boundary to
move across one of the data points. Methods based on changing w using the gradi-
ent of the error function cannot then be applied, because the gradient is zero almost
everywhere.

We therefore consider an alternative error function known as the perceptron cri-
terion. To derive this, we note that we are seeking a weight vector w such that
patterns xn in class C1 will have wTφ(xn) > 0, whereas patterns xn in class C2
have wTφ(xn) < 0. Using the t ∈ {−1, +1} target coding scheme it follows that
we would like all patterns to satisfy wTφ(xn)tn > 0. The perceptron criterion
associates zero error with any pattern that is correctly classiﬁed, whereas for a mis-
classiﬁed pattern xn it tries to minimize the quantity −wTφ(xn)tn. The perceptron
criterion is therefore given by

EP(w) = −

n∈M

wTφntn

(4.54)

Frank Rosenblatt
1928–1969

Rosenblatt’s perceptron played an
important role in the history of ma-
chine learning.
Initially, Rosenblatt
simulated the perceptron on an IBM
704 computer at Cornell
in 1957,
but by the early 1960s he had built
special-purpose hardware that provided a direct, par-
allel implementation of perceptron learning. Many of
his ideas were encapsulated in “Principles of Neuro-
dynamics: Perceptrons and the Theory of Brain Mech-
anisms” published in 1962. Rosenblatt’s work was
criticized by Marvin Minksy, whose objections were
published in the book “Perceptrons”, co-authored with

Seymour Papert. This book was widely misinter-
preted at the time as showing that neural networks
were fatally ﬂawed and could only learn solutions for
linearly separable problems.
In fact, it only proved
such limitations in the case of single-layer networks
such as the perceptron and merely conjectured (in-
correctly) that they applied to more general network
models. Unfortunately, however, this book contributed
to the substantial decline in research funding for neu-
ral computing, a situation that was not reversed un-
til the mid-1980s. Today, there are many hundreds,
if not thousands, of applications of neural networks
in widespread use, with examples in areas such as
handwriting recognition and information retrieval be-
ing used routinely by millions of people.

194

4. LINEAR MODELS FOR CLASSIFICATION

Section 3.1.3

where M denotes the set of all misclassiﬁed patterns. The contribution to the error
associated with a particular misclassiﬁed pattern is a linear function of w in regions
of w space where the pattern is misclassiﬁed and zero in regions where it is correctly
classiﬁed. The total error function is therefore piecewise linear.

We now apply the stochastic gradient descent algorithm to this error function.

The change in the weight vector w is then given by

w(τ +1) = w(τ ) − η∇EP(w) = w(τ ) + ηφntn

(4.55)

where η is the learning rate parameter and τ is an integer that indexes the steps of
the algorithm. Because the perceptron function y(x, w) is unchanged if we multiply
w by a constant, we can set the learning rate parameter η equal to 1 without of
generality. Note that, as the weight vector evolves during training, the set of patterns
that are misclassiﬁed will change.

The perceptron learning algorithm has a simple interpretation, as follows. We
cycle through the training patterns in turn, and for each pattern xn we evaluate the
perceptron function (4.52).
If the pattern is correctly classiﬁed, then the weight
vector remains unchanged, whereas if it is incorrectly classiﬁed, then for class C1
we add the vector φ(xn) onto the current estimate of weight vector w while for
class C2 we subtract the vector φ(xn) from w. The perceptron learning algorithm is
illustrated in Figure 4.7.
If we consider the effect of a single update in the perceptron learning algorithm,
we see that the contribution to the error from a misclassiﬁed pattern will be reduced
because from (4.55) we have

−w(τ +1)Tφntn = −w(τ )Tφntn − (φntn)Tφntn < −w(τ )Tφntn

(4.56)
where we have set η = 1, and made use of φntn2 > 0. Of course, this does
not imply that the contribution to the error function from the other misclassiﬁed
patterns will have been reduced. Furthermore, the change in weight vector may have
caused some previously correctly classiﬁed patterns to become misclassiﬁed. Thus
the perceptron learning rule is not guaranteed to reduce the total error function at
each stage.

However, the perceptron convergence theorem states that if there exists an ex-
act solution (in other words, if the training data set is linearly separable), then the
perceptron learning algorithm is guaranteed to ﬁnd an exact solution in a ﬁnite num-
ber of steps. Proofs of this theorem can be found for example in Rosenblatt (1962),
Block (1962), Nilsson (1965), Minsky and Papert (1969), Hertz et al. (1991), and
Bishop (1995a). Note, however, that the number of steps required to achieve con-
vergence could still be substantial, and in practice, until convergence is achieved,
we will not be able to distinguish between a nonseparable problem and one that is
simply slow to converge.

Even when the data set is linearly separable, there may be many solutions, and
which one is found will depend on the initialization of the parameters and on the or-
der of presentation of the data points. Furthermore, for data sets that are not linearly
separable, the perceptron learning algorithm will never converge.

4.1. Discriminant Functions

195

1

0.5

0

−0.5

−1

−1

1

0.5

0

−0.5

−1

−1

1

0.5

0

−0.5

−1

−1

1

0.5

0

−0.5

−1

−1

−0.5

0

0.5

1

−0.5

0

0.5

1

−0.5

0

0.5

1

−0.5

0

0.5

1

Figure 4.7 Illustration of the convergence of the perceptron learning algorithm, showing data points from two
classes (red and blue) in a two-dimensional feature space (φ1, φ2). The top left plot shows the initial parameter
vector w shown as a black arrow together with the corresponding decision boundary (black line), in which the
arrow points towards the decision region which classiﬁed as belonging to the red class. The data point circled
in green is misclassiﬁed and so its feature vector is added to the current weight vector, giving the new decision
boundary shown in the top right plot. The bottom left plot shows the next misclassiﬁed point to be considered,
indicated by the green circle, and its feature vector is again added to the weight vector giving the decision
boundary shown in the bottom right plot for which all data points are correctly classiﬁed.

196

4. LINEAR MODELS FOR CLASSIFICATION

Figure 4.8 Illustration of the Mark 1 perceptron hardware. The photograph on the left shows how the inputs
were obtained using a simple camera system in which an input scene, in this case a printed character, was
illuminated by powerful lights, and an image focussed onto a 20 × 20 array of cadmium sulphide photocells,
giving a primitive 400 pixel image. The perceptron also had a patch board, shown in the middle photograph,
which allowed different conﬁgurations of input features to be tried. Often these were wired up at random to
demonstrate the ability of the perceptron to learn without the need for precise wiring, in contrast to a modern
digital computer. The photograph on the right shows one of the racks of adaptive weights. Each weight was
implemented using a rotary variable resistor, also called a potentiometer, driven by an electric motor thereby
allowing the value of the weight to be adjusted automatically by the learning algorithm.

Aside from difﬁculties with the learning algorithm, the perceptron does not pro-
vide probabilistic outputs, nor does it generalize readily to K > 2 classes. The most
important limitation, however, arises from the fact that (in common with all of the
models discussed in this chapter and the previous one) it is based on linear com-
binations of ﬁxed basis functions. More detailed discussions of the limitations of
perceptrons can be found in Minsky and Papert (1969) and Bishop (1995a).

Analogue hardware implementations of the perceptron were built by Rosenblatt,
based on motor-driven variable resistors to implement the adaptive parameters wj.
These are illustrated in Figure 4.8. The inputs were obtained from a simple camera
system based on an array of photo-sensors, while the basis functions φ could be
chosen in a variety of ways, for example based on simple ﬁxed functions of randomly
chosen subsets of pixels from the input image. Typical applications involved learning
to discriminate simple shapes or characters.

At the same time that the perceptron was being developed, a closely related
system called the adaline, which is short for ‘adaptive linear element’, was being
explored by Widrow and co-workers. The functional form of the model was the same
as for the perceptron, but a different approach to training was adopted (Widrow and
Hoff, 1960; Widrow and Lehr, 1990).

4.2. Probabilistic Generative Models

We turn next to a probabilistic view of classiﬁcation and show how models with
linear decision boundaries arise from simple assumptions about the distribution of
the data. In Section 1.5.4, we discussed the distinction between the discriminative
and the generative approaches to classiﬁcation. Here we shall adopt a generative

4.2. Probabilistic Generative Models

197

Figure 4.9 Plot of the logistic sigmoid function
σ(a) deﬁned by (4.59), shown in
red, together with the scaled pro-
bit function Φ(λa), for λ2 = π/8,
shown in dashed blue, where Φ(a)
is deﬁned by (4.114). The scal-
ing factor π/8 is chosen so that the
derivatives of the two curves are
equal for a = 0.

1

0.5

0





−5

0

5

approach in which we model the class-conditional densities p(x|Ck), as well as the
class priors p(Ck), and then use these to compute posterior probabilities p(Ck|x)
through Bayes’ theorem.
Consider ﬁrst of all the case of two classes. The posterior probability for class

C1 can be written as

p(C1|x) =
=

p(x|C1)p(C1)

p(x|C1)p(C1) + p(x|C2)p(C2)
1 + exp(−a)

= σ(a)

1

where we have deﬁned

a = ln p(x|C1)p(C1)
p(x|C2)p(C2)
and σ(a) is the logistic sigmoid function deﬁned by

σ(a) =

1

1 + exp(−a)

which is plotted in Figure 4.9. The term ‘sigmoid’ means S-shaped. This type of
function is sometimes also called a ‘squashing function’ because it maps the whole
real axis into a ﬁnite interval. The logistic sigmoid has been encountered already
in earlier chapters and plays an important role in many classiﬁcation algorithms. It
satisﬁes the following symmetry property

as is easily veriﬁed. The inverse of the logistic sigmoid is given by

σ(−a) = 1 − σ(a)

a = ln

σ
1 − σ

and is known as the logit function. It represents the log of the ratio of probabilities
ln [p(C1|x)/p(C2|x)] for the two classes, also known as the log odds.

(4.57)

(4.58)

(4.59)

(4.60)

(4.61)








198

4. LINEAR MODELS FOR CLASSIFICATION

Note that in (4.57) we have simply rewritten the posterior probabilities in an
equivalent form, and so the appearance of the logistic sigmoid may seem rather vac-
uous. However, it will have signiﬁcance provided a(x) takes a simple functional
form. We shall shortly consider situations in which a(x) is a linear function of x, in
which case the posterior probability is governed by a generalized linear model.

For the case of K > 2 classes, we have

p(Ck|x) =

=

p(x|Ck)p(Ck)
j p(x|Cj)p(Cj)
exp(ak)
j exp(aj)

ak = ln p(x|Ck)p(Ck).

(4.62)

(4.63)

which is known as the normalized exponential and can be regarded as a multiclass
generalization of the logistic sigmoid. Here the quantities ak are deﬁned by

The normalized exponential is also known as the softmax function, as it represents
a smoothed version of the ‘max’ function because, if ak 
 aj for all j = k, then
p(Ck|x)  1, and p(Cj|x)  0.
We now investigate the consequences of choosing speciﬁc forms for the class-
conditional densities, looking ﬁrst at continuous input variables x and then dis-
cussing brieﬂy the case of discrete inputs.

4.2.1 Continuous inputs
Let us assume that the class-conditional densities are Gaussian and then explore
the resulting form for the posterior probabilities. To start with, we shall assume that
all classes share the same covariance matrix. Thus the density for class Ck is given
by

1

1

p(x|Ck) =

(x − µk)TΣ−1(x − µk)
Consider ﬁrst the case of two classes. From (4.57) and (4.58), we have

|Σ|1/2

(2π)D/2

exp

1
2

−

where we have deﬁned

p(C1|x) = σ(wTx + w0)

w = Σ−1(µ1 − µ2)
1 Σ−1µ1 +
w0 = −

1
2 µT

1
2 µT

2 Σ−1µ2 + ln p(C1)
p(C2) .

.

(4.64)

(4.65)

(4.66)

(4.67)

We see that the quadratic terms in x from the exponents of the Gaussian densities
have cancelled (due to the assumption of common covariance matrices) leading to
a linear function of x in the argument of the logistic sigmoid. This result is illus-
trated for the case of a two-dimensional input space x in Figure 4.10. The resulting

4.2. Probabilistic Generative Models

199

Figure 4.10 The left-hand plot shows the class-conditional densities for two classes, denoted red and blue.
On the right is the corresponding posterior probability p(C1|x), which is given by a logistic sigmoid of a linear
function of x. The surface in the right-hand plot is coloured using a proportion of red ink given by p(C1|x) and a
proportion of blue ink given by p(C2|x) = 1 − p(C1|x).

decision boundaries correspond to surfaces along which the posterior probabilities
p(Ck|x) are constant and so will be given by linear functions of x, and therefore
the decision boundaries are linear in input space. The prior probabilities p(Ck) enter
only through the bias parameter w0 so that changes in the priors have the effect of
making parallel shifts of the decision boundary and more generally of the parallel
contours of constant posterior probability.

For the general case of K classes we have, from (4.62) and (4.63),

where we have deﬁned

ak(x) = wT

k x + wk0

wk = Σ−1µk
1
wk0 = −
2 µT

k Σ−1µk + ln p(Ck).

(4.68)

(4.69)

(4.70)

We see that the ak(x) are again linear functions of x as a consequence of the cancel-
lation of the quadratic terms due to the shared covariances. The resulting decision
boundaries, corresponding to the minimum misclassiﬁcation rate, will occur when
two of the posterior probabilities (the two largest) are equal, and so will be deﬁned
by linear functions of x, and so again we have a generalized linear model.

If we relax the assumption of a shared covariance matrix and allow each class-
conditional density p(x|Ck) to have its own covariance matrix Σk, then the earlier
cancellations will no longer occur, and we will obtain quadratic functions of x, giv-
ing rise to a quadratic discriminant. The linear and quadratic decision boundaries
are illustrated in Figure 4.11.

4. LINEAR MODELS FOR CLASSIFICATION

200

2.5

2

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−2.5



N

−2

−1

0

1

2

Figure 4.11 The left-hand plot shows the class-conditional densities for three classes each having a Gaussian
distribution, coloured red, green, and blue, in which the red and green classes have the same covariance matrix.
The right-hand plot shows the corresponding posterior probabilities, in which the RGB colour vector represents
the posterior probabilities for the respective three classes. The decision boundaries are also shown. Notice that
the boundary between the red and green classes, which have the same covariance matrix, is linear, whereas
those between the other pairs of classes are quadratic.

4.2.2 Maximum likelihood solution
Once we have speciﬁed a parametric functional form for the class-conditional
densities p(x|Ck), we can then determine the values of the parameters, together with
the prior class probabilities p(Ck), using maximum likelihood. This requires a data
set comprising observations of x along with their corresponding class labels.
Consider ﬁrst the case of two classes, each having a Gaussian class-conditional
density with a shared covariance matrix, and suppose we have a data set {xn, tn}
where n = 1, . . . , N. Here tn = 1 denotes class C1 and tn = 0 denotes class C2. We
denote the prior class probability p(C1) = π, so that p(C2) = 1 − π. For a data point
xn from class C1, we have tn = 1 and hence

p(xn,C1) = p(C1)p(xn|C1) = πN (xn|µ1, Σ).

Similarly for class C2, we have tn = 0 and hence

p(xn,C2) = p(C2)p(xn|C2) = (1 − π)N (xn|µ2, Σ).

Thus the likelihood function is given by

p(t|π, µ1, µ2, Σ) =

n=1

[πN (xn|µ1, Σ)]tn [(1 − π)N (xn|µ2, Σ)]1−tn

(4.71)

where t = (t1, . . . , tN )T. As usual, it is convenient to maximize the log of the
likelihood function. Consider ﬁrst the maximization with respect to π. The terms in



N

n=1



N

n=1






n=1

n=1

n=1

N

N

N






1
N1

n=1

n=1

n=1

N

N

N




n=1

N

N

Exercise 4.9

201

(4.72)

(4.74)

(4.75)

(4.76)

4.2. Probabilistic Generative Models

the log likelihood function that depend on π are

{tn ln π + (1 − tn) ln(1 − π)} .

Setting the derivative with respect to π equal to zero and rearranging, we obtain

π =

1
N

tn = N1
N

=

N1

N1 + N2

(4.73)

where N1 denotes the total number of data points in class C1, and N2 denotes the total
number of data points in class C2. Thus the maximum likelihood estimate for π is
simply the fraction of points in class C1 as expected. This result is easily generalized
to the multiclass case where again the maximum likelihood estimate of the prior
probability associated with class Ck is given by the fraction of the training set points
assigned to that class.
Now consider the maximization with respect to µ1. Again we can pick out of

the log likelihood function those terms that depend on µ1 giving

tn lnN (xn|µ1, Σ) = −

tn(xn − µ1)TΣ−1(xn − µ1) + const.

1
2

Setting the derivative with respect to µ1 to zero and rearranging, we obtain

which is simply the mean of all the input vectors xn assigned to class C1. By a
similar argument, the corresponding result for µ2 is given by

µ1 =

tnxn

µ2 =

1
N2

(1 − tn)xn

which again is the mean of all the input vectors xn assigned to class C2.
Finally, consider the maximum likelihood solution for the shared covariance
matrix Σ. Picking out the terms in the log likelihood function that depend on Σ, we
have

1
2

1
2

−

−

n=1
N
2

= −

tn ln|Σ| −

1
2

(1 − tn) ln|Σ| −

tn(xn − µ1)TΣ−1(xn − µ1)
1
2

(1 − tn)(xn − µ2)TΣ−1(xn − µ2)

ln|Σ| −

N
2

Tr

Σ−1S

(4.77)





D

n∈C1

n∈C2



D

202

4. LINEAR MODELS FOR CLASSIFICATION

where we have deﬁned

S1 =

S = N1
N
1
N1
1
N2

S2 =

S2

S1 + N2
N
(xn − µ1)(xn − µ1)T

(xn − µ2)(xn − µ2)T.

(4.78)

(4.79)

(4.80)

Exercise 4.10

Section 2.3.7

Section 8.2.2

Exercise 4.11

Using the standard result for the maximum likelihood solution for a Gaussian distri-
bution, we see that Σ = S, which represents a weighted average of the covariance
matrices associated with each of the two classes separately.

This result is easily extended to the K class problem to obtain the corresponding
maximum likelihood solutions for the parameters in which each class-conditional
density is Gaussian with a shared covariance matrix. Note that the approach of ﬁtting
Gaussian distributions to the classes is not robust to outliers, because the maximum
likelihood estimation of a Gaussian is not robust.

4.2.3 Discrete features
Let us now consider the case of discrete feature values xi. For simplicity, we
begin by looking at binary feature values xi ∈ {0, 1} and discuss the extension to
more general discrete features shortly. If there are D inputs, then a general distribu-
tion would correspond to a table of 2D numbers for each class, containing 2D − 1
independent variables (due to the summation constraint). Because this grows expo-
nentially with the number of features, we might seek a more restricted representa-
tion. Here we will make the naive Bayes assumption in which the feature values are
treated as independent, conditioned on the class Ck. Thus we have class-conditional
distributions of the form

p(x|Ck) =

i=1

ki(1 − µki)1−xi
µxi

(4.81)

which contain D independent parameters for each class. Substituting into (4.63) then
gives

ak(x) =

i=1

{xi ln µki + (1 − xi) ln(1 − µki)} + ln p(Ck)

(4.82)

which again are linear functions of the input values xi. For the case of K = 2 classes,
we can alternatively consider the logistic sigmoid formulation given by (4.57). Anal-
ogous results are obtained for discrete variables each of which can take M > 2
states.

4.2.4 Exponential family
As we have seen, for both Gaussian distributed and discrete inputs, the posterior
class probabilities are given by generalized linear models with logistic sigmoid (K =













4.3. Probabilistic Discriminative Models

203

2 classes) or softmax (K � 2 classes) activation functions. These are particular cases
of a more general result obtained by assuming that the class-conditional densities
p(x|Ck) are members of the exponential family of distributions.
distribution of x can be written in the form

Using the form (2.194) for members of the exponential family, we see that the

p(x|λk) = h(x)g(λk) exp

k u(x)
λT

.

(4.83)

We now restrict attention to the subclass of such distributions for which u(x) = x.
Then we make use of (2.236) to introduce a scaling parameter s, so that we obtain
the restricted set of exponential family class-conditional densities of the form

p(x|λk, s) =

1
s

h

1
s

x

g(λk) exp

1
s

λT
k x

.

(4.84)

Note that we are allowing each class to have its own parameter vector λk but we are
assuming that the classes share the same scale parameter s.

For the two-class problem, we substitute this expression for the class-conditional
densities into (4.58) and we see that the posterior class probability is again given by
a logistic sigmoid acting on a linear function a(x) which is given by

a(x) = (λ1 − λ2)Tx + ln g(λ1) − ln g(λ2) + ln p(C1) − ln p(C2).

(4.85)

Similarly, for the K-class problem, we substitute the class-conditional density ex-
pression into (4.63) to give

ak(x) = λT

k x + ln g(λk) + ln p(Ck)

(4.86)

and so again is a linear function of x.

4.3. Probabilistic Discriminative Models

For the two-class classiﬁcation problem, we have seen that the posterior probability
of class C1 can be written as a logistic sigmoid acting on a linear function of x, for a
wide choice of class-conditional distributions p(x|Ck). Similarly, for the multiclass
case, the posterior probability of class Ck is given by a softmax transformation of a
linear function of x. For speciﬁc choices of the class-conditional densities p(x|Ck),
we have used maximum likelihood to determine the parameters of the densities as
well as the class priors p(Ck) and then used Bayes’ theorem to ﬁnd the posterior class
probabilities.
However, an alternative approach is to use the functional form of the generalized
linear model explicitly and to determine its parameters directly by using maximum
likelihood. We shall see that there is an efﬁcient algorithm ﬁnding such solutions
known as iterative reweighted least squares, or IRLS.

The indirect approach to ﬁnding the parameters of a generalized linear model,
by ﬁtting class-conditional densities and class priors separately and then applying

204

4. LINEAR MODELS FOR CLASSIFICATION

x2

1

0

−1

−1

0

x1

1

1

φ2

0.5

0

0

0.5

φ1

1

Figure 4.12 Illustration of the role of nonlinear basis functions in linear classiﬁcation models. The left plot
shows the original input space (x1, x2) together with data points from two classes labelled red and blue. Two
‘Gaussian’ basis functions φ1(x) and φ2(x) are deﬁned in this space with centres shown by the green crosses
and with contours shown by the green circles. The right-hand plot shows the corresponding feature space
(φ1, φ2) together with the linear decision boundary obtained given by a logistic regression model of the form
discussed in Section 4.3.2. This corresponds to a nonlinear decision boundary in the original input space,
shown by the black curve in the left-hand plot.

Bayes’ theorem, represents an example of generative modelling, because we could
take such a model and generate synthetic data by drawing values of x from the
marginal distribution p(x). In the direct approach, we are maximizing a likelihood
function deﬁned through the conditional distribution p(Ck|x), which represents a
form of discriminative training. One advantage of the discriminative approach is
that there will typically be fewer adaptive parameters to be determined, as we shall
see shortly. It may also lead to improved predictive performance, particularly when
the class-conditional density assumptions give a poor approximation to the true dis-
tributions.

4.3.1 Fixed basis functions
So far in this chapter, we have considered classiﬁcation models that work di-
rectly with the original input vector x. However, all of the algorithms are equally
applicable if we ﬁrst make a ﬁxed nonlinear transformation of the inputs using a
vector of basis functions φ(x). The resulting decision boundaries will be linear in
the feature space φ, and these correspond to nonlinear decision boundaries in the
original x space, as illustrated in Figure 4.12. Classes that are linearly separable
in the feature space φ(x) need not be linearly separable in the original observation
space x. Note that as in our discussion of linear models for regression, one of the

4.3. Probabilistic Discriminative Models

205






basis functions is typically set to a constant, say φ0(x) = 1, so that the correspond-
ing parameter w0 plays the role of a bias. For the remainder of this chapter, we shall
include a ﬁxed basis function transformation φ(x), as this will highlight some useful
similarities to the regression models discussed in Chapter 3.

For many problems of practical interest, there is signiﬁcant overlap between
the class-conditional densities p(x|Ck). This corresponds to posterior probabilities
p(Ck|x), which, for at least some values of x, are not 0 or 1. In such cases, the opti-
mal solution is obtained by modelling the posterior probabilities accurately and then
applying standard decision theory, as discussed in Chapter 1. Note that nonlinear
transformations φ(x) cannot remove such class overlap. Indeed, they can increase
the level of overlap, or create overlap where none existed in the original observation
space. However, suitable choices of nonlinearity can make the process of modelling
the posterior probabilities easier.

Such ﬁxed basis function models have important limitations, and these will be
resolved in later chapters by allowing the basis functions themselves to adapt to the
data. Notwithstanding these limitations, models with ﬁxed nonlinear basis functions
play an important role in applications, and a discussion of such models will intro-
duce many of the key concepts needed for an understanding of their more complex
counterparts.

4.3.2 Logistic regression
We begin our treatment of generalized linear models by considering the problem
of two-class classiﬁcation. In our discussion of generative approaches in Section 4.2,
we saw that under rather general assumptions, the posterior probability of class C1
can be written as a logistic sigmoid acting on a linear function of the feature vector
φ so that

p(C1|φ) = y(φ) = σ

(4.87)
with p(C2|φ) = 1 − p(C1|φ). Here σ(·) is the logistic sigmoid function deﬁned by
(4.59). In the terminology of statistics, this model is known as logistic regression,
although it should be emphasized that this is a model for classiﬁcation rather than
regression.

wTφ

For an M-dimensional feature space φ, this model has M adjustable parameters.
By contrast, if we had ﬁtted Gaussian class conditional densities using maximum
likelihood, we would have used 2M parameters for the means and M(M + 1)/2
parameters for the (shared) covariance matrix. Together with the class prior p(C1),
this gives a total of M(M +5)/2+1 parameters, which grows quadratically with M,
in contrast to the linear dependence on M of the number of parameters in logistic
regression. For large values of M, there is a clear advantage in working with the
logistic regression model directly.

We now use maximum likelihood to determine the parameters of the logistic
regression model. To do this, we shall make use of the derivative of the logistic sig-
moid function, which can conveniently be expressed in terms of the sigmoid function
itself

dσ
da

= σ(1 − σ).

(4.88)

Section 3.6

Exercise 4.12





n=1

N

N

n=1

N

Exercise 4.13

Section 3.1.1

Exercise 4.14

E(w) = − ln p(t|w) = −

{tn ln yn + (1 − tn) ln(1 − yn)}

(4.90)

where yn = σ(an) and an = wTφn. Taking the gradient of the error function with
respect to w, we obtain

∇E(w) =

(yn − tn)φn

n=1

(4.91)

where we have made use of (4.88). We see that the factor involving the derivative
of the logistic sigmoid has cancelled, leading to a simpliﬁed form for the gradient
of the log likelihood. In particular, the contribution to the gradient from data point
n is given by the ‘error’ yn − tn between the target value and the prediction of the
model, times the basis function vector φn. Furthermore, comparison with (3.13)
shows that this takes precisely the same form as the gradient of the sum-of-squares
error function for the linear regression model.

If desired, we could make use of the result (4.91) to give a sequential algorithm
in which patterns are presented one at a time, in which each of the weight vectors is
updated using (3.22) in which ∇En is the nth term in (4.91).
It is worth noting that maximum likelihood can exhibit severe over-ﬁtting for
data sets that are linearly separable. This arises because the maximum likelihood so-
lution occurs when the hyperplane corresponding to σ = 0.5, equivalent to wTφ =
0, separates the two classes and the magnitude of w goes to inﬁnity. In this case, the
logistic sigmoid function becomes inﬁnitely steep in feature space, corresponding to
a Heaviside step function, so that every training point from each class k is assigned
a posterior probability p(Ck|x) = 1. Furthermore, there is typically a continuum
of such solutions because any separating hyperplane will give rise to the same pos-
terior probabilities at the training data points, as will be seen later in Figure 10.13.
Maximum likelihood provides no way to favour one such solution over another, and
which solution is found in practice will depend on the choice of optimization algo-
rithm and on the parameter initialization. Note that the problem will arise even if
the number of data points is large compared with the number of parameters in the
model, so long as the training data set is linearly separable. The singularity can be
avoided by inclusion of a prior and ﬁnding a MAP solution for w, or equivalently by
adding a regularization term to the error function.

206

4. LINEAR MODELS FOR CLASSIFICATION

For a data set {φn, tn}, where tn ∈ {0, 1} and φn = φ(xn), with n =

1, . . . , N, the likelihood function can be written

p(t|w) =

n {1 − yn}1−tn
ytn

(4.89)

where t = (t1, . . . , tN )T and yn = p(C1|φn). As usual, we can deﬁne an error
function by taking the negative logarithm of the likelihood, which gives the cross-
entropy error function in the form








n=1

N

N

n=1



N



N

4.3. Probabilistic Discriminative Models

207

4.3.3 Iterative reweighted least squares
In the case of the linear regression models discussed in Chapter 3, the maxi-
mum likelihood solution, on the assumption of a Gaussian noise model, leads to a
closed-form solution. This was a consequence of the quadratic dependence of the
log likelihood function on the parameter vector w. For logistic regression, there
is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid
function. However, the departure from a quadratic form is not substantial. To be
precise, the error function is concave, as we shall see shortly, and hence has a unique
minimum. Furthermore, the error function can be minimized by an efﬁcient iterative
technique based on the Newton-Raphson iterative optimization scheme, which uses a
local quadratic approximation to the log likelihood function. The Newton-Raphson
update, for minimizing a function E(w), takes the form (Fletcher, 1987; Bishop and
Nabney, 2008)

(4.92)
where H is the Hessian matrix whose elements comprise the second derivatives of
E(w) with respect to the components of w.

w(new) = w(old) − H−1∇E(w).

Let us ﬁrst of all apply the Newton-Raphson method to the linear regression
model (3.3) with the sum-of-squares error function (3.12). The gradient and Hessian
of this error function are given by

∇E(w) =

(wTφn − tn)φn = ΦTΦw − ΦTt

H = ∇∇E(w) =

φnφT

n = ΦTΦ

w(new) = w(old) − (ΦTΦ)−1

= (ΦTΦ)−1ΦTt

ΦTΦw(old) − ΦTt

(4.95)

which we recognize as the standard least-squares solution. Note that the error func-
tion in this case is quadratic and hence the Newton-Raphson formula gives the exact
solution in one step.

Now let us apply the Newton-Raphson update to the cross-entropy error function
(4.90) for the logistic regression model. From (4.91) we see that the gradient and
Hessian of this error function are given by

∇E(w) =

n=1

(yn − tn)φn = ΦT(y − t)

H = ∇∇E(w) =

n=1

yn(1 − yn)φnφT

n = ΦTRΦ

(4.93)

(4.94)

(4.96)

(4.97)

Section 3.1.1

where Φ is the N × M design matrix, whose nth row is given by φT
Raphson update then takes the form

n. The Newton-







208

4. LINEAR MODELS FOR CLASSIFICATION

Rnn = yn(1 − yn).

where we have made use of (4.88). Also, we have introduced the N × N diagonal
matrix R with elements
(4.98)
We see that the Hessian is no longer constant but depends on w through the weight-
ing matrix R, corresponding to the fact that the error function is no longer quadratic.
Using the property 0 < yn < 1, which follows from the form of the logistic sigmoid
function, we see that uTHu > 0 for an arbitrary vector u, and so the Hessian matrix
H is positive deﬁnite. It follows that the error function is a concave function of w
and hence has a unique minimum.

The Newton-Raphson update formula for the logistic regression model then be-

Exercise 4.15

comes

w(new) = w(old) − (ΦTRΦ)−1ΦT(y − t)

= (ΦTRΦ)−1
= (ΦTRΦ)−1ΦTRz

ΦTRΦw(old) − ΦT(y − t)

where z is an N-dimensional vector with elements

z = Φw(old) − R−1(y − t).

(4.99)

(4.100)

We see that the update formula (4.99) takes the form of a set of normal equations for a
weighted least-squares problem. Because the weighing matrix R is not constant but
depends on the parameter vector w, we must apply the normal equations iteratively,
each time using the new weight vector w to compute a revised weighing matrix
R. For this reason, the algorithm is known as iterative reweighted least squares, or
IRLS (Rubin, 1983). As in the weighted least-squares problem, the elements of the
diagonal weighting matrix R can be interpreted as variances because the mean and
variance of t in the logistic regression model are given by

E[t] = σ(x) = y
var[t] = E[t2] − E[t]2 = σ(x) − σ(x)2 = y(1 − y)

(4.101)
(4.102)
where we have used the property t2 = t for t ∈ {0, 1}. In fact, we can interpret IRLS
as the solution to a linearized problem in the space of the variable a = wTφ. The
quantity zn, which corresponds to the nth element of z, can then be given a simple
interpretation as an effective target value in this space obtained by making a local
linear approximation to the logistic sigmoid function around the current operating
point w(old)

an(w)  an(w(old)) +
nw(old) −

= φT

w(old)

dan
dyn
(yn − tn)
yn(1 − yn)

(tn − yn)
= zn.

(4.103)




N






n=1

K

N

N

K

k=1



N



K

4.3. Probabilistic Discriminative Models

209

4.3.4 Multiclass logistic regression
In our discussion of generative models for multiclass classiﬁcation, we have
seen that for a large class of distributions, the posterior probabilities are given by a
softmax transformation of linear functions of the feature variables, so that

p(Ck|φ) = yk(φ) =

exp(ak)
j exp(aj)

where the ‘activations’ ak are given by

ak = wT

k φ.

(4.104)

(4.105)

(4.106)

There we used maximum likelihood to determine separately the class-conditional
densities and the class priors and then found the corresponding posterior probabilities
using Bayes’ theorem, thereby implicitly determining the parameters {wk}. Here we
consider the use of maximum likelihood to determine the parameters {wk} of this
model directly. To do this, we will require the derivatives of yk with respect to all of
the activations aj. These are given by

∂yk
∂aj

= yk(Ikj − yj)

where Ikj are the elements of the identity matrix.

Next we write down the likelihood function. This is most easily done using
the 1-of-K coding scheme in which the target vector tn for a feature vector φn
belonging to class Ck is a binary vector with all elements zero except for element k,
which equals one. The likelihood function is then given by

p(T|w1, . . . , wK) =

n=1

k=1

p(Ck|φn)tnk =

ytnk
nk

(4.107)

where ynk = yk(φn), and T is an N × K matrix of target variables with elements
tnk. Taking the negative logarithm then gives

E(w1, . . . , wK) = − ln p(T|w1, . . . , wK) = −

tnk ln ynk

(4.108)

n=1

k=1

which is known as the cross-entropy error function for the multiclass classiﬁcation
problem.

We now take the gradient of the error function with respect to one of the param-
eter vectors wj. Making use of the result (4.106) for the derivatives of the softmax
function, we obtain

∇wj E(w1, . . . , wK) =

(ynj − tnj) φn

n=1

(4.109)

Section 4.2

Exercise 4.17

Exercise 4.18



N




210

4. LINEAR MODELS FOR CLASSIFICATION

k tnk = 1. Once again, we see the same form arising
where we have made use of
for the gradient as was found for the sum-of-squares error function with the linear
model and the cross-entropy error for the logistic regression model, namely the prod-
uct of the error (ynj − tnj) times the basis function φn. Again, we could use this
to formulate a sequential algorithm in which patterns are presented one at a time, in
which each of the weight vectors is updated using (3.22).

We have seen that the derivative of the log likelihood function for a linear regres-
sion model with respect to the parameter vector w for a data point n took the form
of the ‘error’ yn − tn times the feature vector φn. Similarly, for the combination
of logistic sigmoid activation function and cross-entropy error function (4.90), and
for the softmax activation function with the multiclass cross-entropy error function
(4.108), we again obtain this same simple form. This is an example of a more general
result, as we shall see in Section 4.3.6.

To ﬁnd a batch algorithm, we again appeal to the Newton-Raphson update to
obtain the corresponding IRLS algorithm for the multiclass problem. This requires
evaluation of the Hessian matrix that comprises blocks of size M × M in which
block j, k is given by

Exercise 4.20

∇wk∇wj E(w1, . . . , wK) = −

ynk(Ikj − ynj)φnφT
n.

n=1

(4.110)

As with the two-class problem, the Hessian matrix for the multiclass logistic regres-
sion model is positive deﬁnite and so the error function again has a unique minimum.
Practical details of IRLS for the multiclass case can be found in Bishop and Nabney
(2008).

4.3.5 Probit regression
We have seen that, for a broad range of class-conditional distributions, described
by the exponential family, the resulting posterior class probabilities are given by a
logistic (or softmax) transformation acting on a linear function of the feature vari-
ables. However, not all choices of class-conditional density give rise to such a simple
form for the posterior probabilities (for instance, if the class-conditional densities are
modelled using Gaussian mixtures). This suggests that it might be worth exploring
other types of discriminative probabilistic model. For the purposes of this chapter,
however, we shall return to the two-class case, and again remain within the frame-
work of generalized linear models so that

p(t = 1|a) = f(a)
where a = wTφ, and f(·) is the activation function.
One way to motivate an alternative choice for the link function is to consider a
noisy threshold model, as follows. For each input φn, we evaluate an = wTφn and
then we set the target value according to

(4.111)

tn = 1 if an � θ
tn = 0 otherwise.

(4.112)






a



4.3. Probabilistic Discriminative Models

211

Figure 4.13 Schematic example of a probability density p(θ)
shown by the blue curve, given in this example by a mixture
of two Gaussians, along with its cumulative distribution function
f (a), shown by the red curve. Note that the value of the blue
curve at any point, such as that indicated by the vertical green
line, corresponds to the slope of the red curve at the same point.
Conversely, the value of the red curve at this point corresponds
to the area under the blue curve indicated by the shaded green
region. In the stochastic threshold model, the class label takes
the value t = 1 if the value of a = wTφ exceeds a threshold, oth-
erwise it takes the value t = 0. This is equivalent to an activation
function given by the cumulative distribution function f (a).

1

0.8

0.6

0.4

0.2

0

0

1

2

3

4

If the value of θ is drawn from a probability density p(θ), then the corresponding
activation function will be given by the cumulative distribution function

f(a) =

p(θ) dθ

a

−∞

as illustrated in Figure 4.13.

As a speciﬁc example, suppose that the density p(θ) is given by a zero mean,
unit variance Gaussian. The corresponding cumulative distribution function is given
by

Φ(a) =

N (θ|0, 1) dθ

−∞

which is known as the probit function. It has a sigmoidal shape and is compared
with the logistic sigmoid function in Figure 4.9. Note that the use of a more gen-
eral Gaussian distribution does not change the model because this is equivalent to
a re-scaling of the linear coefﬁcients w. Many numerical packages provide for the
evaluation of a closely related function deﬁned by

(4.113)

(4.114)

erf(a) =

2
√π

a

0

exp(−θ2/2) dθ

(4.115)

Exercise 4.21

and known as the erf function or error function (not to be confused with the error
function of a machine learning model). It is related to the probit function by

Φ(a) =

1
2

1 +

1
√2

erf(a)

.

(4.116)

The generalized linear model based on a probit activation function is known as probit
regression.

We can determine the parameters of this model using maximum likelihood, by a
straightforward extension of the ideas discussed earlier. In practice, the results found
using probit regression tend to be similar to those of logistic regression. We shall,

212

4. LINEAR MODELS FOR CLASSIFICATION









however, ﬁnd another use for the probit model when we discuss Bayesian treatments
of logistic regression in Section 4.5.

One issue that can occur in practical applications is that of outliers, which can
arise for instance through errors in measuring the input vector x or through misla-
belling of the target value t. Because such points can lie a long way to the wrong side
of the ideal decision boundary, they can seriously distort the classiﬁer. Note that the
logistic and probit regression models behave differently in this respect because the
tails of the logistic sigmoid decay asymptotically like exp(−x) for x → ∞, whereas
for the probit activation function they decay like exp(−x2), and so the probit model
can be signiﬁcantly more sensitive to outliers.
However, both the logistic and the probit models assume the data is correctly
labelled. The effect of mislabelling is easily incorporated into a probabilistic model
by introducing a probability 	 that the target value t has been ﬂipped to the wrong
value (Opper and Winther, 2000a), leading to a target value distribution for data point
x of the form

p(t|x) = (1 − 	)σ(x) + 	(1 − σ(x))

(4.117)
where σ(x) is the activation function with input vector x. Here 	 may be set in
advance, or it may be treated as a hyperparameter whose value is inferred from the
data.

= 	 + (1 − 2	)σ(x)

4.3.6 Canonical link functions
For the linear regression model with a Gaussian noise distribution, the error
function, corresponding to the negative log likelihood, is given by (3.12). If we take
the derivative with respect to the parameter vector w of the contribution to the error
function from a data point n, this takes the form of the ‘error’ yn − tn times the
feature vector φn, where yn = wTφn. Similarly, for the combination of the logistic
sigmoid activation function and the cross-entropy error function (4.90), and for the
softmax activation function with the multiclass cross-entropy error function (4.108),
we again obtain this same simple form. We now show that this is a general result
of assuming a conditional distribution for the target variable from the exponential
family, along with a corresponding choice for the activation function known as the
canonical link function.

We again make use of the restricted form (4.84) of exponential family distribu-
tions. Note that here we are applying the assumption of exponential family distribu-
tion to the target variable t, in contrast to Section 4.2.4 where we applied it to the
input vector x. We therefore consider conditional distributions of the target variable
of the form

p(t|η, s) =

1
s

h

t
s

g(η) exp

.

(4.118)

ηt
s

Using the same line of argument as led to the derivation of the result (2.226), we see
that the conditional mean of t, which we denote by y, is given by

y ≡ E[t|η] = −s

d
dη

ln g(η).

(4.119)







N






n=1

N

N





n=1

N

d
dηn

N

4.4. The Laplace Approximation

213

Thus y and η must related, and we denote this relation through η = ψ(y).

Following Nelder and Wedderburn (1972), we deﬁne a generalized linear model
to be one for which y is a nonlinear function of a linear combination of the input (or
feature) variables so that

y = f(wTφ)

(4.120)
where f(·) is known as the activation function in the machine learning literature, and
f−1(·) is known as the link function in statistics.
Now consider the log likelihood function for this model, which, as a function of
η, is given by

ln p(t|η, s) =

n=1

ln p(tn|η, s) =

ln g(ηn) + ηntn
s

+ const

(4.121)

where we are assuming that all observations share a common scale parameter (which
corresponds to the noise variance for a Gaussian distribution for instance) and so s
is independent of n. The derivative of the log likelihood with respect to the model
parameters w is then given by

∇w ln p(t|η, s) =

ln g(ηn) + tn
s

dηn
dyn

dyn
dan∇an

=

n=1

1
s {tn − yn} ψ(yn)f(an)φn

(4.122)

where an = wTφn, and we have used yn = f(an) together with the result (4.119)
for E[t|η]. We now see that there is a considerable simpliﬁcation if we choose a
particular form for the link function f−1(y) given by

f−1(y) = ψ(y)

(4.123)

which gives f(ψ(y)) = y and hence f(ψ)ψ(y) = 1. Also, because a = f−1(y),
we have a = ψ and hence f(a)ψ(y) = 1. In this case, the gradient of the error
function reduces to

∇ ln E(w) =

1
s

{yn − tn}φn.

n=1

(4.124)

For the Gaussian s = β−1, whereas for the logistic model s = 1.

4.4. The Laplace Approximation

In Section 4.5 we shall discuss the Bayesian treatment of logistic regression. As
we shall see, this is more complex than the Bayesian treatment of linear regression
models, discussed in Sections 3.3 and 3.5. In particular, we cannot integrate exactly



















.

214

Chapter 10
Chapter 11

4. LINEAR MODELS FOR CLASSIFICATION

over the parameter vector w since the posterior distribution is no longer Gaussian.
It is therefore necessary to introduce some form of approximation. Later in the
book we shall consider a range of techniques based on analytical approximations
and numerical sampling.

Here we introduce a simple, but widely used, framework called the Laplace ap-
proximation, that aims to ﬁnd a Gaussian approximation to a probability density
deﬁned over a set of continuous variables. Consider ﬁrst the case of a single contin-
uous variable z, and suppose the distribution p(z) is deﬁned by

f(z) dz is the normalization coefﬁcient. We shall suppose that the
where Z =
value of Z is unknown. In the Laplace method the goal is to ﬁnd a Gaussian approx-
imation q(z) which is centred on a mode of the distribution p(z). The ﬁrst step is to
ﬁnd a mode of p(z), in other words a point z0 such that p(z0) = 0, or equivalently

p(z) =

f(z)

1
Z

df(z)
dz

= 0.

z=z0

A Gaussian distribution has the property that its logarithm is a quadratic function
of the variables. We therefore consider a Taylor expansion of ln f(z) centred on the
mode z0 so that

where

Note that the ﬁrst-order term in the Taylor expansion does not appear since z0 is a
local maximum of the distribution. Taking the exponential we obtain

ln f(z)  ln f(z0) −

1
2 A(z − z0)2

A = −

d2
dz2 ln f(z)

.

z=z0

f(z)  f(z0) exp

A
2

(z − z0)2

−

We can then obtain a normalized distribution q(z) by making use of the standard
result for the normalization of a Gaussian, so that

q(z) =

1/2

exp

A
2π

A
2

−

(z − z0)2

.

(4.130)

The Laplace approximation is illustrated in Figure 4.14. Note that the Gaussian
approximation will only be well deﬁned if its precision A > 0, in other words the
stationary point z0 must be a local maximum, so that the second derivative of f(z)
at the point z0 is negative.

(4.125)

(4.126)

(4.127)

(4.128)

(4.129)

4.4. The Laplace Approximation

215



40

30

20

10

0
−2



4





0.8

0.6

0.4

0.2

0
−2

−1

0

1

2

3

Figure 4.14 Illustration of the Laplace approximation applied to the distribution p(z) ∝ exp(−z2/2)σ(20z + 4)
where σ(z) is the logistic sigmoid function deﬁned by σ(z) = (1 + e−z)−1. The left plot shows the normalized
distribution p(z) in yellow, together with the Laplace approximation centred on the mode z0 of p(z) in red. The
right plot shows the negative logarithms of the corresponding curves.

−1

0

1

2

3

4

We can extend the Laplace method to approximate a distribution p(z) = f(z)/Z
deﬁned over an M-dimensional space z. At a stationary point z0 the gradient ∇f(z)
will vanish. Expanding around this stationary point we have

ln f(z)  ln f(z0) −

(z − z0)TA(z − z0)

1
2

where the M × M Hessian matrix A is deﬁned by

A = − ∇∇ ln f(z)|z=z0

(4.131)

(4.132)

and ∇ is the gradient operator. Taking the exponential of both sides we obtain

f(z)  f(z0) exp

(z − z0)TA(z − z0)

.

(4.133)

1
2

−

The distribution q(z) is proportional to f(z) and the appropriate normalization coef-
ﬁcient can be found by inspection, using the standard result (2.43) for a normalized
multivariate Gaussian, giving

q(z) = |A|1/2
(2π)M/2

exp

1
2

−

(z − z0)TA(z − z0)

= N (z|z0, A−1)

(4.134)

where |A| denotes the determinant of A. This Gaussian distribution will be well
deﬁned provided its precision matrix, given by A, is positive deﬁnite, which implies
that the stationary point z0 must be a local maximum, not a minimum or a saddle
point.

In order to apply the Laplace approximation we ﬁrst need to ﬁnd the mode z0,
and then evaluate the Hessian matrix at that mode. In practice a mode will typi-
cally be found by running some form of numerical optimization algorithm (Bishop








(



+

)*

216

4. LINEAR MODELS FOR CLASSIFICATION

and Nabney, 2008). Many of the distributions encountered in practice will be mul-
timodal and so there will be different Laplace approximations according to which
mode is being considered. Note that the normalization constant Z of the true distri-
bution does not need to be known in order to apply the Laplace method. As a result
of the central limit theorem, the posterior distribution for a model is expected to
become increasingly better approximated by a Gaussian as the number of observed
data points is increased, and so we would expect the Laplace approximation to be
most useful in situations where the number of data points is relatively large.

One major weakness of the Laplace approximation is that, since it is based on a
Gaussian distribution, it is only directly applicable to real variables. In other cases
it may be possible to apply the Laplace approximation to a transformation of the
variable. For instance if 0 � τ < ∞ then we can consider a Laplace approximation
of ln τ . The most serious limitation of the Laplace framework, however, is that
it is based purely on the aspects of the true distribution at a speciﬁc value of the
variable, and so can fail to capture important global properties. In Chapter 10 we
shall consider alternative approaches which adopt a more global perspective.

4.4.1 Model comparison and BIC
As well as approximating the distribution p(z) we can also obtain an approxi-

mation to the normalization constant Z. Using the approximation (4.133) we have

Z =

f(z) dz

 f(z0)

= f(z0)

(2π)M/2
|A|1/2

exp

1
2

−

(z − z0)TA(z − z0)

dz

(4.135)

where we have noted that the integrand is Gaussian and made use of the standard
result (2.43) for a normalized Gaussian distribution. We can use the result (4.135) to
obtain an approximation to the model evidence which, as discussed in Section 3.4,
plays a central role in Bayesian model comparison.

Consider a data set D and a set of models {Mi} having parameters {θi}. For
each model we deﬁne a likelihood function p(D|θi,Mi). If we introduce a prior
p(θi|Mi) over the parameters, then we are interested in computing the model evi-
dence p(D|Mi) for the various models. From now on we omit the conditioning on
Mi to keep the notation uncluttered. From Bayes’ theorem the model evidence is
given by

(4.136)
Identifying f(θ) = p(D|θ)p(θ) and Z = p(D), and applying the result (4.135), we
obtain

p(D|θ)p(θ) dθ.

p(D) =

ln p(D)  ln p(D|θMAP) + ln p(θMAP) + M
2

1
2

ln|A|

(4.137)

ln(2π) −
Occam factor

Exercise 4.22

Exercise 4.23

Section 3.5.3

4.5. Bayesian Logistic Regression

217

where θMAP is the value of θ at the mode of the posterior distribution, and A is the
Hessian matrix of second derivatives of the negative log posterior

A = −∇∇ ln p(D|θMAP)p(θMAP) = −∇∇ ln p(θMAP|D).

(4.138)

The ﬁrst term on the right hand side of (4.137) represents the log likelihood evalu-
ated using the optimized parameters, while the remaining three terms comprise the
‘Occam factor’ which penalizes model complexity.

If we assume that the Gaussian prior distribution over parameters is broad, and
that the Hessian has full rank, then we can approximate (4.137) very roughly using

ln p(D)  ln p(D|θMAP) −

1
2 M ln N

(4.139)

where N is the number of data points, M is the number of parameters in θ and
we have omitted additive constants. This is known as the Bayesian Information
Criterion (BIC) or the Schwarz criterion (Schwarz, 1978). Note that, compared to
AIC given by (1.73), this penalizes model complexity more heavily.

Complexity measures such as AIC and BIC have the virtue of being easy to
evaluate, but can also give misleading results. In particular, the assumption that the
Hessian matrix has full rank is often not valid since many of the parameters are not
‘well-determined’. We can use the result (4.137) to obtain a more accurate estimate
of the model evidence starting from the Laplace approximation, as we illustrate in
the context of neural networks in Section 5.7.

4.5. Bayesian Logistic Regression

We now turn to a Bayesian treatment of logistic regression. Exact Bayesian infer-
ence for logistic regression is intractable. In particular, evaluation of the posterior
distribution would require normalization of the product of a prior distribution and a
likelihood function that itself comprises a product of logistic sigmoid functions, one
for every data point. Evaluation of the predictive distribution is similarly intractable.
Here we consider the application of the Laplace approximation to the problem of
Bayesian logistic regression (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b).

4.5.1 Laplace approximation
Recall from Section 4.4 that the Laplace approximation is obtained by ﬁnding
the mode of the posterior distribution and then ﬁtting a Gaussian centred at that
mode. This requires evaluation of the second derivatives of the log posterior, which
is equivalent to ﬁnding the Hessian matrix.

Because we seek a Gaussian representation for the posterior distribution, it is

natural to begin with a Gaussian prior, which we write in the general form

p(w) = N (w|m0, S0)

(4.140)



1
2
N










n=1

N



218

4. LINEAR MODELS FOR CLASSIFICATION

where m0 and S0 are ﬁxed hyperparameters. The posterior distribution over w is
given by

(4.141)
where t = (t1, . . . , tN )T. Taking the log of both sides, and substituting for the prior
distribution using (4.140), and for the likelihood function using (4.89), we obtain

p(w|t) ∝ p(w)p(t|w)

ln p(w|t) = −

(w − m0)TS−1

0 (w − m0)

+

n=1

{tn ln yn + (1 − tn) ln(1 − yn)} + const

(4.142)

where yn = σ(wTφn). To obtain a Gaussian approximation to the posterior dis-
tribution, we ﬁrst maximize the posterior distribution to give the MAP (maximum
posterior) solution wMAP, which deﬁnes the mean of the Gaussian. The covariance
is then given by the inverse of the matrix of second derivatives of the negative log
likelihood, which takes the form

SN = −∇∇ ln p(w|t) = S−1

0 +

yn(1 − yn)φnφT
n.

(4.143)

The Gaussian approximation to the posterior distribution therefore takes the form

q(w) = N (w|wMAP, SN ).

(4.144)

Having obtained a Gaussian approximation to the posterior distribution, there
remains the task of marginalizing with respect to this distribution in order to make
predictions.

4.5.2 Predictive distribution
The predictive distribution for class C1, given a new feature vector φ(x), is
obtained by marginalizing with respect to the posterior distribution p(w|t), which is
itself approximated by a Gaussian distribution q(w) so that

p(C1|φ, t) =

p(C1|φ, w)p(w|t) dw 

σ(wTφ)q(w) dw

(4.145)

with the corresponding probability for class C2 given by p(C2|φ, t) = 1− p(C1|φ, t).
To evaluate the predictive distribution, we ﬁrst note that the function σ(wTφ) de-
pends on w only through its projection onto φ. Denoting a = wTφ, we have

σ(wTφ) =

δ(a − wTφ)σ(a) da
where δ(·) is the Dirac delta function. From this we obtain

(4.146)

σ(wTφ)q(w) dw =

σ(a)p(a) da

(4.147)

























4.5. Bayesian Logistic Regression

219

(4.148)

where

p(a) =

δ(a − wTφ)q(w) dw.

We can evaluate p(a) by noting that the delta function imposes a linear constraint
on w and so forms a marginal distribution from the joint distribution q(w) by inte-
grating out all directions orthogonal to φ. Because q(w) is Gaussian, we know from
Section 2.3.2 that the marginal distribution will also be Gaussian. We can evaluate
the mean and covariance of this distribution by taking moments, and interchanging
the order of integration over a and w, so that

µa = E[a] =

p(a)a da =

q(w)wTφ dw = wT

MAPφ

(4.149)

where we have used the result (4.144) for the variational posterior distribution q(w).
Similarly

a = var[a] =
σ2

p(a)

da

a2 − E[a]2
N φ)2

=

q(w)

(wTφ)2 − (mT

dw = φTSN φ.

(4.150)

Note that the distribution of a takes the same form as the predictive distribution
(3.58) for the linear regression model, with the noise variance set to zero. Thus our
variational approximation to the predictive distribution becomes

p(C1|t) =

σ(a)p(a) da =

σ(a)N (a|µa, σ2

a) da.

(4.151)

This result can also be derived directly by making use of the results for the marginal
of a Gaussian distribution given in Section 2.3.2.

The integral over a represents the convolution of a Gaussian with a logistic sig-
moid, and cannot be evaluated analytically. We can, however, obtain a good approx-
imation (Spiegelhalter and Lauritzen, 1990; MacKay, 1992b; Barber and Bishop,
1998a) by making use of the close similarity between the logistic sigmoid function
σ(a) deﬁned by (4.59) and the probit function Φ(a) deﬁned by (4.114). In order to
obtain the best approximation to the logistic function we need to re-scale the hori-
zontal axis, so that we approximate σ(a) by Φ(λa). We can ﬁnd a suitable value of
λ by requiring that the two functions have the same slope at the origin, which gives
λ2 = π/8. The similarity of the logistic sigmoid and the probit function, for this
choice of λ, is illustrated in Figure 4.9.

The advantage of using a probit function is that its convolution with a Gaussian
can be expressed analytically in terms of another probit function. Speciﬁcally we
can show that

Φ(λa)N (a|µ, σ2) da = Φ

µ

(λ−2 + σ2)1/2

.

(4.152)

Exercise 4.24

Exercise 4.25

Exercise 4.26



















n





220

4. LINEAR MODELS FOR CLASSIFICATION

We now apply the approximation σ(a)  Φ(λa) to the probit functions appearing
on both sides of this equation, leading to the following approximation for the convo-
lution of a logistic sigmoid with a Gaussian

σ(a)N (a|µ, σ2) da  σ

κ(σ2)µ

(4.153)

where we have deﬁned

(4.154)
Applying this result to (4.151) we obtain the approximate predictive distribution

κ(σ2) = (1 + πσ2/8)−1/2.

in the form

κ(σ2

a)µa

p(C1|φ, t) = σ

where µa and σ2
ﬁned by (4.154).

a are deﬁned by (4.149) and (4.150), respectively, and κ(σ2

(4.155)
a) is de-
Note that the decision boundary corresponding to p(C1|φ, t) = 0.5 is given by
µa = 0, which is the same as the decision boundary obtained by using the MAP
value for w. Thus if the decision criterion is based on minimizing misclassiﬁca-
tion rate, with equal prior probabilities, then the marginalization over w has no ef-
fect. However, for more complex decision criteria it will play an important role.
Marginalization of the logistic sigmoid model under a Gaussian approximation to
the posterior distribution will be illustrated in the context of variational inference in
Figure 10.13.

Exercises

4.1 ( ) Given a set of data points {xn}, we can deﬁne the convex hull to be the set of
(4.156)

all points x given by

x =

αnxn

where αn � 0 and
n αn = 1. Consider a second set of points {yn} together with
their corresponding convex hull. By deﬁnition, the two sets of points will be linearly
wTxn + w0 > 0 for all
separable if there exists a vector
wTyn + w0 < 0 for all yn. Show that if their convex hulls intersect, the two
xn, and
sets of points cannot be linearly separable, and conversely that if they are linearly
separable, their convex hulls do not intersect.

w and a scalar w0 such that

4.2 ( ) www Consider the minimization of a sum-of-squares error function (4.15),
and suppose that all of the target vectors in the training set satisfy a linear constraint

aTtn + b = 0

(4.157)

where tn corresponds to the nth row of the matrix T in (4.15). Show that as a
consequence of this constraint, the elements of the model prediction y(x) given by
the least-squares solution (4.17) also satisfy this constraint, so that

aTy(x) + b = 0.

(4.158)

Exercises

221

To do so, assume that one of the basis functions φ0(x) = 1 so that the corresponding
parameter w0 plays the role of a bias.

4.3 ( ) Extend the result of Exercise 4.2 to show that if multiple linear constraints
are satisﬁed simultaneously by the target vectors, then the same constraints will also
be satisﬁed by the least-squares prediction of a linear model.

4.4 () www Show that maximization of the class separation criterion given by (4.23)
with respect to w, using a Lagrange multiplier to enforce the constraint wTw = 1,
leads to the result that w ∝ (m2 − m1).

4.5 () By making use of (4.20), (4.23), and (4.24), show that the Fisher criterion (4.25)

can be written in the form (4.26).

4.6 () Using the deﬁnitions of the between-class and within-class covariance matrices
given by (4.27) and (4.28), respectively, together with (4.34) and (4.36) and the
choice of target values described in Section 4.1.5, show that the expression (4.33)
that minimizes the sum-of-squares error function can be written in the form (4.37).

4.7 () www Show that the logistic sigmoid function (4.59) satisﬁes the property

σ(−a) = 1 − σ(a) and that its inverse is given by σ−1(y) = ln{y/(1 − y)}.

4.8 () Using (4.57) and (4.58), derive the result (4.65) for the posterior class probability
in the two-class generative model with Gaussian densities, and verify the results
(4.66) and (4.67) for the parameters w and w0.

4.9 () www Consider a generative classiﬁcation model for K classes deﬁned by
prior class probabilities p(Ck) = πk and general class-conditional densities p(φ|Ck)
where φ is the input feature vector. Suppose we are given a training data set {φn, tn}
where n = 1, . . . , N, and tn is a binary target vector of length K that uses the 1-of-
K coding scheme, so that it has components tnj = Ijk if pattern n is from class Ck.
Assuming that the data points are drawn independently from this model, show that
the maximum-likelihood solution for the prior probabilities is given by



N

πk = Nk
N

where Nk is the number of data points assigned to class Ck.

4.10 ( ) Consider the classiﬁcation model of Exercise 4.9 and now suppose that the
class-conditional densities are given by Gaussian distributions with a shared covari-
ance matrix, so that

p(φ|Ck) = N (φ|µk, Σ).

Show that the maximum likelihood solution for the mean of the Gaussian distribution
for class Ck is given by

µk =

1
Nk

tnkφn

n=1

(4.161)

(4.159)

(4.160)



K

k=1



N

222

4. LINEAR MODELS FOR CLASSIFICATION

which represents the mean of those feature vectors assigned to class Ck. Similarly,
show that the maximum likelihood solution for the shared covariance matrix is given
by

Σ =

Nk
N

Sk

(4.162)

(4.163)

where

Sk =

1
Nk

tnk(φn − µk)(φn − µk)T.

n=1

Thus Σ is given by a weighted average of the covariances of the data associated with
each class, in which the weighting coefﬁcients are given by the prior probabilities of
the classes.

4.11 ( ) Consider a classiﬁcation problem with K classes for which the feature vector
φ has M components each of which can take L discrete states. Let the values of the
components be represented by a 1-of-L binary coding scheme. Further suppose that,
conditioned on the class Ck, the M components of φ are independent, so that the
class-conditional density factorizes with respect to the feature vector components.
Show that the quantities ak given by (4.63), which appear in the argument to the
softmax function describing the posterior class probabilities, are linear functions of
the components of φ. Note that this represents an example of the naive Bayes model
which is discussed in Section 8.2.2.

4.12 () www Verify the relation (4.88) for the derivative of the logistic sigmoid func-

tion deﬁned by (4.59).

4.13 () www By making use of the result (4.88) for the derivative of the logistic sig-
moid, show that the derivative of the error function (4.90) for the logistic regression
model is given by (4.91).

4.14 () Show that for a linearly separable data set, the maximum likelihood solution
for the logistic regression model is obtained by ﬁnding a vector w whose decision
boundary wTφ(x) = 0 separates the classes and then taking the magnitude of w to
inﬁnity.

4.15 ( ) Show that the Hessian matrix H for the logistic regression model, given by
(4.97), is positive deﬁnite. Here R is a diagonal matrix with elements yn(1 − yn),
and yn is the output of the logistic regression model for input vector xn. Hence show
that the error function is a concave function of w and that it has a unique minimum.
4.16 () Consider a binary classiﬁcation problem in which each observation xn is known
to belong to one of two classes, corresponding to t = 0 and t = 1, and suppose that
the procedure for collecting training data is imperfect, so that training points are
sometimes mislabelled. For every data point xn, instead of having a value t for the
class label, we have instead a value πn representing the probability that tn = 1.
Given a probabilistic model p(t = 1|φ), write down the log likelihood function
appropriate to such a data set.

Exercises

223

4.17 () www Show that the derivatives of the softmax activation function (4.104),

where the ak are deﬁned by (4.105), are given by (4.106).

4.18 () Using the result (4.91) for the derivatives of the softmax activation function,

show that the gradients of the cross-entropy error (4.108) are given by (4.109).

4.19 () www Write down expressions for the gradient of the log likelihood, as well
as the corresponding Hessian matrix, for the probit regression model deﬁned in Sec-
tion 4.3.5. These are the quantities that would be required to train such a model using
IRLS.

4.20 ( ) Show that the Hessian matrix for the multiclass logistic regression problem,
deﬁned by (4.110), is positive semideﬁnite. Note that the full Hessian matrix for
this problem is of size M K × M K, where M is the number of parameters and K
is the number of classes. To prove the positive semideﬁnite property, consider the
product uTHu where u is an arbitrary vector of length M K, and then apply Jensen’s
inequality.

4.21 () Show that the probit function (4.114) and the erf function (4.115) are related by

(4.116).

4.22 () Using the result (4.135), derive the expression (4.137) for the log model evi-

dence under the Laplace approximation.

4.23 ( ) www In this exercise, we derive the BIC result (4.139) starting from the
Laplace approximation to the model evidence given by (4.137). Show that if the
prior over parameters is Gaussian of the form p(θ) = N (θ|m, V0), the log model
evidence under the Laplace approximation takes the form

0 (θMAP − m) −

1
2

1
2

(θMAP − m)TV−1

ln p(D)  ln p(D|θMAP) −
ln|H| + const
where H is the matrix of second derivatives of the log likelihood ln p(D|θ) evaluated
at θMAP. Now assume that the prior is broad so that V−1
is small and the second
0
term on the right-hand side above can be neglected. Furthermore, consider the case
of independent, identically distributed data so that H is the sum of terms one for each
data point. Show that the log model evidence can then be written approximately in
the form of the BIC expression (4.139).

4.24 ( ) Use the results from Section 2.3.2 to derive the result (4.151) for the marginal-
ization of the logistic regression model with respect to a Gaussian posterior distribu-
tion over the parameters w.

4.25 ( ) Suppose we wish to approximate the logistic sigmoid σ(a) deﬁned by (4.59)
by a scaled probit function Φ(λa), where Φ(a) is deﬁned by (4.114). Show that if
λ is chosen so that the derivatives of the two functions are equal at a = 0, then
λ2 = π/8.

224

4. LINEAR MODELS FOR CLASSIFICATION

4.26 ( )

In this exercise, we prove the relation (4.152) for the convolution of a probit
function with a Gaussian distribution. To do this, show that the derivative of the left-
hand side with respect to µ is equal to the derivative of the right-hand side, and then
integrate both sides with respect to µ and then show that the constant of integration
vanishes. Note that before differentiating the left-hand side, it is convenient ﬁrst
to introduce a change of variable given by a = µ + σz so that the integral over a
is replaced by an integral over z. When we differentiate the left-hand side of the
relation (4.152), we will then obtain a Gaussian integral over z that can be evaluated
analytically.

5

Neural

Networks

In Chapters 3 and 4 we considered models for regression and classiﬁcation that com-
prised linear combinations of ﬁxed basis functions. We saw that such models have
useful analytical and computational properties but that their practical applicability
was limited by the curse of dimensionality. In order to apply such models to large-
scale problems, it is necessary to adapt the basis functions to the data.

Support vector machines (SVMs), discussed in Chapter 7, address this by ﬁrst
deﬁning basis functions that are centred on the training data points and then selecting
a subset of these during training. One advantage of SVMs is that, although the
training involves nonlinear optimization, the objective function is convex, and so the
solution of the optimization problem is relatively straightforward. The number of
basis functions in the resulting models is generally much smaller than the number of
training points, although it is often still relatively large and typically increases with
the size of the training set. The relevance vector machine, discussed in Section 7.2,
also chooses a subset from a ﬁxed set of basis functions and typically results in much

225

226

5. NEURAL NETWORKS

sparser models. Unlike the SVM it also produces probabilistic outputs, although this
is at the expense of a nonconvex optimization during training.

An alternative approach is to ﬁx the number of basis functions in advance but
allow them to be adaptive, in other words to use parametric forms for the basis func-
tions in which the parameter values are adapted during training. The most successful
model of this type in the context of pattern recognition is the feed-forward neural
network, also known as the multilayer perceptron, discussed in this chapter. In fact,
‘multilayer perceptron’ is really a misnomer, because the model comprises multi-
ple layers of logistic regression models (with continuous nonlinearities) rather than
multiple perceptrons (with discontinuous nonlinearities). For many applications, the
resulting model can be signiﬁcantly more compact, and hence faster to evaluate, than
a support vector machine having the same generalization performance. The price to
be paid for this compactness, as with the relevance vector machine, is that the like-
lihood function, which forms the basis for network training, is no longer a convex
function of the model parameters. In practice, however, it is often worth investing
substantial computational resources during the training phase in order to obtain a
compact model that is fast at processing new data.

The term ‘neural network’ has its origins in attempts to ﬁnd mathematical rep-
resentations of information processing in biological systems (McCulloch and Pitts,
1943; Widrow and Hoff, 1960; Rosenblatt, 1962; Rumelhart et al., 1986). Indeed,
it has been used very broadly to cover a wide range of different models, many of
which have been the subject of exaggerated claims regarding their biological plau-
sibility. From the perspective of practical applications of pattern recognition, how-
ever, biological realism would impose entirely unnecessary constraints. Our focus in
this chapter is therefore on neural networks as efﬁcient models for statistical pattern
recognition. In particular, we shall restrict our attention to the speciﬁc class of neu-
ral networks that have proven to be of greatest practical value, namely the multilayer
perceptron.

We begin by considering the functional form of the network model, including
the speciﬁc parameterization of the basis functions, and we then discuss the prob-
lem of determining the network parameters within a maximum likelihood frame-
work, which involves the solution of a nonlinear optimization problem. This requires
the evaluation of derivatives of the log likelihood function with respect to the net-
work parameters, and we shall see how these can be obtained efﬁciently using the
technique of error backpropagation. We shall also show how the backpropagation
framework can be extended to allow other derivatives to be evaluated, such as the
Jacobian and Hessian matrices. Next we discuss various approaches to regulariza-
tion of neural network training and the relationships between them. We also consider
some extensions to the neural network model, and in particular we describe a gen-
eral framework for modelling conditional probability distributions known as mixture
density networks. Finally, we discuss the use of Bayesian treatments of neural net-
works. Additional background on neural network models can be found in Bishop
(1995a).







M




i=1

D

M

5.1. Feed-forward Network Functions

227

5.1. Feed-forward Network Functions

The linear models for regression and classiﬁcation discussed in Chapters 3 and 4, re-
spectively, are based on linear combinations of ﬁxed nonlinear basis functions φj(x)
and take the form

y(x, w) = f

wjφj(x)

j=1

(5.1)

where f(·) is a nonlinear activation function in the case of classiﬁcation and is the
identity in the case of regression. Our goal is to extend this model by making the
basis functions φj(x) depend on parameters and then to allow these parameters to
be adjusted, along with the coefﬁcients {wj}, during training. There are, of course,
many ways to construct parametric nonlinear basis functions. Neural networks use
basis functions that follow the same form as (5.1), so that each basis function is itself
a nonlinear function of a linear combination of the inputs, where the coefﬁcients in
the linear combination are adaptive parameters.

This leads to the basic neural network model, which can be described a series
of functional transformations. First we construct M linear combinations of the input
variables x1, . . . , xD in the form

aj =

w(1)
ji xi + w(1)

j0

(5.2)

where j = 1, . . . , M, and the superscript (1) indicates that the corresponding param-
eters are in the ﬁrst ‘layer’ of the network. We shall refer to the parameters w(1)
ji as
weights and the parameters w(1)
j0 as biases, following the nomenclature of Chapter 3.
The quantities aj are known as activations. Each of them is then transformed using
a differentiable, nonlinear activation function h(·) to give

zj = h(aj).

(5.3)

These quantities correspond to the outputs of the basis functions in (5.1) that, in the
context of neural networks, are called hidden units. The nonlinear functions h(·) are
generally chosen to be sigmoidal functions such as the logistic sigmoid or the ‘tanh’
function. Following (5.1), these values are again linearly combined to give output
unit activations

ak =

w(2)
kj zj + w(2)

k0

j=1

(5.4)

where k = 1, . . . , K, and K is the total number of outputs. This transformation cor-
responds to the second layer of the network, and again the w(2)
k0 are bias parameters.
Finally, the output unit activations are transformed using an appropriate activation
function to give a set of network outputs yk. The choice of activation function is
determined by the nature of the data and the assumed distribution of target variables

Exercise 5.1





M





D





228

5. NEURAL NETWORKS

Figure 5.1 Network diagram for the two-
layer neural network corre-
sponding to (5.7). The input,
hidden, and output variables
are represented by nodes, and
the weight parameters are rep-
resented by links between the
nodes,
in which the bias pa-
rameters are denoted by links
coming from additional
input
and hidden variables x0 and
z0. Arrows denote the direc-
tion of information ﬂow through
the network during forward
propagation.

xD

inputs

x1

x0

hidden units

w(1)
M D

w(2)
KM

zM

z1

z0

yK

outputs

y1

w(2)
10

and follows the same considerations as for linear models discussed in Chapters 3 and
4. Thus for standard regression problems, the activation function is the identity so
that yk = ak. Similarly, for multiple binary classiﬁcation problems, each output unit
activation is transformed using a logistic sigmoid function so that

where

yk = σ(ak)

σ(a) =

1

1 + exp(−a) .

(5.5)

(5.6)

Finally, for multiclass problems, a softmax activation function of the form (4.62)
is used. The choice of output unit activation function is discussed in detail in Sec-
tion 5.2.

We can combine these various stages to give the overall network function that,

for sigmoidal output unit activation functions, takes the form

yk(x, w) = σ

w(2)
kj h

w(1)
ji xi + w(1)

j0

+ w(2)
k0

(5.7)

j=1

i=1

where the set of all weight and bias parameters have been grouped together into a
vector w. Thus the neural network model is simply a nonlinear function from a set
of input variables {xi} to a set of output variables {yk} controlled by a vector w of
adjustable parameters.
This function can be represented in the form of a network diagram as shown
in Figure 5.1. The process of evaluating (5.7) can then be interpreted as a forward
propagation of information through the network. It should be emphasized that these
diagrams do not represent probabilistic graphical models of the kind to be consid-
ered in Chapter 8 because the internal nodes represent deterministic variables rather
than stochastic ones. For this reason, we have adopted a slightly different graphical




i=0

D

M







D



5.1. Feed-forward Network Functions

229

notation for the two kinds of model. We shall see later how to give a probabilistic
interpretation to a neural network.

As discussed in Section 3.1, the bias parameters in (5.2) can be absorbed into
the set of weight parameters by deﬁning an additional input variable x0 whose value
is clamped at x0 = 1, so that (5.2) takes the form

aj =

w(1)

ji xi.

(5.8)

We can similarly absorb the second-layer biases into the second-layer weights, so
that the overall network function becomes

yk(x, w) = σ

w(2)
kj h

w(1)

ji xi

.

(5.9)

j=0

i=0

As can be seen from Figure 5.1, the neural network model comprises two stages
of processing, each of which resembles the perceptron model of Section 4.1.7, and
for this reason the neural network is also known as the multilayer perceptron, or
MLP. A key difference compared to the perceptron, however, is that the neural net-
work uses continuous sigmoidal nonlinearities in the hidden units, whereas the per-
ceptron uses step-function nonlinearities. This means that the neural network func-
tion is differentiable with respect to the network parameters, and this property will
play a central role in network training.

If the activation functions of all the hidden units in a network are taken to be
linear, then for any such network we can always ﬁnd an equivalent network without
hidden units. This follows from the fact that the composition of successive linear
transformations is itself a linear transformation. However, if the number of hidden
units is smaller than either the number of input or output units, then the transforma-
tions that the network can generate are not the most general possible linear trans-
formations from inputs to outputs because information is lost in the dimensionality
reduction at the hidden units. In Section 12.4.2, we show that networks of linear
units give rise to principal component analysis. In general, however, there is little
interest in multilayer networks of linear units.

The network architecture shown in Figure 5.1 is the most commonly used one
in practice. However, it is easily generalized, for instance by considering additional
layers of processing each consisting of a weighted linear combination of the form
(5.4) followed by an element-wise transformation using a nonlinear activation func-
tion. Note that there is some confusion in the literature regarding the terminology
for counting the number of layers in such networks. Thus the network in Figure 5.1
may be described as a 3-layer network (which counts the number of layers of units,
and treats the inputs as units) or sometimes as a single-hidden-layer network (which
counts the number of layers of hidden units). We recommend a terminology in which
Figure 5.1 is called a two-layer network, because it is the number of layers of adap-
tive weights that is important for determining the network properties.

Another generalization of the network architecture is to include skip-layer con-
nections, each of which is associated with a corresponding adaptive parameter. For



inputs

x2

x1



230

5. NEURAL NETWORKS

Figure 5.2 Example of a neural network having a
general feed-forward topology. Note that
each hidden and output unit has an
associated bias parameter (omitted for
clarity).

z1

z2

z3

y2

outputs

y1

instance, in a two-layer network these would go directly from inputs to outputs. In
principle, a network with sigmoidal hidden units can always mimic skip layer con-
nections (for bounded input values) by using a sufﬁciently small ﬁrst-layer weight
that, over its operating range, the hidden unit is effectively linear, and then com-
pensating with a large weight value from the hidden unit to the output. In practice,
however, it may be advantageous to include skip-layer connections explicitly.

Furthermore, the network can be sparse, with not all possible connections within
a layer being present. We shall see an example of a sparse network architecture when
we consider convolutional neural networks in Section 5.5.6.

Because there is a direct correspondence between a network diagram and its
mathematical function, we can develop more general network mappings by con-
sidering more complex network diagrams. However, these must be restricted to a
feed-forward architecture, in other words to one having no closed directed cycles, to
ensure that the outputs are deterministic functions of the inputs. This is illustrated
with a simple example in Figure 5.2. Each (hidden or output) unit in such a network
computes a function given by

zk = h

wkjzj

j

(5.10)

where the sum runs over all units that send connections to unit k (and a bias param-
eter is included in the summation). For a given set of values applied to the inputs of
the network, successive application of (5.10) allows the activations of all units in the
network to be evaluated including those of the output units.

The approximation properties of feed-forward networks have been widely stud-
ied (Funahashi, 1989; Cybenko, 1989; Hornik et al., 1989; Stinchecombe and White,
1989; Cotter, 1990; Ito, 1991; Hornik, 1991; Kreinovich, 1991; Ripley, 1996) and
found to be very general. Neural networks are therefore said to be universal ap-
proximators. For example, a two-layer network with linear outputs can uniformly
approximate any continuous function on a compact input domain to arbitrary accu-
racy provided the network has a sufﬁciently large number of hidden units. This result
holds for a wide range of hidden unit activation functions, but excluding polynomi-
als. Although such theorems are reassuring, the key problem is how to ﬁnd suitable
parameter values given a set of training data, and in later sections of this chapter we

Figure 5.3 Illustration of
the ca-
pability of a multilayer perceptron
to approximate four different func-
tions comprising (a) f (x) = x2, (b)
f (x) = sin(x), (c), f (x) = |x|,
and (d) f (x) = H(x) where H(x)
is the Heaviside step function.
In
each case, N = 50 data points,
shown as blue dots, have been sam-
pled uniformly in x over the interval
(−1, 1) and the corresponding val-
ues of f (x) evaluated. These data
points are then used to train a two-
layer network having 3 hidden units
with ‘tanh’ activation functions and
linear output units. The resulting
network functions are shown by the
red curves, and the outputs of the
three hidden units are shown by the
three dashed curves.

5.1. Feed-forward Network Functions

231

(a)

(c)

(b)

(d)

will show that there exist effective solutions to this problem based on both maximum
likelihood and Bayesian approaches.

The capability of a two-layer network to model a broad range of functions is
illustrated in Figure 5.3. This ﬁgure also shows how individual hidden units work
collaboratively to approximate the ﬁnal function. The role of hidden units in a simple
classiﬁcation problem is illustrated in Figure 5.4 using the synthetic classiﬁcation
data set described in Appendix A.

5.1.1 Weight-space symmetries
One property of feed-forward networks, which will play a role when we consider
Bayesian model comparison, is that multiple distinct choices for the weight vector
w can all give rise to the same mapping function from inputs to outputs (Chen et al.,
1993). Consider a two-layer network of the form shown in Figure 5.1 with M hidden
units having ‘tanh’ activation functions and full connectivity in both layers. If we
change the sign of all of the weights and the bias feeding into a particular hidden
unit, then, for a given input pattern, the sign of the activation of the hidden unit will
be reversed, because ‘tanh’ is an odd function, so that tanh(−a) = − tanh(a). This
transformation can be exactly compensated by changing the sign of all of the weights
leading out of that hidden unit. Thus, by changing the signs of a particular group of
weights (and a bias), the input–output mapping function represented by the network
is unchanged, and so we have found two different weight vectors that give rise to
the same mapping function. For M hidden units, there will be M such ‘sign-ﬂip’

232

5. NEURAL NETWORKS

Figure 5.4 Example of the solution of a simple two-
class classiﬁcation problem involving
synthetic data using a neural network
having two inputs, two hidden units with
‘tanh’ activation functions, and a single
output having a logistic sigmoid activa-
tion function. The dashed blue lines
show the z = 0.5 contours for each of
the hidden units, and the red line shows
the y = 0.5 decision surface for the net-
work. For comparison, the green line
denotes the optimal decision boundary
computed from the distributions used to
generate the data.

3

2

1

0

−1

−2

−2

−1

0

1

2

symmetries, and thus any given weight vector will be one of a set 2M equivalent
weight vectors .

Similarly, imagine that we interchange the values of all of the weights (and the
bias) leading both into and out of a particular hidden unit with the corresponding
values of the weights (and bias) associated with a different hidden unit. Again, this
clearly leaves the network input–output mapping function unchanged, but it corre-
sponds to a different choice of weight vector. For M hidden units, any given weight
vector will belong to a set of M! equivalent weight vectors associated with this inter-
change symmetry, corresponding to the M! different orderings of the hidden units.
The network will therefore have an overall weight-space symmetry factor of M!2M .
For networks with more than two layers of weights, the total level of symmetry will
be given by the product of such factors, one for each layer of hidden units.

It turns out that these factors account for all of the symmetries in weight space
(except for possible accidental symmetries due to speciﬁc choices for the weight val-
ues). Furthermore, the existence of these symmetries is not a particular property of
the ‘tanh’ function but applies to a wide range of activation functions (K˙urkov´a and
Kainen, 1994). In many cases, these symmetries in weight space are of little practi-
cal consequence, although in Section 5.7 we shall encounter a situation in which we
need to take them into account.

5.2. Network Training

So far, we have viewed neural networks as a general class of parametric nonlinear
functions from a vector x of input variables to a vector y of output variables. A
simple approach to the problem of determining the network parameters is to make an
analogy with the discussion of polynomial curve ﬁtting in Section 1.1, and therefore
to minimize a sum-of-squares error function. Given a training set comprising a set
of input vectors {xn}, where n = 1, . . . , N, together with a corresponding set of









n=1

N

N

N



N

n=1

target vectors {tn}, we minimize the error function

E(w) =

1
2

y(xn, w) − tn2.

5.2. Network Training

233

(5.11)

However, we can provide a much more general view of network training by ﬁrst
giving a probabilistic interpretation to the network outputs. We have already seen
many advantages of using probabilistic predictions in Section 1.5.4. Here it will also
provide us with a clearer motivation both for the choice of output unit nonlinearity
and the choice of error function.

We start by discussing regression problems, and for the moment we consider
a single target variable t that can take any real value. Following the discussions
in Section 1.2.5 and 3.1, we assume that t has a Gaussian distribution with an x-
dependent mean, which is given by the output of the neural network, so that

p(t|x, w) = N

t|y(x, w), β−1

(5.12)

where β is the precision (inverse variance) of the Gaussian noise. Of course this
is a somewhat restrictive assumption, and in Section 5.6 we shall see how to extend
this approach to allow for more general conditional distributions. For the conditional
distribution given by (5.12), it is sufﬁcient to take the output unit activation function
to be the identity, because such a network can approximate any continuous function
from x to y. Given a data set of N independent, identically distributed observations
X = {x1, . . . , xN}, along with corresponding target values t = {t1, . . . , tN}, we
can construct the corresponding likelihood function

p(t|X, w, β) =

p(tn|xn, w, β).

n=1

Taking the negative logarithm, we obtain the error function

β
2

{y(xn, w) − tn}2 −

N
2

ln β + N
2

ln(2π)

(5.13)

which can be used to learn the parameters w and β. In Section 5.7, we shall dis-
cuss the Bayesian treatment of neural networks, while here we consider a maximum
likelihood approach. Note that in the neural networks literature, it is usual to con-
sider the minimization of an error function rather than the maximization of the (log)
likelihood, and so here we shall follow this convention. Consider ﬁrst the determi-
nation of w. Maximizing the likelihood function is equivalent to minimizing the
sum-of-squares error function given by

E(w) =

1
2

{y(xn, w) − tn}2

n=1

(5.14)




n=1

N

N






234

5. NEURAL NETWORKS

where we have discarded additive and multiplicative constants. The value of w found
by minimizing E(w) will be denoted wML because it corresponds to the maximum
likelihood solution. In practice, the nonlinearity of the network function y(xn, w)
causes the error E(w) to be nonconvex, and so in practice local maxima of the
likelihood may be found, corresponding to local minima of the error function, as
discussed in Section 5.2.1.

Having found wML, the value of β can be found by minimizing the negative log

likelihood to give

1
βML

=

1
N

{y(xn, wML) − tn}2.

(5.15)

Note that this can be evaluated once the iterative optimization required to ﬁnd wML
is completed. If we have multiple target variables, and we assume that they are inde-
pendent conditional on x and w with shared noise precision β, then the conditional
distribution of the target values is given by

p(t|x, w) = N

t|y(x, w), β−1I

.

(5.16)

Following the same argument as for a single target variable, we see that the maximum
likelihood weights are determined by minimizing the sum-of-squares error function
(5.11). The noise precision is then given by

Exercise 5.2

Exercise 5.3

1
βML

=

1
N K

y(xn, wML) − tn2

n=1

(5.17)

where K is the number of target variables. The assumption of independence can be
dropped at the expense of a slightly more complex optimization problem.

Recall from Section 4.3.6 that there is a natural pairing of the error function
(given by the negative log likelihood) and the output unit activation function. In the
regression case, we can view the network as having an output activation function that
is the identity, so that yk = ak. The corresponding sum-of-squares error function
has the property

∂E
∂ak

= yk − tk

(5.18)

which we shall make use of when discussing error backpropagation in Section 5.3.
Now consider the case of binary classiﬁcation in which we have a single target
variable t such that t = 1 denotes class C1 and t = 0 denotes class C2. Following
the discussion of canonical link functions in Section 4.3.6, we consider a network
having a single output whose activation function is a logistic sigmoid

y = σ(a) ≡

1

1 + exp(−a)

(5.19)

so that 0 � y(x, w) � 1. We can interpret y(x, w) as the conditional probability
p(C1|x), with p(C2|x) given by 1 − y(x, w). The conditional distribution of targets
given inputs is then a Bernoulli distribution of the form

p(t|x, w) = y(x, w)t {1 − y(x, w)}1−t .

(5.20)





n=1

K

N

K

k=1



N



N



K

If we consider a training set of independent observations, then the error function,
which is given by the negative log likelihood, is then a cross-entropy error function
of the form

E(w) = −

{tn ln yn + (1 − tn) ln(1 − yn)}

(5.21)

where yn denotes y(xn, w). Note that there is no analogue of the noise precision β
because the target values are assumed to be correctly labelled. However, the model
is easily extended to allow for labelling errors. Simard et al. (2003) found that using
the cross-entropy error function instead of the sum-of-squares for a classiﬁcation
problem leads to faster training as well as improved generalization.

If we have K separate binary classiﬁcations to perform, then we can use a net-
work having K outputs each of which has a logistic sigmoid activation function.
Associated with each output is a binary class label tk ∈ {0, 1}, where k = 1, . . . , K.
If we assume that the class labels are independent, given the input vector, then the
conditional distribution of the targets is

p(t|x, w) =

yk(x, w)tk [1 − yk(x, w)]1−tk .

(5.22)

Taking the negative logarithm of the corresponding likelihood function then gives
the following error function

E(w) = −

n=1

k=1

{tnk ln ynk + (1 − tnk) ln(1 − ynk)}

(5.23)

where ynk denotes yk(xn, w). Again, the derivative of the error function with re-
spect to the activation for a particular output unit takes the form (5.18) just as in the
regression case.

It is interesting to contrast the neural network solution to this problem with the
corresponding approach based on a linear classiﬁcation model of the kind discussed
in Chapter 4. Suppose that we are using a standard two-layer network of the kind
shown in Figure 5.1. We see that the weight parameters in the ﬁrst layer of the
network are shared between the various outputs, whereas in the linear model each
classiﬁcation problem is solved independently. The ﬁrst layer of the network can
be viewed as performing a nonlinear feature extraction, and the sharing of features
between the different outputs can save on computation and can also lead to improved
generalization.

Finally, we consider the standard multiclass classiﬁcation problem in which each
input is assigned to one of K mutually exclusive classes. The binary target variables
tk ∈ {0, 1} have a 1-of-K coding scheme indicating the class, and the network
outputs are interpreted as yk(x, w) = p(tk = 1|x), leading to the following error
function

E(w) = −

n=1

k=1

tkn ln yk(xn, w).

(5.24)

5.2. Network Training

235

Exercise 5.4

Exercise 5.5

Exercise 5.6





236

5. NEURAL NETWORKS

Figure 5.5 Geometrical view of the error function E(w) as
a surface sitting over weight space. Point wA is
a local minimum and wB is the global minimum.
At any point wC, the local gradient of the error
surface is given by the vector ∇E.

E(w)

wA

wB

wC

w1

w2

∇E

Following the discussion of Section 4.3.4, we see that the output unit activation
function, which corresponds to the canonical link, is given by the softmax function

yk(x, w) =

(5.25)

exp(ak(x, w))

exp(aj(x, w))

j

which satisﬁes 0 � yk � 1 and
k yk = 1. Note that the yk(x, w) are unchanged
if a constant is added to all of the ak(x, w), causing the error function to be constant
for some directions in weight space. This degeneracy is removed if an appropriate
regularization term (Section 5.5) is added to the error function.

Once again, the derivative of the error function with respect to the activation for

Exercise 5.7

a particular output unit takes the familiar form (5.18).

In summary, there is a natural choice of both output unit activation function
and matching error function, according to the type of problem being solved. For re-
gression we use linear outputs and a sum-of-squares error, for (multiple independent)
binary classiﬁcations we use logistic sigmoid outputs and a cross-entropy error func-
tion, and for multiclass classiﬁcation we use softmax outputs with the corresponding
multiclass cross-entropy error function. For classiﬁcation problems involving two
classes, we can use a single logistic sigmoid output, or alternatively we can use a
network with two outputs having a softmax output activation function.

5.2.1 Parameter optimization
We turn next to the task of ﬁnding a weight vector w which minimizes the
chosen function E(w). At this point, it is useful to have a geometrical picture of the
error function, which we can view as a surface sitting over weight space as shown in
Figure 5.5. First note that if we make a small step in weight space from w to w+ δw
then the change in the error function is δE  δwT∇E(w), where the vector ∇E(w)
points in the direction of greatest rate of increase of the error function. Because the
error E(w) is a smooth continuous function of w, its smallest value will occur at a

Section 5.1.1







5.2. Network Training

237

point in weight space such that the gradient of the error function vanishes, so that

(5.26)
as otherwise we could make a small step in the direction of −∇E(w) and thereby
further reduce the error. Points at which the gradient vanishes are called stationary
points, and may be further classiﬁed into minima, maxima, and saddle points.

∇E(w) = 0

Our goal is to ﬁnd a vector w such that E(w) takes its smallest value. How-
ever, the error function typically has a highly nonlinear dependence on the weights
and bias parameters, and so there will be many points in weight space at which the
gradient vanishes (or is numerically very small). Indeed, from the discussion in Sec-
tion 5.1.1 we see that for any point w that is a local minimum, there will be other
points in weight space that are equivalent minima. For instance, in a two-layer net-
work of the kind shown in Figure 5.1, with M hidden units, each point in weight
space is a member of a family of M!2M equivalent points.

Furthermore, there will typically be multiple inequivalent stationary points and
in particular multiple inequivalent minima. A minimum that corresponds to the
smallest value of the error function for any weight vector is said to be a global
minimum. Any other minima corresponding to higher values of the error function
are said to be local minima. For a successful application of neural networks, it may
not be necessary to ﬁnd the global minimum (and in general it will not be known
whether the global minimum has been found) but it may be necessary to compare
several local minima in order to ﬁnd a sufﬁciently good solution.

Because there is clearly no hope of ﬁnding an analytical solution to the equa-
tion ∇E(w) = 0 we resort to iterative numerical procedures. The optimization of
continuous nonlinear functions is a widely studied problem and there exists an ex-
tensive literature on how to solve it efﬁciently. Most techniques involve choosing
some initial value w(0) for the weight vector and then moving through weight space
in a succession of steps of the form

w(τ +1) = w(τ ) + ∆w(τ )

(5.27)

where τ labels the iteration step. Different algorithms involve different choices for
the weight vector update ∆w(τ ). Many algorithms make use of gradient information
and therefore require that, after each update, the value of ∇E(w) is evaluated at
the new weight vector w(τ +1). In order to understand the importance of gradient
information, it is useful to consider a local approximation to the error function based
on a Taylor expansion.

5.2.2 Local quadratic approximation
Insight into the optimization problem, and into the various techniques for solv-
ing it, can be obtained by considering a local quadratic approximation to the error
function.

Consider the Taylor expansion of E(w) around some point

w in weight space

E(w)  E(

w) + (w −

w)Tb +

1
2

(w −

w)TH(w −

w)

(5.28)







238

5. NEURAL NETWORKS






b
b


w=

where cubic and higher terms have been omitted. Here b is deﬁned to be the gradient
of E evaluated at

w

b ≡ ∇E|w=
and the Hessian matrix H = ∇∇E has elements

w

(H)ij ≡

∂E

∂wi∂wj

w

.

(5.30)

From (5.28), the corresponding local approximation to the gradient is given by

∇E  b + H(w −

w).

For points w that are sufﬁciently close to
approximations for the error and its gradient.

w, these expressions will give reasonable

Consider the particular case of a local quadratic approximation around a point
w that is a minimum of the error function. In this case there is no linear term,
because ∇E = 0 at w, and (5.28) becomes

E(w) = E(w) +

1
2

(w − w)TH(w − w)

where the Hessian H is evaluated at w. In order to interpret this geometrically,
consider the eigenvalue equation for the Hessian matrix

(5.29)

(5.31)

(5.32)

(5.33)

Hui = λiui

where the eigenvectors ui form a complete orthonormal set (Appendix C) so that

(5.34)
We now expand (w − w) as a linear combination of the eigenvectors in the form
(5.35)

i uj = δij.
uT

αiui.

w − w =

i

This can be regarded as a transformation of the coordinate system in which the origin
is translated to the point w, and the axes are rotated to align with the eigenvectors
(through the orthogonal matrix whose columns are the ui), and is discussed in more
detail in Appendix C. Substituting (5.35) into (5.32), and using (5.33) and (5.34),
allows the error function to be written in the form

E(w) = E(w) +

1
2

i

λiα2
i .

A matrix H is said to be positive deﬁnite if, and only if,

vTHv > 0

for all v.

(5.36)

(5.37)





i

i

w2

the error

Figure 5.6 In the neighbourhood of a min-
imum w,
function
can be approximated by a
quadratic. Contours of con-
stant error are then ellipses
whose axes are aligned with
the eigenvectors ui of the Hes-
sian matrix, with lengths that
are inversely proportional to the
square roots of the correspond-
ing eigenvectors λi.

λ−1/2
2

5.2. Network Training

239

u2

w

u1

w1

λ−1/2
1

Because the eigenvectors {ui} form a complete set, an arbitrary vector v can be
written in the form
(5.38)

v =

ciui.

From (5.33) and (5.34), we then have

vTHv =

c2
i λi

(5.39)

Exercise 5.10

Exercise 5.11

and so H will be positive deﬁnite if, and only if, all of its eigenvalues are positive.
In the new coordinate system, whose basis vectors are given by the eigenvectors
{ui}, the contours of constant E are ellipses centred on the origin, as illustrated
in Figure 5.6. For a one-dimensional weight space, a stationary point w will be a
minimum if

∂2E
∂w2

w

> 0.

(5.40)

Exercise 5.12

The corresponding result in D-dimensions is that the Hessian matrix, evaluated at
w, should be positive deﬁnite.

5.2.3 Use of gradient information
As we shall see in Section 5.3, it is possible to evaluate the gradient of an error
function efﬁciently by means of the backpropagation procedure. The use of this
gradient information can lead to signiﬁcant improvements in the speed with which
the minima of the error function can be located. We can see why this is so, as follows.
In the quadratic approximation to the error function, given in (5.28), the error
surface is speciﬁed by the quantities b and H, which contain a total of W (W +
3)/2 independent elements (because the matrix H is symmetric), where W is the
dimensionality of w (i.e., the total number of adaptive parameters in the network).
The location of the minimum of this quadratic approximation therefore depends on
O(W 2) parameters, and we should not expect to be able to locate the minimum until
we have gathered O(W 2) independent pieces of information. If we do not make
use of gradient information, we would expect to have to perform O(W 2) function

Exercise 5.13

240

5. NEURAL NETWORKS

evaluations, each of which would require O(W ) steps. Thus, the computational
effort needed to ﬁnd the minimum using such an approach would be O(W 3).

Now compare this with an algorithm that makes use of the gradient information.
Because each evaluation of ∇E brings W items of information, we might hope to
ﬁnd the minimum of the function in O(W ) gradient evaluations. As we shall see,
by using error backpropagation, each such evaluation takes only O(W ) steps and so
the minimum can now be found in O(W 2) steps. For this reason, the use of gradient
information forms the basis of practical algorithms for training neural networks.



N

5.2.4 Gradient descent optimization
The simplest approach to using gradient information is to choose the weight
update in (5.27) to comprise a small step in the direction of the negative gradient, so
that

w(τ +1) = w(τ ) − η∇E(w(τ ))

(5.41)
where the parameter η > 0 is known as the learning rate. After each such update, the
gradient is re-evaluated for the new weight vector and the process repeated. Note that
the error function is deﬁned with respect to a training set, and so each step requires
that the entire training set be processed in order to evaluate ∇E. Techniques that
use the whole data set at once are called batch methods. At each step the weight
vector is moved in the direction of the greatest rate of decrease of the error function,
and so this approach is known as gradient descent or steepest descent. Although
such an approach might intuitively seem reasonable, in fact it turns out to be a poor
algorithm, for reasons discussed in Bishop and Nabney (2008).

For batch optimization, there are more efﬁcient methods, such as conjugate gra-
dients and quasi-Newton methods, which are much more robust and much faster
than simple gradient descent (Gill et al., 1981; Fletcher, 1987; Nocedal and Wright,
1999). Unlike gradient descent, these algorithms have the property that the error
function always decreases at each iteration unless the weight vector has arrived at a
local or global minimum.

In order to ﬁnd a sufﬁciently good minimum, it may be necessary to run a
gradient-based algorithm multiple times, each time using a different randomly cho-
sen starting point, and comparing the resulting performance on an independent vali-
dation set.

There is, however, an on-line version of gradient descent that has proved useful
in practice for training neural networks on large data sets (Le Cun et al., 1989).
Error functions based on maximum likelihood for a set of independent observations
comprise a sum of terms, one for each data point

E(w) =

En(w).

n=1

(5.42)

On-line gradient descent, also known as sequential gradient descent or stochastic
gradient descent, makes an update to the weight vector based on one data point at a
time, so that

w(τ +1) = w(τ ) − η∇En(w(τ )).

(5.43)

5.3. Error Backpropagation

241

This update is repeated by cycling through the data either in sequence or by selecting
points at random with replacement. There are of course intermediate scenarios in
which the updates are based on batches of data points.

One advantage of on-line methods compared to batch methods is that the former
handle redundancy in the data much more efﬁciently. To see, this consider an ex-
treme example in which we take a data set and double its size by duplicating every
data point. Note that this simply multiplies the error function by a factor of 2 and so
is equivalent to using the original error function. Batch methods will require double
the computational effort to evaluate the batch error function gradient, whereas on-
line methods will be unaffected. Another property of on-line gradient descent is the
possibility of escaping from local minima, since a stationary point with respect to
the error function for the whole data set will generally not be a stationary point for
each data point individually.

Nonlinear optimization algorithms, and their practical application to neural net-

work training, are discussed in detail in Bishop and Nabney (2008).

5.3. Error Backpropagation

Our goal in this section is to ﬁnd an efﬁcient technique for evaluating the gradient
of an error function E(w) for a feed-forward neural network. We shall see that
this can be achieved using a local message passing scheme in which information is
sent alternately forwards and backwards through the network and is known as error
backpropagation, or sometimes simply as backprop.

It should be noted that the term backpropagation is used in the neural com-
puting literature to mean a variety of different things. For instance, the multilayer
perceptron architecture is sometimes called a backpropagation network. The term
backpropagation is also used to describe the training of a multilayer perceptron us-
ing gradient descent applied to a sum-of-squares error function. In order to clarify
the terminology, it is useful to consider the nature of the training process more care-
fully. Most training algorithms involve an iterative procedure for minimization of an
error function, with adjustments to the weights being made in a sequence of steps. At
each such step, we can distinguish between two distinct stages. In the ﬁrst stage, the
derivatives of the error function with respect to the weights must be evaluated. As
we shall see, the important contribution of the backpropagation technique is in pro-
viding a computationally efﬁcient method for evaluating such derivatives. Because
it is at this stage that errors are propagated backwards through the network, we shall
use the term backpropagation speciﬁcally to describe the evaluation of derivatives.
In the second stage, the derivatives are then used to compute the adjustments to be
made to the weights. The simplest such technique, and the one originally considered
by Rumelhart et al. (1986), involves gradient descent. It is important to recognize
that the two stages are distinct. Thus, the ﬁrst stage, namely the propagation of er-
rors backwards through the network in order to evaluate derivatives, can be applied
to many other kinds of network and not just the multilayer perceptron. It can also be
applied to error functions other that just the simple sum-of-squares, and to the eval-






n=1

N

k

i

yk =

wkixi

En =

1
2

(ynk − tnk)2

∂En
∂wji

= (ynj − tnj)xni

(5.45)

(5.46)

(5.47)

where ynk = yk(xn, w). The gradient of this error function with respect to a weight
wji is given by

which can be interpreted as a ‘local’ computation involving the product of an ‘error
signal’ ynj − tnj associated with the output end of the link wji and the variable xni
associated with the input end of the link. In Section 4.3.2, we saw how a similar
formula arises with the logistic sigmoid activation function together with the cross
entropy error function, and similarly for the softmax activation function together
with its matching cross-entropy error function. We shall now see how this simple
result extends to the more complex setting of multilayer feed-forward networks.

In a general feed-forward network, each unit computes a weighted sum of its

inputs of the form

aj =

wjizi

i

(5.48)

242

5. NEURAL NETWORKS

uation of other derivatives such as the Jacobian and Hessian matrices, as we shall
see later in this chapter. Similarly, the second stage of weight adjustment using the
calculated derivatives can be tackled using a variety of optimization schemes, many
of which are substantially more powerful than simple gradient descent.

5.3.1 Evaluation of error-function derivatives
We now derive the backpropagation algorithm for a general network having ar-
bitrary feed-forward topology, arbitrary differentiable nonlinear activation functions,
and a broad class of error function. The resulting formulae will then be illustrated
using a simple layered network structure having a single layer of sigmoidal hidden
units together with a sum-of-squares error.

Many error functions of practical interest, for instance those deﬁned by maxi-
mum likelihood for a set of i.i.d. data, comprise a sum of terms, one for each data
point in the training set, so that

E(w) =

En(w).

(5.44)

Here we shall consider the problem of evaluating ∇En(w) for one such term in the
error function. This may be used directly for sequential optimization, or the results
can be accumulated over the training set in the case of batch methods.

Consider ﬁrst a simple linear model in which the outputs yk are linear combina-

tions of the input variables xi so that

together with an error function that, for a particular input pattern n, takes the form

5.3. Error Backpropagation

243

where zi is the activation of a unit, or input, that sends a connection to unit j, and wji
is the weight associated with that connection. In Section 5.1, we saw that biases can
be included in this sum by introducing an extra unit, or input, with activation ﬁxed
at +1. We therefore do not need to deal with biases explicitly. The sum in (5.48) is
transformed by a nonlinear activation function h(·) to give the activation zj of unit j
in the form
(5.49)
Note that one or more of the variables zi in the sum in (5.48) could be an input, and
similarly, the unit j in (5.49) could be an output.

zj = h(aj).

For each pattern in the training set, we shall suppose that we have supplied the
corresponding input vector to the network and calculated the activations of all of
the hidden and output units in the network by successive application of (5.48) and
(5.49). This process is often called forward propagation because it can be regarded
as a forward ﬂow of information through the network.

Now consider the evaluation of the derivative of En with respect to a weight
wji. The outputs of the various units will depend on the particular input pattern n.
However, in order to keep the notation uncluttered, we shall omit the subscript n
from the network variables. First we note that En depends on the weight wji only
via the summed input aj to unit j. We can therefore apply the chain rule for partial
derivatives to give

∂En
∂wji

= ∂En
∂aj

∂aj
∂wji

.

We now introduce a useful notation

δj ≡

∂En
∂aj

(5.50)

(5.51)

where the δ’s are often referred to as errors for reasons we shall see shortly. Using
(5.48), we can write

(5.52)

(5.53)

∂aj
∂wji

= zi.

Substituting (5.51) and (5.52) into (5.50), we then obtain

∂En
∂wji

= δjzi.

Equation (5.53) tells us that the required derivative is obtained simply by multiplying
the value of δ for the unit at the output end of the weight by the value of z for the unit
at the input end of the weight (where z = 1 in the case of a bias). Note that this takes
the same form as for the simple linear model considered at the start of this section.
Thus, in order to evaluate the derivatives, we need only to calculate the value of δj
for each hidden and output unit in the network, and then apply (5.53).

As we have seen already, for the output units, we have

δk = yk − tk

(5.54)




k

δj ≡

∂En
∂ak

∂ak
∂aj

∂En
∂aj

=

(5.55)

where the sum runs over all units k to which unit j sends connections. The arrange-
ment of units and weights is illustrated in Figure 5.7. Note that the units labelled k
could include other hidden units and/or output units. In writing down (5.55), we are
making use of the fact that variations in aj give rise to variations in the error func-
tion only through variations in the variables ak. If we now substitute the deﬁnition
of δ given by (5.51) into (5.55), and make use of (5.48) and (5.49), we obtain the
following backpropagation formula

δj = h(aj)

wkjδk

k

(5.56)

which tells us that the value of δ for a particular hidden unit can be obtained by
propagating the δ’s backwards from units higher up in the network, as illustrated
in Figure 5.7. Note that the summation in (5.56) is taken over the ﬁrst index on
wkj (corresponding to backward propagation of information through the network),
whereas in the forward propagation equation (5.10) it is taken over the second index.
Because we already know the values of the δ’s for the output units, it follows that
by recursively applying (5.56) we can evaluate the δ’s for all of the hidden units in a
feed-forward network, regardless of its topology.

The backpropagation procedure can therefore be summarized as follows.

Error Backpropagation

1. Apply an input vector xn to the network and forward propagate through
the network using (5.48) and (5.49) to ﬁnd the activations of all the hidden
and output units.

2. Evaluate the δk for all the output units using (5.54).
3. Backpropagate the δ’s using (5.56) to obtain δj for each hidden unit in the

network.

4. Use (5.53) to evaluate the required derivatives.

244

5. NEURAL NETWORKS

Figure 5.7 Illustration of the calculation of δj for hidden unit j by
backpropagation of the δ’s from those units k to which
unit j sends connections. The blue arrow denotes the
direction of information ﬂow during forward propagation,
and the red arrows indicate the backward propagation
of error information.

zi

wji

δj

zj

wkj

δk

δ1

provided we are using the canonical link as the output-unit activation function. To
evaluate the δ’s for hidden units, we again make use of the chain rule for partial
derivatives,






k=1

i=0

K

n

D

M

5.3. Error Backpropagation

245

For batch methods, the derivative of the total error E can then be obtained by
repeating the above steps for each pattern in the training set and then summing over
all patterns:

∂E
∂wji

=

∂En
∂wji

.

(5.57)

In the above derivation we have implicitly assumed that each hidden or output unit in
the network has the same activation function h(·). The derivation is easily general-
ized, however, to allow different units to have individual activation functions, simply
by keeping track of which form of h(·) goes with which unit.

5.3.2 A simple example
The above derivation of the backpropagation procedure allowed for general
forms for the error function, the activation functions, and the network topology. In
order to illustrate the application of this algorithm, we shall consider a particular
example. This is chosen both for its simplicity and for its practical importance, be-
cause many applications of neural networks reported in the literature make use of
this type of network. Speciﬁcally, we shall consider a two-layer network of the form
illustrated in Figure 5.1, together with a sum-of-squares error, in which the output
units have linear activation functions, so that yk = ak, while the hidden units have
logistic sigmoid activation functions given by

where

A useful feature of this function is that its derivative can be expressed in a par-

ticularly simple form:

(5.60)
We also consider a standard sum-of-squares error function, so that for pattern n the
error is given by

h(a) = 1 − h(a)2.

h(a) ≡ tanh(a)
tanh(a) = ea − e−a
ea + e−a .

En =

1
2

(yk − tk)2

w(1)

ji xi

zj = tanh(aj)

yk =

w(2)

kj zj.

j=0

(5.58)

(5.59)

(5.61)

(5.62)

(5.63)

(5.64)

where yk is the activation of output unit k, and tk is the corresponding target, for a
particular input pattern xn.

For each pattern in the training set in turn, we ﬁrst perform a forward propagation

using

aj =



K

Next we compute the δ’s for each output unit using
δk = yk − tk.

Then we backpropagate these to obtain δs for the hidden units using

(5.65)

(5.66)

246

5. NEURAL NETWORKS

δj = (1 − z2
j )

k=1

wkjδk.

Finally, the derivatives with respect to the ﬁrst-layer and second-layer weights are
given by

∂En
∂w(1)
ji

= δjxi,

∂En
∂w(2)
kj

= δkzj.

(5.67)

5.3.3 Efﬁciency of backpropagation
One of the most important aspects of backpropagation is its computational efﬁ-
ciency. To understand this, let us examine how the number of computer operations
required to evaluate the derivatives of the error function scales with the total number
W of weights and biases in the network. A single evaluation of the error function
(for a given input pattern) would require O(W ) operations, for sufﬁciently large W .
This follows from the fact that, except for a network with very sparse connections,
the number of weights is typically much greater than the number of units, and so the
bulk of the computational effort in forward propagation is concerned with evaluat-
ing the sums in (5.48), with the evaluation of the activation functions representing a
small overhead. Each term in the sum in (5.48) requires one multiplication and one
addition, leading to an overall computational cost that is O(W ).

An alternative approach to backpropagation for computing the derivatives of the
error function is to use ﬁnite differences. This can be done by perturbing each weight
in turn, and approximating the derivatives by the expression

∂En
∂wji

= En(wji + 	) − En(wji)

	

+ O(	)

(5.68)

where 	  1. In a software simulation, the accuracy of the approximation to the
derivatives can be improved by making 	 smaller, until numerical roundoff problems
arise. The accuracy of the ﬁnite differences method can be improved signiﬁcantly
by using symmetrical central differences of the form

∂En
∂wji

= En(wji + 	) − En(wji − 	)

2	

+ O(	2).

(5.69)

Exercise 5.14

In this case, the O(	) corrections cancel, as can be veriﬁed by Taylor expansion on
the right-hand side of (5.69), and so the residual corrections are O(	2). The number
of computational steps is, however, roughly doubled compared with (5.68).

The main problem with numerical differentiation is that the highly desirable
O(W ) scaling has been lost. Each forward propagation requires O(W ) steps, and

Figure 5.8 Illustration of a modular pattern
recognition system in which the
Jacobian matrix can be used
to backpropagate error signals
from the outputs through to ear-
lier modules in the system.

u

x

5.3. Error Backpropagation

247

v

z

w

y



there are W weights in the network each of which must be perturbed individually, so
that the overall scaling is O(W 2).

However, numerical differentiation plays an important role in practice, because a
comparison of the derivatives calculated by backpropagation with those obtained us-
ing central differences provides a powerful check on the correctness of any software
implementation of the backpropagation algorithm. When training networks in prac-
tice, derivatives should be evaluated using backpropagation, because this gives the
greatest accuracy and numerical efﬁciency. However, the results should be compared
with numerical differentiation using (5.69) for some test cases in order to check the
correctness of the implementation.

5.3.4 The Jacobian matrix
We have seen how the derivatives of an error function with respect to the weights
can be obtained by the propagation of errors backwards through the network. The
technique of backpropagation can also be applied to the calculation of other deriva-
tives. Here we consider the evaluation of the Jacobian matrix, whose elements are
given by the derivatives of the network outputs with respect to the inputs

Jki ≡

∂yk
∂xi

(5.70)

where each such derivative is evaluated with all other inputs held ﬁxed. Jacobian
matrices play a useful role in systems built from a number of distinct modules, as
illustrated in Figure 5.8. Each module can comprise a ﬁxed or adaptive function,
which can be linear or nonlinear, so long as it is differentiable. Suppose we wish
to minimize an error function E with respect to the parameter w in Figure 5.8. The
derivative of the error function is given by

∂E
∂w

=

k,j

∂E
∂yk

∂yk
∂zj

∂zj
∂w

(5.71)

in which the Jacobian matrix for the red module in Figure 5.8 appears in the middle
term.

Because the Jacobian matrix provides a measure of the local sensitivity of the
outputs to changes in each of the input variables, it also allows any known errors ∆xi




=

=

i





j

j

associated with the inputs to be propagated through the trained network in order to
estimate their contribution ∆yk to the errors at the outputs, through the relation

∆yk 

∂yk
∂xi

∆xi

(5.72)

which is valid provided the |∆xi| are small. In general, the network mapping rep-
resented by a trained neural network will be nonlinear, and so the elements of the
Jacobian matrix will not be constants but will depend on the particular input vector
used. Thus (5.72) is valid only for small perturbations of the inputs, and the Jacobian
itself must be re-evaluated for each new input vector.

The Jacobian matrix can be evaluated using a backpropagation procedure that is
similar to the one derived earlier for evaluating the derivatives of an error function
with respect to the weights. We start by writing the element Jki in the form

Jki = ∂yk
∂xi

∂yk
∂aj

∂aj
∂xi

wji

∂yk
∂aj

(5.73)

where we have made use of (5.48). The sum in (5.73) runs over all units j to which
the input unit i sends connections (for example, over all units in the ﬁrst hidden
layer in the layered topology considered earlier). We now write down a recursive
backpropagation formula to determine the derivatives ∂yk/∂aj

248

5. NEURAL NETWORKS

∂yk
∂aj

=

l

∂yk
∂al

∂al
∂aj

= h(aj)

wlj

∂yk
∂al

l

(5.74)

where the sum runs over all units l to which unit j sends connections (corresponding
to the ﬁrst index of wlj). Again, we have made use of (5.48) and (5.49). This
backpropagation starts at the output units for which the required derivatives can be
found directly from the functional form of the output-unit activation function. For
instance, if we have individual sigmoidal activation functions at each output unit,
then

∂yk
∂aj
whereas for softmax outputs we have

= δkjσ(aj)

(5.75)

(5.76)

∂yk
∂aj

= δkjyk − ykyj.

We can summarize the procedure for evaluating the Jacobian matrix as follows.
Apply the input vector corresponding to the point in input space at which the Ja-
cobian matrix is to be found, and forward propagate in the usual way to obtain the

5.4. The Hessian Matrix

249

activations of all of the hidden and output units in the network. Next, for each row
k of the Jacobian matrix, corresponding to the output unit k, backpropagate using
the recursive relation (5.74), starting with (5.75) or (5.76), for all of the hidden units
in the network. Finally, use (5.73) to do the backpropagation to the inputs. The
Jacobian can also be evaluated using an alternative forward propagation formalism,
which can be derived in an analogous way to the backpropagation approach given
here.

Again, the implementation of such algorithms can be checked by using numeri-

cal differentiation in the form

∂yk
∂xi

= yk(xi + 	) − yk(xi − 	)

2	

+ O(	2)

(5.77)

which involves 2D forward propagations for a network having D inputs.

Exercise 5.15

5.4. The Hessian Matrix

We have shown how the technique of backpropagation can be used to obtain the ﬁrst
derivatives of an error function with respect to the weights in the network. Back-
propagation can also be used to evaluate the second derivatives of the error, given
by

∂2E

∂wji∂wlk

.

(5.78)

Note that it is sometimes convenient to consider all of the weight and bias parameters
as elements wi of a single vector, denoted w, in which case the second derivatives
form the elements Hij of the Hessian matrix H, where i, j ∈ {1, . . . , W} and W is
the total number of weights and biases. The Hessian plays an important role in many
aspects of neural computing, including the following:

1. Several nonlinear optimization algorithms used for training neural networks
are based on considerations of the second-order properties of the error surface,
which are controlled by the Hessian matrix (Bishop and Nabney, 2008).

2. The Hessian forms the basis of a fast procedure for re-training a feed-forward

network following a small change in the training data (Bishop, 1991).

3. The inverse of the Hessian has been used to identify the least signiﬁcant weights

in a network as part of network ‘pruning’ algorithms (Le Cun et al., 1990).

4. The Hessian plays a central role in the Laplace approximation for a Bayesian
neural network (see Section 5.7). Its inverse is used to determine the predic-
tive distribution for a trained network, its eigenvalues determine the values of
hyperparameters, and its determinant is used to evaluate the model evidence.

Various approximation schemes have been used to evaluate the Hessian matrix
for a neural network. However, the Hessian can also be calculated exactly using an
extension of the backpropagation technique.





k







250

5. NEURAL NETWORKS

An important consideration for many applications of the Hessian is the efﬁciency
with which it can be evaluated. If there are W parameters (weights and biases) in the
network, then the Hessian matrix has dimensions W × W and so the computational
effort needed to evaluate the Hessian will scale like O(W 2) for each pattern in the
data set. As we shall see, there are efﬁcient methods for evaluating the Hessian
whose scaling is indeed O(W 2).

5.4.1 Diagonal approximation
Some of the applications for the Hessian matrix discussed above require the
inverse of the Hessian, rather than the Hessian itself. For this reason, there has
been some interest in using a diagonal approximation to the Hessian, in other words
one that simply replaces the off-diagonal elements with zeros, because its inverse is
trivial to evaluate. Again, we shall consider an error function that consists of a sum
of terms, one for each pattern in the data set, so that E =
n En. The Hessian can
then be obtained by considering one pattern at a time, and then summing the results
over all patterns. From (5.48), the diagonal elements of the Hessian, for pattern n,
can be written

∂2En
∂w2
ji

= ∂2En
∂a2
j

z2
i .

(5.79)

Using (5.48) and (5.49), the second derivatives on the right-hand side of (5.79) can
be found recursively using the chain rule of differential calculus to give a backprop-
agation equation of the form

∂2En
∂a2
j

= h(aj)2

k

k

wkjwkj

∂2En
∂ak∂ak

+ h(aj)

wkj

∂En
∂ak

.

(5.80)

If we now neglect off-diagonal elements in the second-derivative terms, we obtain
(Becker and Le Cun, 1989; Le Cun et al., 1990)

∂2En
∂a2
j

= h(aj)2

w2
kj

∂2En
∂a2
k

k

+ h(aj)

wkj

∂En
∂ak

.

k

(5.81)

Note that the number of computational steps required to evaluate this approximation
is O(W ), where W is the total number of weight and bias parameters in the network,
compared with O(W 2) for the full Hessian.

Ricotti et al. (1988) also used the diagonal approximation to the Hessian, but
they retained all terms in the evaluation of ∂2En/∂a2
j and so obtained exact expres-
sions for the diagonal terms. Note that this no longer has O(W ) scaling. The major
problem with diagonal approximations, however, is that in practice the Hessian is
typically found to be strongly nondiagonal, and so these approximations, which are
driven mainly be computational convenience, must be treated with care.



N




n=1

N

N




E =

n=1

N

N

5.4. The Hessian Matrix

251

5.4.2 Outer product approximation
When neural networks are applied to regression problems, it is common to use

a sum-of-squares error function of the form

1
2

(yn − tn)2

(5.82)

where we have considered the case of a single output in order to keep the notation
simple (the extension to several outputs is straightforward). We can then write the
Hessian matrix in the form

H = ∇∇E =

∇yn∇yn +

(yn − tn)∇∇yn.

n=1

(5.83)

If the network has been trained on the data set, and its outputs yn happen to be very
close to the target values tn, then the second term in (5.83) will be small and can
be neglected. More generally, however, it may be appropriate to neglect this term
by the following argument. Recall from Section 1.5.5 that the optimal function that
minimizes a sum-of-squares loss is the conditional average of the target data. The
quantity (yn − tn) is then a random variable with zero mean. If we assume that its
value is uncorrelated with the value of the second derivative term on the right-hand
side of (5.83), then the whole term will average to zero in the summation over n.

By neglecting the second term in (5.83), we arrive at the Levenberg–Marquardt
approximation or outer product approximation (because the Hessian matrix is built
up from a sum of outer products of vectors), given by

H 

bnbT
n

n=1

(5.84)

where bn = ∇yn = ∇an because the activation function for the output units is
simply the identity. Evaluation of the outer product approximation for the Hessian
is straightforward as it only involves ﬁrst derivatives of the error function, which
can be evaluated efﬁciently in O(W ) steps using standard backpropagation. The
elements of the matrix can then be found in O(W 2) steps by simple multiplication.
It is important to emphasize that this approximation is only likely to be valid for a
network that has been trained appropriately, and that for a general network mapping
the second derivative terms on the right-hand side of (5.83) will typically not be
negligible.

In the case of the cross-entropy error function for a network with logistic sigmoid

output-unit activation functions, the corresponding approximation is given by

Exercise 5.16

Exercise 5.17

Exercise 5.19

Exercise 5.20

An analogous result can be obtained for multiclass networks having softmax output-
unit activation functions.

H 

n=1

yn(1 − yn)bnbT
n.

(5.85)



N











252

5. NEURAL NETWORKS

5.4.3 Inverse Hessian
We can use the outer-product approximation to develop a computationally ef-
ﬁcient procedure for approximating the inverse of the Hessian (Hassibi and Stork,
1993). First we write the outer-product approximation in matrix notation as

HN =

bnbT
n

n=1

(5.86)

where bn ≡ ∇wan is the contribution to the gradient of the output unit activation
arising from data point n. We now derive a sequential procedure for building up the
Hessian by including data points one at a time. Suppose we have already obtained
the inverse Hessian using the ﬁrst L data points. By separating off the contribution
from data point L + 1, we obtain

In order to evaluate the inverse of the Hessian, we now consider the matrix identity

HL+1 = HL + bL+1bT

L+1.

(5.87)

M + vvT

−1 = M−1 −

(M−1v)

vTM−1
1 + vTM−1v

(5.88)

where I is the unit matrix, which is simply a special case of the Woodbury identity
(C.7). If we now identify HL with M and bL+1 with v, we obtain

H−1

L+1 = H−1

L −

H−1
1 + bT

L bL+1bT
L+1H−1

L+1H−1
L bL+1

L

.

(5.89)

Exercise 5.21

In this way, data points are sequentially absorbed until L+1 = N and the whole data
set has been processed. This result therefore represents a procedure for evaluating
the inverse of the Hessian using a single pass through the data set. The initial matrix
H0 is chosen to be αI, where α is a small quantity, so that the algorithm actually
ﬁnds the inverse of H + αI. The results are not particularly sensitive to the precise
value of α. Extension of this algorithm to networks having more than one output is
straightforward.

We note here that the Hessian matrix can sometimes be calculated indirectly as
part of the network training algorithm. In particular, quasi-Newton nonlinear opti-
mization algorithms gradually build up an approximation to the inverse of the Hes-
sian during training. Such algorithms are discussed in detail in Bishop and Nabney
(2008).

5.4.4 Finite differences
As in the case of the ﬁrst derivatives of the error function, we can ﬁnd the second
derivatives by using ﬁnite differences, with accuracy limited by numerical precision.
If we perturb each possible pair of weights in turn, we obtain

∂wji∂wlk

∂2E

=

1
4	2 {E(wji + 	, wlk + 	) − E(wji + 	, wlk − 	)
−E(wji − 	, wlk + 	) + E(wji − 	, wlk − 	)} + O(	2).

(5.90)





5.4. The Hessian Matrix

253

Again, by using a symmetrical central differences formulation, we ensure that the
residual errors are O(	2) rather than O(	). Because there are W 2 elements in the
Hessian matrix, and because the evaluation of each element requires four forward
propagations each needing O(W ) operations (per pattern), we see that this approach
will require O(W 3) operations to evaluate the complete Hessian. It therefore has
poor scaling properties, although in practice it is very useful as a check on the soft-
ware implementation of backpropagation methods.

A more efﬁcient version of numerical differentiation can be found by applying
central differences to the ﬁrst derivatives of the error function, which are themselves
calculated using backpropagation. This gives

∂2E

∂wji∂wlk

=

1
2	

∂E
∂wji

(wlk + 	) −

∂E
∂wji

(wlk − 	)

+ O(	2).

(5.91)

Because there are now only W weights to be perturbed, and because the gradients
can be evaluated in O(W ) steps, we see that this method gives the Hessian in O(W 2)
operations.

5.4.5 Exact evaluation of the Hessian
So far, we have considered various approximation schemes for evaluating the
Hessian matrix or its inverse. The Hessian can also be evaluated exactly, for a net-
work of arbitrary feed-forward topology, using extension of the technique of back-
propagation used to evaluate ﬁrst derivatives, which shares many of its desirable
features including computational efﬁciency (Bishop, 1991; Bishop, 1992). It can be
applied to any differentiable error function that can be expressed as a function of
the network outputs and to networks having arbitrary differentiable activation func-
tions. The number of computational steps needed to evaluate the Hessian scales
like O(W 2). Similar algorithms have also been considered by Buntine and Weigend
(1993).

Here we consider the speciﬁc case of a network having two layers of weights,
for which the required equations are easily derived. We shall use indices i and i
to denote inputs, indices j and j to denoted hidden units, and indices k and k to
denote outputs. We ﬁrst deﬁne

δk = ∂En
∂ak

,

Mkk ≡

∂2En
∂ak∂ak

(5.92)

where En is the contribution to the error from data point n. The Hessian matrix for
this network can then be considered in three separate blocks as follows.

1. Both weights in the second layer:

∂2En
kj ∂w(2)
kj

∂w(2)

= zjzjMkk.

(5.93)

Exercise 5.22



k





k





254

5. NEURAL NETWORKS

2. Both weights in the ﬁrst layer:

∂w(1)

= xixih(aj)Ijj

∂2En
ji ∂w(1)
ji
+xixih(aj)h(aj)

w(2)

kjδk

k
w(2)
kjw(2)

kj Mkk.

(5.94)

3. One weight in each layer:

∂2En
ji ∂w(2)
kj

∂w(1)

= xih(aj)

δkIjj + zj

w(2)

kjHkk

.

(5.95)

k

Exercise 5.23

Here Ijj is the j, j element of the identity matrix. If one or both of the weights is
a bias term, then the corresponding expressions are obtained simply by setting the
appropriate activation(s) to 1. Inclusion of skip-layer connections is straightforward.

5.4.6 Fast multiplication by the Hessian
For many applications of the Hessian, the quantity of interest is not the Hessian
matrix H itself but the product of H with some vector v. We have seen that the
evaluation of the Hessian takes O(W 2) operations, and it also requires storage that is
O(W 2). The vector vTH that we wish to calculate, however, has only W elements,
so instead of computing the Hessian as an intermediate step, we can instead try to
ﬁnd an efﬁcient approach to evaluating vTH directly in a way that requires only
O(W ) operations.

To do this, we ﬁrst note that

vTH = vT∇(∇E)

(5.96)

where ∇ denotes the gradient operator in weight space. We can then write down
the standard forward-propagation and backpropagation equations for the evaluation
of ∇E and apply (5.96) to these equations to give a set of forward-propagation and
backpropagation equations for the evaluation of vTH (Møller, 1993; Pearlmutter,
1994). This corresponds to acting on the original forward-propagation and back-
propagation equations with a differential operator vT∇. Pearlmutter (1994) used the
notation R{·} to denote the operator vT∇, and we shall follow this convention. The
analysis is straightforward and makes use of the usual rules of differential calculus,
together with the result

R{w} = v.

(5.97)

The technique is best illustrated with a simple example, and again we choose a
two-layer network of the form shown in Figure 5.1, with linear output units and a
sum-of-squares error function. As before, we consider the contribution to the error
function from one pattern in the data set. The required vector is then obtained as






k

j

i





j

i




j

5.4. The Hessian Matrix

255

usual by summing over the contributions from each of the patterns separately. For
the two-layer network, the forward-propagation equations are given by

We now act on these equations using the R{·} operator to obtain a set of forward
propagation equations in the form

aj =

wjixi

zj = h(aj)
yk =

wkjzj.

vjixi

R{aj} =
R{zj} = h(aj)R{aj}
R{yk} =

wkjR{zj} +

vkjzj

(5.98)

(5.99)

(5.100)

(5.101)

(5.102)

(5.103)

(5.104)
(5.105)

(5.106)

where vji is the element of the vector v that corresponds to the weight wji. Quan-
tities of the form R{zj}, R{aj} and R{yk} are to be regarded as new variables
whose values are found using the above equations.
Because we are considering a sum-of-squares error function, we have the fol-

lowing standard backpropagation expressions:
δk = yk − tk
δj = h(aj)

wkjδk.

Again, we act on these equations with the R{·} operator to obtain a set of backprop-
agation equations in the form

R{δk} = R{yk}
R{δj} = h(aj)R{aj}

wkjδk

k

+ h(aj)

vkjδk + h(aj)

k

wkjR{δk}.

k

(5.107)

Finally, we have the usual equations for the ﬁrst derivatives of the error

∂E
∂wkj
∂E
∂wji

= δkzj

= δjxi

(5.108)

(5.109)







and acting on these with the R{·} operator, we obtain expressions for the elements
of the vector vTH

= R{δk}zj + δkR{zj}
= xiR{δj}.

(5.110)

(5.111)

R

R

∂E
∂wkj
∂E
∂wji

256

5. NEURAL NETWORKS

The implementation of this algorithm involves the introduction of additional
variables R{aj}, R{zj} and R{δj} for the hidden units and R{δk} and R{yk}
for the output units. For each input pattern, the values of these quantities can be
found using the above results, and the elements of vTH are then given by (5.110)
and (5.111). An elegant aspect of this technique is that the equations for evaluating
vTH mirror closely those for standard forward and backward propagation, and so the
extension of existing software to compute this product is typically straightforward.
If desired, the technique can be used to evaluate the full Hessian matrix by
choosing the vector v to be given successively by a series of unit vectors of the
form (0, 0, . . . , 1, . . . , 0) each of which picks out one column of the Hessian. This
leads to a formalism that is analytically equivalent to the backpropagation procedure
of Bishop (1992), as described in Section 5.4.5, though with some loss of efﬁciency
due to redundant calculations.

5.5. Regularization in Neural Networks

The number of input and outputs units in a neural network is generally determined
by the dimensionality of the data set, whereas the number M of hidden units is a free
parameter that can be adjusted to give the best predictive performance. Note that M
controls the number of parameters (weights and biases) in the network, and so we
might expect that in a maximum likelihood setting there will be an optimum value
of M that gives the best generalization performance, corresponding to the optimum
balance between under-ﬁtting and over-ﬁtting. Figure 5.9 shows an example of the
effect of different values of M for the sinusoidal regression problem.

The generalization error, however, is not a simple function of M due to the
presence of local minima in the error function, as illustrated in Figure 5.10. Here
we see the effect of choosing multiple random initializations for the weight vector
for a range of values of M. The overall best validation set performance in this
case occurred for a particular solution having M = 8. In practice, one approach to
choosing M is in fact to plot a graph of the kind shown in Figure 5.10 and then to
choose the speciﬁc solution having the smallest validation set error.

There are, however, other ways to control the complexity of a neural network
model in order to avoid over-ﬁtting. From our discussion of polynomial curve ﬁtting
in Chapter 1, we see that an alternative approach is to choose a relatively large value
for M and then to control complexity by the addition of a regularization term to the
error function. The simplest regularizer is the quadratic, giving a regularized error



of the form

E(w) = E(w) + λ
2

wTw.

(5.112)

This regularizer is also known as weight decay and has been discussed at length
in Chapter 3. The effective model complexity is then determined by the choice of
the regularization coefﬁcient λ. As we have seen previously, this regularizer can be
interpreted as the negative logarithm of a zero-mean Gaussian prior distribution over
the weight vector w.

5.5.1 Consistent Gaussian priors
One of the limitations of simple weight decay in the form (5.112) is that is
inconsistent with certain scaling properties of network mappings. To illustrate this,
consider a multilayer perceptron network having two layers of weights and linear
output units, which performs a mapping from a set of input variables {xi} to a set
of output variables {yk}. The activations of the hidden units in the ﬁrst hidden layer

Figure 5.10 Plot of the sum-of-squares test-set
error for the polynomial data set ver-
sus the number of hidden units in the
network, with 30 random starts for
each network size, showing the ef-
fect of local minima. For each new
start, the weight vector was initial-
ized by sampling from an isotropic
Gaussian distribution having a mean
of zero and a variance of 10.

160

140

120

100

80

60

0

2

4

6

8

10

5.5. Regularization in Neural Networks

257

1

0

−1

M = 1

1

0

−1

0

1

0

M = 10

M = 3

1

0

−1

1

0

1

Figure 5.9 Examples of two-layer networks trained on 10 data points drawn from the sinusoidal data set. The
graphs show the result of ﬁtting networks having M = 1, 3 and 10 hidden units, respectively, by minimizing a
sum-of-squares error function using a scaled conjugate-gradient algorithm.











wji =

yk →

xi →

j

i





b
a

i

(5.113)

(5.114)

(5.115)

(5.116)

(5.117)

(5.118)

(5.119)
(5.120)

258

5. NEURAL NETWORKS

take the form

while the activations of the output units are given by

zj = h

wjixi + wj0

yk =

wkjzj + wk0.

Suppose we perform a linear transformation of the input data of the form

xi = axi + b.

Exercise 5.24

Then we can arrange for the mapping performed by the network to be unchanged
by making a corresponding linear transformation of the weights and biases from the
inputs to the units in the hidden layer of the form

Similarly, a linear transformation of the output variables of the network of the form

can be achieved by making a transformation of the second-layer weights and biases
using

1
a

wji

wji →
wj0 →

wj0 = wj0 −

wji.

yk = cyk + d

wkj →
wk0 →

wkj = cwkj
wk0 = cwk0 + d.

If we train one network using the original data and one network using data for which
the input and/or target variables are transformed by one of the above linear transfor-
mations, then consistency requires that we should obtain equivalent networks that
differ only by the linear transformation of the weights as given. Any regularizer
should be consistent with this property, otherwise it arbitrarily favours one solution
over another, equivalent one. Clearly, simple weight decay (5.112), that treats all
weights and biases on an equal footing, does not satisfy this property.

We therefore look for a regularizer which is invariant under the linear trans-
formations (5.116), (5.117), (5.119) and (5.120). These require that the regularizer
should be invariant to re-scaling of the weights and to shifts of the biases. Such a
regularizer is given by

λ1
2

w2 + λ2
2

w∈W1

w∈W2

w2

(5.121)

where W1 denotes the set of weights in the ﬁrst layer, W2 denotes the set of weights
in the second layer, and biases are excluded from the summations. This regularizer








w∈W1

k

1
2

−






5.5. Regularization in Neural Networks

259

will remain unchanged under the weight transformations provided the regularization
parameters are re-scaled using λ1 → a1/2λ1 and λ2 → c−1/2λ2.

The regularizer (5.121) corresponds to a prior of the form

p(w|α1, α2) ∝ exp

α1
2

−

w2 −

α2
2

w2

.

(5.122)

w∈W2

Note that priors of this form are improper (they cannot be normalized) because the
bias parameters are unconstrained. The use of improper priors can lead to difﬁculties
in selecting regularization coefﬁcients and in model comparison within the Bayesian
framework, because the corresponding evidence is zero. It is therefore common to
include separate priors for the biases (which then break shift invariance) having their
own hyperparameters. We can illustrate the effect of the resulting four hyperpa-
rameters by drawing samples from the prior and plotting the corresponding network
functions, as shown in Figure 5.11.

More generally, we can consider priors in which the weights are divided into

any number of groups Wk so that

p(w) ∝ exp

αkw2

k

where

w2

k =

w2
j .

j∈Wk

(5.123)

(5.124)

As a special case of this prior, if we choose the groups to correspond to the sets
of weights associated with each of the input units, and we optimize the marginal
likelihood with respect to the corresponding parameters αk, we obtain automatic
relevance determination as discussed in Section 7.2.2.

5.5.2 Early stopping
An alternative to regularization as a way of controlling the effective complexity
of a network is the procedure of early stopping. The training of nonlinear network
models corresponds to an iterative reduction of the error function deﬁned with re-
spect to a set of training data. For many of the optimization algorithms used for
network training, such as conjugate gradients, the error is a nonincreasing function
of the iteration index. However, the error measured with respect to independent data,
generally called a validation set, often shows a decrease at ﬁrst, followed by an in-
crease as the network starts to over-ﬁt. Training can therefore be stopped at the point
of smallest error with respect to the validation data set, as indicated in Figure 5.12,
in order to obtain a network having good generalization performance.

The behaviour of the network in this case is sometimes explained qualitatively
in terms of the effective number of degrees of freedom in the network, in which this
number starts out small and then to grows during the training process, corresponding
to a steady increase in the effective complexity of the model. Halting training before





1 = 1, αb
αw

1 = 1, αw

2 = 10, αb

2 = 1

1

−0.5

0

0.5

1

1 = 1000, αb
αw

1 = 1000, αw

2 = 1, αb

2 = 1

260

5. NEURAL NETWORKS

1 = 1, αb
αw

1 = 1, αw

2 = 1, αb

2 = 1

−0.5

0

0.5

1 = 1000, αb
αw

1 = 100, αw

2 = 1, αb

2 = 1

−0.5

0

0.5

4

2

0

−2

−4

−6

−1

5

0

−5

−10

−1

40

20

0

−20

−40

−60

−1

5

0

−5

1

−10

−1

−0.5

0

0.5

1

Figure 5.11 Illustration of the effect of the hyperparameters governing the prior distribution over weights and
biases in a two-layer network having a single input, a single linear output, and 12 hidden units having ‘tanh’
2 , which represent
activation functions. The priors are governed by four hyperparameters αb
the precisions of the Gaussian distributions of the ﬁrst-layer biases, ﬁrst-layer weights, second-layer biases, and
2 governs the vertical scale of functions (note
second-layer weights, respectively. We see that the parameter αw
the different vertical axis ranges on the top two diagrams), αw
1 governs the horizontal scale of variations in the
2, whose
function values, and αb
effect is not illustrated here, governs the range of vertical offsets of the functions.

1 governs the horizontal range over which variations occur. The parameter αb

2, and αw

1 , αb

1, αw

a minimum of the training error has been reached then represents a way of limiting
the effective network complexity.

In the case of a quadratic error function, we can verify this insight, and show
that early stopping should exhibit similar behaviour to regularization using a sim-
ple weight-decay term. This can be understood from Figure 5.13, in which the axes
in weight space have been rotated to be parallel to the eigenvectors of the Hessian
matrix. If, in the absence of weight decay, the weight vector starts at the origin and
proceeds during training along a path that follows the local negative gradient vec-
tor, then the weight vector will move initially parallel to the w2 axis through a point
w and then move towards the minimum of the error func-
corresponding roughly to
tion wML. This follows from the shape of the error surface and the widely differing
w is therefore similar to weight
eigenvalues of the Hessian. Stopping at a point near
decay. The relationship between early stopping and weight decay can be made quan-
titative, thereby showing that the quantity τ η (where τ is the iteration index, and η
is the learning rate parameter) plays the role of the reciprocal of the regularization

Exercise 5.25

5.5. Regularization in Neural Networks

261

0.25

0.2

0.15

0

10

20

30

40

50

0.45

0.4

0.35

0

10

20

30

40

50

Figure 5.12 An illustration of the behaviour of training set error (left) and validation set error (right) during a
typical training session, as a function of the iteration step, for the sinusoidal data set. The goal of achieving
the best generalization performance suggests that training should be stopped at the point shown by the vertical
dashed lines, corresponding to the minimum of the validation set error.

parameter λ. The effective number of parameters in the network therefore grows
during the course of training.

5.5.3 Invariances
In many applications of pattern recognition, it is known that predictions should
be unchanged, or invariant, under one or more transformations of the input vari-
ables. For example, in the classiﬁcation of objects in two-dimensional images, such
as handwritten digits, a particular object should be assigned the same classiﬁcation
irrespective of its position within the image (translation invariance) or of its size
(scale invariance). Such transformations produce signiﬁcant changes in the raw
data, expressed in terms of the intensities at each of the pixels in the image, and
yet should give rise to the same output from the classiﬁcation system. Similarly
in speech recognition, small levels of nonlinear warping along the time axis, which
preserve temporal ordering, should not change the interpretation of the signal.

If sufﬁciently large numbers of training patterns are available, then an adaptive
model such as a neural network can learn the invariance, at least approximately. This
involves including within the training set a sufﬁciently large number of examples of
the effects of the various transformations. Thus, for translation invariance in an im-
age, the training set should include examples of objects at many different positions.
This approach may be impractical, however, if the number of training examples
is limited, or if there are several invariants (because the number of combinations of
transformations grows exponentially with the number of such transformations). We
therefore seek alternative approaches for encouraging an adaptive model to exhibit
the required invariances. These can broadly be divided into four categories:

1. The training set is augmented using replicas of the training patterns, trans-
formed according to the desired invariances. For instance, in our digit recog-
nition example, we could make multiple copies of each example in which the



w

w2

wML

w1

e

262

5. NEURAL NETWORKS

Figure 5.13 A schematic illustration of why
early stopping can give similar
results to weight decay in the
case of a quadratic error func-
tion. The ellipse shows a con-
tour of constant error, and wML
denotes the minimum of the er-
ror function.
If the weight vector
starts at the origin and moves ac-
cording to the local negative gra-
dient direction, then it will follow
the path shown by the curve. By
stopping training early, a weight
vector
w is found that is qual-
itatively similar to that obtained
with a simple weight-decay reg-
ularizer and training to the mini-
mum of the regularized error, as
can be seen by comparing with
Figure 3.15.

digit is shifted to a different position in each image.

2. A regularization term is added to the error function that penalizes changes in
the model output when the input is transformed. This leads to the technique of
tangent propagation, discussed in Section 5.5.4.

3. Invariance is built into the pre-processing by extracting features that are invari-
ant under the required transformations. Any subsequent regression or classi-
ﬁcation system that uses such features as inputs will necessarily also respect
these invariances.

4. The ﬁnal option is to build the invariance properties into the structure of a neu-
ral network (or into the deﬁnition of a kernel function in the case of techniques
such as the relevance vector machine). One way to achieve this is through the
use of local receptive ﬁelds and shared weights, as discussed in the context of
convolutional neural networks in Section 5.5.6.

Approach 1 is often relatively easy to implement and can be used to encourage com-
plex invariances such as those illustrated in Figure 5.14. For sequential training
algorithms, this can be done by transforming each input pattern before it is presented
to the model so that, if the patterns are being recycled, a different transformation
(drawn from an appropriate distribution) is added each time. For batch methods, a
similar effect can be achieved by replicating each data point a number of times and
transforming each copy independently. The use of such augmented data can lead to
signiﬁcant improvements in generalization (Simard et al., 2003), although it can also
be computationally costly.

Approach 2 leaves the data set unchanged but modiﬁes the error function through
the addition of a regularizer. In Section 5.5.5, we shall show that this approach is
closely related to approach 2.

5.5. Regularization in Neural Networks

263

Figure 5.14 Illustration of the synthetic warping of a handwritten digit. The original image is shown on the
left. On the right, the top row shows three examples of warped digits, with the corresponding displacement
ﬁelds shown on the bottom row. These displacement ﬁelds are generated by sampling random displacements
∆x, ∆y ∈ (0, 1) at each pixel and then smoothing by convolution with Gaussians of width 0.01, 30 and 60
respectively.

One advantage of approach 3 is that it can correctly extrapolate well beyond the
range of transformations included in the training set. However, it can be difﬁcult
to ﬁnd hand-crafted features with the required invariances that do not also discard
information that can be useful for discrimination.

5.5.4 Tangent propagation
We can use regularization to encourage models to be invariant to transformations
of the input through the technique of tangent propagation (Simard et al., 1992).
Consider the effect of a transformation on a particular input vector xn. Provided the
transformation is continuous (such as translation or rotation, but not mirror reﬂection
for instance), then the transformed pattern will sweep out a manifold M within the
D-dimensional input space. This is illustrated in Figure 5.15, for the case of D =
2 for simplicity. Suppose the transformation is governed by a single parameter ξ
(which might be rotation angle for instance). Then the subspace M swept out by xn

Figure 5.15 Illustration of a two-dimensional

input space
showing the effect of a continuous transforma-
tion on a particular input vector xn. A one-
dimensional transformation, parameterized by
the continuous variable ξ, applied to xn causes
it to sweep out a one-dimensional manifold M.
Locally, the effect of the transformation can be
approximated by the tangent vector τ n.

x2

τ n

xn

M

ξ

x1










i=1

D

D



.





i=1

D













264

5. NEURAL NETWORKS

will be one-dimensional, and will be parameterized by ξ. Let the vector that results
from acting on xn by this transformation be denoted by s(xn, ξ), which is deﬁned
so that s(x, 0) = x. Then the tangent to the curve M is given by the directional
derivative τ = ∂s/∂ξ, and the tangent vector at the point xn is given by

τ n = ∂s(xn, ξ)

∂ξ

ξ=0

(5.125)

Under a transformation of the input vector, the network output vector will, in general,
change. The derivative of output k with respect to ξ is given by

∂yk
∂ξ

=

ξ=0

∂yk
∂xi

∂xi
∂ξ

ξ=0

=

Jkiτi

(5.126)

where Jki is the (k, i) element of the Jacobian matrix J, as discussed in Section 5.3.4.
The result (5.126) can be used to modify the standard error function, so as to encour-
age local invariance in the neighbourhood of the data points, by the addition to the
original error function E of a regularization function Ω to give a total error function
of the form

E = E + λΩ

(5.127)

where λ is a regularization coefﬁcient and

Ω =

1
2

n

k

∂ynk
∂ξ

2

=

1
2

ξ=0

n

k

i=1

2

.

Jnkiτni

(5.128)

Exercise 5.26

The regularization function will be zero when the network mapping function is in-
variant under the transformation in the neighbourhood of each pattern vector, and
the value of the parameter λ determines the balance between ﬁtting the training data
and learning the invariance property.

In a practical implementation, the tangent vector τ n can be approximated us-
ing ﬁnite differences, by subtracting the original vector xn from the corresponding
vector after transformation using a small value of ξ, and then dividing by ξ. This is
illustrated in Figure 5.16.

The regularization function depends on the network weights through the Jaco-
bian J. A backpropagation formalism for computing the derivatives of the regu-
larizer with respect to the network weights is easily obtained by extension of the
techniques introduced in Section 5.3.

If the transformation is governed by L parameters (e.g., L = 3 for the case of
translations combined with in-plane rotations in a two-dimensional image), then the
manifold M will have dimensionality L, and the corresponding regularizer is given
by the sum of terms of the form (5.128), one for each transformation. If several
transformations are considered at the same time, and the network mapping is made
invariant to each separately, then it will be (locally) invariant to combinations of the
transformations (Simard et al., 1992).

5.5. Regularization in Neural Networks

265

Figure 5.16 Illustration
showing
(a) the original image x of a hand-
written digit, (b) the tangent vector
τ corresponding to an inﬁnitesimal
clockwise rotation, (c) the result of
adding a small contribution from the
tangent vector to the original image
giving x + τ with  = 15 degrees,
and (d) the true image rotated for
comparison.

(a)

(c)

(b)

(d)



A related technique, called tangent distance, can be used to build invariance
properties into distance-based methods such as nearest-neighbour classiﬁers (Simard
et al., 1993).

5.5.5 Training with transformed data
We have seen that one way to encourage invariance of a model to a set of trans-
formations is to expand the training set using transformed versions of the original
input patterns. Here we show that this approach is closely related to the technique of
tangent propagation (Bishop, 1995b; Leen, 1995).

As in Section 5.5.4, we shall consider a transformation governed by a single
parameter ξ and described by the function s(x, ξ), with s(x, 0) = x. We shall
also consider a sum-of-squares error function. The error function for untransformed
inputs can be written (in the inﬁnite data set limit) in the form

E =

1
2

{y(x) − t}2p(t|x)p(x) dx dt

(5.129)

as discussed in Section 1.5.5. Here we have considered a network having a single
output, in order to keep the notation uncluttered.
If we now consider an inﬁnite
number of copies of each data point, each of which is perturbed by the transformation















.



2





-





.



266

5. NEURAL NETWORKS

in which the parameter ξ is drawn from a distribution p(ξ), then the error function
deﬁned over this expanded data set can be written as

E =

1
2

{y(s(x, ξ)) − t}2p(t|x)p(x)p(ξ) dx dt dξ.

(5.130)

We now assume that the distribution p(ξ) has zero mean with small variance, so that
we are only considering small transformations of the original input vectors. We can
then expand the transformation function as a Taylor series in powers of ξ to give

s(x, ξ) = s(x, 0) + ξ

s(x, ξ)

+ ξ2
2

∂2
∂ξ2 s(x, ξ)

ξ=0

+ O(ξ3)

∂
∂ξ

ξ=0

1
2 ξ2τ  + O(ξ3)

= x + ξτ +

where τ  denotes the second derivative of s(x, ξ) with respect to ξ evaluated at ξ = 0.
This allows us to expand the model function to give

y(s(x, ξ)) = y(x) + ξτ T∇y(x) + ξ2

2

(τ )T ∇y(x) + τ T∇∇y(x)τ

+ O(ξ3).

Substituting into the mean error function (5.130) and expanding, we then have

E =

1
2

{y(x) − t}2p(t|x)p(x) dx dt

+ E[ξ]

{y(x) − t}τ T∇y(x)p(t|x)p(x) dx dt

+ E[ξ2]

{y(x) − t}

1
2

(τ )T ∇y(x) + τ T∇∇y(x)τ

+

τ T∇y(x)

p(t|x)p(x) dx dt + O(ξ3).

Because the distribution of transformations has zero mean we have E[ξ] = 0. Also,
we shall denote E[ξ2] by λ. Omitting terms of O(ξ3), the average error function then
becomes

(5.131)
where E is the original sum-of-squares error, and the regularization term Ω takes the
form

E = E + λΩ

Ω =

{y(x) − E[t|x]}
τ T∇y(x)

2

+

p(x) dx

1
2

(τ )T ∇y(x) + τ T∇∇y(x)τ

(5.132)

in which we have performed the integration over t.







We can further simplify this regularization term as follows. In Section 1.5.5 we
saw that the function that minimizes the sum-of-squares error is given by the condi-
tional average E[t|x] of the target values t. From (5.131) we see that the regularized
error will equal the unregularized sum-of-squares plus terms which are O(ξ), and so
the network function that minimizes the total error will have the form

y(x) = E[t|x] + O(ξ).

(5.133)

Thus, to leading order in ξ, the ﬁrst term in the regularizer vanishes and we are left
with

Ω =

1
2

τ T∇y(x)

2

p(x) dx

(5.134)

which is equivalent to the tangent propagation regularizer (5.128).

If we consider the special case in which the transformation of the inputs simply
consists of the addition of random noise, so that x → x + ξ, then the regularizer
takes the form
(5.135)

Ω =

1
2

∇y(x)2 p(x) dx

which is known as Tikhonov regularization (Tikhonov and Arsenin, 1977; Bishop,
1995b). Derivatives of this regularizer with respect to the network weights can be
found using an extended backpropagation algorithm (Bishop, 1993). We see that, for
small noise amplitudes, Tikhonov regularization is related to the addition of random
noise to the inputs, which has been shown to improve generalization in appropriate
circumstances (Sietsma and Dow, 1991).

5.5.6 Convolutional networks
Another approach to creating models that are invariant to certain transformation
of the inputs is to build the invariance properties into the structure of a neural net-
work. This is the basis for the convolutional neural network (Le Cun et al., 1989;
LeCun et al., 1998), which has been widely applied to image data.

Consider the speciﬁc task of recognizing handwritten digits. Each input image
comprises a set of pixel intensity values, and the desired output is a posterior proba-
bility distribution over the ten digit classes. We know that the identity of the digit is
invariant under translations and scaling as well as (small) rotations. Furthermore, the
network must also exhibit invariance to more subtle transformations such as elastic
deformations of the kind illustrated in Figure 5.14. One simple approach would be to
treat the image as the input to a fully connected network, such as the kind shown in
Figure 5.1. Given a sufﬁciently large training set, such a network could in principle
yield a good solution to this problem and would learn the appropriate invariances by
example.

However, this approach ignores a key property of images, which is that nearby
pixels are more strongly correlated than more distant pixels. Many of the modern
approaches to computer vision exploit this property by extracting local features that
depend only on small subregions of the image. Information from such features can
then be merged in later stages of processing in order to detect higher-order features

5.5. Regularization in Neural Networks

267

Exercise 5.27

268

5. NEURAL NETWORKS

Input image

Convolutional layer

Sub-sampling
layer

Figure 5.17 Diagram illustrating part of a convolutional neural network, showing a layer of convolu-
tional units followed by a layer of subsampling units. Several successive pairs of such
layers may be used.

and ultimately to yield information about the image as whole. Also, local features
that are useful in one region of the image are likely to be useful in other regions of
the image, for instance if the object of interest is translated.

These notions are incorporated into convolutional neural networks through three
mechanisms: (i) local receptive ﬁelds, (ii) weight sharing, and (iii) subsampling. The
structure of a convolutional network is illustrated in Figure 5.17. In the convolutional
layer the units are organized into planes, each of which is called a feature map. Units
in a feature map each take inputs only from a small subregion of the image, and all
of the units in a feature map are constrained to share the same weight values. For
instance, a feature map might consist of 100 units arranged in a 10 × 10 grid, with
each unit taking inputs from a 5×5 pixel patch of the image. The whole feature map
therefore has 25 adjustable weight parameters plus one adjustable bias parameter.
Input values from a patch are linearly combined using the weights and the bias, and
the result transformed by a sigmoidal nonlinearity using (5.1). If we think of the units
as feature detectors, then all of the units in a feature map detect the same pattern but
at different locations in the input image. Due to the weight sharing, the evaluation
of the activations of these units is equivalent to a convolution of the image pixel
intensities with a ‘kernel’ comprising the weight parameters. If the input image is
shifted, the activations of the feature map will be shifted by the same amount but will
otherwise be unchanged. This provides the basis for the (approximate) invariance of

5.5. Regularization in Neural Networks

269

the network outputs to translations and distortions of the input image. Because we
will typically need to detect multiple features in order to build an effective model,
there will generally be multiple feature maps in the convolutional layer, each having
its own set of weight and bias parameters.

The outputs of the convolutional units form the inputs to the subsampling layer
of the network. For each feature map in the convolutional layer, there is a plane of
units in the subsampling layer and each unit takes inputs from a small receptive ﬁeld
in the corresponding feature map of the convolutional layer. These units perform
subsampling. For instance, each subsampling unit might take inputs from a 2 × 2
unit region in the corresponding feature map and would compute the average of
those inputs, multiplied by an adaptive weight with the addition of an adaptive bias
parameter, and then transformed using a sigmoidal nonlinear activation function.
The receptive ﬁelds are chosen to be contiguous and nonoverlapping so that there
are half the number of rows and columns in the subsampling layer compared with
the convolutional layer. In this way, the response of a unit in the subsampling layer
will be relatively insensitive to small shifts of the image in the corresponding regions
of the input space.

In a practical architecture, there may be several pairs of convolutional and sub-
sampling layers. At each stage there is a larger degree of invariance to input trans-
formations compared to the previous layer. There may be several feature maps in a
given convolutional layer for each plane of units in the previous subsampling layer,
so that the gradual reduction in spatial resolution is then compensated by an increas-
ing number of features. The ﬁnal layer of the network would typically be a fully
connected, fully adaptive layer, with a softmax output nonlinearity in the case of
multiclass classiﬁcation.

The whole network can be trained by error minimization using backpropagation
to evaluate the gradient of the error function. This involves a slight modiﬁcation
of the usual backpropagation algorithm to ensure that the shared-weight constraints
are satisﬁed. Due to the use of local receptive ﬁelds, the number of weights in
the network is smaller than if the network were fully connected. Furthermore, the
number of independent parameters to be learned from the data is much smaller still,
due to the substantial numbers of constraints on the weights.

5.5.7 Soft weight sharing
One way to reduce the effective complexity of a network with a large number
of weights is to constrain weights within certain groups to be equal. This is the
technique of weight sharing that was discussed in Section 5.5.6 as a way of building
translation invariance into networks used for image interpretation. It is only appli-
cable, however, to particular problems in which the form of the constraints can be
speciﬁed in advance. Here we consider a form of soft weight sharing (Nowlan and
Hinton, 1992) in which the hard constraint of equal weights is replaced by a form
of regularization in which groups of weights are encouraged to have similar values.
Furthermore, the division of weights into groups, the mean weight value for each
group, and the spread of values within the groups are all determined as part of the
learning process.

Exercise 5.28














j=1

j=1

M

M

i



270

5. NEURAL NETWORKS

Section 2.3.9

Exercise 5.29

Recall that the simple weight decay regularizer, given in (5.112), can be viewed
as the negative log of a Gaussian prior distribution over the weights. We can encour-
age the weight values to form several groups, rather than just one group, by consid-
ering instead a probability distribution that is a mixture of Gaussians. The centres
and variances of the Gaussian components, as well as the mixing coefﬁcients, will be
considered as adjustable parameters to be determined as part of the learning process.
Thus, we have a probability density of the form

where

p(w) =

p(wi)

p(wi) =

πjN (wi|µj, σ2
j )

(5.136)

(5.137)

and πj are the mixing coefﬁcients. Taking the negative logarithm then leads to a
regularization function of the form

Ω(w) = −

ln

i

πjN (wi|µj, σ2
j )

The total error function is then given by

E(w) = E(w) + λΩ(w)

.

(5.138)

(5.139)

where λ is the regularization coefﬁcient. This error is minimized both with respect
to the weights wi and with respect to the parameters {πj, µj, σj} of the mixture
model. If the weights were constant, then the parameters of the mixture model could
be determined by using the EM algorithm discussed in Chapter 9. However, the dis-
tribution of weights is itself evolving during the learning process, and so to avoid nu-
merical instability, a joint optimization is performed simultaneously over the weights
and the mixture-model parameters. This can be done using a standard optimization
algorithm such as conjugate gradients or quasi-Newton methods.

In order to minimize the total error function, it is necessary to be able to evaluate
its derivatives with respect to the various adjustable parameters. To do this it is con-
venient to regard the {πj} as prior probabilities and to introduce the corresponding
posterior probabilities which, following (2.192), are given by Bayes’ theorem in the
form

γj(w) =

πjN (w|µj, σ2
j )
k) .
k πkN (w|µk, σ2

(5.140)

The derivatives of the total error function with respect to the weights are then given
by

∂
E
∂wi

= ∂E
∂wi

+ λ

γj(wi)

j

(wi − µj)

σ2
j

.

(5.141)






account of the constraints

easily computed to give

E
∂
∂µj

E
∂
∂σj

i






i

Exercise 5.30

Exercise 5.31

5.5. Regularization in Neural Networks

271

The effect of the regularization term is therefore to pull each weight towards the
centre of the jth Gaussian, with a force proportional to the posterior probability of
that Gaussian for the given weight. This is precisely the kind of effect that we are
seeking.

Derivatives of the error with respect to the centres of the Gaussians are also

= λ

γj(wi)

(µi − wj)

σ2
j

(5.142)

which has a simple intuitive interpretation, because it pushes µj towards an aver-
age of the weight values, weighted by the posterior probabilities that the respective
weight parameters were generated by component j. Similarly, the derivatives with
respect to the variances are given by

= λ

γj(wi)

1
σj −

(wi − µj)2

σ3
j

(5.143)

which drives σj towards the weighted average of the squared deviations of the weights
around the corresponding centre µj, where the weighting coefﬁcients are again given
by the posterior probability that each weight is generated by component j. Note that
in a practical implementation, new variables ηj deﬁned by

j = exp(ηj)
σ2

(5.144)

are introduced, and the minimization is performed with respect to the ηj. This en-
sures that the parameters σj remain positive. It also has the effect of discouraging
pathological solutions in which one or more of the σj goes to zero, corresponding
to a Gaussian component collapsing onto one of the weight parameter values. Such
solutions are discussed in more detail in the context of Gaussian mixture models in
Section 9.2.1.

For the derivatives with respect to the mixing coefﬁcients πj, we need to take

πj = 1,

0 � πi � 1

(5.145)

j

which follow from the interpretation of the πj as prior probabilities. This can be
done by expressing the mixing coefﬁcients in terms of a set of auxiliary variables
{ηj} using the softmax function given by

πj =

exp(ηj)
M
k=1 exp(ηk)

.

(5.146)

Exercise 5.32

The derivatives of the regularized error function with respect to the {ηj} then take
the form



272

5. NEURAL NETWORKS

Figure 5.18 The left ﬁgure shows a two-link robot arm,
in which the Cartesian coordinates (x1, x2) of the end ef-
fector are determined uniquely by the two joint angles θ1
and θ2 and the (ﬁxed) lengths L1 and L2 of the arms. This
is know as the forward kinematics of the arm.
In prac-
tice, we have to ﬁnd the joint angles that will give rise to a
desired end effector position and, as shown in the right ﬁg-
ure, this inversekinematicshas two solutions correspond-
ing to ‘elbow up’ and ‘elbow down’.



L2

θ2

L1

∂
E
∂ηj

=

i

{πj − γj(wi)} .

Exercise 5.33

We see that πj is therefore driven towards the average posterior probability for com-
ponent j.

5.6. Mixture Density Networks

The goal of supervised learning is to model a conditional distribution p(t|x), which
for many simple regression problems is chosen to be Gaussian. However, practical
machine learning problems can often have signiﬁcantly non-Gaussian distributions.
These can arise, for example, with inverse problems in which the distribution can be
multimodal, in which case the Gaussian assumption can lead to very poor predic-
tions.

As a simple example of an inverse problem, consider the kinematics of a robot
arm, as illustrated in Figure 5.18. The forward problem involves ﬁnding the end ef-
fector position given the joint angles and has a unique solution. However, in practice
we wish to move the end effector of the robot to a speciﬁc position, and to do this we
must set appropriate joint angles. We therefore need to solve the inverse problem,
which has two solutions as seen in Figure 5.18.

Forward problems often corresponds to causality in a physical system and gen-
erally have a unique solution. For instance, a speciﬁc pattern of symptoms in the
human body may be caused by the presence of a particular disease. In pattern recog-
nition, however, we typically have to solve an inverse problem, such as trying to
predict the presence of a disease given a set of symptoms. If the forward problem
involves a many-to-one mapping, then the inverse problem will have multiple solu-
tions. For instance, several different diseases may result in the same symptoms.

In the robotics example, the kinematics is deﬁned by geometrical equations, and
the multimodality is readily apparent. However, in many machine learning problems
the presence of multimodality, particularly in problems involving spaces of high di-
mensionality, can be less obvious. For tutorial purposes, however, we shall consider
a simple toy problem for which we can easily visualize the multimodality. Data for
this problem is generated by sampling a variable x uniformly over the interval (0, 1),
to give a set of values {xn}, and the corresponding target values tn are obtained

(x1, x2)

(x1, x2)

elbow
up

θ1

elbow
down

(5.147)



K




1



5.6. Mixture Density Networks

Figure 5.19 On the left is the data
set for a simple ‘forward problem’ in
which the red curve shows the result
of ﬁtting a two-layer neural network
by minimizing the sum-of-squares
error function. The corresponding
inverse problem, shown on the right,
is obtained by exchanging the roles
of x and t. Here the same net-
work trained again by minimizing the
sum-of-squares error function gives
a very poor ﬁt to the data due to the
multimodality of the data set.

1

0

0

1

0

0

273

1

by computing the function xn + 0.3 sin(2πxn) and then adding uniform noise over
the interval (−0.1, 0.1). The inverse problem is then obtained by keeping the same
data points but exchanging the roles of x and t. Figure 5.19 shows the data sets for
the forward and inverse problems, along with the results of ﬁtting two-layer neural
networks having 6 hidden units and a single linear output unit by minimizing a sum-
of-squares error function. Least squares corresponds to maximum likelihood under
a Gaussian assumption. We see that this leads to a very poor model for the highly
non-Gaussian inverse problem.

We therefore seek a general framework for modelling conditional probability
distributions. This can be achieved by using a mixture model for p(t|x) in which
both the mixing coefﬁcients as well as the component densities are ﬂexible functions
of the input vector x, giving rise to the mixture density network. For any given value
of x, the mixture model provides a general formalism for modelling an arbitrary
conditional density function p(t|x). Provided we consider a sufﬁciently ﬂexible
network, we then have a framework for approximating arbitrary conditional distri-
butions.

Here we shall develop the model explicitly for Gaussian components, so that

p(t|x) =

k=1

πk(x)N

t|µk(x), σ2

k(x)

.

(5.148)

This is an example of a heteroscedastic model since the noise variance on the data
is a function of the input vector x. Instead of Gaussians, we can use other distribu-
tions for the components, such as Bernoulli distributions if the target variables are
binary rather than continuous. We have also specialized to the case of isotropic co-
variances for the components, although the mixture density network can readily be
extended to allow for general covariance matrices by representing the covariances
using a Cholesky factorization (Williams, 1996). Even with isotropic components,
the conditional distribution p(t|x) does not assume factorization with respect to the
components of t (in contrast to the standard sum-of-squares regression model) as a
consequence of the mixture distribution.

We now take the various parameters of the mixture model, namely the mixing
k(x), to be governed by

coefﬁcients πk(x), the means µk(x), and the variances σ2



K

θ



274

5. NEURAL NETWORKS

xD

x1

θM

θ1

p(t|x)

t

Figure 5.20 The mixturedensitynetwork can represent general conditional probability densities p(t|x)
by considering a parametric mixture model for the distribution of t whose parameters are
determined by the outputs of a neural network that takes x as its input vector.

the outputs of a conventional neural network that takes x as its input. The structure
of this mixture density network is illustrated in Figure 5.20. The mixture density
network is closely related to the mixture of experts discussed in Section 14.5.3. The
principle difference is that in the mixture density network the same function is used
to predict the parameters of all of the component densities as well as the mixing co-
efﬁcients, and so the nonlinear hidden units are shared amongst the input-dependent
functions.

The neural network in Figure 5.20 can, for example, be a two-layer network
having sigmoidal (‘tanh’) hidden units. If there are L components in the mixture
model (5.148), and if t has K components, then the network will have L output unit
k that determine the mixing coefﬁcients πk(x), K outputs
activations denoted by aπ
k that determine the kernel widths σk(x), and L × K outputs denoted
denoted by aσ
by aµ
kj that determine the components µkj(x) of the kernel centres µk(x). The total
number of network outputs is given by (K + 2)L, as compared with the usual K
outputs for a network, which simply predicts the conditional means of the target
variables.

The mixing coefﬁcients must satisfy the constraints

πk(x) = 1,

k=1

0 � πk(x) � 1

(5.149)

which can be achieved using a set of softmax outputs

πk(x) =

exp(aπ
k)
K
l=1 exp(aπ
l )

.

(5.150)

Similarly, the variances must satisfy σ2
of the exponentials of the corresponding network activations using

k(x) � 0 and so can be represented in terms

Finally, because the means µk(x) have real components, they can be represented

σk(x) = exp(aσ

k).

(5.151)



N





k

5.6. Mixture Density Networks

directly by the network output activations

µkj(x) = aµ
kj.

The adaptive parameters of the mixture density network comprise the vector w
of weights and biases in the neural network, that can be set by maximum likelihood,
or equivalently by minimizing an error function deﬁned to be the negative logarithm
of the likelihood. For independent data, this error function takes the form

E(w) = −

ln

n=1

k=1

πk(xn, w)N

tn|µk(xn, w), σ2

k(xn, w)

(5.153)

where we have made the dependencies on w explicit.

In order to minimize the error function, we need to calculate the derivatives of
the error E(w) with respect to the components of w. These can be evaluated by
using the standard backpropagation procedure, provided we obtain suitable expres-
sions for the derivatives of the error with respect to the output-unit activations. These
represent error signals δ for each pattern and for each output unit, and can be back-
propagated to the hidden units and the error function derivatives evaluated in the
usual way. Because the error function (5.153) is composed of a sum of terms, one
for each training data point, we can consider the derivatives for a particular pattern
n and then ﬁnd the derivatives of E by summing over all patterns.

Because we are dealing with mixture distributions, it is convenient to view the
mixing coefﬁcients πk(x) as x-dependent prior probabilities and to introduce the
corresponding posterior probabilities given by

where Nnk denotes N (tn|µk(xn), σ2
ing coefﬁcients are given by

k(xn)).

The derivatives with respect to the network output activations governing the mix-

Similarly, the derivatives with respect to the output activations controlling the com-
ponent means are given by

γk(t|x) = πkNnk
l=1 πlNnl

K

∂En
∂aπ
k

= πk − γk.

∂En
∂aµ
kl

= γk

µkl − tl

σ2
k

.

Finally, the derivatives with respect to the output activations controlling the compo-
nent variances are given by

∂En
∂aσ
k

= −γk

t − µk2

σ3
k

1
σk

−

.

(5.157)

Exercise 5.34

Exercise 5.35

Exercise 5.36



275

(5.152)

(5.154)

(5.155)

(5.156)












276

5. NEURAL NETWORKS

Figure 5.21 (a) Plot of the mixing
coefﬁcients πk(x) as a function of
x for the three kernel functions in a
mixture density network trained on
the data shown in Figure 5.19. The
model has three Gaussian compo-
nents, and uses a two-layer multi-
layer perceptron with ﬁve ‘tanh’ sig-
moidal units in the hidden layer, and
nine outputs (corresponding to the 3
means and 3 variances of the Gaus-
sian components and the 3 mixing
coefﬁcients). At both small and large
values of x, where the conditional
probability density of the target data
is unimodal, only one of
the ker-
nels has a high value for its prior
probability, while at intermediate val-
ues of x, where the conditional den-
sity is trimodal, the three mixing co-
efﬁcients have comparable values.
(b) Plots of the means µk(x) using
the same colour coding as for the
mixing coefﬁcients.
(c) Plot of the
contours of the corresponding con-
ditional probability density of the tar-
get data for the same mixture den-
sity network.
the ap-
proximate conditional mode, shown
by the red points, of the conditional
density.

(d) Plot of

1

0

1

0

0

0

1

0

1

0

0

0

1

1

(b)

(d)

1

1



(a)

(c)



K

We illustrate the use of a mixture density network by returning to the toy ex-
ample of an inverse problem shown in Figure 5.19. Plots of the mixing coefﬁ-
cients πk(x), the means µk(x), and the conditional density contours corresponding
to p(t|x), are shown in Figure 5.21. The outputs of the neural network, and hence the
parameters in the mixture model, are necessarily continuous single-valued functions
of the input variables. However, we see from Figure 5.21(c) that the model is able to
produce a conditional density that is unimodal for some values of x and trimodal for
other values by modulating the amplitudes of the mixing components πk(x).

Once a mixture density network has been trained, it can predict the conditional
density function of the target data for any given value of the input vector. This
conditional density represents a complete description of the generator of the data, so
far as the problem of predicting the value of the output vector is concerned. From
this density function we can calculate more speciﬁc quantities that may be of interest
in different applications. One of the simplest of these is the mean, corresponding to
the conditional average of the target data, and is given by

E [t|x] =

tp(t|x) dt =

k=1

πk(x)µk(x)

(5.158)




k=1

⎧⎨⎩σ2

	

'''''µk(x) −



K

'''''2

⎫⎬⎭ (5.160)

(5.159)

277

5.7. Bayesian Neural Networks

where we have used (5.148). Because a standard network trained by least squares
is approximating the conditional mean, we see that a mixture density network can
reproduce the conventional least-squares result as a special case. Of course, as we
have already noted, for a multimodal distribution the conditional mean is of limited
value.

We can similarly evaluate the variance of the density function about the condi-

Exercise 5.37

tional average, to give

s2(x) = E
K

=

t − E[t|x]2 |x
k(x) +
πk(x)

πl(x)µl(x)

l=1

where we have used (5.148) and (5.158). This is more general than the corresponding
least-squares result because the variance is a function of x.

We have seen that for multimodal distributions, the conditional mean can give
a poor representation of the data. For instance, in controlling the simple robot arm
shown in Figure 5.18, we need to pick one of the two possible joint angle settings
in order to achieve the desired end-effector location, whereas the average of the two
solutions is not itself a solution.
In such cases, the conditional mode may be of
more value. Because the conditional mode for the mixture density network does not
have a simple analytical solution, this would require numerical iteration. A simple
alternative is to take the mean of the most probable component (i.e., the one with the
largest mixing coefﬁcient) at each value of x. This is shown for the toy data set in
Figure 5.21(d).

5.7. Bayesian Neural Networks

So far, our discussion of neural networks has focussed on the use of maximum like-
lihood to determine the network parameters (weights and biases). Regularized max-
imum likelihood can be interpreted as a MAP (maximum posterior) approach in
which the regularizer can be viewed as the logarithm of a prior parameter distribu-
tion. However, in a Bayesian treatment we need to marginalize over the distribution
of parameters in order to make predictions.

In Section 3.3, we developed a Bayesian solution for a simple linear regression
model under the assumption of Gaussian noise. We saw that the posterior distribu-
tion, which is Gaussian, could be evaluated exactly and that the predictive distribu-
tion could also be found in closed form. In the case of a multilayered network, the
highly nonlinear dependence of the network function on the parameter values means
that an exact Bayesian treatment can no longer be found. In fact, the log of the pos-
terior distribution will be nonconvex, corresponding to the multiple local minima in
the error function.

The technique of variational inference, to be discussed in Chapter 10, has been
applied to Bayesian neural networks using a factorized Gaussian approximation



N

278

5. NEURAL NETWORKS

to the posterior distribution (Hinton and van Camp, 1993) and also using a full-
covariance Gaussian (Barber and Bishop, 1998a; Barber and Bishop, 1998b). The
most complete treatment, however, has been based on the Laplace approximation
(MacKay, 1992c; MacKay, 1992b) and forms the basis for the discussion given here.
We will approximate the posterior distribution by a Gaussian, centred at a mode of
the true posterior. Furthermore, we shall assume that the covariance of this Gaus-
sian is small so that the network function is approximately linear with respect to the
parameters over the region of parameter space for which the posterior probability is
signiﬁcantly nonzero. With these two approximations, we will obtain models that
are analogous to the linear regression and classiﬁcation models discussed in earlier
chapters and so we can exploit the results obtained there. We can then make use of
the evidence framework to provide point estimates for the hyperparameters and to
compare alternative models (for example, networks having different numbers of hid-
den units). To start with, we shall discuss the regression case and then later consider
the modiﬁcations needed for solving classiﬁcation tasks.

5.7.1 Posterior parameter distribution
Consider the problem of predicting a single continuous target variable t from
a vector x of inputs (the extension to multiple targets is straightforward). We shall
suppose that the conditional distribution p(t|x) is Gaussian, with an x-dependent
mean given by the output of a neural network model y(x, w), and with precision
(inverse variance) β

p(t|x, w, β) = N (t|y(x, w), β−1).

(5.161)

Similarly, we shall choose a prior distribution over the weights w that is Gaussian of
the form

(5.162)
For an i.i.d. data set of N observations x1, . . . , xN , with a corresponding set of target
values D = {t1, . . . , tN}, the likelihood function is given by

p(w|α) = N (w|0, α−1I).

p(D|w, β) =

n=1

N (tn|y(xn, w), β−1)

and so the resulting posterior distribution is then

p(w|D, α, β) ∝ p(w|α)p(D|w, β).

(5.163)

(5.164)

which, as a consequence of the nonlinear dependence of y(x, w) on w, will be non-
Gaussian.

We can ﬁnd a Gaussian approximation to the posterior distribution by using the
Laplace approximation. To do this, we must ﬁrst ﬁnd a (local) maximum of the
posterior, and this must be done using iterative numerical optimization. As usual, it
is convenient to maximize the logarithm of the posterior, which can be written in the





n=1

N






Exercise 5.38

We can therefore make use of the general result (2.115) for the marginal p(t) to give

p(t|x, w, β)  N

t|y(x, wMAP) + gT(w − wMAP), β−1

.

(5.171)

p(t|x,D, α, β) = N

t|y(x, wMAP), σ2(x)

(5.172)

279

(5.165)

(5.167)

(5.168)





5.7. Bayesian Neural Networks

form

ln p(w|D) = −

α
2

wTw −

β
2

{y(xn, w) − tn}2 + const

which corresponds to a regularized sum-of-squares error function. Assuming for
the moment that α and β are ﬁxed, we can ﬁnd a maximum of the posterior, which
we denote wMAP, by standard nonlinear optimization algorithms such as conjugate
gradients, using error backpropagation to evaluate the required derivatives.

Having found a mode wMAP, we can then build a local Gaussian approximation
by evaluating the matrix of second derivatives of the negative log posterior distribu-
tion. From (5.165), this is given by

A = −∇∇ ln p(w|D, α, β) = αI + βH

(5.166)

where H is the Hessian matrix comprising the second derivatives of the sum-of-
squares error function with respect to the components of w. Algorithms for comput-
ing and approximating the Hessian were discussed in Section 5.4. The corresponding
Gaussian approximation to the posterior is then given from (4.134) by

q(w|D) = N (w|wMAP, A−1).

Similarly, the predictive distribution is obtained by marginalizing with respect

to this posterior distribution

p(t|x,D) =

p(t|x, w)q(w|D) dw.

However, even with the Gaussian approximation to the posterior, this integration is
still analytically intractable due to the nonlinearity of the network function y(x, w)
as a function of w. To make progress, we now assume that the posterior distribution
has small variance compared with the characteristic scales of w over which y(x, w)
is varying. This allows us to make a Taylor series expansion of the network function
around wMAP and retain only the linear terms

y(x, w)  y(x, wMAP) + gT(w − wMAP)

(5.169)

where we have deﬁned

(5.170)
With this approximation, we now have a linear-Gaussian model with a Gaussian
distribution for p(w) and a Gaussian for p(t|w) whose mean is a linear function of
w of the form

g = ∇wy(x, w)|w=wMAP .

280

5. NEURAL NETWORKS





N

n=1

Exercise 5.39

This is easily evaluated by making use of the Laplace approximation result (4.135).
Taking logarithms then gives

p(D|α, β) =

p(D|w, β)p(w|α) dw.

(5.174)

where the input-dependent variance is given by

σ2(x) = β−1 + gTA−1g.

(5.173)
We see that the predictive distribution p(t|x,D) is a Gaussian whose mean is given
by the network function y(x, wMAP) with the parameter set to their MAP value. The
variance has two terms, the ﬁrst of which arises from the intrinsic noise on the target
variable, whereas the second is an x-dependent term that expresses the uncertainty
in the interpolant due to the uncertainty in the model parameters w. This should
be compared with the corresponding predictive distribution for the linear regression
model, given by (3.58) and (3.59).

5.7.2 Hyperparameter optimization
So far, we have assumed that the hyperparameters α and β are ﬁxed and known.
We can make use of the evidence framework, discussed in Section 3.5, together with
the Gaussian approximation to the posterior obtained using the Laplace approxima-
tion, to obtain a practical procedure for choosing the values of such hyperparameters.
The marginal likelihood, or evidence, for the hyperparameters is obtained by

integrating over the network weights

ln p(D|α, β)  −E(wMAP) −
where W is the total number of parameters in w, and the regularized error function
is deﬁned by

ln(2π) (5.175)

ln β −

ln|A| + W
2

ln α + N
2

N
2

1
2

E(wMAP) = β
2

{y(xn, wMAP) − tn}2 + α
2

wT

MAPwMAP.

(5.176)

We see that this takes the same form as the corresponding result (3.86) for the linear
regression model.

In the evidence framework, we make point estimates for α and β by maximizing
ln p(D|α, β). Consider ﬁrst the maximization with respect to α, which can be done
by analogy with the linear regression case discussed in Section 3.5.2. We ﬁrst deﬁne
the eigenvalue equation

(5.177)
where H is the Hessian matrix comprising the second derivatives of the sum-of-
squares error function, evaluated at w = wMAP. By analogy with (3.92), we obtain

βHui = λiui

α =

γ

wT

MAPwMAP

(5.178)




i=1

W

5.7. Bayesian Neural Networks

where γ represents the effective number of parameters and is deﬁned by

γ =

λi

α + λi

.

281

(5.179)

Section 3.5.3

Section 5.1.1

Note that this result was exact for the linear regression case. For the nonlinear neural
network, however, it ignores the fact that changes in α will cause changes in the
Hessian H, which in turn will change the eigenvalues. We have therefore implicitly
ignored terms involving the derivatives of λi with respect to α.

Similarly, from (3.95) we see that maximizing the evidence with respect to β

gives the re-estimation formula

1
β

=

N

1

N − γ

n=1

{y(xn, wMAP) − tn}2.

(5.180)

As with the linear model, we need to alternate between re-estimation of the hyper-
parameters α and β and updating of the posterior distribution. The situation with
a neural network model is more complex, however, due to the multimodality of the
posterior distribution. As a consequence, the solution for wMAP found by maximiz-
ing the log posterior will depend on the initialization of w. Solutions that differ only
as a consequence of the interchange and sign reversal symmetries in the hidden units
are identical so far as predictions are concerned, and it is irrelevant which of the
equivalent solutions is found. However, there may be inequivalent solutions as well,
and these will generally yield different values for the optimized hyperparameters.

In order to compare different models, for example neural networks having differ-
ent numbers of hidden units, we need to evaluate the model evidence p(D). This can
be approximated by taking (5.175) and substituting the values of α and β obtained
from the iterative optimization of these hyperparameters. A more careful evaluation
is obtained by marginalizing over α and β, again by making a Gaussian approxima-
tion (MacKay, 1992c; Bishop, 1995a). In either case, it is necessary to evaluate the
determinant |A| of the Hessian matrix. This can be problematic in practice because
the determinant, unlike the trace, is sensitive to the small eigenvalues that are often
difﬁcult to determine accurately.

The Laplace approximation is based on a local quadratic expansion around a
mode of the posterior distribution over weights. We have seen in Section 5.1.1 that
any given mode in a two-layer network is a member of a set of M!2M equivalent
modes that differ by interchange and sign-change symmetries, where M is the num-
ber of hidden units. When comparing networks having different numbers of hid-
den units, this can be taken into account by multiplying the evidence by a factor of
M!2M .

5.7.3 Bayesian neural networks for classiﬁcation
So far, we have used the Laplace approximation to develop a Bayesian treat-
ment of neural network regression models. We now discuss the modiﬁcations to





N

this framework that arise when it is applied to classiﬁcation. Here we shall con-
sider a network having a single logistic sigmoid output corresponding to a two-class
classiﬁcation problem. The extension to networks with multiclass softmax outputs
is straightforward. We shall build extensively on the analogous results for linear
classiﬁcation models discussed in Section 4.5, and so we encourage the reader to
familiarize themselves with that material before studying this section.

The log likelihood function for this model is given by

ln p(D|w) =

= 1N {tn ln yn + (1 − tn) ln(1 − yn)}

n

(5.181)

where tn ∈ {0, 1} are the target values, and yn ≡ y(xn, w). Note that there is no
hyperparameter β, because the data points are assumed to be correctly labelled. As
before, the prior is taken to be an isotropic Gaussian of the form (5.162).

The ﬁrst stage in applying the Laplace framework to this model is to initialize
the hyperparameter α, and then to determine the parameter vector w by maximizing
the log posterior distribution. This is equivalent to minimizing the regularized error
function

E(w) = − ln p(D|w) + α
2

wTw

(5.182)

and can be achieved using error backpropagation combined with standard optimiza-
tion algorithms, as discussed in Section 5.3.

Having found a solution wMAP for the weight vector, the next step is to eval-
uate the Hessian matrix H comprising the second derivatives of the negative log
likelihood function. This can be done, for instance, using the exact method of Sec-
tion 5.4.5, or using the outer product approximation given by (5.85). The second
derivatives of the negative log posterior can again be written in the form (5.166), and
the Gaussian approximation to the posterior is then given by (5.167).

To optimize the hyperparameter α, we again maximize the marginal likelihood,

which is easily shown to take the form

ln p(D|α)  −E(wMAP) −

1
2

ln|A| + W
2

ln α + const

(5.183)

where the regularized error function is deﬁned by

E(wMAP) = −

n=1

{tn ln yn + (1 − tn) ln(1 − yn)} + α
2

wT

MAPwMAP (5.184)

in which yn ≡ y(xn, wMAP). Maximizing this evidence function with respect to α
again leads to the re-estimation equation given by (5.178).
The use of the evidence procedure to determine α is illustrated in Figure 5.22

for the synthetic two-dimensional data discussed in Appendix A.

Finally, we need the predictive distribution, which is deﬁned by (5.168). Again,
this integration is intractable due to the nonlinearity of the network function. The

282

5. NEURAL NETWORKS

Exercise 5.40

Exercise 5.41










5.7. Bayesian Neural Networks

283

Figure 5.22 Illustration of

the evidence framework
applied to a synthetic two-class data set.
The green curve shows the optimal de-
cision boundary, the black curve shows
the result of ﬁtting a two-layer network
with 8 hidden units by maximum likeli-
hood, and the red curve shows the re-
sult of including a regularizer in which
α is optimized using the evidence pro-
cedure, starting from the initial value
α = 0. Note that the evidence proce-
dure greatly reduces the over-ﬁtting of
the network.

3

2

1

0

−1

−2

−2

−1

0

1

2

simplest approximation is to assume that the posterior distribution is very narrow
and hence make the approximation

p(t|x,D)  p(t|x, wMAP).

(5.185)

We can improve on this, however, by taking account of the variance of the posterior
distribution. In this case, a linear approximation for the network outputs, as was used
in the case of regression, would be inappropriate due to the logistic sigmoid output-
unit activation function that constrains the output to lie in the range (0, 1). Instead,
we make a linear approximation for the output unit activation in the form

a(x, w)  aMAP(x) + bT(w − wMAP)

(5.186)
where aMAP(x) = a(x, wMAP), and the vector b ≡ ∇a(x, wMAP) can be found by
backpropagation.
Because we now have a Gaussian approximation for the posterior distribution
over w, and a model for a that is a linear function of w, we can now appeal to the
results of Section 4.5.2. The distribution of output unit activation values, induced by
the distribution over network weights, is given by

δ

p(a|x,D) =

a − aMAP(x) − bT(x)(w − wMAP)

q(w|D) dw (5.187)
where q(w|D) is the Gaussian approximation to the posterior distribution given by
(5.167). From Section 4.5.2, we see that this distribution is Gaussian with mean
aMAP ≡ a(x, wMAP), and variance

a(x) = bT(x)A−1b(x).
σ2

(5.188)

Finally, to obtain the predictive distribution, we must marginalize over a using

p(t = 1|x,D) =

σ(a)p(a|x,D) da.

(5.189)






284

5. NEURAL NETWORKS

3

2

1

0

−1

−2

−2

−1

0

1

2

3

2

1

0

−1

−2

−2

−1

0

1

2

Figure 5.23 An illustration of the Laplace approximation for a Bayesian neural network having 8 hidden units
with ‘tanh’ activation functions and a single logistic-sigmoid output unit. The weight parameters were found using
scaled conjugate gradients, and the hyperparameter α was optimized using the evidence framework. On the left
is the result of using the simple approximation (5.185) based on a point estimate wMAP of the parameters,
in which the green curve shows the y = 0.5 decision boundary, and the other contours correspond to output
probabilities of y = 0.1, 0.3, 0.7, and 0.9. On the right is the corresponding result obtained using (5.190). Note
that the effect of marginalization is to spread out the contours and to make the predictions less conﬁdent, so
that at each input point x, the posterior probabilities are shifted towards 0.5, while the y = 0.5 contour itself is
unaffected.

The convolution of a Gaussian with a logistic sigmoid is intractable. We therefore
apply the approximation (4.153) to (5.189) giving

p(t = 1|x,D) = σ

κ(σ2

a)bTwMAP

(5.190)

where κ(·) is deﬁned by (4.154). Recall that both σ2
ﬁcation data set described in Appendix A.

Figure 5.23 shows an example of this framework applied to the synthetic classi-

a and b are functions of x.

Exercises

5.1 ( ) Consider a two-layer network function of the form (5.7) in which the hidden-
unit nonlinear activation functions g(·) are given by logistic sigmoid functions of the
form
(5.191)

σ(a) = {1 + exp(−a)}−1 .

Show that there exists an equivalent network, which computes exactly the same func-
tion, but with hidden unit activation functions given by tanh(a) where the tanh func-
tion is deﬁned by (5.59). Hint: ﬁrst ﬁnd the relation between σ(a) and tanh(a), and
then show that the parameters of the two networks differ by linear transformations.

5.2 () www Show that maximizing the likelihood function under the conditional
distribution (5.16) for a multioutput neural network is equivalent to minimizing the
sum-of-squares error function (5.11).

Exercises

285

5.3 ( ) Consider a regression problem involving multiple target variables in which it
is assumed that the distribution of the targets, conditioned on the input vector x, is a
Gaussian of the form

p(t|x, w) = N (t|y(x, w), Σ)

(5.192)
where y(x, w) is the output of a neural network with input vector x and weight
vector w, and Σ is the covariance of the assumed Gaussian noise on the targets.
Given a set of independent observations of x and t, write down the error function
that must be minimized in order to ﬁnd the maximum likelihood solution for w, if
we assume that Σ is ﬁxed and known. Now assume that Σ is also to be determined
from the data, and write down an expression for the maximum likelihood solution
for Σ. Note that the optimizations of w and Σ are now coupled, in contrast to the
case of independent target variables discussed in Section 5.2.

5.4 ( ) Consider a binary classiﬁcation problem in which the target values are t ∈
{0, 1}, with a network output y(x, w) that represents p(t = 1|x), and suppose that
there is a probability 	 that the class label on a training data point has been incorrectly
set. Assuming independent and identically distributed data, write down the error
function corresponding to the negative log likelihood. Verify that the error function
(5.21) is obtained when 	 = 0. Note that this error function makes the model robust
to incorrectly labelled data, in contrast to the usual error function.

5.5 () www Show that maximizing likelihood for a multiclass neural network model
in which the network outputs have the interpretation yk(x, w) = p(tk = 1|x) is
equivalent to the minimization of the cross-entropy error function (5.24).

5.6 () www Show the derivative of the error function (5.21) with respect to the
activation ak for an output unit having a logistic sigmoid activation function satisﬁes
(5.18).

5.7 () Show the derivative of the error function (5.24) with respect to the activation ak

for output units having a softmax activation function satisﬁes (5.18).

5.8 () We saw in (4.88) that the derivative of the logistic sigmoid activation function
can be expressed in terms of the function value itself. Derive the corresponding result
for the ‘tanh’ activation function deﬁned by (5.59).

5.9 () www The error function (5.21) for binary classiﬁcation problems was de-
rived for a network having a logistic-sigmoid output activation function, so that
0 � y(x, w) � 1, and data having target values t ∈ {0, 1}. Derive the correspond-
ing error function if we consider a network having an output −1 � y(x, w) � 1
and target values t = 1 for class C1 and t = −1 for class C2. What would be the
appropriate choice of output unit activation function?

5.10 () www Consider a Hessian matrix H with eigenvector equation (5.33). By
setting the vector v in (5.39) equal to each of the eigenvectors ui in turn, show that
H is positive deﬁnite if, and only if, all of its eigenvalues are positive.







286

5. NEURAL NETWORKS

5.11 ( ) www Consider a quadratic error function deﬁned by (5.32), in which the
Hessian matrix H has an eigenvalue equation given by (5.33). Show that the con-
tours of constant error are ellipses whose axes are aligned with the eigenvectors ui,
with lengths that are inversely proportional to the square root of the corresponding
eigenvalues λi.

5.12 ( ) www By considering the local Taylor expansion (5.32) of an error function
about a stationary point w, show that the necessary and sufﬁcient condition for the
stationary point to be a local minimum of the error function is that the Hessian matrix
H, deﬁned by (5.30) with

w = w, be positive deﬁnite.

5.13 ()

Show that as a consequence of the symmetry of the Hessian matrix H, the
number of independent elements in the quadratic error function (5.28) is given by
W (W + 3)/2.

5.14 () By making a Taylor expansion, verify that the terms that are O(	) cancel on the

right-hand side of (5.69).

5.15 ( ) In Section 5.3.4, we derived a procedure for evaluating the Jacobian matrix of a
neural network using a backpropagation procedure. Derive an alternative formalism
for ﬁnding the Jacobian based on forward propagation equations.

5.16 () The outer product approximation to the Hessian matrix for a neural network
using a sum-of-squares error function is given by (5.84). Extend this result to the
case of multiple outputs.

5.17 () Consider a squared loss function of the form

E =

1
2

{y(x, w) − t}2 p(x, t) dx dt

(5.193)

where y(x, w) is a parametric function such as a neural network. The result (1.89)
shows that the function y(x, w) that minimizes this error is given by the conditional
expectation of t given x. Use this result to show that the second derivative of E with
respect to two elements wr and ws of the vector w, is given by

∂2E

∂wr∂ws

=

∂y
∂wr

∂y
∂ws

p(x) dx.

(5.194)

Note that, for a ﬁnite sample from p(x), we obtain (5.84).

5.18 () Consider a two-layer network of the form shown in Figure 5.1 with the addition
of extra parameters corresponding to skip-layer connections that go directly from
the inputs to the outputs. By extending the discussion of Section 5.3.2, write down
the equations for the derivatives of the error function with respect to these additional
parameters.

5.19 () www Derive the expression (5.85) for the outer product approximation to
the Hessian matrix for a network having a single output with a logistic sigmoid
output-unit activation function and a cross-entropy error function, corresponding to
the result (5.84) for the sum-of-squares error function.

Exercises

287

5.20 () Derive an expression for the outer product approximation to the Hessian matrix
for a network having K outputs with a softmax output-unit activation function and
a cross-entropy error function, corresponding to the result (5.84) for the sum-of-
squares error function.

5.21 (  ) Extend the expression (5.86) for the outer product approximation of the Hes-
sian matrix to the case of K > 1 output units. Hence, derive a recursive expression
analogous to (5.87) for incrementing the number N of patterns and a similar expres-
sion for incrementing the number K of outputs. Use these results, together with the
identity (5.88), to ﬁnd sequential update expressions analogous to (5.89) for ﬁnding
the inverse of the Hessian by incrementally including both extra patterns and extra
outputs.

5.22 ( ) Derive the results (5.93), (5.94), and (5.95) for the elements of the Hessian
matrix of a two-layer feed-forward network by application of the chain rule of cal-
culus.

5.23 ( ) Extend the results of Section 5.4.5 for the exact Hessian of a two-layer network

to include skip-layer connections that go directly from inputs to outputs.

5.24 () Verify that the network function deﬁned by (5.113) and (5.114) is invariant un-
der the transformation (5.115) applied to the inputs, provided the weights and biases
are simultaneously transformed using (5.116) and (5.117). Similarly, show that the
network outputs can be transformed according (5.118) by applying the transforma-
tion (5.119) and (5.120) to the second-layer weights and biases.

5.25 (  ) www Consider a quadratic error function of the form

E = E0 +

1
2

(w − w)TH(w − w)

(5.195)

where w represents the minimum, and the Hessian matrix H is positive deﬁnite and
constant. Suppose the initial weight vector w(0) is chosen to be at the origin and is
updated using simple gradient descent

w(τ ) = w(τ−1) − ρ∇E

(5.196)

where τ denotes the step number, and ρ is the learning rate (which is assumed to be
small). Show that, after τ steps, the components of the weight vector parallel to the
eigenvectors of H can be written

w(τ )
j = {1 − (1 − ρηj)τ} w

j

(5.197)

where wj = wTuj, and uj and ηj are the eigenvectors and eigenvalues, respectively,
of H so that

(5.198)
Show that as τ → ∞, this gives w(τ ) → w as expected, provided |1 − ρηj| < 1.
Now suppose that training is halted after a ﬁnite number τ of steps. Show that the

Huj = ηjuj.




k

1
2






i

(5.201)

(5.202)

(5.203)

(5.204)

(5.205)

288

5. NEURAL NETWORKS

components of the weight vector parallel to the eigenvectors of the Hessian satisfy

w(τ )
j  w
|  |w

j when ηj 
 (ρτ)−1
j| when ηj  (ρτ)−1.

|w(τ )

j

(5.199)

(5.200)

Compare this result with the discussion in Section 3.5.3 of regularization with simple
weight decay, and hence show that (ρτ)−1 is analogous to the regularization param-
eter λ. The above results also show that the effective number of parameters in the
network, as deﬁned by (3.91), grows as the training progresses.

5.26 ( ) Consider a multilayer perceptron with arbitrary feed-forward topology, which
is to be trained by minimizing the tangent propagation error function (5.127) in
which the regularizing function is given by (5.128). Show that the regularization
term Ω can be written as a sum over patterns of terms of the form

Ωn =

(Gyk)2

where G is a differential operator deﬁned by
τi

G ≡

i

∂
∂xi

.

By acting on the forward propagation equations

zj = h(aj),

aj =

wjizi

with the operator G, show that Ωn can be evaluated by forward propagation using
the following equations:

αj = h(aj)βj,

βj =

wjiαi.

i

where we have deﬁned the new variables

αj ≡ Gzj,

βj ≡ Gaj.

Now show that the derivatives of Ωn with respect to a weight wrs in the network can
be written in the form

where we have deﬁned

∂Ωn
∂wrs

=

k

αk {φkrzs + δkrαs}

δkr ≡

∂yk
∂ar

,

φkr ≡ Gδkr.

(5.206)

(5.207)

Write down the backpropagation equations for δkr, and hence derive a set of back-
propagation equations for the evaluation of the φkr.



5.27 ( ) www Consider the framework for training with transformed data in the
special case in which the transformation consists simply of the addition of random
noise x → x + ξ where ξ has a Gaussian distribution with zero mean and unit
covariance. By following an argument analogous to that of Section 5.5.5, show that
the resulting regularizer reduces to the Tikhonov form (5.135).

5.28 () www Consider a neural network, such as the convolutional network discussed
in Section 5.5.6, in which multiple weights are constrained to have the same value.
Discuss how the standard backpropagation algorithm must be modiﬁed in order to
ensure that such constraints are satisﬁed when evaluating the derivatives of an error
function with respect to the adjustable parameters in the network.

5.29 () www Verify the result (5.141).

5.30 () Verify the result (5.142).

5.31 () Verify the result (5.143).
5.32 ( ) Show that the derivatives of the mixing coefﬁcients {πk}, deﬁned by (5.146),

with respect to the auxiliary parameters {ηj} are given by

Exercises

289

∂πk
∂ηj

= δjkπj − πjπk.

(5.208)

Hence, by making use of the constraint

k πk = 1, derive the result (5.147).

5.33 () Write down a pair of equations that express the Cartesian coordinates (x1, x2)
for the robot arm shown in Figure 5.18 in terms of the joint angles θ1 and θ2 and
the lengths L1 and L2 of the links. Assume the origin of the coordinate system is
given by the attachment point of the lower arm. These equations deﬁne the ‘forward
kinematics’ of the robot arm.

5.34 () www Derive the result (5.155) for the derivative of the error function with
respect to the network output activations controlling the mixing coefﬁcients in the
mixture density network.

5.35 () Derive the result (5.156) for the derivative of the error function with respect
to the network output activations controlling the component means in the mixture
density network.

5.36 () Derive the result (5.157) for the derivative of the error function with respect to
the network output activations controlling the component variances in the mixture
density network.

5.37 () Verify the results (5.158) and (5.160) for the conditional mean and variance of

the mixture density network model.

5.38 () Using the general result (2.115), derive the predictive distribution (5.172) for

the Laplace approximation to the Bayesian neural network model.

290

5. NEURAL NETWORKS

5.39 () www Make use of the Laplace approximation result (4.135) to show that the
evidence function for the hyperparameters α and β in the Bayesian neural network
model can be approximated by (5.175).

5.40 () www Outline the modiﬁcations needed to the framework for Bayesian neural
networks, discussed in Section 5.7.3, to handle multiclass problems using networks
having softmax output-unit activation functions.

5.41 ( ) By following analogous steps to those given in Section 5.7.1 for regression
networks, derive the result (5.183) for the marginal likelihood in the case of a net-
work having a cross-entropy error function and logistic-sigmoid output-unit activa-
tion function.

6

Kernel
Methods

In Chapters 3 and 4, we considered linear parametric models for regression and
classiﬁcation in which the form of the mapping y(x, w) from input x to output y
is governed by a vector w of adaptive parameters. During the learning phase, a
set of training data is used either to obtain a point estimate of the parameter vector
or to determine a posterior distribution over this vector. The training data is then
discarded, and predictions for new inputs are based purely on the learned parameter
vector w. This approach is also used in nonlinear parametric models such as neural
networks.

However, there is a class of pattern recognition techniques, in which the training
data points, or a subset of them, are kept and used also during the prediction phase.
For instance, the Parzen probability density model comprised a linear combination
of ‘kernel’ functions each one centred on one of the training data points. Similarly,
in Section 2.5.2 we introduced a simple technique for classiﬁcation called nearest
neighbours, which involved assigning to each new test vector the same label as the

Chapter 5

Section 2.5.1

291

292

6. KERNEL METHODS

closest example from the training set. These are examples of memory-based methods
that involve storing the entire training set in order to make predictions for future data
points. They typically require a metric to be deﬁned that measures the similarity of
any two vectors in input space, and are generally fast to ‘train’ but slow at making
predictions for test data points.

Many linear parametric models can be re-cast into an equivalent ‘dual represen-
tation’ in which the predictions are also based on linear combinations of a kernel
function evaluated at the training data points. As we shall see, for models which are
based on a ﬁxed nonlinear feature space mapping φ(x), the kernel function is given
by the relation

k(x, x) = φ(x)Tφ(x).

(6.1)

From this deﬁnition, we see that the kernel is a symmetric function of its arguments
so that k(x, x) = k(x, x). The kernel concept was introduced into the ﬁeld of pat-
tern recognition by Aizerman et al. (1964) in the context of the method of potential
functions, so-called because of an analogy with electrostatics. Although neglected
for many years, it was re-introduced into machine learning in the context of large-
margin classiﬁers by Boser et al.
(1992) giving rise to the technique of support
vector machines. Since then, there has been considerable interest in this topic, both
in terms of theory and applications. One of the most signiﬁcant developments has
been the extension of kernels to handle symbolic objects, thereby greatly expanding
the range of problems that can be addressed.

The simplest example of a kernel function is obtained by considering the identity
mapping for the feature space in (6.1) so that φ(x) = x, in which case k(x, x) =
xTx. We shall refer to this as the linear kernel.

The concept of a kernel formulated as an inner product in a feature space allows
us to build interesting extensions of many well-known algorithms by making use of
the kernel trick, also known as kernel substitution. The general idea is that, if we have
an algorithm formulated in such a way that the input vector x enters only in the form
of scalar products, then we can replace that scalar product with some other choice of
kernel. For instance, the technique of kernel substitution can be applied to principal
component analysis in order to develop a nonlinear variant of PCA (Sch¨olkopf et al.,
1998). Other examples of kernel substitution include nearest-neighbour classiﬁers
and the kernel Fisher discriminant (Mika et al., 1999; Roth and Steinhage, 2000;
Baudat and Anouar, 2000).

There are numerous forms of kernel functions in common use, and we shall en-
counter several examples in this chapter. Many have the property of being a function
only of the difference between the arguments, so that k(x, x) = k(x − x), which
are known as stationary kernels because they are invariant to translations in input
space. A further specialization involves homogeneous kernels, also known as ra-
dial basis functions, which depend only on the magnitude of the distance (typically
Euclidean) between the arguments so that k(x, x) = k(x − x).
brich (2002), and Shawe-Taylor and Cristianini (2004).

For recent textbooks on kernel methods, see Sch¨olkopf and Smola (2002), Her-

Chapter 7

Section 12.3

Section 6.3





N

n=1









N

n=1





n=1

N

6.1. Dual Representations

293

6.1. Dual Representations

Many linear models for regression and classiﬁcation can be reformulated in terms of
a dual representation in which the kernel function arises naturally. This concept will
play an important role when we consider support vector machines in the next chapter.
Here we consider a linear regression model whose parameters are determined by
minimizing a regularized sum-of-squares error function given by

J(w) =

1
2

wTφ(xn) − tn

2 + λ
2

wTw

(6.2)

where λ � 0. If we set the gradient of J(w) with respect to w equal to zero, we see
that the solution for w takes the form of a linear combination of the vectors φ(xn),
with coefﬁcients that are functions of w, of the form

w = −

1
λ

wTφ(xn) − tn

φ(xn) =

anφ(xn) = ΦTa

(6.3)

where Φ is the design matrix, whose nth row is given by φ(xn)T. Here the vector
a = (a1, . . . , aN )T, and we have deﬁned

an = −

1
λ

wTφ(xn) − tn

.

(6.4)

Instead of working with the parameter vector w, we can now reformulate the least-
squares algorithm in terms of the parameter vector a, giving rise to a dual represen-
tation. If we substitute w = ΦTa into J(w), we obtain

J(a) =

1
2

aTΦΦTΦΦTa − aTΦΦTt +

1
2

tTt + λ
2

aTΦΦTa

(6.5)

where t = (t1, . . . , tN )T. We now deﬁne the Gram matrix K = ΦΦT, which is an
N × N symmetric matrix with elements

Knm = φ(xn)Tφ(xm) = k(xn, xm)

(6.6)

where we have introduced the kernel function k(x, x) deﬁned by (6.1). In terms of
the Gram matrix, the sum-of-squares error function can be written as

J(a) =

1
2

aTKKa − aTKt +

1
2

tTt + λ
2

aTKa.

(6.7)

Setting the gradient of J(a) with respect to a to zero, we obtain the following solu-
tion

a = (K + λIN )−1 t.

(6.8)

If we substitute this back into the linear regression model, we obtain the following
prediction for a new input x

y(x) = wTφ(x) = aTΦφ(x) = k(x)T (K + λIN )−1 t

(6.9)

where we have deﬁned the vector k(x) with elements kn(x) = k(xn, x). Thus we
see that the dual formulation allows the solution to the least-squares problem to be
expressed entirely in terms of the kernel function k(x, x). This is known as a dual
formulation because, by noting that the solution for a can be expressed as a linear
combination of the elements of φ(x), we recover the original formulation in terms of
the parameter vector w. Note that the prediction at x is given by a linear combination
of the target values from the training set. In fact, we have already obtained this result,
using a slightly different notation, in Section 3.3.3.

In the dual formulation, we determine the parameter vector a by inverting an
N × N matrix, whereas in the original parameter space formulation we had to invert
an M × M matrix in order to determine w. Because N is typically much larger
than M, the dual formulation does not seem to be particularly useful. However, the
advantage of the dual formulation, as we shall see, is that it is expressed entirely in
terms of the kernel function k(x, x). We can therefore work directly in terms of
kernels and avoid the explicit introduction of the feature vector φ(x), which allows
us implicitly to use feature spaces of high, even inﬁnite, dimensionality.

The existence of a dual representation based on the Gram matrix is a property of
many linear models, including the perceptron. In Section 6.4, we will develop a dual-
ity between probabilistic linear models for regression and the technique of Gaussian
processes. Duality will also play an important role when we discuss support vector
machines in Chapter 7.

Exercise 6.1

Exercise 6.2




i=1

M




294

6. KERNEL METHODS

6.2. Constructing Kernels

In order to exploit kernel substitution, we need to be able to construct valid kernel
functions. One approach is to choose a feature space mapping φ(x) and then use
this to ﬁnd the corresponding kernel, as is illustrated in Figure 6.1. Here the kernel
function is deﬁned for a one-dimensional input space by

k(x, x) = φ(x)Tφ(x) =

φi(x)φi(x)

(6.10)

where φi(x) are the basis functions.

An alternative approach is to construct kernel functions directly. In this case,
we must ensure that the function we choose is a valid kernel, in other words that it
corresponds to a scalar product in some (perhaps inﬁnite dimensional) feature space.
As a simple example, consider a kernel function given by

k(x, z) =

xTz

2

.

(6.11)

1

0.5

0

−0.5

−1

−1

0.04

0.02

0

−1

1

0.75

0.5

0.25

0

−1

0.04

0.02

0

−1

0

1

0

1

6.2. Constructing Kernels

295

1

0.75

0.5

0.25

0
−1

0.04

0.02

0

−1

1

1

0

0

1

1




0

0



Figure 6.1 Illustration of the construction of kernel functions starting from a corresponding set of basis func-
tions. In each column the lower plot shows the kernel function k(x, x) deﬁned by (6.10) plotted as a function of
x for x = 0, while the upper plot shows the corresponding basis functions given by polynomials (left column),
‘Gaussians’ (centre column), and logistic sigmoids (right column).

If we take the particular case of a two-dimensional input space x = (x1, x2) we
can expand out the terms and thereby identify the corresponding nonlinear feature
mapping

k(x, z) =

2 = (x1z1 + x2z2)2
2z2
2

xTz
= x2
1 + 2x1z1x2z2 + x2
1z2
1,√2x1x2, x2
= (x2
= φ(x)Tφ(z).

1,√2z1z2, z2
2)T

2)(z2

(6.12)

2)T and
We see that the feature mapping takes the form φ(x) = (x2
therefore comprises all possible second order terms, with a speciﬁc weighting be-
tween them.

1,√2x1x2, x2

More generally, however, we need a simple way to test whether a function con-
stitutes a valid kernel without having to construct the function φ(x) explicitly. A
necessary and sufﬁcient condition for a function k(x, x) to be a valid kernel (Shawe-
Taylor and Cristianini, 2004) is that the Gram matrix K, whose elements are given by
k(xn, xm), should be positive semideﬁnite for all possible choices of the set {xn}.
Note that a positive semideﬁnite matrix is not the same thing as a matrix whose
elements are nonnegative.

One powerful technique for constructing new kernels is to build them out of
simpler kernels as building blocks. This can be done using the following properties:

Appendix C

296

6. KERNEL METHODS

























Techniques for Constructing New Kernels.

Given valid kernels k1(x, x) and k2(x, x), the following new kernels will also
be valid:

k(x, x) = ck1(x, x)
k(x, x) = f(x)k1(x, x)f(x)
k(x, x) = q (k1(x, x))
k(x, x) = exp (k1(x, x))
k(x, x) = k1(x, x) + k2(x, x)
k(x, x) = k1(x, x)k2(x, x)
k(x, x) = k3 (φ(x), φ(x))
k(x, x) = xTAx
k(x, x) = ka(xa, xa) + kb(xb, xb)
k(x, x) = ka(xa, xa)kb(xb, xb)

(6.13)
(6.14)
(6.15)
(6.16)
(6.17)
(6.18)
(6.19)
(6.20)
(6.21)
(6.22)
where c > 0 is a constant, f(·) is any function, q(·) is a polynomial with nonneg-
ative coefﬁcients, φ(x) is a function from x to RM , k3(·,·) is a valid kernel in
RM , A is a symmetric positive semideﬁnite matrix, xa and xb are variables (not
necessarily disjoint) with x = (xa, xb), and ka and kb are valid kernel functions
over their respective spaces.

Equipped with these properties, we can now embark on the construction of more
complex kernels appropriate to speciﬁc applications. We require that the kernel
k(x, x) be symmetric and positive semideﬁnite and that it expresses the appropriate
form of similarity between x and x according to the intended application. Here we
consider a few common examples of kernel functions. For a more extensive discus-
sion of ‘kernel engineering’, see Shawe-Taylor and Cristianini (2004).

We saw that the simple polynomial kernel k(x, x) =

xTx

2 contains only
If we consider the slightly generalized kernel k(x, x) =
2 with c > 0, then the corresponding feature mapping φ(x) contains con-
M

terms of degree two.
xTx + c
stant and linear terms as well as terms of order two. Similarly, k(x, x) =
contains all monomials of order M. For instance, if x and x are two images, then
the kernel represents a particular weighted sum of all possible products of M pixels
in the ﬁrst image with M pixels in the second image. This can similarly be gener-
M
alized to include all terms up to degree M by considering k(x, x) =
with c > 0. Using the results (6.17) and (6.18) for combining kernels we see that
these will all be valid kernel functions.

xTx + c

xTx

Another commonly used kernel takes the form

k(x, x) = exp

−x − x2/2σ2

(6.23)

and is often called a ‘Gaussian’ kernel. Note, however, that in this context it is
not interpreted as a probability density, and hence the normalization coefﬁcient is










Exercise 6.11

Exercise 6.12

6.2. Constructing Kernels

297

(6.24)

(6.25)

omitted. We can see that this is a valid kernel by expanding the square

x − x2 = xTx + (x)Tx − 2xTx

to give

k(x, x) = exp

−xTx/2σ2

exp

xTx/σ2

exp

−(x)Tx/2σ2

and then making use of (6.14) and (6.16), together with the validity of the linear
kernel k(x, x) = xTx. Note that the feature vector that corresponds to the Gaussian
kernel has inﬁnite dimensionality.

The Gaussian kernel is not restricted to the use of Euclidean distance. If we use
kernel substitution in (6.24) to replace xTx with a nonlinear kernel κ(x, x), we
obtain

k(x, x) = exp

−

1
2σ2 (κ(x, x) + κ(x, x) − 2κ(x, x))

.

(6.26)

An important contribution to arise from the kernel viewpoint has been the exten-
sion to inputs that are symbolic, rather than simply vectors of real numbers. Kernel
functions can be deﬁned over objects as diverse as graphs, sets, strings, and text doc-
uments. Consider, for instance, a ﬁxed set and deﬁne a nonvectorial space consisting
of all possible subsets of this set. If A1 and A2 are two such subsets then one simple
choice of kernel would be

k(A1, A2) = 2|A1∩A2|

(6.27)
where A1 ∩ A2 denotes the intersection of sets A1 and A2, and |A| denotes the
number of subsets in A. This is a valid kernel function because it can be shown to
correspond to an inner product in a feature space.

One powerful approach to the construction of kernels starts from a probabilistic
generative model (Haussler, 1999), which allows us to apply generative models in a
discriminative setting. Generative models can deal naturally with missing data and
in the case of hidden Markov models can handle sequences of varying length. By
contrast, discriminative models generally give better performance on discriminative
tasks than generative models. It is therefore of some interest to combine these two
approaches (Lasserre et al., 2006). One way to combine them is to use a generative
model to deﬁne a kernel, and then use this kernel in a discriminative approach.

Given a generative model p(x) we can deﬁne a kernel by

k(x, x) = p(x)p(x).

(6.28)

This is clearly a valid kernel function because we can interpret it as an inner product
in the one-dimensional feature space deﬁned by the mapping p(x). It says that two
inputs x and x are similar if they both have high probabilities. We can use (6.13) and
(6.17) to extend this class of kernels by considering sums over products of different
probability distributions, with positive weighting coefﬁcients p(i), of the form

k(x, x) =

p(x|i)p(x|i)p(i).

i

(6.29)

















Z

N

	

298

6. KERNEL METHODS

This is equivalent, up to an overall multiplicative constant, to a mixture distribution
in which the components factorize, with the index i playing the role of a ‘latent’
variable. Two inputs x and x will give a large value for the kernel function, and
hence appear similar, if they have signiﬁcant probability under a range of different
components. Taking the limit of an inﬁnite sum, we can also consider kernels of the
form

k(x, x) =

p(x|z)p(x|z)p(z) dz

(6.30)

where z is a continuous latent variable.

Now suppose that our data consists of ordered sequences of length L so that
an observation is given by X = {x1, . . . , xL}. A popular generative model for
sequences is the hidden Markov model, which expresses the distribution p(X) as a
marginalization over a corresponding sequence of hidden states Z = {z1, . . . , zL}.
We can use this approach to deﬁne a kernel function measuring the similarity of two
sequences X and X by extending the mixture representation (6.29) to give

k(X, X) =

p(X|Z)p(X|Z)p(Z)

(6.31)

so that both observed sequences are generated by the same hidden sequence Z. This
model can easily be extended to allow sequences of differing length to be compared.
An alternative technique for using generative models to deﬁne kernel functions
is known as the Fisher kernel (Jaakkola and Haussler, 1999). Consider a parametric
generative model p(x|θ) where θ denotes the vector of parameters. The goal is to
ﬁnd a kernel that measures the similarity of two input vectors x and x induced by the
generative model. Jaakkola and Haussler (1999) consider the gradient with respect
to θ, which deﬁnes a vector in a ‘feature’ space having the same dimensionality as
θ. In particular, they consider the Fisher score

Section 9.2

Section 13.2

Exercise 6.13

from which the Fisher kernel is deﬁned by

g(θ, x) = ∇θ ln p(x|θ)

Here F is the Fisher information matrix, given by

k(x, x) = g(θ, x)TF−1g(θ, x).

(6.32)

(6.33)

F = Ex

g(θ, x)g(θ, x)T

(6.34)
where the expectation is with respect to x under the distribution p(x|θ). This can
be motivated from the perspective of information geometry (Amari, 1998), which
considers the differential geometry of the space of model parameters. Here we sim-
ply note that the presence of the Fisher information matrix causes this kernel to be
invariant under a nonlinear re-parameterization of the density model θ → ψ(θ).
In practice, it is often infeasible to evaluate the Fisher information matrix. One
approach is simply to replace the expectation in the deﬁnition of the Fisher informa-
tion with the sample average, giving

1
N

F 

g(θ, xn)g(θ, xn)T.

(6.35)

n=1








N

6.3. Radial Basis Function Networks

299

Section 12.1.3

Section 6.4.7

This is the covariance matrix of the Fisher scores, and so the Fisher kernel corre-
sponds to a whitening of these scores. More simply, we can just omit the Fisher
information matrix altogether and use the noninvariant kernel

k(x, x) = g(θ, x)Tg(θ, x).

(6.36)

An application of Fisher kernels to document retrieval is given by Hofmann (2000).

A ﬁnal example of a kernel function is the sigmoidal kernel given by

k(x, x) = tanh

axTx + b

(6.37)

whose Gram matrix in general is not positive semideﬁnite. This form of kernel
has, however, been used in practice (Vapnik, 1995), possibly because it gives kernel
expansions such as the support vector machine a superﬁcial resemblance to neural
network models. As we shall see, in the limit of an inﬁnite number of basis functions,
a Bayesian neural network with an appropriate prior reduces to a Gaussian process,
thereby providing a deeper link between neural networks and kernel methods.

6.3. Radial Basis Function Networks

In Chapter 3, we discussed regression models based on linear combinations of ﬁxed
basis functions, although we did not discuss in detail what form those basis functions
might take. One choice that has been widely used is that of radial basis functions,
which have the property that each basis function depends only on the radial distance
(typically Euclidean) from a centre µj, so that φj(x) = h(x − µj).
Historically, radial basis functions were introduced for the purpose of exact func-
tion interpolation (Powell, 1987). Given a set of input vectors {x1, . . . , xN} along
with corresponding target values {t1, . . . , tN}, the goal is to ﬁnd a smooth function
f(x) that ﬁts every target value exactly, so that f(xn) = tn for n = 1, . . . , N. This
is achieved by expressing f(x) as a linear combination of radial basis functions, one
centred on every data point

f(x) =

wnh(x − xn).

n=1

(6.38)

The values of the coefﬁcients {wn} are found by least squares, and because there
are the same number of coefﬁcients as there are constraints, the result is a function
that ﬁts every target value exactly. In pattern recognition applications, however, the
target values are generally noisy, and exact interpolation is undesirable because this
corresponds to an over-ﬁtted solution.

Expansions in radial basis functions also arise from regularization theory (Pog-
gio and Girosi, 1990; Bishop, 1995a). For a sum-of-squares error function with a
regularizer deﬁned in terms of a differential operator, the optimal solution is given
by an expansion in the Green’s functions of the operator (which are analogous to the
eigenvectors of a discrete matrix), again with one basis function centred on each data





N

n=1




n=1

N



300

6. KERNEL METHODS

point. If the differential operator is isotropic then the Green’s functions depend only
on the radial distance from the corresponding data point. Due to the presence of the
regularizer, the solution no longer interpolates the training data exactly.

Another motivation for radial basis functions comes from a consideration of
the interpolation problem when the input (rather than the target) variables are noisy
If the noise on the input variable x is described
(Webb, 1994; Bishop, 1995a).
by a variable ξ having a distribution ν(ξ), then the sum-of-squares error function
becomes

E =

1
2

{y(xn + ξ) − tn}2 ν(ξ) dξ.

Appendix D
Exercise 6.17

Using the calculus of variations, we can optimize with respect to the function f(x)
to give

(6.39)

(6.40)

(6.41)

where the basis functions are given by

y(xn) =

tnh(x − xn)

h(x − xn) = ν(x − xn)
ν(x − xn)

N

.

n=1

We see that there is one basis function centred on every data point. This is known as
the Nadaraya-Watson model and will be derived again from a different perspective
in Section 6.3.1. If the noise distribution ν(ξ) is isotropic, so that it is a function
only of ξ, then the basis functions will be radial.
n h(x − xn) = 1
for any value of x. The effect of such normalization is shown in Figure 6.2. Normal-
ization is sometimes used in practice as it avoids having regions of input space where
all of the basis functions take small values, which would necessarily lead to predic-
tions in such regions that are either small or controlled purely by the bias parameter.

Note that the basis functions (6.41) are normalized, so that

Another situation in which expansions in normalized radial basis functions arise
is in the application of kernel density estimation to the problem of regression, as we
shall discuss in Section 6.3.1.

Because there is one basis function associated with every data point, the corre-
sponding model can be computationally costly to evaluate when making predictions
for new data points. Models have therefore been proposed (Broomhead and Lowe,
1988; Moody and Darken, 1989; Poggio and Girosi, 1990), which retain the expan-
sion in radial basis functions but where the number M of basis functions is smaller
than the number N of data points. Typically, the number of basis functions, and the
locations µi of their centres, are determined based on the input data {xn} alone. The
basis functions are then kept ﬁxed and the coefﬁcients {wi} are determined by least
squares by solving the usual set of linear equations, as discussed in Section 3.1.1.

6.3. Radial Basis Function Networks

301

1

0.8

0.6

0.4

0.2

0
−1

−0.5

0

0.5

1

1

0.8

0.6

0.4

0.2

0
−1

Figure 6.2 Plot of a set of Gaussian basis functions on the left, together with the corresponding normalized
basis functions on the right.

−0.5

0

0.5

1



N

Section 9.1

Section 2.5.1

One of the simplest ways of choosing basis function centres is to use a randomly
chosen subset of the data points. A more systematic approach is called orthogonal
least squares (Chen et al., 1991). This is a sequential selection process in which at
each step the next data point to be chosen as a basis function centre corresponds to
the one that gives the greatest reduction in the sum-of-squares error. Values for the
expansion coefﬁcients are determined as part of the algorithm. Clustering algorithms
such as K-means have also been used, which give a set of basis function centres that
no longer coincide with training data points.

6.3.1 Nadaraya-Watson model
In Section 3.3.3, we saw that the prediction of a linear regression model for a
new input x takes the form of a linear combination of the training set target values
with coefﬁcients given by the ‘equivalent kernel’ (3.62) where the equivalent kernel
satisﬁes the summation constraint (3.64).

We can motivate the kernel regression model (3.61) from a different perspective,
starting with kernel density estimation. Suppose we have a training set {xn, tn} and
we use a Parzen density estimator to model the joint distribution p(x, t), so that

p(x, t) =

1
N

f(x − xn, t − tn)

n=1

(6.42)

where f(x, t) is the component density function, and there is one such component
centred on each data point. We now ﬁnd an expression for the regression function
y(x), corresponding to the conditional average of the target variable conditioned on










m

m

n

n

−∞








−∞

∞

=

m

n

N

y(x) =

g(x − xn)tn
g(x − xm)
k(x, xn)tn

k(x, xn) = g(x − xn)
g(x − xm)

g(x) =

∞

f(x, t) dt.

k(x, xn) = 1.

n=1

where n, m = 1, . . . , N and the kernel function k(x, xn) is given by

and we have deﬁned

The result (6.45) is known as the Nadaraya-Watson model, or kernel regression
(Nadaraya, 1964; Watson, 1964). For a localized kernel function, it has the prop-
erty of giving more weight to the data points xn that are close to x. Note that the
kernel (6.46) satisﬁes the summation constraint

302

6. KERNEL METHODS

the input variable, which is given by

y(x) = E[t|x] =

tp(t|x) dt

∞

−∞
tp(x, t) dt

p(x, t) dt

=

=

tf(x − xn, t − tn) dt

f(x − xm, t − tm) dt

.

(6.43)

We now assume for simplicity that the component density functions have zero mean
so that

f(x, t)t dt = 0

for all values of x. Using a simple change of variable, we then obtain

(6.44)

(6.45)

(6.46)

(6.47)







0

Figure 6.3 Illustration of the Nadaraya-Watson kernel
regression model using isotropic Gaussian kernels, for the
sinusoidal data set. The original sine function is shown
by the green curve, the data points are shown in blue,
and each is the centre of an isotropic Gaussian kernel.
The resulting regression function, given by the condi-
tional mean, is shown by the red line, along with the two-
standard-deviation region for the conditional distribution
p(t|x) shown by the red shading. The blue ellipse around
each data point shows one standard deviation contour for
the corresponding kernel. These appear noncircular due
to the different scales on the horizontal and vertical axes.

1.5

1

0.5

0

−0.5

−1

−1.5

6.4. Gaussian Processes

303

0.2

0.4

0.6

0.8

1

In fact, this model deﬁnes not only a conditional expectation but also a full

conditional distribution given by

p(t|x) = p(t, x)

p(t, x) dt

=

f(x − xn, t − tn)

(6.48)

n

f(x − xm, t − tm) dt

m

Exercise 6.18

from which other expectations can be evaluated.

As an illustration we consider the case of a single input variable x in which
f(x, t) is given by a zero-mean isotropic Gaussian over the variable z = (x, t) with
variance σ2. The corresponding conditional distribution (6.48) is given by a Gaus-
sian mixture, and is shown, together with the conditional mean, for the sinusoidal
synthetic data set in Figure 6.3.

An obvious extension of this model is to allow for more ﬂexible forms of Gaus-
sian components, for instance having different variance parameters for the input and
target variables. More generally, we could model the joint distribution p(t, x) using
a Gaussian mixture model, trained using techniques discussed in Chapter 9 (Ghahra-
mani and Jordan, 1994), and then ﬁnd the corresponding conditional distribution
p(t|x). In this latter case we no longer have a representation in terms of kernel func-
tions evaluated at the training set data points. However, the number of components
in the mixture model can be smaller than the number of training set points, resulting
in a model that is faster to evaluate for test data points. We have thereby accepted an
increased computational cost during the training phase in order to have a model that
is faster at making predictions.

6.4. Gaussian Processes

In Section 6.1, we introduced kernels by applying the concept of duality to a non-
probabilistic model for regression. Here we extend the role of kernels to probabilis-

304

6. KERNEL METHODS

tic discriminative models, leading to the framework of Gaussian processes. We shall
thereby see how kernels arise naturally in a Bayesian setting.

In Chapter 3, we considered linear regression models of the form y(x, w) =
wTφ(x) in which w is a vector of parameters and φ(x) is a vector of ﬁxed nonlinear
basis functions that depend on the input vector x. We showed that a prior distribution
over w induced a corresponding prior distribution over functions y(x, w). Given a
training data set, we then evaluated the posterior distribution over w and thereby
obtained the corresponding posterior distribution over regression functions, which
in turn (with the addition of noise) implies a predictive distribution p(t|x) for new
input vectors x.
In the Gaussian process viewpoint, we dispense with the parametric model and
instead deﬁne a prior probability distribution over functions directly. At ﬁrst sight, it
might seem difﬁcult to work with a distribution over the uncountably inﬁnite space of
functions. However, as we shall see, for a ﬁnite training set we only need to consider
the values of the function at the discrete set of input values xn corresponding to the
training set and test set data points, and so in practice we can work in a ﬁnite space.
Models equivalent to Gaussian processes have been widely studied in many dif-
ferent ﬁelds. For instance, in the geostatistics literature Gaussian process regression
is known as kriging (Cressie, 1993). Similarly, ARMA (autoregressive moving aver-
age) models, Kalman ﬁlters, and radial basis function networks can all be viewed as
forms of Gaussian process models. Reviews of Gaussian processes from a machine
learning perspective can be found in MacKay (1998), Williams (1999), and MacKay
(2003), and a comparison of Gaussian process models with alternative approaches is
given in Rasmussen (1996). See also Rasmussen and Williams (2006) for a recent
textbook on Gaussian processes.

6.4.1 Linear regression revisited
In order to motivate the Gaussian process viewpoint, let us return to the linear
regression example and re-derive the predictive distribution by working in terms
of distributions over functions y(x, w). This will provide a speciﬁc example of a
Gaussian process.

Consider a model deﬁned in terms of a linear combination of M ﬁxed basis

functions given by the elements of the vector φ(x) so that

y(x) = wTφ(x)

(6.49)

where x is the input vector and w is the M-dimensional weight vector. Now consider
a prior distribution over w given by an isotropic Gaussian of the form

p(w) = N (w|0, α−1I)

(6.50)

governed by the hyperparameter α, which represents the precision (inverse variance)
of the distribution. For any given value of w, the deﬁnition (6.49) deﬁnes a partic-
ular function of x. The probability distribution over w deﬁned by (6.50) therefore
induces a probability distribution over functions y(x). In practice, we wish to eval-
uate this function at speciﬁc values of x, for example at the training data points



	



	

(6.52)

(6.53)

(6.54)

6.4. Gaussian Processes

305

x1, . . . , xN . We are therefore interested in the joint distribution of the function val-
ues y(x1), . . . , y(xN ), which we denote by the vector y with elements yn = y(xn)
for n = 1, . . . , N. From (6.49), this vector is given by

y = Φw

(6.51)
where Φ is the design matrix with elements Φnk = φk(xn). We can ﬁnd the proba-
bility distribution of y as follows. First of all we note that y is a linear combination of
Gaussian distributed variables given by the elements of w and hence is itself Gaus-
sian. We therefore need only to ﬁnd its mean and covariance, which are given from
(6.50) by

E[y] = ΦE[w] = 0

cov[y] = E

= ΦE
where K is the Gram matrix with elements

yyT

wwT

ΦT =

1
α

ΦΦT = K

Knm = k(xn, xm) =

1
α

φ(xn)Tφ(xm)

and k(x, x) is the kernel function.

This model provides us with a particular example of a Gaussian process. In gen-
eral, a Gaussian process is deﬁned as a probability distribution over functions y(x)
such that the set of values of y(x) evaluated at an arbitrary set of points x1, . . . , xN
jointly have a Gaussian distribution. In cases where the input vector x is two di-
mensional, this may also be known as a Gaussian random ﬁeld. More generally, a
stochastic process y(x) is speciﬁed by giving the joint probability distribution for
any ﬁnite set of values y(x1), . . . , y(xN ) in a consistent manner.

A key point about Gaussian stochastic processes is that the joint distribution
over N variables y1, . . . , yN is speciﬁed completely by the second-order statistics,
namely the mean and the covariance. In most applications, we will not have any
prior knowledge about the mean of y(x) and so by symmetry we take it to be zero.
This is equivalent to choosing the mean of the prior over weight values p(w|α) to
be zero in the basis function viewpoint. The speciﬁcation of the Gaussian process is
then completed by giving the covariance of y(x) evaluated at any two values of x,
which is given by the kernel function

E [y(xn)y(xm)] = k(xn, xm).

(6.55)

For the speciﬁc case of a Gaussian process deﬁned by the linear regression model
(6.49) with a weight prior (6.50), the kernel function is given by (6.54).

We can also deﬁne the kernel function directly, rather than indirectly through a
choice of basis function. Figure 6.4 shows samples of functions drawn from Gaus-
sian processes for two different choices of kernel function. The ﬁrst of these is a
‘Gaussian’ kernel of the form (6.23), and the second is the exponential kernel given
by

k(x, x) = exp (−θ |x − x|)
(6.56)
which corresponds to the Ornstein-Uhlenbeck process originally introduced by Uh-
lenbeck and Ornstein (1930) to describe Brownian motion.

Exercise 2.31

306

6. KERNEL METHODS

Figure 6.4 Samples from Gaus-
sian processes for a ‘Gaussian’ ker-
nel (left) and an exponential kernel
(right).

3

1.5

0

−1.5

−3

−1

−0.5

3

1.5

0

−1.5

−3

−1

0

0.5

1

−0.5

0

0.5

1



6.4.2 Gaussian processes for regression
In order to apply Gaussian process models to the problem of regression, we need

to take account of the noise on the observed target values, which are given by

tn = yn + 	n

(6.57)

where yn = y(xn), and 	n is a random noise variable whose value is chosen inde-
pendently for each observation n. Here we shall consider noise processes that have
a Gaussian distribution, so that

p(tn|yn) = N (tn|yn, β−1)

(6.58)

where β is a hyperparameter representing the precision of the noise. Because the
noise is independent for each data point, the joint distribution of the target values
t = (t1, . . . , tN )T conditioned on the values of y = (y1, . . . , yN )T is given by an
isotropic Gaussian of the form

(6.59)
where IN denotes the N × N unit matrix. From the deﬁnition of a Gaussian process,
the marginal distribution p(y) is given by a Gaussian whose mean is zero and whose
covariance is deﬁned by a Gram matrix K so that

p(t|y) = N (t|y, β−1IN )

p(y) = N (y|0, K).

(6.60)

The kernel function that determines K is typically chosen to express the property
that, for points xn and xm that are similar, the corresponding values y(xn) and
y(xm) will be more strongly correlated than for dissimilar points. Here the notion
of similarity will depend on the application.

In order to ﬁnd the marginal distribution p(t), conditioned on the input values
x1, . . . , xN , we need to integrate over y. This can be done by making use of the
results from Section 2.3.3 for the linear-Gaussian model. Using (2.115), we see that
the marginal distribution of t is given by

p(t) =

p(t|y)p(y) dy = N (t|0, C)

(6.61)









6.4. Gaussian Processes

307

(6.62)

where the covariance matrix C has elements

C(xn, xm) = k(xn, xm) + β−1δnm.

This result reﬂects the fact that the two Gaussian sources of randomness, namely
that associated with y(x) and that associated with 	, are independent and so their
covariances simply add.

One widely used kernel function for Gaussian process regression is given by the
exponential of a quadratic form, with the addition of constant and linear terms to
give

k(xn, xm) = θ0 exp

−

θ1
2 xn − xm2

+ θ2 + θ3xT

nxm.

(6.63)

Note that the term involving θ3 corresponds to a parametric model that is a linear
function of the input variables. Samples from this prior are plotted for various values
of the parameters θ0, . . . , θ3 in Figure 6.5, and Figure 6.6 shows a set of points sam-
pled from the joint distribution (6.60) along with the corresponding values deﬁned
by (6.61).

So far, we have used the Gaussian process viewpoint to build a model of the
joint distribution over sets of data points. Our goal in regression, however, is to
make predictions of the target variables for new inputs, given a set of training data.
Let us suppose that tN = (t1, . . . , tN )T, corresponding to input values x1, . . . , xN ,
comprise the observed training set, and our goal is to predict the target variable tN +1
for a new input vector xN +1. This requires that we evaluate the predictive distri-
bution p(tN +1|tN ). Note that this distribution is conditioned also on the variables
x1, . . . , xN and xN +1. However, to keep the notation simple we will not show these
conditioning variables explicitly.

To ﬁnd the conditional distribution p(tN +1|t), we begin by writing down the
joint distribution p(tN +1), where tN +1 denotes the vector (t1, . . . , tN , tN +1)T. We
then apply the results from Section 2.3.1 to obtain the required conditional distribu-
tion, as illustrated in Figure 6.7.

From (6.61), the joint distribution over t1, . . . , tN +1 will be given by

p(tN +1) = N (tN +1|0, CN +1)

(6.64)
where CN +1 is an (N + 1) × (N + 1) covariance matrix with elements given by
(6.62). Because this joint distribution is Gaussian, we can apply the results from
Section 2.3.1 to ﬁnd the conditional Gaussian distribution. To do this, we partition
the covariance matrix as follows

CN +1 =

CN k
kT
c

(6.65)

where CN is the N × N covariance matrix with elements given by (6.62) for n, m =
1, . . . , N, the vector k has elements k(xn, xN +1) for n = 1, . . . , N, and the scalar

308

6. KERNEL METHODS

(1.00, 4.00, 0.00, 0.00)

0

−0.5
0.5
(1.00, 0.25, 0.00, 0.00)

3

1.5

0

−1.5

−3

−1

3

1.5

0

−1.5

−3

−1

9

4.5

0

−4.5

1

−9

−1

9

4.5

0

−4.5

−9

−1

(9.00, 4.00, 0.00, 0.00)

−0.5

0

0.5

1

(1.00, 4.00, 10.00, 0.00)

−0.5

0

0.5

1

3

1.5

0

−1.5

−3

−1

4

2

0

−2

−4

−1

(1.00, 64.00, 0.00, 0.00)

0

−0.5
0.5
(1.00, 4.00, 0.00, 5.00)

1

−0.5

0

0.5

1

−0.5

0

0.5

1

Figure 6.5 Samples from a Gaussian process prior deﬁned by the covariance function (6.63). The title above
each plot denotes (θ0, θ1, θ2, θ3).

c = k(xN +1, xN +1) + β−1. Using the results (2.81) and (2.82), we see that the con-
ditional distribution p(tN +1|t) is a Gaussian distribution with mean and covariance
given by

m(xN +1) = kTC−1
N t
σ2(xN +1) = c − kTC−1
N k.

(6.66)
(6.67)

These are the key results that deﬁne Gaussian process regression. Because the vector
k is a function of the test point input value xN +1, we see that the predictive distribu-
tion is a Gaussian whose mean and variance both depend on xN +1. An example of
Gaussian process regression is shown in Figure 6.8.

The only restriction on the kernel function is that the covariance matrix given by
(6.62) must be positive deﬁnite. If λi is an eigenvalue of K, then the corresponding
eigenvalue of C will be λi + β−1. It is therefore sufﬁcient that the kernel matrix
k(xn, xm) be positive semideﬁnite for any pair of points xn and xm, so that λi � 0,
because any eigenvalue λi that is zero will still give rise to a positive eigenvalue
for C because β > 0. This is the same restriction on the kernel function discussed
earlier, and so we can again exploit all of the techniques in Section 6.2 to construct



−1

N

−3

Figure 6.6 Illustration of

the sampling of data
points {tn} from a Gaussian process.
The blue curve shows a sample func-
tion from the Gaussian process prior
over functions, and the red points
show the values of yn obtained by
evaluating the function at a set of in-
put values {xn}. The correspond-
ing values of {tn}, shown in green,
are obtained by adding independent
Gaussian noise to each of the {yn}.

t

3

0

6.4. Gaussian Processes

309

0

x

1

suitable kernels.

tion of xN +1, in the form

Note that the mean (6.66) of the predictive distribution can be written, as a func-

Exercise 6.21

m(xN +1) =

ank(xn, xN +1)

(6.68)

n=1

where an is the nth component of C−1
N t. Thus, if the kernel function k(xn, xm)
depends only on the distance xn − xm, then we obtain an expansion in radial
basis functions.
The results (6.66) and (6.67) deﬁne the predictive distribution for Gaussian pro-
cess regression with an arbitrary kernel function k(xn, xm). In the particular case in
which the kernel function k(x, x) is deﬁned in terms of a ﬁnite set of basis functions,
we can derive the results obtained previously in Section 3.3.2 for linear regression
starting from the Gaussian process viewpoint.

For such models, we can therefore obtain the predictive distribution either by
taking a parameter space viewpoint and using the linear regression result or by taking
a function space viewpoint and using the Gaussian process result.

The central computational operation in using Gaussian processes will involve
the inversion of a matrix of size N × N, for which standard methods require O(N 3)
computations. By contrast, in the basis function model we have to invert a matrix
SN of size M × M, which has O(M 3) computational complexity. Note that for
both viewpoints, the matrix inversion must be performed once for the given training
set. For each new test point, both methods require a vector-matrix multiply, which
has cost O(N 2) in the Gaussian process case and O(M 2) for the linear basis func-
tion model. If the number M of basis functions is smaller than the number N of
data points, it will be computationally more efﬁcient to work in the basis function

310

6. KERNEL METHODS

Figure 6.7 Illustration of

the mechanism of
Gaussian process regression for
the case of one training point and
one test point, in which the red el-
lipses show contours of the joint dis-
tribution p(t1, t2). Here t1 is the
training data point, and condition-
ing on the value of t1, correspond-
ing to the vertical blue line, we ob-
tain p(t2|t1) shown as a function of
t2 by the green curve.

1

0

−1

t2

m(x2)

t1

−1

0

1

framework. However, an advantage of a Gaussian processes viewpoint is that we
can consider covariance functions that can only be expressed in terms of an inﬁnite
number of basis functions.

For large training data sets, however, the direct application of Gaussian process
methods can become infeasible, and so a range of approximation schemes have been
developed that have better scaling with training set size than the exact approach
(Gibbs, 1997; Tresp, 2001; Smola and Bartlett, 2001; Williams and Seeger, 2001;
Csat´o and Opper, 2002; Seeger et al., 2003). Practical issues in the application of
Gaussian processes are discussed in Bishop and Nabney (2008).

We have introduced Gaussian process regression for the case of a single tar-
get variable. The extension of this formalism to multiple target variables, known
as co-kriging (Cressie, 1993), is straightforward. Various other extensions of Gaus-

Exercise 6.23

Figure 6.8 Illustration of Gaussian process re-
gression applied to the sinusoidal
data set in Figure A.6 in which the
three right-most data points have
been omitted. The green curve
shows the sinusoidal function from
which the data points, shown in
blue, are obtained by sampling and
addition of Gaussian noise. The
red line shows the mean of
the
Gaussian process predictive distri-
bution, and the shaded region cor-
responds to plus and minus two
standard deviations. Notice how
the uncertainty increases in the re-
gion to the right of the data points.

1

0.5

0

−0.5

−1

0

0.2

0.4

0.6

0.8

1



1
2



1
2

6.4. Gaussian Processes

311

sian process regression have also been considered, for purposes such as modelling
the distribution over low-dimensional manifolds for unsupervised learning (Bishop
et al., 1998a) and the solution of stochastic differential equations (Graepel, 2003).

6.4.3 Learning the hyperparameters
The predictions of a Gaussian process model will depend, in part, on the choice
of covariance function. In practice, rather than ﬁxing the covariance function, we
may prefer to use a parametric family of functions and then infer the parameter
values from the data. These parameters govern such things as the length scale of the
correlations and the precision of the noise and correspond to the hyperparameters in
a standard parametric model.

Techniques for learning the hyperparameters are based on the evaluation of the
likelihood function p(t|θ) where θ denotes the hyperparameters of the Gaussian pro-
cess model. The simplest approach is to make a point estimate of θ by maximizing
the log likelihood function. Because θ represents a set of hyperparameters for the
regression problem, this can be viewed as analogous to the type 2 maximum like-
lihood procedure for linear regression models. Maximization of the log likelihood
can be done using efﬁcient gradient-based optimization algorithms such as conjugate
gradients (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and Nabney, 2008).

The log likelihood function for a Gaussian process regression model is easily

evaluated using the standard form for a multivariate Gaussian distribution, giving

ln p(t|θ) = −

ln|CN| −

tTC−1

N t −

N
2

ln(2π).

(6.69)

For nonlinear optimization, we also need the gradient of the log likelihood func-
tion with respect to the parameter vector θ. We shall assume that evaluation of the
derivatives of CN is straightforward, as would be the case for the covariance func-
tions considered in this chapter. Making use of the result (C.21) for the derivative of
C−1
N , together with the result (C.22) for the derivative of ln|CN|, we obtain

∂
∂θi

ln p(t|θ) = −

1
2

Tr

C−1
N

∂CN
∂θi

+

1
2

tTC−1
N

∂CN
∂θi

C−1
N t.

(6.70)

Because ln p(t|θ) will in general be a nonconvex function, it can have multiple max-
ima.
It is straightforward to introduce a prior over θ and to maximize the log poste-
rior using gradient-based methods. In a fully Bayesian treatment, we need to evaluate
marginals over θ weighted by the product of the prior p(θ) and the likelihood func-
tion p(t|θ). In general, however, exact marginalization will be intractable, and we
must resort to approximations.
The Gaussian process regression model gives a predictive distribution whose
mean and variance are functions of the input vector x. However, we have assumed
that the contribution to the predictive variance arising from the additive noise, gov-
erned by the parameter β, is a constant. For some problems, known as heteroscedas-
tic, the noise variance itself will also depend on x. To model this, we can extend the

Section 3.5





2



312

6. KERNEL METHODS

for Gaussian processes,

Figure 6.9 Samples from the ARD
prior
in
which the kernel function is given by
(6.71). The left plot corresponds to
η1 = η2 = 1, and the right plot cor-
responds to η1 = 1, η2 = 0.01.

Gaussian process framework by introducing a second Gaussian process to represent
the dependence of β on the input x (Goldberg et al., 1998). Because β is a variance,
and hence nonnegative, we use the Gaussian process to model ln β(x).

6.4.4 Automatic relevance determination
In the previous section, we saw how maximum likelihood could be used to de-
termine a value for the correlation length-scale parameter in a Gaussian process.
This technique can usefully be extended by incorporating a separate parameter for
each input variable (Rasmussen and Williams, 2006). The result, as we shall see, is
that the optimization of these parameters by maximum likelihood allows the relative
importance of different inputs to be inferred from the data. This represents an exam-
ple in the Gaussian process context of automatic relevance determination, or ARD,
which was originally formulated in the framework of neural networks (MacKay,
1994; Neal, 1996). The mechanism by which appropriate inputs are preferred is
discussed in Section 7.2.2.

Consider a Gaussian process with a two-dimensional input space x = (x1, x2),

having a kernel function of the form

k(x, x) = θ0 exp

1
2

−

ηi(xi − xi)2

.

i=1

(6.71)

Samples from the resulting prior over functions y(x) are shown for two different
settings of the precision parameters ηi in Figure 6.9. We see that, as a particu-
lar parameter ηi becomes small, the function becomes relatively insensitive to the
corresponding input variable xi. By adapting these parameters to a data set using
maximum likelihood, it becomes possible to detect input variables that have little
effect on the predictive distribution, because the corresponding values of ηi will be
small. This can be useful in practice because it allows such inputs to be discarded.
ARD is illustrated using a simple synthetic data set having three inputs x1, x2 and x3
(Nabney, 2002) in Figure 6.10. The target variable t, is generated by sampling 100
values of x1 from a Gaussian, evaluating the function sin(2πx1), and then adding





D





D

6.4. Gaussian Processes

313

Figure 6.10 Illustration of automatic rele-
vance determination in a Gaus-
sian process for a synthetic prob-
lem having three inputs x1, x2,
and x3,
for which the curves
show the corresponding values of
the hyperparameters η1 (red), η2
(green), and η3 (blue) as a func-
tion of the number of iterations
when optimizing the marginal
likelihood. Details are given in
the text. Note the logarithmic
scale on the vertical axis.

102

100

10−2

10−4

0

20

40

60

80

100

Gaussian noise. Values of x2 are given by copying the corresponding values of x1
and adding noise, and values of x3 are sampled from an independent Gaussian dis-
tribution. Thus x1 is a good predictor of t, x2 is a more noisy predictor of t, and x3
has only chance correlations with t. The marginal likelihood for a Gaussian process
with ARD parameters η1, η2, η3 is optimized using the scaled conjugate gradients
algorithm. We see from Figure 6.10 that η1 converges to a relatively large value, η2
converges to a much smaller value, and η3 becomes very small indicating that x3 is
irrelevant for predicting t.

The ARD framework is easily incorporated into the exponential-quadratic kernel
(6.63) to give the following form of kernel function, which has been found useful for
applications of Gaussian processes to a range of regression problems

k(xn, xm) = θ0 exp

1
2

−

ηi(xni − xmi)2

i=1

+ θ2 + θ3

xnixmi

(6.72)

i=1

where D is the dimensionality of the input space.

6.4.5 Gaussian processes for classiﬁcation
In a probabilistic approach to classiﬁcation, our goal is to model the posterior
probabilities of the target variable for a new input vector, given a set of training
data. These probabilities must lie in the interval (0, 1), whereas a Gaussian process
model makes predictions that lie on the entire real axis. However, we can easily
adapt Gaussian processes to classiﬁcation problems by transforming the output of
the Gaussian process using an appropriate nonlinear activation function.

Consider ﬁrst the two-class problem with a target variable t ∈ {0, 1}. If we de-
ﬁne a Gaussian process over a function a(x) and then transform the function using
a logistic sigmoid y = σ(a), given by (4.59), then we will obtain a non-Gaussian
stochastic process over functions y(x) where y ∈ (0, 1). This is illustrated for the
case of a one-dimensional input space in Figure 6.11 in which the probability distri-

314

6. KERNEL METHODS

10

5

0

−5

−10

−1

−0.5

0

0.5

1

1

0.75

0.5

0.25

0
−1

−0.5

0

0.5

1

Figure 6.11 The left plot shows a sample from a Gaussian process prior over functions a(x), and the right plot
shows the result of transforming this sample using a logistic sigmoid function.

bution over the target variable t is then given by the Bernoulli distribution

p(t|a) = σ(a)t(1 − σ(a))1−t.

(6.73)

As usual, we denote the training set inputs by x1, . . . , xN with corresponding
observed target variables t = (t1, . . . , tN )T. We also consider a single test point
xN +1 with target value tN +1. Our goal is to determine the predictive distribution
p(tN +1|t), where we have left the conditioning on the input variables implicit. To do
this we introduce a Gaussian process prior over the vector aN +1, which has compo-
nents a(x1), . . . , a(xN +1). This in turn deﬁnes a non-Gaussian process over tN +1,
and by conditioning on the training data tN we obtain the required predictive distri-
bution. The Gaussian process prior for aN +1 takes the form
p(aN +1) = N (aN +1|0, CN +1).

(6.74)

Unlike the regression case, the covariance matrix no longer includes a noise term
because we assume that all of the training data points are correctly labelled. How-
ever, for numerical reasons it is convenient to introduce a noise-like term governed
by a parameter ν that ensures that the covariance matrix is positive deﬁnite. Thus
the covariance matrix CN +1 has elements given by

C(xn, xm) = k(xn, xm) + νδnm

(6.75)

where k(xn, xm) is any positive semideﬁnite kernel function of the kind considered
in Section 6.2, and the value of ν is typically ﬁxed in advance. We shall assume that
the kernel function k(x, x) is governed by a vector θ of parameters, and we shall
later discuss how θ may be learned from the training data.

For two-class problems, it is sufﬁcient to predict p(tN +1 = 1|tN ) because the
value of p(tN +1 = 0|tN ) is then given by 1 − p(tN +1 = 1|tN ). The required








predictive distribution is given by

p(tN +1 = 1|tN ) =

p(tN +1 = 1|aN +1)p(aN +1|tN ) daN +1

(6.76)

where p(tN +1 = 1|aN +1) = σ(aN +1).
This integral is analytically intractable, and so may be approximated using sam-
pling methods (Neal, 1997). Alternatively, we can consider techniques based on
an analytical approximation. In Section 4.5.2, we derived the approximate formula
(4.153) for the convolution of a logistic sigmoid with a Gaussian distribution. We
can use this result to evaluate the integral in (6.76) provided we have a Gaussian
approximation to the posterior distribution p(aN +1|tN ). The usual justiﬁcation for a
Gaussian approximation to a posterior distribution is that the true posterior will tend
to a Gaussian as the number of data points increases as a consequence of the central
limit theorem. In the case of Gaussian processes, the number of variables grows with
the number of data points, and so this argument does not apply directly. However, if
we consider increasing the number of data points falling in a ﬁxed region of x space,
then the corresponding uncertainty in the function a(x) will decrease, again leading
asymptotically to a Gaussian (Williams and Barber, 1998).

Three different approaches to obtaining a Gaussian approximation have been
considered. One technique is based on variational inference (Gibbs and MacKay,
2000) and makes use of the local variational bound (10.144) on the logistic sigmoid.
This allows the product of sigmoid functions to be approximated by a product of
Gaussians thereby allowing the marginalization over aN to be performed analyti-
cally. The approach also yields a lower bound on the likelihood function p(tN|θ).
The variational framework for Gaussian process classiﬁcation can also be extended
to multiclass (K > 2) problems by using a Gaussian approximation to the softmax
function (Gibbs, 1997).

A second approach uses expectation propagation (Opper and Winther, 2000b;
Minka, 2001b; Seeger, 2003). Because the true posterior distribution is unimodal, as
we shall see shortly, the expectation propagation approach can give good results.

6.4.6 Laplace approximation
The third approach to Gaussian process classiﬁcation is based on the Laplace
approximation, which we now consider in detail. In order to evaluate the predictive
distribution (6.76), we seek a Gaussian approximation to the posterior distribution
over aN +1, which, using Bayes’ theorem, is given by

p(aN +1|tN ) =
=

=

=

p(aN +1, aN|tN ) daN
1

p(tN )

1

p(tN )

p(aN +1, aN )p(tN|aN +1, aN ) daN
p(aN +1|aN )p(aN )p(tN|aN ) daN

p(aN +1|aN )p(aN|tN ) daN

(6.77)

6.4. Gaussian Processes

315

Section 2.3

Section 10.1

Section 10.7

Section 4.4



N



N



1
2
N

316

6. KERNEL METHODS

where we have used p(tN|aN +1, aN ) = p(tN|aN ). The conditional distribution
p(aN +1|aN ) is obtained by invoking the results (6.66) and (6.67) for Gaussian pro-
cess regression, to give

p(aN +1|aN ) = N (aN +1|kTC−1

N aN , c − kTC−1

N k).

(6.78)

We can therefore evaluate the integral in (6.77) by ﬁnding a Laplace approximation
for the posterior distribution p(aN|tN ), and then using the standard result for the
convolution of two Gaussian distributions.
The prior p(aN ) is given by a zero-mean Gaussian process with covariance ma-

trix CN , and the data term (assuming independence of the data points) is given by

p(tN|aN ) =

n=1

σ(an)tn(1 − σ(an))1−tn =

eantnσ(−an).

n=1

(6.79)

We then obtain the Laplace approximation by Taylor expanding the logarithm of
p(aN|tN ), which up to an additive normalization constant is given by the quantity

Ψ(aN ) = ln p(aN ) + ln p(tN|aN )

= −

N C−1
aT

N aN −

N
2

ln(2π) −

1
2

ln|CN| + tT

N aN

ln(1 + ean) + const.

−

n=1

(6.80)

First we need to ﬁnd the mode of the posterior distribution, and this requires that we
evaluate the gradient of Ψ(aN ), which is given by

∇Ψ(aN ) = tN − σN − C−1

N aN

(6.81)

where σN is a vector with elements σ(an). We cannot simply ﬁnd the mode by
setting this gradient to zero, because σN depends nonlinearly on aN , and so we
resort to an iterative scheme based on the Newton-Raphson method, which gives rise
to an iterative reweighted least squares (IRLS) algorithm. This requires the second
derivatives of Ψ(aN ), which we also require for the Laplace approximation anyway,
and which are given by

N

∇∇Ψ(aN ) = −WN − C−1

(6.82)
where WN is a diagonal matrix with elements σ(an)(1− σ(an)), and we have used
the result (4.88) for the derivative of the logistic sigmoid function. Note that these
diagonal elements lie in the range (0, 1/4), and hence WN is a positive deﬁnite
matrix. Because CN (and hence its inverse) is positive deﬁnite by construction, and
because the sum of two positive deﬁnite matrices is also positive deﬁnite, we see
that the Hessian matrix A = −∇∇Ψ(aN ) is positive deﬁnite and so the posterior
distribution p(aN|tN ) is log convex and therefore has a single mode that is the global

Section 4.3.3

Exercise 6.24

6.4. Gaussian Processes

317

maximum. The posterior distribution is not Gaussian, however, because the Hessian
is a function of aN .

Using the Newton-Raphson formula (4.92), the iterative update equation for aN

is given by

N = CN (I + WN CN )−1 {tN − σN + WN aN} .
anew

(6.83)

These equations are iterated until they converge to the mode which we denote by
N . At the mode, the gradient ∇Ψ(aN ) will vanish, and hence a
a

N will satisfy

N = CN (tN − σN ).
a

(6.84)

Once we have found the mode a

N of the posterior, we can evaluate the Hessian

matrix given by

H = −∇∇Ψ(aN ) = WN + C−1
where the elements of WN are evaluated using a
proximation to the posterior distribution p(aN|tN ) given by

N

(6.85)
N . This deﬁnes our Gaussian ap-

q(aN ) = N (aN|a

N , H−1).

(6.86)



Exercise 6.25

Exercise 6.26

We can now combine this with (6.78) and hence evaluate the integral (6.77). Because
this corresponds to a linear-Gaussian model, we can use the general result (2.115) to
give

N + CN )−1k.

E[aN +1|tN ] = kT(tN − σN )
var[aN +1|tN ] = c − kT(W−1

(6.87)
(6.88)
Now that we have a Gaussian distribution for p(aN +1|tN ), we can approximate
the integral (6.76) using the result (4.153). As with the Bayesian logistic regression
model of Section 4.5, if we are only interested in the decision boundary correspond-
ing to p(tN +1|tN ) = 0.5, then we need only consider the mean and we can ignore
the effect of the variance.
We also need to determine the parameters θ of the covariance function. One
approach is to maximize the likelihood function given by p(tN|θ) for which we need
expressions for the log likelihood and its gradient. If desired, suitable regularization
terms can also be added, leading to a penalized maximum likelihood solution. The
likelihood function is deﬁned by

p(tN|θ) =

p(tN|aN )p(aN|θ) daN .

(6.89)

This integral is analytically intractable, so again we make use of the Laplace approx-
imation. Using the result (4.135), we obtain the following approximation for the log
of the likelihood function

ln p(tN|θ) = Ψ(a

N ) −

1
2

ln|WN + C−1

N | + N
2

ln(2π)

(6.90)





	



N



∂θj

N



318

6. KERNEL METHODS

N ) = ln p(a

N|θ) + ln p(tN|a

N ). We also need to evaluate the gradient
where Ψ(a
of ln p(tN|θ) with respect to the parameter vector θ. Note that changes in θ will
cause changes in a
N , leading to additional terms in the gradient. Thus, when we
differentiate (6.90) with respect to θ, we obtain two sets of terms, the ﬁrst arising
from the dependence of the covariance matrix CN on θ, and the rest arising from
dependence of a

The terms arising from the explicit dependence on θ can be found by using

N on θ.

(6.80) together with the results (C.21) and (C.22), and are given by

∂ ln p(tN|θ)

=

1
2

N C−1
aT
N

C−1

N a

N

∂CN
∂θj

1
2

−

Tr

(I + CN WN )−1WN

∂CN
∂θj

.

(6.91)

To compute the terms arising from the dependence of a

N on θ, we note that
the Laplace approximation has been constructed such that Ψ(aN ) has zero gradient
at aN = a
N ) gives no contribution to the gradient as a result of its
dependence on a
N . This leaves the following contribution to the derivative with
respect to a component θj of θ

N , and so Ψ(a

1
2

−

∂ ln|WN + C−1
N |

∂a
n

∂a
n
∂θj

n=1

1
2

= −

(I + CN WN )−1CN

nn

n=1

n(1 − σ
σ

n)(1 − 2σ

n) ∂a
n
∂θj

(6.92)

n = σ(a

where σ
deﬁnition of WN . We can evaluate the derivative of a
entiating the relation (6.84) with respect to θj to give

n), and again we have used the result (C.22) together with the
N with respect to θj by differ-

∂a
n
∂θj

= ∂CN
∂θj

(tN − σN ) − CN WN

∂a
n
∂θj

.

Rearranging then gives

∂a
n
∂θj

= (I + WN CN )−1 ∂CN
∂θj

(tN − σN ).

(6.93)

(6.94)

Combining (6.91), (6.92), and (6.94), we can evaluate the gradient of the log
likelihood function, which can be used with standard nonlinear optimization algo-
rithms in order to determine a value for θ.

We can illustrate the application of the Laplace approximation for Gaussian pro-
cesses using the synthetic two-class data set shown in Figure 6.12. Extension of the
Laplace approximation to Gaussian processes involving K > 2 classes, using the
softmax activation function, is straightforward (Williams and Barber, 1998).

Appendix A

6.4. Gaussian Processes

319

2

0

−2

−2

0

2

Figure 6.12 Illustration of the use of a Gaussian process for classiﬁcation, showing the data on the left together
with the optimal decision boundary from the true distribution in green, and the decision boundary from the
Gaussian process classiﬁer in black. On the right is the predicted posterior probability for the blue and red
classes together with the Gaussian process decision boundary.

6.4.7 Connection to neural networks
We have seen that the range of functions which can be represented by a neural
network is governed by the number M of hidden units, and that, for sufﬁciently
large M, a two-layer network can approximate any given function with arbitrary
accuracy.
In the framework of maximum likelihood, the number of hidden units
needs to be limited (to a level dependent on the size of the training set) in order
to avoid over-ﬁtting. However, from a Bayesian perspective it makes little sense to
limit the number of parameters in the network according to the size of the training
set.

In a Bayesian neural network, the prior distribution over the parameter vector
w, in conjunction with the network function f(x, w), produces a prior distribution
over functions from y(x) where y is the vector of network outputs. Neal (1996)
has shown that, for a broad class of prior distributions over w, the distribution of
functions generated by a neural network will tend to a Gaussian process in the limit
M → ∞. It should be noted, however, that in this limit the output variables of the
neural network become independent. One of the great merits of neural networks is
that the outputs share the hidden units and so they can ‘borrow statistical strength’
from each other, that is, the weights associated with each hidden unit are inﬂuenced
by all of the output variables not just by one of them. This property is therefore lost
in the Gaussian process limit.

We have seen that a Gaussian process is determined by its covariance (kernel)
function. Williams (1998) has given explicit forms for the covariance in the case of
two speciﬁc choices for the hidden unit activation function (probit and Gaussian).
These kernel functions k(x, x) are nonstationary, i.e. they cannot be expressed as
a function of the difference x − x, as a consequence of the Gaussian weight prior
being centred on zero which breaks translation invariance in weight space.

320

6. KERNEL METHODS

Exercises

By working directly with the covariance function we have implicitly marginal-
ized over the distribution of weights. If the weight prior is governed by hyperpa-
rameters, then their values will determine the length scales of the distribution over
functions, as can be understood by studying the examples in Figure 5.11 for the case
of a ﬁnite number of hidden units. Note that we cannot marginalize out the hyperpa-
rameters analytically, and must instead resort to techniques of the kind discussed in
Section 6.4.

6.1 ( ) www Consider the dual formulation of the least squares linear regression
problem given in Section 6.1. Show that the solution for the components an of
the vector a can be expressed as a linear combination of the elements of the vector
φ(xn). Denoting these coefﬁcients by the vector w, show that the dual of the dual
formulation is given by the original representation in terms of the parameter vector
w.

6.2 ( )

In this exercise, we develop a dual formulation of the perceptron learning
algorithm. Using the perceptron learning rule (4.55), show that the learned weight
vector w can be written as a linear combination of the vectors tnφ(xn) where tn ∈
{−1, +1}. Denote the coefﬁcients of this linear combination by αn and derive a
formulation of the perceptron learning algorithm, and the predictive function for the
perceptron, in terms of the αn. Show that the feature vector φ(x) enters only in the
form of the kernel function k(x, x) = φ(x)Tφ(x).

6.3 () The nearest-neighbour classiﬁer (Section 2.5.2) assigns a new input vector x
to the same class as that of the nearest input vector xn from the training set, where
in the simplest case, the distance is deﬁned by the Euclidean metric x − xn2. By
expressing this rule in terms of scalar products and then making use of kernel sub-
stitution, formulate the nearest-neighbour classiﬁer for a general nonlinear kernel.

6.4 ()

In Appendix C, we give an example of a matrix that has positive elements but
that has a negative eigenvalue and hence that is not positive deﬁnite. Find an example
of the converse property, namely a 2 × 2 matrix with positive eigenvalues yet that
has at least one negative element.

6.5 () www Verify the results (6.13) and (6.14) for constructing valid kernels.

6.6 () Verify the results (6.15) and (6.16) for constructing valid kernels.

6.7 () www Verify the results (6.17) and (6.18) for constructing valid kernels.

6.8 () Verify the results (6.19) and (6.20) for constructing valid kernels.

6.9 () Verify the results (6.21) and (6.22) for constructing valid kernels.

6.10 () Show that an excellent choice of kernel for learning a function f(x) is given
by k(x, x) = f(x)f(x) by showing that a linear learning machine based on this
kernel will always ﬁnd a solution proportional to f(x).




N

(6.97)

(6.98)

w =

n=1

αnφ(xn) + w⊥

show that the value of w that minimizes J(w) takes the form of a linear combination
of the basis functions φ(xn) for n = 1, . . . , N.

6.17 ( ) www Consider the sum-of-squares error function (6.39) for data having
noisy inputs, where ν(ξ) is the distribution of the noise. Use the calculus of vari-
ations to minimize this error function with respect to the function y(x), and hence
show that the optimal solution is given by an expansion of the form (6.40) in which
the basis functions are given by (6.41).

Exercises

321

6.11 () By making use of the expansion (6.25), and then expanding the middle factor
as a power series, show that the Gaussian kernel (6.23) can be expressed as the inner
product of an inﬁnite-dimensional feature vector.

6.12 ( ) www Consider the space of all possible subsets A of a given ﬁxed set D.
Show that the kernel function (6.27) corresponds to an inner product in a feature
space of dimensionality 2|D| deﬁned by the mapping φ(A) where A is a subset of D
and the element φU (A), indexed by the subset U, is given by

φU (A) =

1,
if U ⊆ A;
0, otherwise.

(6.95)

Here U ⊆ A denotes that U is either a subset of A or is equal to A.

6.13 () Show that the Fisher kernel, deﬁned by (6.33), remains invariant if we make
a nonlinear transformation of the parameter vector θ → ψ(θ), where the function
ψ(·) is invertible and differentiable.

6.14 () www Write down the form of the Fisher kernel, deﬁned by (6.33), for the
case of a distribution p(x|µ) = N (x|µ, S) that is Gaussian with mean µ and ﬁxed
covariance S.

6.15 () By considering the determinant of a 2 × 2 Gram matrix, show that a positive-

deﬁnite kernel function k(x, x) satisﬁes the Cauchy-Schwartz inequality

k(x1, x2)2 � k(x1, x1)k(x2, x2).

(6.96)

6.16 ( ) Consider a parametric model governed by the parameter vector w together
with a data set of input values x1, . . . , xN and a nonlinear feature mapping φ(x).
Suppose that the dependence of the error function on w takes the form

J(w) = f(wTφ(x1), . . . , wTφ(xN )) + g(wTw)

where g(·) is a monotonically increasing function. By writing w in the form





N

E =

1
2

{y(xn − ξn) − tn}2 g(ξn) dξn.

n=1

(6.99)

By minimizing E with respect to the function y(z) using the calculus of variations
(Appendix D), show that optimal solution for y(x) is given by a Nadaraya-Watson
kernel regression solution of the form (6.45) with a kernel of the form (6.46).

6.20 ( ) www Verify the results (6.66) and (6.67).

6.21 ( ) www Consider a Gaussian process regression model in which the kernel
function is deﬁned in terms of a ﬁxed set of nonlinear basis functions. Show that the
predictive distribution is identical to the result (3.58) obtained in Section 3.3.2 for the
Bayesian linear regression model. To do this, note that both models have Gaussian
predictive distributions, and so it is only necessary to show that the conditional mean
and variance are the same. For the mean, make use of the matrix identity (C.6), and
for the variance, make use of the matrix identity (C.7).

6.22 ( ) Consider a regression problem with N training set input vectors x1, . . . , xN
and L test set input vectors xN +1, . . . , xN +L, and suppose we deﬁne a Gaussian
process prior over functions t(x). Derive an expression for the joint predictive dis-
tribution for t(xN +1), . . . , t(xN +L), given the values of t(x1), . . . , t(xN ). Show the
marginal of this distribution for one of the test observations tj where N + 1 � j �
N + L is given by the usual Gaussian process regression result (6.66) and (6.67).

6.23 ( ) www Consider a Gaussian process regression model in which the target
variable t has dimensionality D. Write down the conditional distribution of tN +1
for a test input vector xN +1, given a training set of input vectors x1, . . . , xN +1 and
corresponding target observations t1, . . . , tN .

6.24 () Show that a diagonal matrix W whose elements satisfy 0 < Wii < 1 is positive
deﬁnite. Show that the sum of two positive deﬁnite matrices is itself positive deﬁnite.

322

6. KERNEL METHODS

6.18 () Consider a Nadaraya-Watson model with one input variable x and one target
variable t having Gaussian components with isotropic covariances, so that the co-
variance matrix is given by σ2I where I is the unit matrix. Write down expressions
for the conditional density p(t|x) and for the conditional mean E[t|x] and variance
var[t|x], in terms of the kernel function k(x, xn).

6.19 ( ) Another viewpoint on kernel regression comes from a consideration of re-
gression problems in which the input variables as well as the target variables are
corrupted with additive noise. Suppose each target value tn is generated as usual
by taking a function y(zn) evaluated at a point zn, and adding Gaussian noise. The
value of zn is not directly observed, however, but only a noise corrupted version
xn = zn + ξn where the random variable ξ is governed by some distribution g(ξ).
Consider a set of observations {xn, tn}, where n = 1, . . . , N, together with a cor-
responding sum-of-squares error function deﬁned by averaging over the distribution
of input noise to give

Exercises

323

6.25 () www Using the Newton-Raphson formula (4.92), derive the iterative update
N of the posterior distribution in the Gaussian

formula (6.83) for ﬁnding the mode a
process classiﬁcation model.

6.26 () Using the result (2.115), derive the expressions (6.87) and (6.88) for the mean
and variance of the posterior distribution p(aN +1|tN ) in the Gaussian process clas-
siﬁcation model.

6.27 (  ) Derive the result (6.90) for the log likelihood function in the Laplace approx-
imation framework for Gaussian process classiﬁcation. Similarly, derive the results
(6.91), (6.92), and (6.94) for the terms in the gradient of the log likelihood.

7

Sparse Kernel

Machines

In the previous chapter, we explored a variety of learning algorithms based on non-
linear kernels. One of the signiﬁcant limitations of many such algorithms is that
the kernel function k(xn, xm) must be evaluated for all possible pairs xn and xm
of training points, which can be computationally infeasible during training and can
lead to excessive computation times when making predictions for new data points.
In this chapter we shall look at kernel-based algorithms that have sparse solutions,
so that predictions for new inputs depend only on the kernel function evaluated at a
subset of the training data points.

We begin by looking in some detail at the support vector machine (SVM), which
became popular in some years ago for solving problems in classiﬁcation, regression,
and novelty detection. An important property of support vector machines is that the
determination of the model parameters corresponds to a convex optimization prob-
lem, and so any local solution is also a global optimum. Because the discussion of
support vector machines makes extensive use of Lagrange multipliers, the reader is

325

326

7. SPARSE KERNEL MACHINES

encouraged to review the key concepts covered in Appendix E. Additional infor-
mation on support vector machines can be found in Vapnik (1995), Burges (1998),
Cristianini and Shawe-Taylor (2000), M¨uller et al. (2001), Sch¨olkopf and Smola
(2002), and Herbrich (2002).

The SVM is a decision machine and so does not provide posterior probabilities.
We have already discussed some of the beneﬁts of determining probabilities in Sec-
tion 1.5.4. An alternative sparse kernel technique, known as the relevance vector
machine (RVM), is based on a Bayesian formulation and provides posterior proba-
bilistic outputs, as well as having typically much sparser solutions than the SVM.

Section 7.2

7.1. Maximum Margin Classiﬁers

We begin our discussion of support vector machines by returning to the two-class
classiﬁcation problem using linear models of the form

y(x) = wTφ(x) + b

(7.1)

where φ(x) denotes a ﬁxed feature-space transformation, and we have made the
bias parameter b explicit. Note that we shall shortly introduce a dual representation
expressed in terms of kernel functions, which avoids having to work explicitly in
feature space. The training data set comprises N input vectors x1, . . . , xN , with
corresponding target values t1, . . . , tN where tn ∈ {−1, 1}, and new data points x
are classiﬁed according to the sign of y(x).
We shall assume for the moment that the training data set is linearly separable in
feature space, so that by deﬁnition there exists at least one choice of the parameters
w and b such that a function of the form (7.1) satisﬁes y(xn) > 0 for points having
tn = +1 and y(xn) < 0 for points having tn = −1, so that tny(xn) > 0 for all
training data points.
There may of course exist many such solutions that separate the classes exactly.
In Section 4.1.7, we described the perceptron algorithm that is guaranteed to ﬁnd
a solution in a ﬁnite number of steps. The solution that it ﬁnds, however, will be
dependent on the (arbitrary) initial values chosen for w and b as well as on the
order in which the data points are presented. If there are multiple solutions all of
which classify the training data set exactly, then we should try to ﬁnd the one that
will give the smallest generalization error. The support vector machine approaches
this problem through the concept of the margin, which is deﬁned to be the smallest
distance between the decision boundary and any of the samples, as illustrated in
Figure 7.1.

In support vector machines the decision boundary is chosen to be the one for
which the margin is maximized. The maximum margin solution can be motivated us-
ing computational learning theory, also known as statistical learning theory. How-
ever, a simple insight into the origins of maximum margin has been given by Tong
and Koller (2000) who consider a framework for classiﬁcation based on a hybrid of
generative and discriminative approaches. They ﬁrst model the distribution over in-
put vectors x for each class using a Parzen density estimator with Gaussian kernels

Section 7.1.5








	

7.1. Maximum Margin Classiﬁers

327

y = −1
y = 0

y = 1

y = 1

y = 0

y = −1

margin

Figure 7.1 The margin is deﬁned as the perpendicular distance between the decision boundary and the closest
of the data points, as shown on the left ﬁgure. Maximizing the margin leads to a particular choice of decision
boundary, as shown on the right. The location of this boundary is determined by a subset of the data points,
known as support vectors, which are indicated by the circles.

having a common parameter σ2. Together with the class priors, this deﬁnes an opti-
mal misclassiﬁcation-rate decision boundary. However, instead of using this optimal
boundary, they determine the best hyperplane by minimizing the probability of error
relative to the learned density model. In the limit σ2 → 0, the optimal hyperplane
is shown to be the one having maximum margin. The intuition behind this result is
that as σ2 is reduced, the hyperplane is increasingly dominated by nearby data points
relative to more distant ones. In the limit, the hyperplane becomes independent of
data points that are not support vectors.

We shall see in Figure 10.13 that marginalization with respect to the prior distri-
bution of the parameters in a Bayesian approach for a simple linearly separable data
set leads to a decision boundary that lies in the middle of the region separating the
data points. The large margin solution has similar behaviour.

Recall from Figure 4.1 that the perpendicular distance of a point x from a hyper-
plane deﬁned by y(x) = 0 where y(x) takes the form (7.1) is given by |y(x)|/w.
Furthermore, we are only interested in solutions for which all data points are cor-
rectly classiﬁed, so that tny(xn) > 0 for all n. Thus the distance of a point xn to the
decision surface is given by

tny(xn)
w

= tn(wTφ(xn) + b)

.

w

(7.2)

The margin is given by the perpendicular distance to the closest point xn from the
data set, and we wish to optimize the parameters w and b in order to maximize this
distance. Thus the maximum margin solution is found by solving

arg max

w,b

1
w

min
n

tn

wTφ(xn) + b

(7.3)

where we have taken the factor 1/w outside the optimization over n because w
















n=1

n=1

N

N

N



328

Appendix E

7. SPARSE KERNEL MACHINES

does not depend on n. Direct solution of this optimization problem would be very
complex, and so we shall convert it into an equivalent problem that is much easier
to solve. To do this we note that if we make the rescaling w → κw and b → κb,
then the distance from any point xn to the decision surface, given by tny(xn)/w,
is unchanged. We can use this freedom to set

tn

wTφ(xn) + b

= 1

(7.4)

tn

for the point that is closest to the surface. In this case, all data points will satisfy the
constraints

� 1,

n = 1, . . . , N.

wTφ(xn) + b

(7.5)
This is known as the canonical representation of the decision hyperplane.
In the
case of data points for which the equality holds, the constraints are said to be active,
whereas for the remainder they are said to be inactive. By deﬁnition, there will
always be at least one active constraint, because there will always be a closest point,
and once the margin has been maximized there will be at least two active constraints.
The optimization problem then simply requires that we maximize w−1, which is
equivalent to minimizing w2, and so we have to solve the optimization problem
(7.6)

arg min

1
2w2

w,b

subject to the constraints given by (7.5). The factor of 1/2 in (7.6) is included for
later convenience. This is an example of a quadratic programming problem in which
we are trying to minimize a quadratic function subject to a set of linear inequality
constraints. It appears that the bias parameter b has disappeared from the optimiza-
tion. However, it is determined implicitly via the constraints, because these require
that changes to w be compensated by changes to b. We shall see how this works
shortly.
In order to solve this constrained optimization problem, we introduce Lagrange
multipliers an � 0, with one multiplier an for each of the constraints in (7.5), giving
the Lagrangian function

L(w, b, a) =

1
2w2 −

an

tn(wTφ(xn) + b) − 1

(7.7)

where a = (a1, . . . , aN )T. Note the minus sign in front of the Lagrange multiplier
term, because we are minimizing with respect to w and b, and maximizing with
respect to a. Setting the derivatives of L(w, b, a) with respect to w and b equal to
zero, we obtain the following two conditions

w =

antnφ(xn)

0 =

antn.

n=1

(7.8)

(7.9)






n=1

N

N

n=1



N





1
2

N

N

7.1. Maximum Margin Classiﬁers

329

Eliminating w and b from L(w, b, a) using these conditions then gives the dual
representation of the maximum margin problem in which we maximize

L(a) =

an −

n=1
with respect to a subject to the constraints

m=1

anamtntmk(xn, xm)

(7.10)

n = 1, . . . , N,

(7.11)

(7.12)

an � 0,

antn = 0.

Here the kernel function is deﬁned by k(x, x) = φ(x)Tφ(x). Again, this takes the
form of a quadratic programming problem in which we optimize a quadratic function
of a subject to a set of inequality constraints. We shall discuss techniques for solving
such quadratic programming problems in Section 7.1.1.

The solution to a quadratic programming problem in M variables in general has
computational complexity that is O(M 3). In going to the dual formulation we have
turned the original optimization problem, which involved minimizing (7.6) over M
variables, into the dual problem (7.10), which has N variables. For a ﬁxed set of
basis functions whose number M is smaller than the number N of data points, the
move to the dual problem appears disadvantageous. However, it allows the model to
be reformulated using kernels, and so the maximum margin classiﬁer can be applied
efﬁciently to feature spaces whose dimensionality exceeds the number of data points,
including inﬁnite feature spaces. The kernel formulation also makes clear the role
of the constraint that the kernel function k(x, x) be positive deﬁnite, because this
L(a) is bounded below, giving rise to a well-
ensures that the Lagrangian function
deﬁned optimization problem.

In order to classify new data points using the trained model, we evaluate the sign
of y(x) deﬁned by (7.1). This can be expressed in terms of the parameters {an} and
the kernel function by substituting for w using (7.8) to give

y(x) =

antnk(x, xn) + b.

(7.13)

n=1

Joseph-Louis Lagrange
1736–1813

Although widely considered to be
a French mathematician, Lagrange
was born in Turin in Italy. By the age
of nineteen, he had already made
important contributions mathemat-
ics and had been appointed as Pro-
fessor at the Royal Artillery School in Turin. For many

years, Euler worked hard to persuade Lagrange to
move to Berlin, which he eventually did in 1766 where
he succeeded Euler as Director of Mathematics at
the Berlin Academy. Later he moved to Paris, nar-
rowly escaping with his life during the French revo-
lution thanks to the personal intervention of Lavoisier
(the French chemist who discovered oxygen) who him-
self was later executed at the guillotine. Lagrange
made key contributions to the calculus of variations
and the foundations of dynamics.











m∈S

n∈S

N



330

7. SPARSE KERNEL MACHINES

In Appendix E, we show that a constrained optimization of this form satisﬁes the
Karush-Kuhn-Tucker (KKT) conditions, which in this case require that the following
three properties hold

an � 0
tny(xn) − 1 � 0
an {tny(xn) − 1} = 0.

(7.14)
(7.15)
(7.16)

Thus for every data point, either an = 0 or tny(xn) = 1. Any data point for
which an = 0 will not appear in the sum in (7.13) and hence plays no role in making
predictions for new data points. The remaining data points are called support vectors,
and because they satisfy tny(xn) = 1, they correspond to points that lie on the
maximum margin hyperplanes in feature space, as illustrated in Figure 7.1. This
property is central to the practical applicability of support vector machines. Once
the model is trained, a signiﬁcant proportion of the data points can be discarded and
only the support vectors retained.

Having solved the quadratic programming problem and found a value for a, we
can then determine the value of the threshold parameter b by noting that any support
vector xn satisﬁes tny(xn) = 1. Using (7.13) this gives

tn

amtmk(xn, xm) + b

= 1

(7.17)

where S denotes the set of indices of the support vectors. Although we can solve
this equation for b using an arbitrarily chosen support vector xn, a numerically more
n = 1,
stable solution is obtained by ﬁrst multiplying through by tn, making use of t2
and then averaging these equations over all support vectors and solving for b to give

b =

1
NS

amtmk(xn, xm)

(7.18)

tn −

m∈S

where NS is the total number of support vectors.
For later comparison with alternative models, we can express the maximum-
margin classiﬁer in terms of the minimization of an error function, with a simple
quadratic regularizer, in the form

E∞(y(xn)tn − 1) + λw2

n=1

(7.19)

where E∞(z) is a function that is zero if z � 0 and ∞ otherwise and ensures that
the constraints (7.5) are satisﬁed. Note that as long as the regularization parameter
satisﬁes λ > 0, its precise value plays no role.

Figure 7.2 shows an example of the classiﬁcation resulting from training a sup-
port vector machine on a simple synthetic data set using a Gaussian kernel of the

7.1. Maximum Margin Classiﬁers

331

Figure 7.2 Example of synthetic data from
two classes in two dimensions
showing contours of constant
y(x) obtained from a support
vector machine having a Gaus-
sian kernel function. Also shown
are the decision boundary,
the
margin boundaries, and the sup-
port vectors.

form (6.23). Although the data set is not linearly separable in the two-dimensional
data space x, it is linearly separable in the nonlinear feature space deﬁned implicitly
by the nonlinear kernel function. Thus the training data points are perfectly separated
in the original data space.

This example also provides a geometrical insight into the origin of sparsity in
the SVM. The maximum margin hyperplane is deﬁned by the location of the support
vectors. Other data points can be moved around freely (so long as they remain out-
side the margin region) without changing the decision boundary, and so the solution
will be independent of such data points.

7.1.1 Overlapping class distributions
So far, we have assumed that the training data points are linearly separable in the
feature space φ(x). The resulting support vector machine will give exact separation
of the training data in the original input space x, although the corresponding decision
boundary will be nonlinear. In practice, however, the class-conditional distributions
may overlap, in which case exact separation of the training data can lead to poor
generalization.

We therefore need a way to modify the support vector machine so as to allow
some of the training points to be misclassiﬁed. From (7.19) we see that in the case
of separable classes, we implicitly used an error function that gave inﬁnite error
if a data point was misclassiﬁed and zero error if it was classiﬁed correctly, and
then optimized the model parameters to maximize the margin. We now modify this
approach so that data points are allowed to be on the ‘wrong side’ of the margin
boundary, but with a penalty that increases with the distance from that boundary. For
the subsequent optimization problem, it is convenient to make this penalty a linear
function of this distance. To do this, we introduce slack variables, ξn � 0 where
n = 1, . . . , N, with one slack variable for each training data point (Bennett, 1992;
Cortes and Vapnik, 1995). These are deﬁned by ξn = 0 for data points that are on or
inside the correct margin boundary and ξn = |tn − y(xn)| for other points. Thus a
data point that is on the decision boundary y(xn) = 0 will have ξn = 1, and points

332

7. SPARSE KERNEL MACHINES

Figure 7.3 Illustration of the slack variables ξn � 0.
Data points with circles around them are
support vectors.






n=1

N

N



N

y = −1
y = 0

y = 1

ξ = 0

ξ > 1

ξ < 1

ξ = 0



N

with ξn > 1 will be misclassiﬁed. The exact classiﬁcation constraints (7.5) are then
replaced with

n = 1, . . . , N

tny(xn) � 1 − ξn,

(7.20)
in which the slack variables are constrained to satisfy ξn � 0. Data points for which
ξn = 0 are correctly classiﬁed and are either on the margin or on the correct side
of the margin. Points for which 0 < ξn � 1 lie inside the margin, but on the cor-
rect side of the decision boundary, and those data points for which ξn > 1 lie on
the wrong side of the decision boundary and are misclassiﬁed, as illustrated in Fig-
ure 7.3. This is sometimes described as relaxing the hard margin constraint to give a
soft margin and allows some of the training set data points to be misclassiﬁed. Note
that while slack variables allow for overlapping class distributions, this framework is
still sensitive to outliers because the penalty for misclassiﬁcation increases linearly
with ξ.

Our goal is now to maximize the margin while softly penalizing points that lie

on the wrong side of the margin boundary. We therefore minimize

C

ξn +

1
2w2

(7.21)

where the parameter C > 0 controls the trade-off between the slack variable penalty
and the margin. Because any point that is misclassiﬁed has ξn > 1, it follows that
n ξn is an upper bound on the number of misclassiﬁed points. The parameter C is
therefore analogous to (the inverse of) a regularization coefﬁcient because it controls
the trade-off between minimizing training errors and controlling model complexity.
In the limit C → ∞, we will recover the earlier support vector machine for separable
data.
We now wish to minimize (7.21) subject to the constraints (7.20) together with

ξn � 0. The corresponding Lagrangian is given by

L(w, b, a) =

1
2w2 + C

ξn−

n=1

n=1

an {tny(xn) − 1 + ξn}−

n=1

µnξn (7.22)





N

∂L
∂w

∂L
∂b

∂L
∂ξn






n=1

n=1

N

N

N

N



1
2

N

(7.23)
(7.24)
(7.25)
(7.26)
(7.27)
(7.28)

(7.29)

(7.30)

(7.31)

(7.33)

(7.34)

Using these results to eliminate w, b, and {ξn} from the Lagrangian, we obtain the
dual Lagrangian in the form

L(a) =

n=1

an −

anamtntmk(xn, xm)

(7.32)

n=1

m=1

which is identical to the separable case, except that the constraints are somewhat
different. To see what these constraints are, we note that an � 0 is required because
these are Lagrange multipliers. Furthermore, (7.31) together with µn � 0 implies
an � C. We therefore have to minimize (7.32) with respect to the dual variables
{an} subject to

0 � an � C

antn = 0

n=1

for n = 1, . . . , N, where (7.33) are known as box constraints. This again represents
a quadratic programming problem. If we substitute (7.29) into (7.1), we see that
predictions for new data points are again made by using (7.13).

We can now interpret the resulting solution. As before, a subset of the data
points may have an = 0, in which case they do not contribute to the predictive

Appendix E

where {an � 0} and {µn � 0} are Lagrange multipliers. The corresponding set of
KKT conditions are given by

7.1. Maximum Margin Classiﬁers

333

where n = 1, . . . , N.

We now optimize out w, b, and {ξn} making use of the deﬁnition (7.1) of y(x)

to give

an � 0
tny(xn) − 1 + ξn � 0
an (tny(xn) − 1 + ξn) = 0
µn � 0
ξn � 0
µnξn = 0

= 0 ⇒ w =

antnφ(xn)

= 0 ⇒

antn = 0

= 0 ⇒ an = C − µn.















tn −

n∈M

m∈S

N

N

1
2

N

n=1

N

n=1

m=1

0 � an � 1/N

antn = 0

an � ν.

n=1



334

7. SPARSE KERNEL MACHINES

model (7.13). The remaining data points constitute the support vectors. These have
an > 0 and hence from (7.25) must satisfy

tny(xn) = 1 − ξn.

(7.35)

If an < C, then (7.31) implies that µn > 0, which from (7.28) requires ξn = 0 and
hence such points lie on the margin. Points with an = C can lie inside the margin
and can either be correctly classiﬁed if ξn � 1 or misclassiﬁed if ξn > 1.

To determine the parameter b in (7.1), we note that those support vectors for

which 0 < an < C have ξn = 0 so that tny(xn) = 1 and hence will satisfy

tn

amtmk(xn, xm) + b

= 1.

(7.36)

Again, a numerically stable solution is obtained by averaging to give

b =

1
NM

amtmk(xn, xm)

(7.37)

m∈S

where M denotes the set of indices of data points having 0 < an < C.
An alternative, equivalent formulation of the support vector machine, known as
the ν-SVM, has been proposed by Sch¨olkopf et al. (2000). This involves maximizing

L(a) = −

anamtntmk(xn, xm)

(7.38)

subject to the constraints

(7.39)

(7.40)

(7.41)

This approach has the advantage that the parameter ν, which replaces C, can be
interpreted as both an upper bound on the fraction of margin errors (points for which
ξn > 0 and hence which lie on the wrong side of the margin boundary and which may
or may not be misclassiﬁed) and a lower bound on the fraction of support vectors. An
example of the ν-SVM applied to a synthetic data set is shown in Figure 7.4. Here
Gaussian kernels of the form exp (−γx − x2) have been used, with γ = 0.45.
Although predictions for new inputs are made using only the support vectors,
the training phase (i.e., the determination of the parameters a and b) makes use of
the whole data set, and so it is important to have efﬁcient algorithms for solving



335

7.1. Maximum Margin Classiﬁers

Figure 7.4 Illustration of the ν-SVM applied
to a nonseparable data set in two
dimensions. The support vectors
are indicated by circles.

2

0

−2

−2

0

2

L(a)
the quadratic programming problem. We ﬁrst note that the objective function
given by (7.10) or (7.32) is quadratic and so any local optimum will also be a global
optimum provided the constraints deﬁne a convex region (which they do as a conse-
quence of being linear). Direct solution of the quadratic programming problem us-
ing traditional techniques is often infeasible due to the demanding computation and
memory requirements, and so more practical approaches need to be found. The tech-
nique of chunking (Vapnik, 1982) exploits the fact that the value of the Lagrangian
is unchanged if we remove the rows and columns of the kernel matrix corresponding
to Lagrange multipliers that have value zero. This allows the full quadratic pro-
gramming problem to be broken down into a series of smaller ones, whose goal is
eventually to identify all of the nonzero Lagrange multipliers and discard the others.
Chunking can be implemented using protected conjugate gradients (Burges, 1998).
Although chunking reduces the size of the matrix in the quadratic function from the
number of data points squared to approximately the number of nonzero Lagrange
multipliers squared, even this may be too big to ﬁt in memory for large-scale appli-
cations. Decomposition methods (Osuna et al., 1996) also solve a series of smaller
quadratic programming problems but are designed so that each of these is of a ﬁxed
size, and so the technique can be applied to arbitrarily large data sets. However, it
still involves numerical solution of quadratic programming subproblems and these
can be problematic and expensive. One of the most popular approaches to training
support vector machines is called sequential minimal optimization, or SMO (Platt,
1999). It takes the concept of chunking to the extreme limit and considers just two
Lagrange multipliers at a time. In this case, the subproblem can be solved analyti-
cally, thereby avoiding numerical quadratic programming altogether. Heuristics are
given for choosing the pair of Lagrange multipliers to be considered at each step.
In practice, SMO is found to have a scaling with the number of data points that is
somewhere between linear and quadratic depending on the particular application.

We have seen that kernel functions correspond to inner products in feature spaces
that can have high, or even inﬁnite, dimensionality. By working directly in terms of
the kernel function, without introducing the feature space explicitly, it might there-
fore seem that support vector machines somehow manage to avoid the curse of di-






7. SPARSE KERNEL MACHINES

336

Section 1.4

mensionality. This is not the case, however, because there are constraints amongst
the feature values that restrict the effective dimensionality of feature space. To see
this consider a simple second-order polynomial kernel that we can expand in terms
of its components

k(x, z) =

1 + xTz

2 = (1 + x1z1 + x2z2)2

= 1 + 2x1z1 + 2x2z2 + x2
= (1,√2x1,√2x2, x2
= φ(x)Tφ(z).

1z2

1 + 2x1z1x2z2 + x2

1,√2x1x2, x2

2)(1,√2z1,√2z2, z2

2z2
2

1,√2z1z2, z2
2)T
(7.42)

This kernel function therefore represents an inner product in a feature space having
six dimensions, in which the mapping from input space to feature space is described
by the vector function φ(x). However, the coefﬁcients weighting these different
features are constrained to have speciﬁc forms. Thus any set of points in the original
two-dimensional space x would be constrained to lie exactly on a two-dimensional
nonlinear manifold embedded in the six-dimensional feature space.

We have already highlighted the fact that the support vector machine does not
provide probabilistic outputs but instead makes classiﬁcation decisions for new in-
put vectors. Veropoulos et al. (1999) discuss modiﬁcations to the SVM to allow
the trade-off between false positive and false negative errors to be controlled. How-
ever, if we wish to use the SVM as a module in a larger probabilistic system, then
probabilistic predictions of the class label t for new inputs x are required.

To address this issue, Platt (2000) has proposed ﬁtting a logistic sigmoid to the
outputs of a previously trained support vector machine. Speciﬁcally, the required
conditional probability is assumed to be of the form

p(t = 1|x) = σ (Ay(x) + B)

(7.43)

where y(x) is deﬁned by (7.1). Values for the parameters A and B are found by
minimizing the cross-entropy error function deﬁned by a training set consisting of
pairs of values y(xn) and tn. The data used to ﬁt the sigmoid needs to be independent
of that used to train the original SVM in order to avoid severe over-ﬁtting. This two-
stage approach is equivalent to assuming that the output y(x) of the support vector
machine represents the log-odds of x belonging to class t = 1. Because the SVM
training procedure is not speciﬁcally intended to encourage this, the SVM can give
a poor approximation to the posterior probabilities (Tipping, 2001).

7.1.2 Relation to logistic regression
As with the separable case, we can re-cast the SVM for nonseparable distri-
butions in terms of the minimization of a regularized error function. This will also
allow us to highlight similarities, and differences, compared to the logistic regression
model.

We have seen that for data points that are on the correct side of the margin
boundary, and which therefore satisfy yntn � 1, we have ξn = 0, and for the

Section 4.3.2




n=1

N

N

Figure 7.5 Plot of the ‘hinge’ error function used
in support vector machines, shown
in blue, along with the error function
for logistic regression, rescaled by a
factor of 1/ ln(2) so that it passes
through the point (0, 1), shown in red.
Also shown are the misclassiﬁcation
error in black and the squared error
in green.

7.1. Maximum Margin Classiﬁers

337

E(z)

−2

−1

0

1

z

2

remaining points we have ξn = 1 − yntn. Thus the objective function (7.21) can be
written (up to an overall multiplicative constant) in the form

ESV(yntn) + λw2

(7.44)

where λ = (2C)−1, and ESV(·) is the hinge error function deﬁned by

ESV(yntn) = [1 − yntn]+

(7.45)
where [· ]+ denotes the positive part. The hinge error function, so-called because
of its shape, is plotted in Figure 7.5. It can be viewed as an approximation to the
misclassiﬁcation error, i.e., the error function that ideally we would like to minimize,
which is also shown in Figure 7.5.

When we considered the logistic regression model in Section 4.3.2, we found it
convenient to work with target variable t ∈ {0, 1}. For comparison with the support
vector machine, we ﬁrst reformulate maximum likelihood logistic regression using
the target variable t ∈ {−1, 1}. To do this, we note that p(t = 1|y) = σ(y) where
y(x) is given by (7.1), and σ(y) is the logistic sigmoid function deﬁned by (4.59). It
follows that p(t = −1|y) = 1 − σ(y) = σ(−y), where we have used the properties
of the logistic sigmoid function, and so we can write

p(t|y) = σ(yt).

(7.46)

Exercise 7.6

From this we can construct an error function by taking the negative logarithm of the
likelihood function that, with a quadratic regularizer, takes the form

where

ELR(yntn) + λw2.

n=1

ELR(yt) = ln (1 + exp(−yt)) .

(7.47)

(7.48)

338

7. SPARSE KERNEL MACHINES

For comparison with other error functions, we can divide by ln(2) so that the error
function passes through the point (0, 1). This rescaled error function is also plotted
in Figure 7.5 and we see that it has a similar form to the support vector error function.
The key difference is that the ﬂat region in ESV(yt) leads to sparse solutions.

Both the logistic error and the hinge loss can be viewed as continuous approx-
imations to the misclassiﬁcation error. Another continuous error function that has
sometimes been used to solve classiﬁcation problems is the squared error, which
is again plotted in Figure 7.5. It has the property, however, of placing increasing
emphasis on data points that are correctly classiﬁed but that are a long way from
the decision boundary on the correct side. Such points will be strongly weighted at
the expense of misclassiﬁed points, and so if the objective is to minimize the mis-
classiﬁcation rate, then a monotonically decreasing error function would be a better
choice.

7.1.3 Multiclass SVMs
The support vector machine is fundamentally a two-class classiﬁer. In practice,
however, we often have to tackle problems involving K > 2 classes. Various meth-
ods have therefore been proposed for combining multiple two-class SVMs in order
to build a multiclass classiﬁer.

One commonly used approach (Vapnik, 1998) is to construct K separate SVMs,
in which the kth model yk(x) is trained using the data from class Ck as the positive
examples and the data from the remaining K − 1 classes as the negative examples.
This is known as the one-versus-the-rest approach. However, in Figure 4.2 we saw
that using the decisions of the individual classiﬁers can lead to inconsistent results
in which an input is assigned to multiple classes simultaneously. This problem is
sometimes addressed by making predictions for new inputs x using

y(x) = max

k

yk(x).

(7.49)

Unfortunately, this heuristic approach suffers from the problem that the different
classiﬁers were trained on different tasks, and there is no guarantee that the real-
valued quantities yk(x) for different classiﬁers will have appropriate scales.

Another problem with the one-versus-the-rest approach is that the training sets
are imbalanced. For instance, if we have ten classes each with equal numbers of
training data points, then the individual classiﬁers are trained on data sets comprising
90% negative examples and only 10% positive examples, and the symmetry of the
original problem is lost. A variant of the one-versus-the-rest scheme was proposed
by Lee et al. (2001) who modify the target values so that the positive class has target
+1 and the negative class has target −1/(K − 1).
Weston and Watkins (1999) deﬁne a single objective function for training all
K SVMs simultaneously, based on maximizing the margin from each to remaining
classes. However, this can result in much slower training because, instead of solving
K separate optimization problems each over N data points with an overall cost of
O(KN 2), a single optimization problem of size (K − 1)N must be solved giving an
overall cost of O(K 2N 2).

7.1. Maximum Margin Classiﬁers

339

Another approach is to train K(K −1)/2 different 2-class SVMs on all possible
pairs of classes, and then to classify test points according to which class has the high-
est number of ‘votes’, an approach that is sometimes called one-versus-one. Again,
we saw in Figure 4.2 that this can lead to ambiguities in the resulting classiﬁcation.
Also, for large K this approach requires signiﬁcantly more training time than the
one-versus-the-rest approach. Similarly, to evaluate test points, signiﬁcantly more
computation is required.

The latter problem can be alleviated by organizing the pairwise classiﬁers into
a directed acyclic graph (not to be confused with a probabilistic graphical model)
leading to the DAGSVM (Platt et al., 2000). For K classes, the DAGSVM has a total
of K(K − 1)/2 classiﬁers, and to classify a new test point only K − 1 pairwise
classiﬁers need to be evaluated, with the particular classiﬁers used depending on
which path through the graph is traversed.

A different approach to multiclass classiﬁcation, based on error-correcting out-
put codes, was developed by Dietterich and Bakiri (1995) and applied to support
vector machines by Allwein et al. (2000). This can be viewed as a generalization of
the voting scheme of the one-versus-one approach in which more general partitions
of the classes are used to train the individual classiﬁers. The K classes themselves
are represented as particular sets of responses from the two-class classiﬁers chosen,
and together with a suitable decoding scheme, this gives robustness to errors and to
ambiguity in the outputs of the individual classiﬁers. Although the application of
SVMs to multiclass classiﬁcation problems remains an open issue, in practice the
one-versus-the-rest approach is the most widely used in spite of its ad-hoc formula-
tion and its practical limitations.

There are also single-class support vector machines, which solve an unsuper-
vised learning problem related to probability density estimation. Instead of mod-
elling the density of data, however, these methods aim to ﬁnd a smooth boundary
enclosing a region of high density. The boundary is chosen to represent a quantile of
the density, that is, the probability that a data point drawn from the distribution will
land inside that region is given by a ﬁxed number between 0 and 1 that is speciﬁed in
advance. This is a more restricted problem than estimating the full density but may
be sufﬁcient in speciﬁc applications. Two approaches to this problem using support
vector machines have been proposed. The algorithm of Sch¨olkopf et al. (2001) tries
to ﬁnd a hyperplane that separates all but a ﬁxed fraction ν of the training data from
the origin while at the same time maximizing the distance (margin) of the hyperplane
from the origin, while Tax and Duin (1999) look for the smallest sphere in feature
space that contains all but a fraction ν of the data points. For kernels k(x, x) that
are functions only of x − x, the two algorithms are equivalent.

7.1.4 SVMs for regression
We now extend support vector machines to regression problems while at the
same time preserving the property of sparseness. In simple linear regression, we

Section 3.1.4





n=1

N

N

1
2





z

(7.50)

(7.51)

(7.52)

(7.53)
(7.54)



340

7. SPARSE KERNEL MACHINES

Figure 7.6 Plot of an -insensitive error function (in
red) in which the error increases lin-
early with distance beyond the insen-
sitive region. Also shown for compar-
ison is the quadratic error function (in
green).

E(z)

−	

0


