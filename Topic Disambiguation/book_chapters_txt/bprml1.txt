	

minimize a regularized error function given by

{yn − tn}2 + λ

2w2.

To obtain sparse solutions, the quadratic error function is replaced by an 	-insensitive
error function (Vapnik, 1995), which gives zero error if the absolute difference be-
tween the prediction y(x) and the target t is less than 	 where 	 > 0. A simple
example of an 	-insensitive error function, having a linear cost associated with errors
outside the insensitive region, is given by

E(y(x) − t) =
and is illustrated in Figure 7.6.

0,
|y(x) − t| − 	, otherwise

if |y(x) − t| < 	;

We therefore minimize a regularized error function given by

C

n=1

E(y(xn) − tn) +

1
2w2

where y(x) is given by (7.1). By convention the (inverse) regularization parameter,
denoted C, appears in front of the error term.

As before, we can re-express the optimization problem by introducing slack
variables. For each data point xn, we now need two slack variables ξn � 0 and
ξn � 0, where ξn > 0 corresponds to a point for which tn > y(xn) + 	, and
ξn > 0
corresponds to a point for which tn < y(xn) − 	, as illustrated in Figure 7.7.
The condition for a target point to lie inside the 	-tube is that yn − 	 � tn �
yn+	, where yn = y(xn). Introducing the slack variables allows points to lie outside
the tube provided the slack variables are nonzero, and the corresponding conditions
are

tn � y(xn) + 	 + ξn
tn � y(xn) − 	 −
ξn.









b
b


N

n=1







n=1

N





n=1

N





n=1

n=1

N

N









n=1

n=1

N

N

∂L
∂w

∂L
∂b

∂L
∂ξn
∂L

∂

ξn

341

y + 	

y
y − 	

x

(7.55)

(7.57)

(7.58)

(7.59)

(7.60)

b

b


7.1. Maximum Margin Classiﬁers

Figure 7.7 Illustration of SVM regression, showing
the regression curve together with the -
insensitive ‘tube’. Also shown are exam-
ξ. Points
ples of the slack variables ξ and
above the -tube have ξ > 0 and
ξ = 0,
points below the -tube have ξ = 0 and
ξ > 0, and points inside the -tube have
ξ =

ξ = 0.

y(x)

ξ > 0

ξ > 0

The error function for support vector regression can then be written as

C

(ξn +

ξn) +

1
2w2

ξn � 0 as well as
which must be minimized subject to the constraints ξn � 0 and
(7.53) and (7.54). This can be achieved by introducing Lagrange multipliers an � 0,
an � 0, µn � 0, and

µn � 0 and optimizing the Lagrangian

(ξn +

ξn) +

1
2w2 −

(µnξn +

µn

ξn)

L = C

−

an(	 + ξn + yn − tn) −

an(	 +

ξn − yn + tn).

(7.56)

We now substitute for y(x) using (7.1) and then set the derivatives of the La-

grangian with respect to w, b, ξn, and

ξn to zero, giving

= 0 ⇒ w =

(an −

an)φ(xn)

= 0 ⇒

(an −

an) = 0

= 0 ⇒ an + µn = C
µn = C.
= 0 ⇒

an +

Exercise 7.7

Using these results to eliminate the corresponding variables from the Lagrangian, we
see that the dual problem involves maximizing














m=1

n=1

n=1

n=1

N

N

N

N





−	






0 � an � C
an � C
0 �






n=1

N









342

7. SPARSE KERNEL MACHINES





L(a,

a) = −

1
2

(an −

an)(am −

am)k(xn, xm)

(an +

an) +

(an −

an)tn

(7.61)

an}, where we have introduced the kernel k(x, x) =
with respect to {an} and {
φ(x)Tφ(x). Again, this is a constrained maximization, and to ﬁnd the constraints
an � 0 are both required because these are Lagrange
we note that an � 0 and
multipliers. Also µn � 0 and
µn � 0 together with (7.59) and (7.60), require
an � C and

an � C, and so again we have the box constraints

together with the condition (7.58).

Substituting (7.57) into (7.1), we see that predictions for new inputs can be made

using

y(x) =

(an −

an)k(x, xn) + b

which is again expressed in terms of the kernel function.

The corresponding Karush-Kuhn-Tucker (KKT) conditions, which state that at
the solution the product of the dual variables and the constraints must vanish, are
given by

(7.62)
(7.63)

(7.64)

(7.65)
(7.66)
(7.67)
(7.68)

an(	 + ξn + yn − tn) = 0
ξn − yn + tn) = 0
an(	 +
(C − an)ξn = 0
(C −
ξn = 0.

an)

From these we can obtain several useful results. First of all, we note that a coefﬁcient
an can only be nonzero if 	 + ξn + yn − tn = 0, which implies that the data point
either lies on the upper boundary of the 	-tube (ξn = 0) or lies above the upper
ξn − yn + tn = 0,
boundary (ξn > 0). Similarly, a nonzero value for
and such points must lie either on or below the lower boundary of the 	-tube.
ξn − yn + tn = 0
are incompatible, as is easily seen by adding them together and noting that ξn and
ξn are nonnegative while 	 is strictly positive, and so for every data point xn, either
an or

Furthermore, the two constraints 	 + ξn + yn − tn = 0 and 	 +

an (or both) must be zero.

an implies 	 +

The support vectors are those data points that contribute to predictions given by
an = 0. These are points that
(7.64), in other words those for which either an = 0 or
lie on the boundary of the 	-tube or outside the tube. All points within the tube have


















(an −

m=1

n=1

n=1

n=1

N

N

N

N

1
2








0 � an � C/N
an � C/N
0 �

(an −

an) = 0

N

(an +

n=1

an) � νC.



7.1. Maximum Margin Classiﬁers

343

an = 0. We again have a sparse solution, and the only terms that have to be

an =
evaluated in the predictive model (7.64) are those that involve the support vectors.

The parameter b can be found by considering a data point for which 0 < an <
C, which from (7.67) must have ξn = 0, and from (7.65) must therefore satisfy
	 + yn − tn = 0. Using (7.1) and solving for b, we obtain

b = tn − 	 − wTφ(xn)
(am −

= tn − 	 −

N

m=1

am)k(xn, xm)

(7.69)

where we have used (7.57). We can obtain an analogous result by considering a point
for which 0 <
an < C. In practice, it is better to average over all such estimates of
b.

As with the classiﬁcation case, there is an alternative formulation of the SVM
for regression in which the parameter governing complexity has a more intuitive
interpretation (Sch¨olkopf et al., 2000). In particular, instead of ﬁxing the width 	 of
the insensitive region, we ﬁx instead a parameter ν that bounds the fraction of points
lying outside the tube. This involves maximizing

L(a,

a) = −

(an −

an)(am −

am)k(xn, xm)

+

subject to the constraints

an)tn

(7.70)

(7.71)
(7.72)

(7.73)

(7.74)

Appendix A

It can be shown that there are at most νN data points falling outside the insensitive
tube, while at least νN data points are support vectors and so lie either on the tube
or outside it.

The use of a support vector machine to solve a regression problem is illustrated
using the sinusoidal data set in Figure 7.8. Here the parameters ν and C have been
chosen by hand. In practice, their values would typically be determined by cross-
validation.

344

7. SPARSE KERNEL MACHINES

Figure 7.8 Illustration of the ν-SVM for re-
gression applied to the sinusoidal
synthetic data set using Gaussian
kernels. The predicted regression
curve is shown by the red line, and
the -insensitive tube corresponds
to the shaded region. Also, the
data points are shown in green,
and those with support vectors
are indicated by blue circles.

t

1

0

−1

0

1

x

7.1.5 Computational learning theory
Historically, support vector machines have largely been motivated and analysed
using a theoretical framework known as computational learning theory, also some-
times called statistical learning theory (Anthony and Biggs, 1992; Kearns and Vazi-
rani, 1994; Vapnik, 1995; Vapnik, 1998). This has its origins with Valiant (1984)
who formulated the probably approximately correct, or PAC, learning framework.
The goal of the PAC framework is to understand how large a data set needs to be in
order to give good generalization. It also gives bounds for the computational cost of
learning, although we do not consider these here.

Ex,t [I (f(x;D) = t)] < 	

Suppose that a data set D of size N is drawn from some joint distribution p(x, t)
where x is the input variable and t represents the class label, and that we restrict
attention to ‘noise free’ situations in which the class labels are determined by some
(unknown) deterministic function t = g(x). In PAC learning we say that a function
f(x;D), drawn from a space F of such functions on the basis of the training set
D, has good generalization if its expected error rate is below some pre-speciﬁed
threshold 	, so that
(7.75)
where I(·) is the indicator function, and the expectation is with respect to the dis-
tribution p(x, t). The quantity on the left-hand side is a random variable, because
it depends on the training set D, and the PAC framework requires that (7.75) holds,
with probability greater than 1 − δ, for a data set D drawn randomly from p(x, t).
Here δ is another pre-speciﬁed parameter, and the terminology ‘probably approxi-
mately correct’ comes from the requirement that with high probability (greater than
1− δ), the error rate be small (less than 	). For a given choice of model space F, and
for given parameters 	 and δ, PAC learning aims to provide bounds on the minimum
size N of data set needed to meet this criterion. A key quantity in PAC learning is
the Vapnik-Chervonenkis dimension, or VC dimension, which provides a measure of
the complexity of a space of functions, and which allows the PAC framework to be
extended to spaces containing an inﬁnite number of functions.

The bounds derived within the PAC framework are often described as worst-

7.2. Relevance Vector Machines

345

case, because they apply to any choice for the distribution p(x, t), so long as both
the training and the test examples are drawn (independently) from the same distribu-
tion, and for any choice for the function f(x) so long as it belongs to F. In real-world
applications of machine learning, we deal with distributions that have signiﬁcant reg-
ularity, for example in which large regions of input space carry the same class label.
As a consequence of the lack of any assumptions about the form of the distribution,
the PAC bounds are very conservative, in other words they strongly over-estimate
the size of data sets required to achieve a given generalization performance. For this
reason, PAC bounds have found few, if any, practical applications.

One attempt to improve the tightness of the PAC bounds is the PAC-Bayesian
framework (McAllester, 2003), which considers a distribution over the space F of
functions, somewhat analogous to the prior in a Bayesian treatment. This still con-
siders any possible choice for p(x, t), and so although the bounds are tighter, they
are still very conservative.

7.2. Relevance Vector Machines

Support vector machines have been used in a variety of classiﬁcation and regres-
sion applications. Nevertheless, they suffer from a number of limitations, several
of which have been highlighted already in this chapter. In particular, the outputs of
an SVM represent decisions rather than posterior probabilities. Also, the SVM was
originally formulated for two classes, and the extension to K > 2 classes is prob-
lematic. There is a complexity parameter C, or ν (as well as a parameter 	 in the case
of regression), that must be found using a hold-out method such as cross-validation.
Finally, predictions are expressed as linear combinations of kernel functions that are
centred on training data points and that are required to be positive deﬁnite.

The relevance vector machine or RVM (Tipping, 2001) is a Bayesian sparse ker-
nel technique for regression and classiﬁcation that shares many of the characteristics
of the SVM whilst avoiding its principal limitations. Additionally, it typically leads
to much sparser models resulting in correspondingly faster performance on test data
whilst maintaining comparable generalization error.

In contrast to the SVM we shall ﬁnd it more convenient to introduce the regres-

sion form of the RVM ﬁrst and then consider the extension to classiﬁcation tasks.

7.2.1 RVM for regression
The relevance vector machine for regression is a linear model of the form studied
in Chapter 3 but with a modiﬁed prior that results in sparse solutions. The model
deﬁnes a conditional distribution for a real-valued target variable t, given an input
vector x, which takes the form

p(t|x, w, β) = N (t|y(x), β−1)

(7.76)






n=1

i=1

M

N

N

M

with ﬁxed nonlinear basis functions φi(x), which will typically include a constant
term so that the corresponding weight parameter represents a ‘bias’.

The relevance vector machine is a speciﬁc instance of this model, which is in-
tended to mirror the structure of the support vector machine. In particular, the basis
functions are given by kernels, with one kernel associated with each of the data
points from the training set. The general expression (7.77) then takes the SVM-like
form

y(x) =

wnk(x, xn) + b

(7.78)

where b is a bias parameter. The number of parameters in this case is M = N + 1,
and y(x) has the same form as the predictive model (7.64) for the SVM, except that
the coefﬁcients an are here denoted wn. It should be emphasized that the subsequent
analysis is valid for arbitrary choices of basis function, and for generality we shall
work with the form (7.77). In contrast to the SVM, there is no restriction to positive-
deﬁnite kernels, nor are the basis functions tied in either number or location to the
training data points.

Suppose we are given a set of N observations of the input vector x, which we
n with n = 1, . . . , N. The
denote collectively by a data matrix X whose nth row is xT
corresponding target values are given by t = (t1, . . . , tN )T. Thus, the likelihood
function is given by

p(t|X, w, β) =

p(tn|xn, w, β−1).

(7.79)

n=1

Next we introduce a prior distribution over the parameter vector w and as in
Chapter 3, we shall consider a zero-mean Gaussian prior. However, the key differ-
ence in the RVM is that we introduce a separate hyperparameter αi for each of the
weight parameters wi instead of a single shared hyperparameter. Thus the weight
prior takes the form

p(w|α) =

i=1

N (wi|0, α−1
i )

(7.80)

where αi represents the precision of the corresponding parameter wi, and α denotes
(α1, . . . , αM )T. We shall see that, when we maximize the evidence with respect
to these hyperparameters, a signiﬁcant proportion of them go to inﬁnity, and the
corresponding weight parameters have posterior distributions that are concentrated
at zero. The basis functions associated with these parameters therefore play no role

346

7. SPARSE KERNEL MACHINES

where β = σ−2 is the noise precision (inverse noise variance), and the mean is given
by a linear model of the form

y(x) =

wiφi(x) = wTφ(x)

(7.77)












7.2. Relevance Vector Machines

347

in the predictions made by the model and so are effectively pruned out, resulting in
a sparse model.

Using the result (3.49) for linear regression models, we see that the posterior

distribution for the weights is again Gaussian and takes the form

p(w|t, X, α, β) = N (w|m, Σ)

(7.81)

where the mean and covariance are given by

A + βΦTΦ

m = βΣΦTt
Σ =

(7.82)
(7.83)
where Φ is the N × M design matrix with elements Φni = φi(xn), and A =
diag(αi). Note that in the speciﬁc case of the model (7.78), we have Φ = K, where
K is the symmetric (N + 1) × (N + 1) kernel matrix with elements k(xn, xm).
The values of α and β are determined using type-2 maximum likelihood, also
known as the evidence approximation, in which we maximize the marginal likeli-
hood function obtained by integrating out the weight parameters

−1

p(t|X, α, β) =

p(t|X, w, β)p(w|α) dw.

(7.84)

Because this represents the convolution of two Gaussians, it is readily evaluated to
give the log marginal likelihood in the form

ln p(t|X, α, β) = lnN (t|0, C)

= −

1
2

N ln(2π) + ln|C| + tTC−1t

where t = (t1, . . . , tN )T, and we have deﬁned the N × N matrix C given by

C = β−1I + ΦA−1ΦT.

Our goal is now to maximize (7.85) with respect to the hyperparameters α and
β. This requires only a small modiﬁcation to the results obtained in Section 3.5 for
the evidence approximation in the linear regression model. Again, we can identify
two approaches. In the ﬁrst, we simply set the required derivatives of the marginal
likelihood to zero and obtain the following re-estimation equations

(7.85)

(7.86)

(7.87)

(7.88)

αnew

i

= γi
m2
i

(βnew)−1 = t − Φm2
i γi

N −

Section 3.5

Exercise 7.10

Exercise 7.12

Section 3.5.3

where mi is the ith component of the posterior mean m deﬁned by (7.82). The
quantity γi measures how well the corresponding parameter wi is determined by the
data and is deﬁned by

348

7. SPARSE KERNEL MACHINES








γi = 1 − αiΣii

(7.89)
in which Σii is the ith diagonal component of the posterior covariance Σ given by
(7.83). Learning therefore proceeds by choosing initial values for α and β, evalu-
ating the mean and covariance of the posterior using (7.82) and (7.83), respectively,
and then alternately re-estimating the hyperparameters, using (7.87) and (7.88), and
re-estimating the posterior mean and covariance, using (7.82) and (7.83), until a suit-
able convergence criterion is satisﬁed.

The second approach is to use the EM algorithm, and is discussed in Sec-
tion 9.3.4. These two approaches to ﬁnding the values of the hyperparameters that
maximize the evidence are formally equivalent. Numerically, however, it is found
that the direct optimization approach corresponding to (7.87) and (7.88) gives some-
what faster convergence (Tipping, 2001).

As a result of the optimization, we ﬁnd that a proportion of the hyperparameters
{αi} are driven to large (in principle inﬁnite) values, and so the weight parameters
wi corresponding to these hyperparameters have posterior distributions with mean
and variance both zero. Thus those parameters, and the corresponding basis func-
tions φi(x), are removed from the model and play no role in making predictions for
new inputs. In the case of models of the form (7.78), the inputs xn corresponding to
the remaining nonzero weights are called relevance vectors, because they are iden-
tiﬁed through the mechanism of automatic relevance determination, and are analo-
gous to the support vectors of an SVM. It is worth emphasizing, however, that this
mechanism for achieving sparsity in probabilistic models through automatic rele-
vance determination is quite general and can be applied to any model expressed as
an adaptive linear combination of basis functions.

Having found values α and β for the hyperparameters that maximize the
marginal likelihood, we can evaluate the predictive distribution over t for a new
input x. Using (7.76) and (7.81), this is given by

p(t|x, X, t, α, β) =

= N

p(t|x, w, β)p(w|X, t, α, β) dw
t|mTφ(x), σ2(x)

.

(7.90)

Thus the predictive mean is given by (7.76) with w set equal to the posterior mean
m, and the variance of the predictive distribution is given by

σ2(x) = (β)−1 + φ(x)TΣφ(x)

(7.91)

where Σ is given by (7.83) in which α and β are set to their optimized values α and
β. This is just the familiar result (3.59) obtained in the context of linear regression.
Recall that for localized basis functions, the predictive variance for linear regression
models becomes small in regions of input space where there are no basis functions.
In the case of an RVM with the basis functions centred on data points, the model will
therefore become increasingly certain of its predictions when extrapolating outside
the domain of the data (Rasmussen and Qui˜nonero-Candela, 2005), which of course
is undesirable. The predictive distribution in Gaussian process regression does not

Exercise 9.23

Section 7.2.2

Exercise 7.14

Section 6.4.2

7.2. Relevance Vector Machines

349

Figure 7.9 Illustration of RVM regression us-
ing the same data set, and the
same Gaussian kernel functions,
the
as used in Figure 7.8 for
ν-SVM regression model.
The
mean of
the predictive distribu-
tion for the RVM is shown by the
red line, and the one standard-
deviation predictive distribution is
shown by the shaded region.
Also, the data points are shown
in green, and the relevance vec-
tors are indicated by blue circles.
Note that there are only 3 rele-
vance vectors compared to 7 sup-
port vectors for the ν-SVM in Fig-
ure 7.8.

t

1

0

−1

0

1

x

suffer from this problem. However, the computational cost of making predictions
with a Gaussian processes is typically much higher than with an RVM.

Figure 7.9 shows an example of the RVM applied to the sinusoidal regression
data set. Here the noise precision parameter β is also determined through evidence
maximization. We see that the number of relevance vectors in the RVM is signif-
icantly smaller than the number of support vectors used by the SVM. For a wide
range of regression and classiﬁcation tasks, the RVM is found to give models that
are typically an order of magnitude more compact than the corresponding support
vector machine, resulting in a signiﬁcant improvement in the speed of processing on
test data. Remarkably, this greater sparsity is achieved with little or no reduction in
generalization error compared with the corresponding SVM.

The principal disadvantage of the RVM compared to the SVM is that training
involves optimizing a nonconvex function, and training times can be longer than for a
comparable SVM. For a model with M basis functions, the RVM requires inversion
of a matrix of size M × M, which in general requires O(M 3) computation. In the
speciﬁc case of the SVM-like model (7.78), we have M = N +1. As we have noted,
there are techniques for training SVMs whose cost is roughly quadratic in N. Of
course, in the case of the RVM we always have the option of starting with a smaller
number of basis functions than N + 1. More signiﬁcantly, in the relevance vector
machine the parameters governing complexity and noise variance are determined
automatically from a single training run, whereas in the support vector machine the
parameters C and 	 (or ν) are generally found using cross-validation, which involves
multiple training runs. Furthermore, in the next section we shall derive an alternative
procedure for training the relevance vector machine that improves training speed
signiﬁcantly.

7.2.2 Analysis of sparsity
We have noted earlier that the mechanism of automatic relevance determination
causes a subset of parameters to be driven to zero. We now examine in more detail

350

7. SPARSE KERNEL MACHINES

t2

C

t

t1

t2

ϕ

C

t

t1

Figure 7.10 Illustration of the mechanism for sparsity in a Bayesian linear regression model, showing a training
set vector of target values given by t = (t1, t2)T, indicated by the cross, for a model with one basis vector
ϕ = (φ(x1), φ(x2))T, which is poorly aligned with the target data vector t. On the left we see a model having
only isotropic noise, so that C = β−1I, corresponding to α = ∞, with β set to its most probable value. On
the right we see the same model but with a ﬁnite value of α. In each case the red ellipse corresponds to unit
Mahalanobis distance, with |C| taking the same value for both plots, while the dashed green circle shows the
contrition arising from the noise term β−1. We see that any ﬁnite value of α reduces the probability of the
observed data, and so for the most probable solution the basis vector is removed.

the mechanism of sparsity in the context of the relevance vector machine. In the
process, we will arrive at a signiﬁcantly faster procedure for optimizing the hyper-
parameters compared to the direct techniques given above.

Before proceeding with a mathematical analysis, we ﬁrst give some informal
insight into the origin of sparsity in Bayesian linear models. Consider a data set
comprising N = 2 observations t1 and t2, together with a model having a single
basis function φ(x), with hyperparameter α, along with isotropic noise having pre-
cision β. From (7.85), the marginal likelihood is given by p(t|α, β) = N (t|0, C) in
which the covariance matrix takes the form

C =

1
β

I +

1
α

ϕϕT

(7.92)

where ϕ denotes the N-dimensional vector (φ(x1), φ(x2))T, and similarly t =
(t1, t2)T. Notice that this is just a zero-mean Gaussian process model over t with
covariance C. Given a particular observation for t, our goal is to ﬁnd α and β by
maximizing the marginal likelihood. We see from Figure 7.10 that, if there is a poor
alignment between the direction of ϕ and that of the training data vector t, then the
corresponding hyperparameter α will be driven to ∞, and the basis vector will be
pruned from the model. This arises because any ﬁnite value for α will always assign
a lower probability to the data, thereby decreasing the value of the density at t, pro-
vided that β is set to its optimal value. We see that any ﬁnite value for α would cause
the distribution to be elongated in a direction away from the data, thereby increasing
the probability mass in regions away from the observed data and hence reducing the
value of the density at the target data vector itself. For the more general case of M







7.2. Relevance Vector Machines

351

basis vectors ϕ1, . . . , ϕM a similar intuition holds, namely that if a particular basis
vector is poorly aligned with the data vector t, then it is likely to be pruned from the
model.

We now investigate the mechanism for sparsity from a more mathematical per-
spective, for a general case involving M basis functions. To motivate this analysis
we ﬁrst note that, in the result (7.87) for re-estimating the parameter αi, the terms on
the right-hand side are themselves also functions of αi. These results therefore rep-
resent implicit solutions, and iteration would be required even to determine a single
αi with all other αj for j = i ﬁxed.
This suggests a different approach to solving the optimization problem for the
RVM, in which we make explicit all of the dependence of the marginal likelihood
(7.85) on a particular αi and then determine its stationary points explicitly (Faul and
Tipping, 2002; Tipping and Faul, 2003). To do this, we ﬁrst pull out the contribution
from αi in the matrix C deﬁned by (7.86) to give

C = β−1I +

α−1
j ϕjϕT

j + α−1

i ϕiϕT
i

j=i
= C−i + α−1
i ϕiϕT
i

where ϕi denotes the ith column of Φ, in other words the N-dimensional vector with
elements (φi(x1), . . . , φi(xN )), in contrast to φn, which denotes the nth row of Φ.
The matrix C−i represents the matrix C with the contribution from basis function i
removed. Using the matrix identities (C.7) and (C.15), the determinant and inverse
of C can then be written

(7.93)

(7.94)

(7.95)

Exercise 7.15

|C| = |C−i||1 + α−1
i ϕT
C−1
−i ϕiϕT
C−1 = C−1
αi + ϕT

−i −

i C−1
−i ϕi|
i C−1
−i
.
−i ϕi

i C−1

Using these results, we can then write the log marginal likelihood function (7.85) in
the form

(7.96)
where L(α−i) is simply the log marginal likelihood with basis function ϕi omitted,
and the quantity λ(αi) is deﬁned by

L(α) = L(α−i) + λ(αi)

λ(αi) =

1
2

ln αi − ln (αi + si) + q2
αi + si

i

(7.97)

and contains all of the dependence on αi. Here we have introduced the two quantities

si = ϕT
qi = ϕT

i C−1
−i ϕi
i C−1
−i t.

(7.98)
(7.99)

Here si is called the sparsity and qi is known as the quality of ϕi, and as we shall
see, a large value of si relative to the value of qi means that the basis function ϕi

352

7. SPARSE KERNEL MACHINES

of

the

Figure 7.11 Plots
log
likelihood λ(αi) versus
marginal
ln αi showing on the left, the single
maximum at a ﬁnite αi for q2
i = 4
and si = 1 (so that q2
i > si) and on
the right, the maximum at αi = ∞
i = 1 and si = 2 (so that
for q2
i < si).
q2

2

0

−2

−4

2

0

−2

−4

−5

0

5

−5

0

5

is more likely to be pruned from the model. The ‘sparsity’ measures the extent to
which basis function ϕi overlaps with the other basis vectors in the model, and the
‘quality’ represents a measure of the alignment of the basis vector ϕn with the error
between the training set values t = (t1, . . . , tN )T and the vector y
−i of predictions
that would result from the model with the vector ϕi excluded (Tipping and Faul,
2003).

The stationary points of the marginal likelihood with respect to αi occur when

the derivative

dλ(αi)

dαi

= α−1
i s2

i − (q2
2(αi + si)2

i − si)

(7.100)

is equal to zero. There are two possible forms for the solution. Recalling that αi � 0,
we see that if q2
i > si, we
can solve for αi to obtain

i < si, then αi → ∞ provides a solution. Conversely, if q2

Exercise 7.16

αi = s2

i

q2
i − si

.

(7.101)

These two solutions are illustrated in Figure 7.11. We see that the relative size of
the quality and sparsity terms determines whether a particular basis vector will be
pruned from the model or not. A more complete analysis (Faul and Tipping, 2002),
based on the second derivatives of the marginal likelihood, conﬁrms these solutions
are indeed the unique maxima of λ(αi).

Note that this approach has yielded a closed-form solution for αi, for given
values of the other hyperparameters. As well as providing insight into the origin of
sparsity in the RVM, this analysis also leads to a practical algorithm for optimizing
the hyperparameters that has signiﬁcant speed advantages. This uses a ﬁxed set
of candidate basis vectors, and then cycles through them in turn to decide whether
each vector should be included in the model or not. The resulting sequential sparse
Bayesian learning algorithm is described below.

Sequential Sparse Bayesian Learning Algorithm

1. If solving a regression problem, initialize β.
2. Initialize using one basis function ϕ1, with hyperparameter α1 set using
(7.101), with the remaining hyperparameters αj for j = i initialized to
inﬁnity, so that only ϕ1 is included in the model.






7.2. Relevance Vector Machines

353

3. Evaluate Σ and m, along with qi and si for all basis functions.
4. Select a candidate basis function ϕi.
5. If q2

the model, then update αi using (7.101).

i > si, and αi < ∞, so that the basis vector ϕi is already included in
i > si, and αi = ∞, then add ϕi to the model, and evaluate hyperpa-
i � si, and αi < ∞ then remove basis function ϕi from the model,

rameter αi using (7.101).

7. If q2

6. If q2

and set αi = ∞.

8. If solving a regression problem, update β.
9. If converged terminate, otherwise go to 3.

Note that if q2
from the model and no action is required.

i � si and αi = ∞, then the basis function ϕi is already excluded

In practice, it is convenient to evaluate the quantities

The quality and sparseness variables can then be expressed in the form

Qi = ϕT
Si = ϕT

i C−1t
i C−1ϕi.

qi =

si =

αiQi
αi − Si
αiSi
αi − Si

.

(7.102)
(7.103)

(7.104)

(7.105)

Exercise 7.17

Note that when αi = ∞, we have qi = Qi and si = Si. Using (C.7), we can write
(7.106)
(7.107)

Qi = βϕT
Si = βϕT

i ΦΣΦTt

i ΦΣΦTϕi

i t − β2ϕT
i ϕi − β2ϕT

where Φ and Σ involve only those basis vectors that correspond to ﬁnite hyperpa-
rameters αi. At each stage the required computations therefore scale like O(M 3),
where M is the number of active basis vectors in the model and is typically much
smaller than the number N of training patterns.

7.2.3 RVM for classiﬁcation
We can extend the relevance vector machine framework to classiﬁcation prob-
lems by applying the ARD prior over weights to a probabilistic linear classiﬁcation
model of the kind studied in Chapter 4. To start with, we consider two-class prob-
lems with a binary target variable t ∈ {0, 1}. The model now takes the form of a
linear combination of basis functions transformed by a logistic sigmoid function

y(x, w) = σ

wTφ(x)

(7.108)



n=1











7. SPARSE KERNEL MACHINES

354

Section 4.4

where σ(·) is the logistic sigmoid function deﬁned by (4.59).
If we introduce a
Gaussian prior over the weight vector w, then we obtain the model that has been
considered already in Chapter 4. The difference here is that in the RVM, this model
uses the ARD prior (7.80) in which there is a separate precision hyperparameter
associated with each weight parameter.

In contrast to the regression model, we can no longer integrate analytically over
the parameter vector w. Here we follow Tipping (2001) and use the Laplace ap-
proximation, which was applied to the closely related problem of Bayesian logistic
regression in Section 4.5.1.

We begin by initializing the hyperparameter vector α. For this given value of
α, we then build a Gaussian approximation to the posterior distribution and thereby
obtain an approximation to the marginal likelihood. Maximization of this approxi-
mate marginal likelihood then leads to a re-estimated value for α, and the process is
repeated until convergence.

Let us consider the Laplace approximation for this model in more detail. For
a ﬁxed value of α, the mode of the posterior distribution over w is obtained by
maximizing

ln p(w|t, α) = ln{p(t|w)p(w|α)} − ln p(t|α)
1
=
{tn ln yn + (1 − tn) ln(1 − yn)} −
2

N

wTAw + const (7.109)

∇ ln p(w|t, α) = ΦT(t − y) − Aw
∇∇ ln p(w|t, α) = −
ΦTBΦ + A

(7.110)
(7.111)
where B is an N × N diagonal matrix with elements bn = yn(1 − yn), the vector
y = (y1, . . . , yN )T, and Φ is the design matrix with elements Φni = φi(xn). Here
we have used the property (4.88) for the derivative of the logistic sigmoid function.
At convergence of the IRLS algorithm, the negative Hessian represents the inverse
covariance matrix for the Gaussian approximation to the posterior distribution.

The mode of the resulting approximation to the posterior distribution, corre-
sponding to the mean of the Gaussian approximation, is obtained setting (7.110) to
zero, giving the mean and covariance of the Laplace approximation in the form

w = A−1ΦT(t − y)
−1
Σ =
ΦTBΦ + A

.

(7.112)

(7.113)

We can now use this Laplace approximation to evaluate the marginal likelihood.
Using the general result (4.135) for an integral evaluated using the Laplace approxi-

Exercise 7.18

where A = diag(αi). This can be done using iterative reweighted least squares
(IRLS) as discussed in Section 4.3.3. For this, we need the gradient vector and
Hessian matrix of the log posterior distribution, which from (7.109) are given by












7.2. Relevance Vector Machines

355

mation, we have

p(t|α) =

p(t|w)p(w|α) dw

(7.114)
If we substitute for p(t|w) and p(w|α) and then set the derivative of the marginal
likelihood with respect to αi equal to zero, we obtain

 p(t|w)p(w|α)(2π)M/2|Σ|1/2.

Exercise 7.19

1
2

−

(w

i )2 +

1
2αi −

1
2

Σii = 0.

Deﬁning γi = 1 − αiΣii and rearranging then gives

i = γi
αnew
i )2
(w

which is identical to the re-estimation formula (7.87) obtained for the regression
RVM.

If we deﬁne

we can write the approximate log marginal likelihood in the form

t = Φw + B−1(t − y)

ln p(t|α, β) = −

1
2

N ln(2π) + ln|C| + (

t)TC−1

t

where

C = B + ΦAΦT.

(7.115)

(7.116)

(7.117)

(7.118)

(7.119)

Appendix A

Section 13.3

This takes the same form as (7.85) in the regression case, and so we can apply the
same analysis of sparsity and obtain the same fast learning algorithm in which we
fully optimize a single hyperparameter αi at each step.

Figure 7.12 shows the relevance vector machine applied to a synthetic classiﬁ-
cation data set. We see that the relevance vectors tend not to lie in the region of the
decision boundary, in contrast to the support vector machine. This is consistent with
our earlier discussion of sparsity in the RVM, because a basis function φi(x) centred
on a data point near the boundary will have a vector ϕi that is poorly aligned with
the training data vector t.

One of the potential advantages of the relevance vector machine compared with
the SVM is that it makes probabilistic predictions. For example, this allows the RVM
to be used to help construct an emission density in a nonlinear extension of the linear
dynamical system for tracking faces in video sequences (Williams et al., 2005).

So far, we have considered the RVM for binary classiﬁcation problems. For
K > 2 classes, we again make use of the probabilistic approach in Section 4.3.4 in
which there are K linear models of the form

ak = wT
k x

(7.120)

exp(ak)


j

N



.

K

(7.121)

(7.122)

The log likelihood function is then given by

ln p(T|w1, . . . , wK) =

ytnk
nk

n=1

k=1

where the target values tnk have a 1-of-K coding for each data point n, and T is a
matrix with elements tnk. Again, the Laplace approximation can be used to optimize
the hyperparameters (Tipping, 2001), in which the model and its Hessian are found
using IRLS. This gives a more principled approach to multiclass classiﬁcation than
the pairwise method used in the support vector machine and also provides probabilis-
tic predictions for new data points. The principal disadvantage is that the Hessian
matrix has size M K×M K, where M is the number of active basis functions, which
gives an additional factor of K 3 in the computational cost of training compared with
the two-class RVM.

The principal disadvantage of the relevance vector machine is the relatively long
training times compared with the SVM. This is offset, however, by the avoidance of
cross-validation runs to set the model complexity parameters. Furthermore, because
it yields sparser models, the computation time on test points, which is usually the
more important consideration in practice, is typically much less.

356

7. SPARSE KERNEL MACHINES

2

0

−2

−2

0

2

Figure 7.12 Example of the relevance vector machine applied to a synthetic data set, in which the left-hand plot
shows the decision boundary and the data points, with the relevance vectors indicated by circles. Comparison
with the results shown in Figure 7.4 for the corresponding support vector machine shows that the RVM gives a
much sparser model. The right-hand plot shows the posterior probability given by the RVM output in which the
proportion of red (blue) ink indicates the probability of that point belonging to the red (blue) class.

which are combined using a softmax function to give outputs

yk(x) =

exp(aj)






n=1

N



Exercises

Exercises

357

7.1 ( ) www Suppose we have a data set of input vectors {xn} with corresponding
target values tn ∈ {−1, 1}, and suppose that we model the density of input vec-
tors within each class separately using a Parzen kernel density estimator (see Sec-
tion 2.5.1) with a kernel k(x, x). Write down the minimum misclassiﬁcation-rate
decision rule assuming the two classes have equal prior probability. Show also that,
if the kernel is chosen to be k(x, x) = xTx, then the classiﬁcation rule reduces to
simply assigning a new input vector to the class having the closest mean. Finally,
show that, if the kernel takes the form k(x, x) = φ(x)Tφ(x), that the classiﬁcation
is based on the closest mean in the feature space φ(x).

7.2 () Show that, if the 1 on the right-hand side of the constraint (7.5) is replaced by
some arbitrary constant γ > 0, the solution for the maximum margin hyperplane is
unchanged.

7.3 ( )

Show that, irrespective of the dimensionality of the data space, a data set
consisting of just two data points, one from each class, is sufﬁcient to determine the
location of the maximum-margin hyperplane.

7.4 ( ) www Show that the value ρ of the margin for the maximum-margin hyper-

plane is given by

(7.123)

(7.124)

(7.125)

where {an} are given by maximizing (7.10) subject to the constraints (7.11) and
(7.12).

7.5 ( ) Show that the values of ρ and {an} in the previous exercise also satisfy

1
ρ2 =

an

1
ρ2 = 2

L(a)

where

L(a) is deﬁned by (7.10). Similarly, show that

1
ρ2 = w2.

7.6 () Consider the logistic regression model with a target variable t ∈ {−1, 1}. If
we deﬁne p(t = 1|y) = σ(y) where y(x) is given by (7.1), show that the negative
log likelihood, with the addition of a quadratic regularization term, takes the form
(7.47).

7.7 () Consider the Lagrangian (7.56) for the regression support vector machine. By
setting the derivatives of the Lagrangian with respect to w, b, ξn, and
ξn to zero and
then back substituting to eliminate the corresponding variables, show that the dual
Lagrangian is given by (7.61).





358

7. SPARSE KERNEL MACHINES

7.8 () www For the regression support vector machine considered in Section 7.1.4,
show that all training data points for which ξn > 0 will have an = C, and similarly
all points for which

ξn > 0 will have

an = C.

7.9 () Verify the results (7.82) and (7.83) for the mean and covariance of the posterior

distribution over weights in the regression RVM.

7.10 ( ) www Derive the result (7.85) for the marginal likelihood function in the
regression RVM, by performing the Gaussian integral over w in (7.84) using the
technique of completing the square in the exponential.

7.11 ( ) Repeat the above exercise, but this time make use of the general result (2.115).

7.12 ( ) www Show that direct maximization of the log marginal likelihood (7.85) for
the regression relevance vector machine leads to the re-estimation equations (7.87)
and (7.88) where γi is deﬁned by (7.89).

7.13 ( )

In the evidence framework for RVM regression, we obtained the re-estimation
formulae (7.87) and (7.88) by maximizing the marginal likelihood given by (7.85).
Extend this approach by inclusion of hyperpriors given by gamma distributions of
the form (B.26) and obtain the corresponding re-estimation formulae for α and β by
maximizing the corresponding posterior probability p(t, α, β|X) with respect to α
and β.

7.14 ( ) Derive the result (7.90) for the predictive distribution in the relevance vector

machine for regression. Show that the predictive variance is given by (7.91).

7.15 ( ) www Using the results (7.94) and (7.95), show that the marginal likelihood
(7.85) can be written in the form (7.96), where λ(αn) is deﬁned by (7.97) and the
sparsity and quality factors are deﬁned by (7.98) and (7.99), respectively.

7.16 () By taking the second derivative of the log marginal likelihood (7.97) for the
regression RVM with respect to the hyperparameter αi, show that the stationary
point given by (7.101) is a maximum of the marginal likelihood.

7.17 ( ) Using (7.83) and (7.86), together with the matrix identity (C.7), show that
the quantities Sn and Qn deﬁned by (7.102) and (7.103) can be written in the form
(7.106) and (7.107).

7.18 () www Show that the gradient vector and Hessian matrix of the log poste-
rior distribution (7.109) for the classiﬁcation relevance vector machine are given by
(7.110) and (7.111).

7.19 ( ) Verify that maximization of the approximate log marginal likelihood function
(7.114) for the classiﬁcation relevance vector machine leads to the result (7.116) for
re-estimation of the hyperparameters.

8

Graphical
Models

Probabilities play a central role in modern pattern recognition. We have seen in
Chapter 1 that probability theory can be expressed in terms of two simple equations
corresponding to the sum rule and the product rule. All of the probabilistic infer-
ence and learning manipulations discussed in this book, no matter how complex,
amount to repeated application of these two equations. We could therefore proceed
to formulate and solve complicated probabilistic models purely by algebraic ma-
nipulation. However, we shall ﬁnd it highly advantageous to augment the analysis
using diagrammatic representations of probability distributions, called probabilistic
graphical models. These offer several useful properties:

1. They provide a simple way to visualize the structure of a probabilistic model

and can be used to design and motivate new models.

2. Insights into the properties of the model, including conditional independence

properties, can be obtained by inspection of the graph.

359

360

8. GRAPHICAL MODELS

3. Complex computations, required to perform inference and learning in sophis-
ticated models, can be expressed in terms of graphical manipulations, in which
underlying mathematical expressions are carried along implicitly.

A graph comprises nodes (also called vertices) connected by links (also known
as edges or arcs). In a probabilistic graphical model, each node represents a random
variable (or group of random variables), and the links express probabilistic relation-
ships between these variables. The graph then captures the way in which the joint
distribution over all of the random variables can be decomposed into a product of
factors each depending only on a subset of the variables. We shall begin by dis-
cussing Bayesian networks, also known as directed graphical models, in which the
links of the graphs have a particular directionality indicated by arrows. The other
major class of graphical models are Markov random ﬁelds, also known as undirected
graphical models, in which the links do not carry arrows and have no directional
signiﬁcance. Directed graphs are useful for expressing causal relationships between
random variables, whereas undirected graphs are better suited to expressing soft con-
straints between random variables. For the purposes of solving inference problems,
it is often convenient to convert both directed and undirected graphs into a different
representation called a factor graph.

In this chapter, we shall focus on the key aspects of graphical models as needed
for applications in pattern recognition and machine learning. More general treat-
ments of graphical models can be found in the books by Whittaker (1990), Lauritzen
(1996), Jensen (1996), Castillo et al. (1997), Jordan (1999), Cowell et al. (1999),
and Jordan (2007).

8.1. Bayesian Networks

In order to motivate the use of directed graphs to describe probability distributions,
consider ﬁrst an arbitrary joint distribution p(a, b, c) over three variables a, b, and c.
Note that at this stage, we do not need to specify anything further about these vari-
ables, such as whether they are discrete or continuous. Indeed, one of the powerful
aspects of graphical models is that a speciﬁc graph can make probabilistic statements
for a broad class of distributions. By application of the product rule of probability
(1.11), we can write the joint distribution in the form

p(a, b, c) = p(c|a, b)p(a, b).

(8.1)

A second application of the product rule, this time to the second term on the right-
hand side of (8.1), gives

p(a, b, c) = p(c|a, b)p(b|a)p(a).

(8.2)

Note that this decomposition holds for any choice of the joint distribution. We now
represent the right-hand side of (8.2) in terms of a simple graphical model as follows.
First we introduce a node for each of the random variables a, b, and c and associate
each node with the corresponding conditional distribution on the right-hand side of

Figure 8.1 A directed graphical model representing the joint probabil-
ity distribution over three variables a, b, and c, correspond-
ing to the decomposition on the right-hand side of (8.2).

a

b

8.1. Bayesian Networks

361

c

(8.2). Then, for each conditional distribution we add directed links (arrows) to the
graph from the nodes corresponding to the variables on which the distribution is
conditioned. Thus for the factor p(c|a, b), there will be links from nodes a and b to
node c, whereas for the factor p(a) there will be no incoming links. The result is the
graph shown in Figure 8.1. If there is a link going from a node a to a node b, then we
say that node a is the parent of node b, and we say that node b is the child of node a.
Note that we shall not make any formal distinction between a node and the variable
to which it corresponds but will simply use the same symbol to refer to both.

An interesting point to note about (8.2) is that the left-hand side is symmetrical
with respect to the three variables a, b, and c, whereas the right-hand side is not.
Indeed, in making the decomposition in (8.2), we have implicitly chosen a particular
ordering, namely a, b, c, and had we chosen a different ordering we would have
obtained a different decomposition and hence a different graphical representation.
We shall return to this point later.

For the moment let us extend the example of Figure 8.1 by considering the joint
distribution over K variables given by p(x1, . . . , xK). By repeated application of
the product rule of probability, this joint distribution can be written as a product of
conditional distributions, one for each of the variables

p(x1, . . . , xK) = p(xK|x1, . . . , xK−1) . . . p(x2|x1)p(x1).

(8.3)

For a given choice of K, we can again represent this as a directed graph having K
nodes, one for each conditional distribution on the right-hand side of (8.3), with each
node having incoming links from all lower numbered nodes. We say that this graph
is fully connected because there is a link between every pair of nodes.

So far, we have worked with completely general joint distributions, so that the
decompositions, and their representations as fully connected graphs, will be applica-
ble to any choice of distribution. As we shall see shortly, it is the absence of links
in the graph that conveys interesting information about the properties of the class of
distributions that the graph represents. Consider the graph shown in Figure 8.2. This
is not a fully connected graph because, for instance, there is no link from x1 to x2 or
from x3 to x7.

We shall now go from this graph to the corresponding representation of the joint
probability distribution written in terms of the product of a set of conditional dis-
tributions, one for each node in the graph. Each such conditional distribution will
be conditioned only on the parents of the corresponding node in the graph. For in-
stance, x5 will be conditioned on x1 and x3. The joint distribution of all 7 variables



K

is therefore given by

p(x1)p(x2)p(x3)p(x4|x1, x2, x3)p(x5|x1, x3)p(x6|x4)p(x7|x4, x5).

(8.4)

The reader should take a moment to study carefully the correspondence between
(8.4) and Figure 8.2.

We can now state in general terms the relationship between a given directed
graph and the corresponding distribution over the variables. The joint distribution
deﬁned by a graph is given by the product, over all of the nodes of the graph, of
a conditional distribution for each node conditioned on the variables corresponding
to the parents of that node in the graph. Thus, for a graph with K nodes, the joint
distribution is given by

362

8. GRAPHICAL MODELS

Figure 8.2 Example of a directed acyclic graph describing the joint
distribution over variables x1, . . . , x7. The corresponding
decomposition of the joint distribution is given by (8.4).

x1

x2

x4

x3

x5

x6

x7

Exercise 8.1

Exercise 8.2

p(x) =

p(xk|pak)

k=1

(8.5)

where pak denotes the set of parents of xk, and x = {x1, . . . , xK}. This key
equation expresses the factorization properties of the joint distribution for a directed
graphical model. Although we have considered each node to correspond to a single
variable, we can equally well associate sets of variables and vector-valued variables
with the nodes of a graph. It is easy to show that the representation on the right-
hand side of (8.5) is always correctly normalized provided the individual conditional
distributions are normalized.

The directed graphs that we are considering are subject to an important restric-
tion namely that there must be no directed cycles, in other words there are no closed
paths within the graph such that we can move from node to node along links follow-
ing the direction of the arrows and end up back at the starting node. Such graphs are
also called directed acyclic graphs, or DAGs. This is equivalent to the statement that
there exists an ordering of the nodes such that there are no links that go from any
node to any lower numbered node.

8.1.1 Example: Polynomial regression
As an illustration of the use of directed graphs to describe probability distri-
butions, we consider the Bayesian polynomial regression model introduced in Sec-




N

N

Figure 8.3 Directed graphical model representing the joint
distribution (8.6) corresponding to the Bayesian
polynomial regression model
introduced in Sec-
tion 1.2.6.

8.1. Bayesian Networks

w

t1

363

tN

(8.6)

tion 1.2.6. The random variables in this model are the vector of polynomial coefﬁ-
cients w and the observed data t = (t1, . . . , tN )T. In addition, this model contains
the input data x = (x1, . . . , xN )T, the noise variance σ2, and the hyperparameter α
representing the precision of the Gaussian prior over w, all of which are parameters
of the model rather than random variables. Focussing just on the random variables
for the moment, we see that the joint distribution is given by the product of the prior
p(w) and N conditional distributions p(tn|w) for n = 1, . . . , N so that

p(t, w) = p(w)

p(tn|w).

n=1

This joint distribution can be represented by a graphical model shown in Figure 8.3.

When we start to deal with more complex models later in the book, we shall ﬁnd
it inconvenient to have to write out multiple nodes of the form t1, . . . , tN explicitly as
in Figure 8.3. We therefore introduce a graphical notation that allows such multiple
nodes to be expressed more compactly, in which we draw a single representative
node tn and then surround this with a box, called a plate, labelled with N indicating
that there are N nodes of this kind. Re-writing the graph of Figure 8.3 in this way,
we obtain the graph shown in Figure 8.4.

We shall sometimes ﬁnd it helpful to make the parameters of a model, as well as

its stochastic variables, explicit. In this case, (8.6) becomes

p(t, w|x, α, σ2) = p(w|α)

p(tn|w, xn, σ2).

n=1

Correspondingly, we can make x and α explicit in the graphical representation. To
do this, we shall adopt the convention that random variables will be denoted by open
circles, and deterministic parameters will be denoted by smaller solid circles. If we
take the graph of Figure 8.4 and include the deterministic parameters, we obtain the
graph shown in Figure 8.5.

When we apply a graphical model to a problem in machine learning or pattern
recognition, we will typically set some of the random variables to speciﬁc observed

Figure 8.4 An alternative, more compact, representation of the graph
shown in Figure 8.3 in which we have introduced a plate
(the box labelled N) that represents N nodes of which only
a single example tn is shown explicitly.

tn

N

w












N



N

 





364

8. GRAPHICAL MODELS

Figure 8.5 This shows the same model as in Figure 8.4 but
with the deterministic parameters shown explicitly
by the smaller solid nodes.

xn

α

w

N

σ2

tn

values, for example the variables {tn} from the training set in the case of polynomial
curve ﬁtting. In a graphical model, we will denote such observed variables by shad-
ing the corresponding nodes. Thus the graph corresponding to Figure 8.5 in which
the variables {tn} are observed is shown in Figure 8.6. Note that the value of w is
not observed, and so w is an example of a latent variable, also known as a hidden
variable. Such variables play a crucial role in many probabilistic models and will
form the focus of Chapters 9 and 12.

Having observed the values {tn} we can, if desired, evaluate the posterior dis-
tribution of the polynomial coefﬁcients w as discussed in Section 1.2.5. For the
moment, we note that this involves a straightforward application of Bayes’ theorem

p(w|T) ∝ p(w)

p(tn|w)

n=1

(8.7)

where again we have omitted the deterministic parameters in order to keep the nota-
tion uncluttered.

In general, model parameters such as w are of little direct interest in themselves,
because our ultimate goal is to make predictions for new input values. Suppose we
x and we wish to ﬁnd the corresponding probability dis-
are given a new input value
t conditioned on the observed data. The graphical model that describes
tribution for
this problem is shown in Figure 8.7, and the corresponding joint distribution of all
of the random variables in this model, conditioned on the deterministic parameters,
is then given by

p(

t, t, w|

x, x, α, σ2) =

p(tn|xn, w, σ2)

p(w|α)p(

n=1

x, w, σ2).
t|

(8.8)

Figure 8.6 As in Figure 8.5 but with the nodes {tn} shaded
to indicate that
the corresponding random vari-
ables have been set to their observed (training set)
values.

xn

α

w

σ2

tn

N

b










b












tn



Figure 8.7 The polynomial regression model, corresponding
to Figure 8.6, showing also a new input value
x
together with the corresponding model prediction
t.

xn

8.1. Bayesian Networks

α

w

ˆt

N

σ2

365

ˆx

The required predictive distribution for
probability, by integrating out the model parameters w so that

t is then obtained, from the sum rule of

p(

t|

x, x, t, α, σ2) ∝

p(

t, t, w|

x, x, α, σ2) dw

where we are implicitly setting the random variables in t to the speciﬁc values ob-
served in the data set. The details of this calculation were discussed in Chapter 3.

8.1.2 Generative models
There are many situations in which we wish to draw samples from a given prob-
ability distribution. Although we shall devote the whole of Chapter 11 to a detailed
discussion of sampling methods, it is instructive to outline here one technique, called
ancestral sampling, which is particularly relevant to graphical models. Consider a
joint distribution p(x1, . . . , xK) over K variables that factorizes according to (8.5)
corresponding to a directed acyclic graph. We shall suppose that the variables have
been ordered such that there are no links from any node to any lower numbered node,
in other words each node has a higher number than any of its parents. Our goal is to
draw a sample

xK from the joint distribution.

x1, . . . ,

To do this, we start with the lowest-numbered node and draw a sample from the
distribution p(x1), which we call
x1. We then work through each of the nodes in or-
der, so that for node n we draw a sample from the conditional distribution p(xn|pan)
in which the parent variables have been set to their sampled values. Note that at each
stage, these parent values will always be available because they correspond to lower-
numbered nodes that have already been sampled. Techniques for sampling from
speciﬁc distributions will be discussed in detail in Chapter 11. Once we have sam-
pled from the ﬁnal variable xK, we will have achieved our objective of obtaining a
sample from the joint distribution. To obtain a sample from some marginal distribu-
tion corresponding to a subset of the variables, we simply take the sampled values
for the required nodes and ignore the sampled values for the remaining nodes. For
example, to draw a sample from the distribution p(x2, x4), we simply sample from
x4 and discard the remaining
the full joint distribution and then retain the values
values {

xj=2,4}.

x2,

366

8. GRAPHICAL MODELS

Figure 8.8 A graphical model representing the process by which
images of objects are created, in which the identity
of an object (a discrete variable) and the position and
orientation of that object (continuous variables) have
independent prior probabilities. The image (a vector
of pixel intensities) has a probability distribution that
is dependent on the identity of the object as well as
on its position and orientation.

Object

Position

Orientation

Image

For practical applications of probabilistic models, it will typically be the higher-
numbered variables corresponding to terminal nodes of the graph that represent the
observations, with lower-numbered nodes corresponding to latent variables. The
primary role of the latent variables is to allow a complicated distribution over the
observed variables to be represented in terms of a model constructed from simpler
(typically exponential family) conditional distributions.

We can interpret such models as expressing the processes by which the observed
data arose. For instance, consider an object recognition task in which each observed
data point corresponds to an image (comprising a vector of pixel intensities) of one
of the objects. In this case, the latent variables might have an interpretation as the
position and orientation of the object. Given a particular observed image, our goal is
to ﬁnd the posterior distribution over objects, in which we integrate over all possible
positions and orientations. We can represent this problem using a graphical model
of the form show in Figure 8.8.

The graphical model captures the causal process (Pearl, 1988) by which the ob-
served data was generated. For this reason, such models are often called generative
models. By contrast, the polynomial regression model described by Figure 8.5 is
not generative because there is no probability distribution associated with the input
variable x, and so it is not possible to generate synthetic data points from this model.
We could make it generative by introducing a suitable prior distribution p(x), at the
expense of a more complex model.

The hidden variables in a probabilistic model need not, however, have any ex-
plicit physical interpretation but may be introduced simply to allow a more complex
joint distribution to be constructed from simpler components.
In either case, the
technique of ancestral sampling applied to a generative model mimics the creation
of the observed data and would therefore give rise to ‘fantasy’ data whose probability
distribution (if the model were a perfect representation of reality) would be the same
as that of the observed data. In practice, producing synthetic observations from a
generative model can prove informative in understanding the form of the probability
distribution represented by that model.

8.1.3 Discrete variables
We have discussed the importance of probability distributions that are members
of the exponential family, and we have seen that this family includes many well-
known distributions as particular cases. Although such distributions are relatively
simple, they form useful building blocks for constructing more complex probability

Section 2.4







k=1

K

K

K



.



Figure 8.9 (a) This fully-connected graph describes a general distribu-
tion over two K-state discrete variables having a total of
K 2 − 1 parameters.
(b) By dropping the link between the
nodes, the number of parameters is reduced to 2(K − 1).

x1

x1

(a)

(b)

8.1. Bayesian Networks

367

x2

x2

distributions, and the framework of graphical models is very useful in expressing the
way in which these building blocks are linked together.

Such models have particularly nice properties if we choose the relationship be-
tween each parent-child pair in a directed graph to be conjugate, and we shall ex-
plore several examples of this shortly. Two cases are particularly worthy of note,
namely when the parent and child node each correspond to discrete variables and
when they each correspond to Gaussian variables, because in these two cases the
relationship can be extended hierarchically to construct arbitrarily complex directed
acyclic graphs. We begin by examining the discrete case.

The probability distribution p(x|µ) for a single discrete variable x having K

possible states (using the 1-of-K representation) is given by

p(x|µ) =

µxk
k

(8.9)

distribution.

and is governed by the parameters µ = (µ1, . . . , µK)T. Due to the constraint
k µk = 1, only K − 1 values for µk need to be speciﬁed in order to deﬁne the
Now suppose that we have two discrete variables, x1 and x2, each of which has
K states, and we wish to model their joint distribution. We denote the probability of
observing both x1k = 1 and x2l = 1 by the parameter µkl, where x1k denotes the
kth component of x1, and similarly for x2l. The joint distribution can be written

p(x1, x2|µ) =

k=1

l=1

µx1kx2l
kl

k

l µkl = 1, this distri-
Because the parameters µkl are subject to the constraint
bution is governed by K 2 − 1 parameters. It is easily seen that the total number of
parameters that must be speciﬁed for an arbitrary joint distribution over M variables
is KM − 1 and therefore grows exponentially with the number M of variables.
Using the product rule, we can factor the joint distribution p(x1, x2) in the form
p(x2|x1)p(x1), which corresponds to a two-node graph with a link going from the
x1 node to the x2 node as shown in Figure 8.9(a). The marginal distribution p(x1)
is governed by K − 1 parameters, as before, Similarly, the conditional distribution
p(x2|x1) requires the speciﬁcation of K − 1 parameters for each of the K possible
values of x1. The total number of parameters that must be speciﬁed in the joint
distribution is therefore (K − 1) + K(K − 1) = K 2 − 1 as before.
Now suppose that the variables x1 and x2 were independent, corresponding to
the graphical model shown in Figure 8.9(b). Each variable is then described by

368

8. GRAPHICAL MODELS

Figure 8.10 This chain of M discrete nodes, each
having K states, requires the speciﬁcation of K − 1 +
(M − 1)K(K − 1) parameters, which grows linearly
with the length M of the chain. In contrast, a fully con-
nected graph of M nodes would have KM − 1 param-
eters, which grows exponentially with M.

x1

x2

xM

a separate multinomial distribution, and the total number of parameters would be
2(K − 1). For a distribution over M independent discrete variables, each having K
states, the total number of parameters would be M(K − 1), which therefore grows
linearly with the number of variables. From a graphical perspective, we have reduced
the number of parameters by dropping links in the graph, at the expense of having a
restricted class of distributions.

More generally, if we have M discrete variables x1, . . . , xM , we can model
the joint distribution using a directed graph with one variable corresponding to each
node. The conditional distribution at each node is given by a set of nonnegative pa-
rameters subject to the usual normalization constraint. If the graph is fully connected
then we have a completely general distribution having KM − 1 parameters, whereas
if there are no links in the graph the joint distribution factorizes into the product of
the marginals, and the total number of parameters is M(K − 1). Graphs having in-
termediate levels of connectivity allow for more general distributions than the fully
factorized one while requiring fewer parameters than the general joint distribution.
As an illustration, consider the chain of nodes shown in Figure 8.10. The marginal
distribution p(x1) requires K − 1 parameters, whereas each of the M − 1 condi-
tional distributions p(xi|xi−1), for i = 2, . . . , M, requires K(K − 1) parameters.
This gives a total parameter count of K − 1 + (M − 1)K(K − 1), which is quadratic
in K and which grows linearly (rather than exponentially) with the length M of the
chain.

An alternative way to reduce the number of independent parameters in a model
is by sharing parameters (also known as tying of parameters). For instance, in the
chain example of Figure 8.10, we can arrange that all of the conditional distributions
p(xi|xi−1), for i = 2, . . . , M, are governed by the same set of K(K−1) parameters.
Together with the K−1 parameters governing the distribution of x1, this gives a total
of K 2 − 1 parameters that must be speciﬁed in order to deﬁne the joint distribution.
We can turn a graph over discrete variables into a Bayesian model by introduc-
ing Dirichlet priors for the parameters. From a graphical point of view, each node
then acquires an additional parent representing the Dirichlet distribution over the pa-
rameters associated with the corresponding discrete node. This is illustrated for the
chain model in Figure 8.11. The corresponding model in which we tie the parame-
ters governing the conditional distributions p(xi|xi−1), for i = 2, . . . , M, is shown
in Figure 8.12.
Another way of controlling the exponential growth in the number of parameters
in models of discrete variables is to use parameterized models for the conditional
distributions instead of complete tables of conditional probability values. To illus-
trate this idea, consider the graph in Figure 8.13 in which all of the nodes represent
binary variables. Each of the parent variables xi is governed by a single parame-

Figure 8.11 An extension of the model of
Figure 8.10 to include Dirich-
let priors over
the param-
eters governing the discrete
distributions.

Figure 8.12 As in Figure 8.11 but with a sin-
gle set of parameters µ shared
amongst all of the conditional
distributions p(xi|xi−1).

µ1

x1

µ1

x1

8.1. Bayesian Networks

µ

x2

369

µM

xM

xM





M

µ2

x2



ter µi representing the probability p(xi = 1), giving M parameters in total for the
parent nodes. The conditional distribution p(y|x1, . . . , xM ), however, would require
2M parameters representing the probability p(y = 1) for each of the 2M possible
settings of the parent variables. Thus in general the number of parameters required
to specify this conditional distribution will grow exponentially with M. We can ob-
tain a more parsimonious form for the conditional distribution by using a logistic
sigmoid function acting on a linear combination of the parent variables, giving

Section 2.4

p(y = 1|x1, . . . , xM ) = σ

w0 +

wixi

= σ(wTx)

(8.10)

i=1

where σ(a) = (1+exp(−a))−1 is the logistic sigmoid, x = (x0, x1, . . . , xM )T is an
(M + 1)-dimensional vector of parent states augmented with an additional variable
x0 whose value is clamped to 1, and w = (w0, w1, . . . , wM )T is a vector of M + 1
parameters. This is a more restricted form of conditional distribution than the general
case but is now governed by a number of parameters that grows linearly with M. In
this sense, it is analogous to the choice of a restrictive form of covariance matrix (for
example, a diagonal matrix) in a multivariate Gaussian distribution. The motivation
for the logistic sigmoid representation was discussed in Section 4.2.

Figure 8.13 A graph comprising M parents x1, . . . , xM and a sin-
gle child y, used to illustrate the idea of parameterized
conditional distributions for discrete variables.

x1

xM

y

⎞⎠
⎞⎠2




i=1

D

D




j∈pai

⎛⎝
⎛⎝



xi

j∈pai

370

8. GRAPHICAL MODELS

8.1.4 Linear-Gaussian models
In the previous section, we saw how to construct joint probability distributions
over a set of discrete variables by expressing the variables as nodes in a directed
acyclic graph. Here we show how a multivariate Gaussian can be expressed as a
directed graph corresponding to a linear-Gaussian model over the component vari-
ables. This allows us to impose interesting structure on the distribution, with the
general Gaussian and the diagonal covariance Gaussian representing opposite ex-
tremes. Several widely used techniques are examples of linear-Gaussian models,
such as probabilistic principal component analysis, factor analysis, and linear dy-
namical systems (Roweis and Ghahramani, 1999). We shall make extensive use of
the results of this section in later chapters when we consider some of these techniques
in detail.

Consider an arbitrary directed acyclic graph over D variables in which node i
represents a single continuous random variable xi having a Gaussian distribution.
The mean of this distribution is taken to be a linear combination of the states of its
parent nodes pai of node i

p(xi|pai) = N

wijxj + bi, vi

(8.11)

where wij and bi are parameters governing the mean, and vi is the variance of the
conditional distribution for xi. The log of the joint distribution is then the log of the
product of these conditionals over all nodes in the graph and hence takes the form

ln p(x) =

ln p(xi|pai)

(8.12)

= −

i=1

1
2vi

xi −

wijxj − bi

j∈pai

+ const

(8.13)

where x = (x1, . . . , xD)T and ‘const’ denotes terms independent of x. We see that
this is a quadratic function of the components of x, and hence the joint distribution
p(x) is a multivariate Gaussian.

We can determine the mean and covariance of the joint distribution recursively
as follows. Each variable xi has (conditional on the states of its parents) a Gaussian
distribution of the form (8.11) and so

xi =

wijxj + bi + √vi	i

(8.14)

where 	i is a zero mean, unit variance Gaussian random variable satisfying E[	i] = 0
and E[	i	j] = Iij, where Iij is the i, j element of the identity matrix. Taking the
expectation of (8.14), we have

E[xi] =

wijE[xj] + bi.

(8.15)

j∈pai

⎧⎨⎩

⎡⎣




⎫⎬⎭⎤⎦


Figure 8.14 A directed graph over three Gaussian variables,

x1

x2

with one missing link.

8.1. Bayesian Networks

371

x3

Thus we can ﬁnd the components of E[x] = (E[x1], . . . , E[xD])T by starting at the
lowest numbered node and working recursively through the graph (here we again
assume that the nodes are numbered such that each node has a higher number than
its parents). Similarly, we can use (8.14) and (8.15) to obtain the i, j element of the
covariance matrix for p(x) in the form of a recursion relation

cov[xi, xj] = E [(xi − E[xi])(xj − E[xj])]

= E

(xi − E[xi])

wjk(xk − E[xk]) + √vj	j

k∈paj

=

wjkcov[xi, xk] + Iijvj

k∈paj

(8.16)

and so the covariance can similarly be evaluated recursively starting from the lowest
numbered node.

Let us consider two extreme cases. First of all, suppose that there are no links
in the graph, which therefore comprises D isolated nodes. In this case, there are no
parameters wij and so there are just D parameters bi and D parameters vi. From
the recursion relations (8.15) and (8.16), we see that the mean of p(x) is given by
(b1, . . . , bD)T and the covariance matrix is diagonal of the form diag(v1, . . . , vD).
The joint distribution has a total of 2D parameters and represents a set of D inde-
pendent univariate Gaussian distributions.

Now consider a fully connected graph in which each node has all lower num-
bered nodes as parents. The matrix wij then has i − 1 entries on the ith row and
hence is a lower triangular matrix (with no entries on the leading diagonal). Then
the total number of parameters wij is obtained by taking the number D2 of elements
in a D× D matrix, subtracting D to account for the absence of elements on the lead-
ing diagonal, and then dividing by 2 because the matrix has elements only below the
diagonal, giving a total of D(D−1)/2. The total number of independent parameters
{wij} and {vi} in the covariance matrix is therefore D(D + 1)/2 corresponding to
a general symmetric covariance matrix.
Graphs having some intermediate level of complexity correspond to joint Gaus-
sian distributions with partially constrained covariance matrices. Consider for ex-
ample the graph shown in Figure 8.14, which has a link missing between variables
x1 and x3. Using the recursion relations (8.15) and (8.16), we see that the mean and
covariance of the joint distribution are given by

µ = (b1, b2 + w21b1, b3 + w32b2 + w32w21b1)T

Σ =

v1

w21v1
v2 + w2

w21v1

21v1
w32w21v1 w32(v2 + w2

w32w21v1
w32(v2 + w2

21v1)
32(v2 + w2

21v1)

21v1) v3 + w2

(8.17)

. (8.18)

Section 2.3

Exercise 8.7

⎛⎝



⎞⎠

372

8. GRAPHICAL MODELS

We can readily extend the linear-Gaussian graphical model to the case in which
the nodes of the graph represent multivariate Gaussian variables. In this case, we can
write the conditional distribution for node i in the form

p(xi|pai) = N

xi

j∈pai

Wijxj + bi, Σi

(8.19)

Section 2.3.6

where now Wij is a matrix (which is nonsquare if xi and xj have different dimen-
sionalities). Again it is easy to verify that the joint distribution over all variables is
Gaussian.

Note that we have already encountered a speciﬁc example of the linear-Gaussian
relationship when we saw that the conjugate prior for the mean µ of a Gaussian
variable x is itself a Gaussian distribution over µ. The joint distribution over x and
µ is therefore Gaussian. This corresponds to a simple two-node graph in which
the node representing µ is the parent of the node representing x. The mean of the
distribution over µ is a parameter controlling a prior, and so it can be viewed as a
hyperparameter. Because the value of this hyperparameter may itself be unknown,
we can again treat it from a Bayesian perspective by introducing a prior over the
hyperparameter, sometimes called a hyperprior, which is again given by a Gaussian
distribution. This type of construction can be extended in principle to any level and is
an illustration of a hierarchical Bayesian model, of which we shall encounter further
examples in later chapters.

8.2. Conditional Independence

An important concept for probability distributions over multiple variables is that of
conditional independence (Dawid, 1980). Consider three variables a, b, and c, and
suppose that the conditional distribution of a, given b and c, is such that it does not
depend on the value of b, so that

p(a|b, c) = p(a|c).

(8.20)

We say that a is conditionally independent of b given c. This can be expressed in a
slightly different way if we consider the joint distribution of a and b conditioned on
c, which we can write in the form

p(a, b|c) = p(a|b, c)p(b|c)
= p(a|c)p(b|c).

(8.21)

where we have used the product rule of probability together with (8.20). Thus we
see that, conditioned on c, the joint distribution of a and b factorizes into the prod-
uct of the marginal distribution of a and the marginal distribution of b (again both
conditioned on c). This says that the variables a and b are statistically independent,
given c. Note that our deﬁnition of conditional independence will require that (8.20),

Figure 8.15 The ﬁrst of three examples of graphs over three variables
a, b, and c used to discuss conditional independence
properties of directed graphical models.

8.2. Conditional Independence

373

c

a

b



or equivalently (8.21), must hold for every possible value of c, and not just for some
values. We shall sometimes use a shorthand notation for conditional independence
(Dawid, 1979) in which

a ⊥⊥ b | c

(8.22)

denotes that a is conditionally independent of b given c and is equivalent to (8.20).

Conditional independence properties play an important role in using probabilis-
tic models for pattern recognition by simplifying both the structure of a model and
the computations needed to perform inference and learning under that model. We
shall see examples of this shortly.

If we are given an expression for the joint distribution over a set of variables in
terms of a product of conditional distributions (i.e., the mathematical representation
underlying a directed graph), then we could in principle test whether any poten-
tial conditional independence property holds by repeated application of the sum and
product rules of probability. In practice, such an approach would be very time con-
suming. An important and elegant feature of graphical models is that conditional
independence properties of the joint distribution can be read directly from the graph
without having to perform any analytical manipulations. The general framework
for achieving this is called d-separation, where the ‘d’ stands for ‘directed’ (Pearl,
1988). Here we shall motivate the concept of d-separation and give a general state-
ment of the d-separation criterion. A formal proof can be found in Lauritzen (1996).

8.2.1 Three example graphs
We begin our discussion of the conditional independence properties of directed
graphs by considering three simple examples each involving graphs having just three
nodes. Together, these will motivate and illustrate the key concepts of d-separation.
The ﬁrst of the three examples is shown in Figure 8.15, and the joint distribution
corresponding to this graph is easily written down using the general result (8.5) to
give

(8.23)
If none of the variables are observed, then we can investigate whether a and b are
independent by marginalizing both sides of (8.23) with respect to c to give

p(a, b, c) = p(a|c)p(b|c)p(c).

p(a, b) =

p(a|c)p(b|c)p(c).

c

In general, this does not factorize into the product p(a)p(b), and so

a ⊥⊥ b | ∅

(8.24)

(8.25)



where ∅ denotes the empty set, and the symbol ⊥⊥ means that the conditional inde-
pendence property does not hold in general. Of course, it may hold for a particular
distribution by virtue of the speciﬁc numerical values associated with the various
conditional probabilities, but it does not follow in general from the structure of the
graph.

Now suppose we condition on the variable c, as represented by the graph of
Figure 8.16. From (8.23), we can easily write down the conditional distribution of a
and b, given c, in the form

p(a, b|c) = p(a, b, c)
= p(a|c)p(b|c)
and so we obtain the conditional independence property

p(c)

a ⊥⊥ b | c.

We can provide a simple graphical interpretation of this result by considering
the path from node a to node b via c. The node c is said to be tail-to-tail with re-
spect to this path because the node is connected to the tails of the two arrows, and
the presence of such a path connecting nodes a and b causes these nodes to be de-
pendent. However, when we condition on node c, as in Figure 8.16, the conditioned
node ‘blocks’ the path from a to b and causes a and b to become (conditionally)
independent.

We can similarly consider the graph shown in Figure 8.17. The joint distribution
corresponding to this graph is again obtained from our general formula (8.5) to give

p(a, b, c) = p(a)p(c|a)p(b|c).

(8.26)

First of all, suppose that none of the variables are observed. Again, we can test to
see if a and b are independent by marginalizing over c to give

374

8. GRAPHICAL MODELS

Figure 8.16 As in Figure 8.15 but where we have conditioned on the

value of variable c.

c

a

b

p(a, b) = p(a)

p(c|a)p(b|c) = p(a)p(b|a).

c

Figure 8.17 The second of our three examples of 3-node
graphs used to motivate the conditional indepen-
dence framework for directed graphical models.

a

c

b

Figure 8.18 As in Figure 8.17 but now conditioning on node c.

a

c

8.2. Conditional Independence

which in general does not factorize into p(a)p(b), and so

a ⊥⊥ b | ∅

375

b

(8.27)

as before.

Now suppose we condition on node c, as shown in Figure 8.18. Using Bayes’

theorem, together with (8.26), we obtain

p(a, b|c) = p(a, b, c)

p(c)

= p(a)p(c|a)p(b|c)
p(c)
= p(a|c)p(b|c)

and so again we obtain the conditional independence property

a ⊥⊥ b | c.

As before, we can interpret these results graphically. The node c is said to be
head-to-tail with respect to the path from node a to node b. Such a path connects
nodes a and b and renders them dependent. If we now observe c, as in Figure 8.18,
then this observation ‘blocks’ the path from a to b and so we obtain the conditional
independence property a ⊥⊥ b | c.
Finally, we consider the third of our 3-node examples, shown by the graph in
Figure 8.19. As we shall see, this has a more subtle behaviour than the two previous
graphs.

The joint distribution can again be written down using our general result (8.5) to

give

p(a, b, c) = p(a)p(b)p(c|a, b).

(8.28)

Consider ﬁrst the case where none of the variables are observed. Marginalizing both
sides of (8.28) over c we obtain

p(a, b) = p(a)p(b)

Figure 8.19 The last of our three examples of 3-node graphs used to
explore conditional independence properties in graphi-
cal models. This graph has rather different properties
from the two previous examples.

a

b

c

376

8. GRAPHICAL MODELS

Figure 8.20 As in Figure 8.19 but conditioning on the value of node
c. In this graph, the act of conditioning induces a depen-
dence between a and b.

a

b

c

and so a and b are independent with no variables observed, in contrast to the two
previous examples. We can write this result as

Now suppose we condition on c, as indicated in Figure 8.20. The conditional distri-
bution of a and b is then given by

a ⊥⊥ b | ∅.

(8.29)

p(a, b|c) = p(a, b, c)

p(c)

= p(a)p(b)p(c|a, b)

p(c)

which in general does not factorize into the product p(a)p(b), and so

a ⊥⊥ b | c.

Thus our third example has the opposite behaviour from the ﬁrst two. Graphically,
we say that node c is head-to-head with respect to the path from a to b because it
connects to the heads of the two arrows. When node c is unobserved, it ‘blocks’
the path, and the variables a and b are independent. However, conditioning on c
‘unblocks’ the path and renders a and b dependent.

There is one more subtlety associated with this third example that we need to
consider. First we introduce some more terminology. We say that node y is a de-
scendant of node x if there is a path from x to y in which each step of the path
follows the directions of the arrows. Then it can be shown that a head-to-head path
will become unblocked if either the node, or any of its descendants, is observed.

In summary, a tail-to-tail node or a head-to-tail node leaves a path unblocked
unless it is observed in which case it blocks the path. By contrast, a head-to-head
node blocks a path if it is unobserved, but once the node, and/or at least one of its
descendants, is observed the path becomes unblocked.

It is worth spending a moment to understand further the unusual behaviour of the
graph of Figure 8.20. Consider a particular instance of such a graph corresponding
to a problem with three binary random variables relating to the fuel system on a car,
as shown in Figure 8.21. The variables are called B, representing the state of a
battery that is either charged (B = 1) or ﬂat (B = 0), F representing the state of
the fuel tank that is either full of fuel (F = 1) or empty (F = 0), and G, which is
the state of an electric fuel gauge and which indicates either full (G = 1) or empty

Exercise 8.10

B






Given the state of the fuel tank and the battery, the fuel gauge reads full with proba-
bilities given by

p(B = 1) = 0.9
p(F = 1) = 0.9.

p(G = 1|B = 1, F = 1) = 0.8
p(G = 1|B = 1, F = 0) = 0.2
p(G = 1|B = 0, F = 1) = 0.2
p(G = 1|B = 0, F = 0) = 0.1

B

F

G

8.2. Conditional Independence

377

F

B

F

G

G

Figure 8.21 An example of a 3-node graph used to illustrate the phenomenon of ‘explaining away’. The three
nodes represent the state of the battery (B), the state of the fuel tank (F ) and the reading on the electric fuel
gauge (G). See the text for details.

(G = 0). The battery is either charged or ﬂat, and independently the fuel tank is
either full or empty, with prior probabilities

so this is a rather unreliable fuel gauge! All remaining probabilities are determined
by the requirement that probabilities sum to one, and so we have a complete speciﬁ-
cation of the probabilistic model.

Before we observe any data, the prior probability of the fuel tank being empty
is p(F = 0) = 0.1. Now suppose that we observe the fuel gauge and discover that
it reads empty, i.e., G = 0, corresponding to the middle graph in Figure 8.21. We
can use Bayes’ theorem to evaluate the posterior probability of the fuel tank being
empty. First we evaluate the denominator for Bayes’ theorem given by

p(G = 0) =

B∈{0,1}

and similarly we evaluate

F∈{0,1}

p(G = 0|B, F )p(B)p(F ) = 0.315

(8.30)

p(G = 0|F = 0) =

B∈{0,1}

and using these results we have

p(G = 0|B, F = 0)p(B) = 0.81

(8.31)

p(F = 0|G = 0) = p(G = 0|F = 0)p(F = 0)

p(G = 0)

 0.257

(8.32)



and so p(F = 0|G = 0) > p(F = 0). Thus observing that the gauge reads empty
makes it more likely that the tank is indeed empty, as we would intuitively expect.
Next suppose that we also check the state of the battery and ﬁnd that it is ﬂat, i.e.,
B = 0. We have now observed the states of both the fuel gauge and the battery, as
shown by the right-hand graph in Figure 8.21. The posterior probability that the fuel
tank is empty given the observations of both the fuel gauge and the battery state is
then given by

p(F = 0|G = 0, B = 0) = p(G = 0|B = 0, F = 0)p(F = 0)

p(G = 0|B = 0, F )p(F )  0.111 (8.33)

378

8. GRAPHICAL MODELS

F∈{0,1}

where the prior probability p(B = 0) has cancelled between numerator and denom-
inator. Thus the probability that the tank is empty has decreased (from 0.257 to
0.111) as a result of the observation of the state of the battery. This accords with our
intuition that ﬁnding out that the battery is ﬂat explains away the observation that the
fuel gauge reads empty. We see that the state of the fuel tank and that of the battery
have indeed become dependent on each other as a result of observing the reading
on the fuel gauge. In fact, this would also be the case if, instead of observing the
fuel gauge directly, we observed the state of some descendant of G. Note that the
probability p(F = 0|G = 0, B = 0)  0.111 is greater than the prior probability
p(F = 0) = 0.1 because the observation that the fuel gauge reads zero still provides
some evidence in favour of an empty fuel tank.

8.2.2 D-separation
We now give a general statement of the d-separation property (Pearl, 1988) for
directed graphs. Consider a general directed graph in which A, B, and C are arbi-
trary nonintersecting sets of nodes (whose union may be smaller than the complete
set of nodes in the graph). We wish to ascertain whether a particular conditional
independence statement A ⊥⊥ B | C is implied by a given directed acyclic graph. To
do so, we consider all possible paths from any node in A to any node in B. Any such
path is said to be blocked if it includes a node such that either

(a) the arrows on the path meet either head-to-tail or tail-to-tail at the node, and the

node is in the set C, or

(b) the arrows meet head-to-head at the node, and neither the node, nor any of its

descendants, is in the set C.

If all paths are blocked, then A is said to be d-separated from B by C, and the joint
distribution over all of the variables in the graph will satisfy A ⊥⊥ B | C.
The concept of d-separation is illustrated in Figure 8.22. In graph (a), the path
from a to b is not blocked by node f because it is a tail-to-tail node for this path
and is not observed, nor is it blocked by node e because, although the latter is a
head-to-head node, it has a descendant c because is in the conditioning set. Thus
the conditional independence statement a ⊥⊥ b | c does not follow from this graph.
In graph (b), the path from a to b is blocked by node f because this is a tail-to-tail
node that is observed, and so the conditional independence property a ⊥⊥ b | f will

8.2. Conditional Independence

379

Figure 8.22 Illustration of the con-
cept of d-separation. See the text for
details.

a

f

(a)

e

c

a

f

b

e

c

(b)



b

N

Section 2.3

be satisﬁed by any distribution that factorizes according to this graph. Note that this
path is also blocked by node e because e is a head-to-head node and neither it nor its
descendant are in the conditioning set.

For the purposes of d-separation, parameters such as α and σ2 in Figure 8.5,
indicated by small ﬁlled circles, behave in the same was as observed nodes. How-
ever, there are no marginal distributions associated with such nodes. Consequently
parameter nodes never themselves have parents and so all paths through these nodes
will always be tail-to-tail and hence blocked. Consequently they play no role in
d-separation.

Another example of conditional independence and d-separation is provided by
the concept of i.i.d. (independent identically distributed) data introduced in Sec-
tion 1.2.4. Consider the problem of ﬁnding the posterior distribution for the mean
of a univariate Gaussian distribution. This can be represented by the directed graph
shown in Figure 8.23 in which the joint distribution is deﬁned by a prior p(µ) to-
gether with a set of conditional distributions p(xn|µ) for n = 1, . . . , N. In practice,
we observe D = {x1, . . . , xN} and our goal is to infer µ. Suppose, for a moment,
that we condition on µ and consider the joint distribution of the observations. Using
d-separation, we note that there is a unique path from any xi to any other xj=i and
that this path is tail-to-tail with respect to the observed node µ. Every such path is
blocked and so the observations D = {x1, . . . , xN} are independent given µ, so that

Figure 8.23 (a) Directed graph corre-
sponding to the problem
of inferring the mean µ of
a univariate Gaussian dis-
tribution from observations
x1, . . . , xN .
(b) The same
graph drawn using the plate
notation.

p(D|µ) =

p(xn|µ).

n=1

µ

(8.34)

µ

x1

xN

xn

(a)

N

(b)

N










N







380

8. GRAPHICAL MODELS

Figure 8.24 A graphical representation of the ‘naive Bayes’
model
Conditioned on the
class label z,
the components of the observed
vector x = (x1, . . . , xD)T are assumed to be
independent.

for classiﬁcation.

x1

z

xD

Section 3.3

However, if we integrate over µ, the observations are in general no longer indepen-
dent

p(D) =

∞

0

p(D|µ)p(µ) dµ =

p(xn).

n=1

(8.35)

Here µ is a latent variable, because its value is not observed.

Another example of a model representing i.i.d. data is the graph in Figure 8.7
corresponding to Bayesian polynomial regression. Here the stochastic nodes corre-
t. We see that the node for w is tail-to-tail with respect to
spond to {tn}, w and
the path from
t to any one of the nodes tn and so we have the following conditional
independence property

t ⊥⊥ tn | w.

(8.36)
Thus, conditioned on the polynomial coefﬁcients w, the predictive distribution for
t is independent of the training data {t1, . . . , tN}. We can therefore ﬁrst use the
training data to determine the posterior distribution over the coefﬁcients w and then
we can discard the training data and use the posterior distribution for w to make
predictions of

t for new input observations

x.

A related graphical structure arises in an approach to classiﬁcation called the
naive Bayes model, in which we use conditional independence assumptions to sim-
plify the model structure. Suppose our observed variable consists of a D-dimensional
vector x = (x1, . . . , xD)T, and we wish to assign observed values of x to one of K
classes. Using the 1-of-K encoding scheme, we can represent these classes by a K-
dimensional binary vector z. We can then deﬁne a generative model by introducing
a multinomial prior p(z|µ) over the class labels, where the kth component µk of µ
is the prior probability of class Ck, together with a conditional distribution p(x|z)
for the observed vector x. The key assumption of the naive Bayes model is that,
conditioned on the class z, the distributions of the input variables x1, . . . , xD are in-
dependent. The graphical representation of this model is shown in Figure 8.24. We
see that observation of z blocks the path between xi and xj for j = i (because such
paths are tail-to-tail at the node z) and so xi and xj are conditionally independent
given z. If, however, we marginalize out z (so that z is unobserved) the tail-to-tail
path from xi to xj is no longer blocked. This tells us that in general the marginal
density p(x) will not factorize with respect to the components of x. We encountered
a simple application of the naive Bayes model in the context of fusing data from
different sources for medical diagnosis in Section 1.5.

If we are given a labelled training set, comprising inputs {x1, . . . , xN} together
with their class labels, then we can ﬁt the naive Bayes model to the training data

8.2. Conditional Independence

381

using maximum likelihood assuming that the data are drawn independently from
the model. The solution is obtained by ﬁtting the model for each class separately
using the correspondingly labelled data. As an example, suppose that the probability
density within each class is chosen to be Gaussian. In this case, the naive Bayes
assumption then implies that the covariance matrix for each Gaussian is diagonal,
and the contours of constant density within each class will be axis-aligned ellipsoids.
The marginal density, however, is given by a superposition of diagonal Gaussians
(with weighting coefﬁcients given by the class priors) and so will no longer factorize
with respect to its components.

The naive Bayes assumption is helpful when the dimensionality D of the input
space is high, making density estimation in the full D-dimensional space more chal-
lenging. It is also useful if the input vector contains both discrete and continuous
variables, since each can be represented separately using appropriate models (e.g.,
Bernoulli distributions for binary observations or Gaussians for real-valued vari-
ables). The conditional independence assumption of this model is clearly a strong
one that may lead to rather poor representations of the class-conditional densities.
Nevertheless, even if this assumption is not precisely satisﬁed, the model may still
give good classiﬁcation performance in practice because the decision boundaries can
be insensitive to some of the details in the class-conditional densities, as illustrated
in Figure 1.27.

We have seen that a particular directed graph represents a speciﬁc decomposition
of a joint probability distribution into a product of conditional probabilities. The
graph also expresses a set of conditional independence statements obtained through
the d-separation criterion, and the d-separation theorem is really an expression of the
equivalence of these two properties. In order to make this clear, it is helpful to think
of a directed graph as a ﬁlter. Suppose we consider a particular joint probability
distribution p(x) over the variables x corresponding to the (nonobserved) nodes of
the graph. The ﬁlter will allow this distribution to pass through if, and only if, it can
be expressed in terms of the factorization (8.5) implied by the graph. If we present to
the ﬁlter the set of all possible distributions p(x) over the set of variables x, then the
subset of distributions that are passed by the ﬁlter will be denoted DF, for directed
factorization. This is illustrated in Figure 8.25. Alternatively, we can use the graph as
a different kind of ﬁlter by ﬁrst listing all of the conditional independence properties
obtained by applying the d-separation criterion to the graph, and then allowing a
distribution to pass only if it satisﬁes all of these properties. If we present all possible
distributions p(x) to this second kind of ﬁlter, then the d-separation theorem tells us
that the set of distributions that will be allowed through is precisely the set DF.
It should be emphasized that the conditional independence properties obtained
from d-separation apply to any probabilistic model described by that particular di-
rected graph. This will be true, for instance, whether the variables are discrete or
continuous or a combination of these. Again, we see that a particular graph is de-
scribing a whole family of probability distributions.

At one extreme we have a fully connected graph that exhibits no conditional in-
dependence properties at all, and which can represent any possible joint probability
distribution over the given variables. The set DF will contain all possible distribu-





k

p(xi|x{j=i}) =

=

p(x1, . . . , xD)

p(x1, . . . , xD) dxi

p(xk|pak)

p(xk|pak) dxi

k

in which the integral is replaced by a summation in the case of discrete variables. We
now observe that any factor p(xk|pak) that does not have any functional dependence
on xi can be taken outside the integral over xi, and will therefore cancel between
numerator and denominator. The only factors that remain will be the conditional
distribution p(xi|pai) for node xi itself, together with the conditional distributions
for any nodes xk such that node xi is in the conditioning set of p(xk|pak), in other
words for which xi is a parent of xk. The conditional p(xi|pai) will depend on the
parents of node xi, whereas the conditionals p(xk|pak) will depend on the children

382

8. GRAPHICAL MODELS

p(x)

DF

Figure 8.25 We can view a graphical model (in this case a directed graph) as a ﬁlter in which a prob-
ability distribution p(x) is allowed through the ﬁlter if, and only if, it satisﬁes the directed
factorization property (8.5). The set of all possible probability distributions p(x) that pass
through the ﬁlter is denoted DF. We can alternatively use the graph to ﬁlter distributions
according to whether they respect all of the conditional independencies implied by the
d-separation properties of the graph. The d-separation theorem says that it is the same
set of distributions DF that will be allowed through this second kind of ﬁlter.

tions p(x). At the other extreme, we have the fully disconnected graph, i.e., one
having no links at all. This corresponds to joint distributions which factorize into the
product of the marginal distributions over the variables comprising the nodes of the
graph.

Note that for any given graph, the set of distributions DF will include any dis-
tributions that have additional independence properties beyond those described by
the graph. For instance, a fully factorized distribution will always be passed through
the ﬁlter implied by any graph over the corresponding set of variables.

We end our discussion of conditional independence properties by exploring the
concept of a Markov blanket or Markov boundary. Consider a joint distribution
p(x1, . . . , xD) represented by a directed graph having D nodes, and consider the
conditional distribution of a particular node with variables xi conditioned on all of
the remaining variables xj=i. Using the factorization property (8.5), we can express
this conditional distribution in the form

8.3. Markov Random Fields

383

Figure 8.26 The Markov blanket of a node xi comprises the set
of parents, children and co-parents of the node.
It
has the property that the conditional distribution of
xi, conditioned on all the remaining variables in the
graph, is dependent only on the variables in the
Markov blanket.

xi

of xi as well as on the co-parents, in other words variables corresponding to parents
of node xk other than node xi. The set of nodes comprising the parents, the children
and the co-parents is called the Markov blanket and is illustrated in Figure 8.26. We
can think of the Markov blanket of a node xi as being the minimal set of nodes that
isolates xi from the rest of the graph. Note that it is not sufﬁcient to include only the
parents and children of node xi because the phenomenon of explaining away means
that observations of the child nodes will not block paths to the co-parents. We must
therefore observe the co-parent nodes also.

8.3. Markov Random Fields

We have seen that directed graphical models specify a factorization of the joint dis-
tribution over a set of variables into a product of local conditional distributions. They
also deﬁne a set of conditional independence properties that must be satisﬁed by any
distribution that factorizes according to the graph. We turn now to the second ma-
jor class of graphical models that are described by undirected graphs and that again
specify both a factorization and a set of conditional independence relations.

A Markov random ﬁeld, also known as a Markov network or an undirected
graphical model (Kindermann and Snell, 1980), has a set of nodes each of which
corresponds to a variable or group of variables, as well as a set of links each of
which connects a pair of nodes. The links are undirected, that is they do not carry
arrows. In the case of undirected graphs, it is convenient to begin with a discussion
of conditional independence properties.

8.3.1 Conditional independence properties
In the case of directed graphs, we saw that it was possible to test whether a par-
ticular conditional independence property holds by applying a graphical test called
d-separation. This involved testing whether or not the paths connecting two sets of
nodes were ‘blocked’. The deﬁnition of blocked, however, was somewhat subtle
due to the presence of paths having head-to-head nodes. We might ask whether it
is possible to deﬁne an alternative graphical semantics for probability distributions
such that conditional independence is determined by simple graph separation. This
is indeed the case and corresponds to undirected graphical models. By removing the

Section 8.2

384

8. GRAPHICAL MODELS

Figure 8.27 An example of an undirected graph in
which every path from any node in set
A to any node in set B passes through
at least one node in set C. Conse-
quently the conditional
independence
property A ⊥⊥ B | C holds for any
probability distribution described by this
graph.

C

B

A

directionality from the links of the graph, the asymmetry between parent and child
nodes is removed, and so the subtleties associated with head-to-head nodes no longer
arise.

Suppose that in an undirected graph we identify three sets of nodes, denoted A,

B, and C, and that we consider the conditional independence property

A ⊥⊥ B | C.

(8.37)

To test whether this property is satisﬁed by a probability distribution deﬁned by a
graph we consider all possible paths that connect nodes in set A to nodes in set
B. If all such paths pass through one or more nodes in set C, then all such paths are
‘blocked’ and so the conditional independence property holds. However, if there is at
least one such path that is not blocked, then the property does not necessarily hold, or
more precisely there will exist at least some distributions corresponding to the graph
that do not satisfy this conditional independence relation. This is illustrated with an
example in Figure 8.27. Note that this is exactly the same as the d-separation crite-
rion except that there is no ‘explaining away’ phenomenon. Testing for conditional
independence in undirected graphs is therefore simpler than in directed graphs.

An alternative way to view the conditional independence test is to imagine re-
moving all nodes in set C from the graph together with any links that connect to
those nodes. We then ask if there exists a path that connects any node in A to any
node in B. If there are no such paths, then the conditional independence property
must hold.

The Markov blanket for an undirected graph takes a particularly simple form,
because a node will be conditionally independent of all other nodes conditioned only
on the neighbouring nodes, as illustrated in Figure 8.28.

8.3.2 Factorization properties
We now seek a factorization rule for undirected graphs that will correspond to
the above conditional independence test. Again, this will involve expressing the joint
distribution p(x) as a product of functions deﬁned over sets of variables that are local
to the graph. We therefore need to decide what is the appropriate notion of locality
in this case.

8.3. Markov Random Fields

385

Figure 8.28 For an undirected graph, the Markov blanket of a node
xi consists of the set of neighbouring nodes.
It has the
property that the conditional distribution of xi, conditioned
on all the remaining variables in the graph, is dependent
only on the variables in the Markov blanket.

If we consider two nodes xi and xj that are not connected by a link, then these
variables must be conditionally independent given all other nodes in the graph. This
follows from the fact that there is no direct path between the two nodes, and all other
paths pass through nodes that are observed, and hence those paths are blocked. This
conditional independence property can be expressed as

p(xi, xj|x\{i,j}) = p(xi|x\{i,j})p(xj|x\{i,j})

(8.38)
where x\{i,j} denotes the set x of all variables with xi and xj removed. The factor-
ization of the joint distribution must therefore be such that xi and xj do not appear
in the same factor in order for the conditional independence property to hold for all
possible distributions belonging to the graph.

This leads us to consider a graphical concept called a clique, which is deﬁned
as a subset of the nodes in a graph such that there exists a link between all pairs of
nodes in the subset. In other words, the set of nodes in a clique is fully connected.
Furthermore, a maximal clique is a clique such that it is not possible to include any
other nodes from the graph in the set without it ceasing to be a clique. These concepts
are illustrated by the undirected graph over four variables shown in Figure 8.29. This
graph has ﬁve cliques of two nodes given by {x1, x2}, {x2, x3}, {x3, x4}, {x4, x2},
and {x1, x3}, as well as two maximal cliques given by {x1, x2, x3} and {x2, x3, x4}.
The set {x1, x2, x3, x4} is not a clique because of the missing link from x1 to x4.
We can therefore deﬁne the factors in the decomposition of the joint distribution
to be functions of the variables in the cliques. In fact, we can consider functions
of the maximal cliques, without loss of generality, because other cliques must be
subsets of maximal cliques. Thus, if {x1, x2, x3} is a maximal clique and we deﬁne
an arbitrary function over this clique, then including another factor deﬁned over a
subset of these variables would be redundant.

Let us denote a clique by C and the set of variables in that clique by xC. Then

Figure 8.29 A four-node undirected graph showing a clique (outlined in

green) and a maximal clique (outlined in blue).

x1

x3

x2

x4




C



1
Z

the joint distribution is written as a product of potential functions ψC(xC) over the
maximal cliques of the graph

p(x) =

ψC(xC).

(8.39)

386

8. GRAPHICAL MODELS

Here the quantity Z, sometimes called the partition function, is a normalization con-
stant and is given by

Z =

ψC(xC)

x

C

(8.40)

which ensures that the distribution p(x) given by (8.39) is correctly normalized.
By considering only potential functions which satisfy ψC(xC) � 0 we ensure that
p(x) � 0. In (8.40) we have assumed that x comprises discrete variables, but the
framework is equally applicable to continuous variables, or a combination of the two,
in which the summation is replaced by the appropriate combination of summation
and integration.

Note that we do not restrict the choice of potential functions to those that have a
speciﬁc probabilistic interpretation as marginal or conditional distributions. This is
in contrast to directed graphs in which each factor represents the conditional distribu-
tion of the corresponding variable, conditioned on the state of its parents. However,
in special cases, for instance where the undirected graph is constructed by starting
with a directed graph, the potential functions may indeed have such an interpretation,
as we shall see shortly.

One consequence of the generality of the potential functions ψC(xC) is that
their product will in general not be correctly normalized. We therefore have to in-
troduce an explicit normalization factor given by (8.40). Recall that for directed
graphs, the joint distribution was automatically normalized as a consequence of the
normalization of each of the conditional distributions in the factorization.

The presence of this normalization constant is one of the major limitations of
undirected graphs. If we have a model with M discrete nodes each having K states,
then the evaluation of the normalization term involves summing over KM states and
so (in the worst case) is exponential in the size of the model. The partition function
is needed for parameter learning because it will be a function of any parameters that
govern the potential functions ψC(xC). However, for evaluation of local conditional
distributions, the partition function is not needed because a conditional is the ratio
of two marginals, and the partition function cancels between numerator and denom-
inator when evaluating this ratio. Similarly, for evaluating local marginal probabil-
ities we can work with the unnormalized joint distribution and then normalize the
marginals explicitly at the end. Provided the marginals only involves a small number
of variables, the evaluation of their normalization coefﬁcient will be feasible.

So far, we have discussed the notion of conditional independence based on sim-
ple graph separation and we have proposed a factorization of the joint distribution
that is intended to correspond to this conditional independence structure. However,
we have not made any formal connection between conditional independence and
factorization for undirected graphs. To do so we need to restrict attention to poten-
tial functions ψC(xC) that are strictly positive (i.e., never zero or negative for any

8.3. Markov Random Fields

387

choice of xC). Given this restriction, we can make a precise relationship between
factorization and conditional independence.

To do this we again return to the concept of a graphical model as a ﬁlter, corre-
sponding to Figure 8.25. Consider the set of all possible distributions deﬁned over
a ﬁxed set of variables corresponding to the nodes of a particular undirected graph.
We can deﬁne UI to be the set of such distributions that are consistent with the set
of conditional independence statements that can be read from the graph using graph
separation. Similarly, we can deﬁne UF to be the set of such distributions that can
be expressed as a factorization of the form (8.39) with respect to the maximal cliques
of the graph. The Hammersley-Clifford theorem (Clifford, 1990) states that the sets
UI and UF are identical.
convenient to express them as exponentials, so that

Because we are restricted to potential functions which are strictly positive it is

ψC(xC) = exp{−E(xC)}

(8.41)

where E(xC) is called an energy function, and the exponential representation is
called the Boltzmann distribution. The joint distribution is deﬁned as the product of
potentials, and so the total energy is obtained by adding the energies of each of the
maximal cliques.

In contrast to the factors in the joint distribution for a directed graph, the po-
tentials in an undirected graph do not have a speciﬁc probabilistic interpretation.
Although this gives greater ﬂexibility in choosing the potential functions, because
there is no normalization constraint, it does raise the question of how to motivate a
choice of potential function for a particular application. This can be done by view-
ing the potential function as expressing which conﬁgurations of the local variables
are preferred to others. Global conﬁgurations that have a relatively high probability
are those that ﬁnd a good balance in satisfying the (possibly conﬂicting) inﬂuences
of the clique potentials. We turn now to a speciﬁc example to illustrate the use of
undirected graphs.

8.3.3 Illustration: Image de-noising
We can illustrate the application of undirected graphs using an example of noise
removal from a binary image (Besag, 1974; Geman and Geman, 1984; Besag, 1986).
Although a very simple example, this is typical of more sophisticated applications.
Let the observed noisy image be described by an array of binary pixel values yi ∈
{−1, +1}, where the index i = 1, . . . , D runs over all pixels. We shall suppose
that the image is obtained by taking an unknown noise-free image, described by
binary pixel values xi ∈ {−1, +1} and randomly ﬂipping the sign of pixels with
some small probability. An example binary image, together with a noise corrupted
image obtained by ﬂipping the sign of the pixels with probability 10%, is shown in
Figure 8.30. Given the noisy image, our goal is to recover the original noise-free
image.

Because the noise level is small, we know that there will be a strong correlation
between xi and yi. We also know that neighbouring pixels xi and xj in an image
are strongly correlated. This prior knowledge can be captured using the Markov

388

8. GRAPHICAL MODELS

Figure 8.30 Illustration of image de-noising using a Markov random ﬁeld. The top row shows the original
binary image on the left and the corrupted image after randomly changing 10% of the pixels on the right. The
bottom row shows the restored images obtained using iterated conditional models (ICM) on the left and using
the graph-cut algorithm on the right.
ICM produces an image where 96% of the pixels agree with the original
image, whereas the corresponding number for graph-cut is 99%.

random ﬁeld model whose undirected graph is shown in Figure 8.31. This graph has
two types of cliques, each of which contains two variables. The cliques of the form
{xi, yi} have an associated energy function that expresses the correlation between
these variables. We choose a very simple energy function for these cliques of the
form −ηxiyi where η is a positive constant. This has the desired effect of giving a
lower energy (thus encouraging a higher probability) when xi and yi have the same
sign and a higher energy when they have the opposite sign.

The remaining cliques comprise pairs of variables {xi, xj} where i and j are
indices of neighbouring pixels. Again, we want the energy to be lower when the
pixels have the same sign than when they have the opposite sign, and so we choose
an energy given by −βxixj where β is a positive constant.
Because a potential function is an arbitrary, nonnegative function over a maximal
clique, we can multiply it by any nonnegative functions of subsets of the clique, or







xi

8.3. Markov Random Fields

389

Figure 8.31 An undirected graphical model representing a
Markov random ﬁeld for image de-noising,
in
which xi is a binary variable denoting the state
of pixel i in the unknown noise-free image, and yi
denotes the corresponding value of pixel i in the
observed noisy image.

yi

equivalently we can add the corresponding energies. In this example, this allows us
to add an extra term hxi for each pixel i in the noise-free image. Such a term has
the effect of biasing the model towards pixel values that have one particular sign in
preference to the other.

The complete energy function for the model then takes the form

E(x, y) = h

xi − β

i

{i,j}

xixj − η

i

xiyi

which deﬁnes a joint distribution over x and y given by

p(x, y) =

1
Z

exp{−E(x, y)}.

(8.42)

(8.43)

We now ﬁx the elements of y to the observed values given by the pixels of the
noisy image, which implicitly deﬁnes a conditional distribution p(x|y) over noise-
free images. This is an example of the Ising model, which has been widely studied in
statistical physics. For the purposes of image restoration, we wish to ﬁnd an image x
having a high probability (ideally the maximum probability). To do this we shall use
a simple iterative technique called iterated conditional modes, or ICM (Kittler and
F¨oglein, 1984), which is simply an application of coordinate-wise gradient ascent.
The idea is ﬁrst to initialize the variables {xi}, which we do by simply setting xi =
yi for all i. Then we take one node xj at a time and we evaluate the total energy
for the two possible states xj = +1 and xj = −1, keeping all other node variables
ﬁxed, and set xj to whichever state has the lower energy. This will either leave
the probability unchanged, if xj is unchanged, or will increase it. Because only
one variable is changed, this is a simple local computation that can be performed
efﬁciently. We then repeat the update for another site, and so on, until some suitable
stopping criterion is satisﬁed. The nodes may be updated in a systematic way, for
instance by repeatedly raster scanning through the image, or by choosing nodes at
random.

If we have a sequence of updates in which every site is visited at least once,
and in which no changes to the variables are made, then by deﬁnition the algorithm

Exercise 8.13

390

8. GRAPHICAL MODELS

Figure 8.32 (a) Example of a directed
graph.
(b) The equivalent undirected
graph.

x1

x1

(a)

(b)

x2

x2

xN−1

xN

xN

xN−1

will have converged to a local maximum of the probability. This need not, however,
correspond to the global maximum.

For the purposes of this simple illustration, we have ﬁxed the parameters to be
β = 1.0, η = 2.1 and h = 0. Note that leaving h = 0 simply means that the prior
probabilities of the two states of xi are equal. Starting with the observed noisy image
as the initial conﬁguration, we run ICM until convergence, leading to the de-noised
image shown in the lower left panel of Figure 8.30. Note that if we set β = 0,
which effectively removes the links between neighbouring pixels, then the global
most probable solution is given by xi = yi for all i, corresponding to the observed
noisy image.

Later we shall discuss a more effective algorithm for ﬁnding high probability so-
lutions called the max-product algorithm, which typically leads to better solutions,
although this is still not guaranteed to ﬁnd the global maximum of the posterior dis-
tribution. However, for certain classes of model, including the one given by (8.42),
there exist efﬁcient algorithms based on graph cuts that are guaranteed to ﬁnd the
global maximum (Greig et al., 1989; Boykov et al., 2001; Kolmogorov and Zabih,
2004). The lower right panel of Figure 8.30 shows the result of applying a graph-cut
algorithm to the de-noising problem.

Exercise 8.14

Section 8.4

8.3.4 Relation to directed graphs
We have introduced two graphical frameworks for representing probability dis-
tributions, corresponding to directed and undirected graphs, and it is instructive to
discuss the relation between these. Consider ﬁrst the problem of taking a model that
is speciﬁed using a directed graph and trying to convert it to an undirected graph. In
some cases this is straightforward, as in the simple example in Figure 8.32. Here the
joint distribution for the directed graph is given as a product of conditionals in the
form

p(x) = p(x1)p(x2|x1)p(x3|x2)··· p(xN|xN−1).

(8.44)

Now let us convert this to an undirected graph representation, as shown in Fig-
ure 8.32. In the undirected graph, the maximal cliques are simply the pairs of neigh-
bouring nodes, and so from (8.39) we wish to write the joint distribution in the form

p(x) =

1
Z

ψ1,2(x1, x2)ψ2,3(x2, x3)··· ψN−1,N (xN−1, xN ).

(8.45)

Figure 8.33 Example of a simple
directed graph (a) and the corre-
sponding moral graph (b).

x1

x3

x1

x3

x2

x2

8.3. Markov Random Fields

391

x4

(a)

x4

(b)

This is easily done by identifying

ψ1,2(x1, x2) = p(x1)p(x2|x1)
ψ2,3(x2, x3) = p(x3|x2)

...

ψN−1,N (xN−1, xN ) = p(xN|xN−1)

where we have absorbed the marginal p(x1) for the ﬁrst node into the ﬁrst potential
function. Note that in this case, the partition function Z = 1.

Let us consider how to generalize this construction, so that we can convert any
distribution speciﬁed by a factorization over a directed graph into one speciﬁed by a
factorization over an undirected graph. This can be achieved if the clique potentials
of the undirected graph are given by the conditional distributions of the directed
graph. In order for this to be valid, we must ensure that the set of variables that
appears in each of the conditional distributions is a member of at least one clique of
the undirected graph. For nodes on the directed graph having just one parent, this is
achieved simply by replacing the directed link with an undirected link. However, for
nodes in the directed graph having more than one parent, this is not sufﬁcient. These
are nodes that have ‘head-to-head’ paths encountered in our discussion of conditional
independence. Consider a simple directed graph over 4 nodes shown in Figure 8.33.
The joint distribution for the directed graph takes the form

p(x) = p(x1)p(x2)p(x3)p(x4|x1, x2, x3).

(8.46)
We see that the factor p(x4|x1, x2, x3) involves the four variables x1, x2, x3, and
x4, and so these must all belong to a single clique if this conditional distribution is
to be absorbed into a clique potential. To ensure this, we add extra links between
all pairs of parents of the node x4. Anachronistically, this process of ‘marrying
the parents’ has become known as moralization, and the resulting undirected graph,
after dropping the arrows, is called the moral graph. It is important to observe that
the moral graph in this example is fully connected and so exhibits no conditional
independence properties, in contrast to the original directed graph.

Thus in general to convert a directed graph into an undirected graph, we ﬁrst add
additional undirected links between all pairs of parents for each node in the graph and

392

8. GRAPHICAL MODELS

Section 8.4

Section 8.2

then drop the arrows on the original links to give the moral graph. Then we initialize
all of the clique potentials of the moral graph to 1. We then take each conditional
distribution factor in the original directed graph and multiply it into one of the clique
potentials. There will always exist at least one maximal clique that contains all of
the variables in the factor as a result of the moralization step. Note that in all cases
the partition function is given by Z = 1.

The process of converting a directed graph into an undirected graph plays an
important role in exact inference techniques such as the junction tree algorithm.
Converting from an undirected to a directed representation is much less common
and in general presents problems due to the normalization constraints.

We saw that in going from a directed to an undirected representation we had to
discard some conditional independence properties from the graph. Of course, we
could always trivially convert any distribution over a directed graph into one over an
undirected graph by simply using a fully connected undirected graph. This would,
however, discard all conditional independence properties and so would be vacuous.
The process of moralization adds the fewest extra links and so retains the maximum
number of independence properties.

We have seen that the procedure for determining the conditional independence
properties is different between directed and undirected graphs. It turns out that the
two types of graph can express different conditional independence properties, and
it is worth exploring this issue in more detail. To do so, we return to the view of
a speciﬁc (directed or undirected) graph as a ﬁlter, so that the set of all possible
distributions over the given variables could be reduced to a subset that respects the
conditional independencies implied by the graph. A graph is said to be a D map
(for ‘dependency map’) of a distribution if every conditional independence statement
satisﬁed by the distribution is reﬂected in the graph. Thus a completely disconnected
graph (no links) will be a trivial D map for any distribution.

Alternatively, we can consider a speciﬁc distribution and ask which graphs have
the appropriate conditional independence properties. If every conditional indepen-
dence statement implied by a graph is satisﬁed by a speciﬁc distribution, then the
graph is said to be an I map (for ‘independence map’) of that distribution. Clearly a
fully connected graph will be a trivial I map for any distribution.

If it is the case that every conditional independence property of the distribution
is reﬂected in the graph, and vice versa, then the graph is said to be a perfect map for

Figure 8.34 Venn diagram illustrating the set of all distributions
P over a given set of variables, together with the
set of distributions D that can be represented as a
perfect map using a directed graph, and the set U
that can be represented as a perfect map using an
undirected graph.

D

U

P

8.4. Inference in Graphical Models

Figure 8.35 A directed graph whose conditional

independence
properties cannot be expressed using an undirected
graph over the same three variables.

A

393

B

C

that distribution. A perfect map is therefore both an I map and a D map.

Consider the set of distributions such that for each distribution there exists a
directed graph that is a perfect map. This set is distinct from the set of distributions
such that for each distribution there exists an undirected graph that is a perfect map.
In addition there are distributions for which neither directed nor undirected graphs
offer a perfect map. This is illustrated as a Venn diagram in Figure 8.34.

Figure 8.35 shows an example of a directed graph that is a perfect map for
a distribution satisfying the conditional independence properties A ⊥⊥ B | ∅ and
A ⊥⊥ B | C. There is no corresponding undirected graph over the same three vari-
ables that is a perfect map.
Conversely, consider the undirected graph over four variables shown in Fig-
ure 8.36. This graph exhibits the properties A ⊥⊥ B | ∅, C ⊥⊥ D | A ∪ B and
A ⊥⊥ B | C ∪ D. There is no directed graph over four variables that implies the same
set of conditional independence properties.
The graphical framework can be extended in a consistent way to graphs that
include both directed and undirected links. These are called chain graphs (Lauritzen
and Wermuth, 1989; Frydenberg, 1990), and contain the directed and undirected
graphs considered so far as special cases. Although such graphs can represent a
broader class of distributions than either directed or undirected alone, there remain
distributions for which even a chain graph cannot provide a perfect map. Chain
graphs are not discussed further in this book.

Figure 8.36 An undirected graph whose conditional

independence
properties cannot be expressed in terms of a directed
graph over the same variables.

A

B

C

D

8.4.

Inference in Graphical Models

We turn now to the problem of inference in graphical models, in which some of
the nodes in a graph are clamped to observed values, and we wish to compute the
posterior distributions of one or more subsets of other nodes. As we shall see, we
can exploit the graphical structure both to ﬁnd efﬁcient algorithms for inference, and

394

8. GRAPHICAL MODELS

Figure 8.37 A graphical representation of Bayes’

See the text for details.

theorem.

x

y

x

y

x

y

(a)

(b)

(c)



to make the structure of those algorithms transparent. Speciﬁcally, we shall see that
many algorithms can be expressed in terms of the propagation of local messages
around the graph. In this section, we shall focus primarily on techniques for exact
inference, and in Chapter 10 we shall consider a number of approximate inference
algorithms.

To start with, let us consider the graphical interpretation of Bayes’ theorem.
Suppose we decompose the joint distribution p(x, y) over two variables x and y into
a product of factors in the form p(x, y) = p(x)p(y|x). This can be represented by
the directed graph shown in Figure 8.37(a). Now suppose we observe the value of
y, as indicated by the shaded node in Figure 8.37(b). We can view the marginal
distribution p(x) as a prior over the latent variable x, and our goal is to infer the
corresponding posterior distribution over x. Using the sum and product rules of
probability we can evaluate

p(y) =

p(y|x)p(x)

x

which can then be used in Bayes’ theorem to calculate

p(x|y) = p(y|x)p(x)

p(y)

.

(8.47)

(8.48)

Thus the joint distribution is now expressed in terms of p(y) and p(x|y). From a
graphical perspective, the joint distribution p(x, y) is now represented by the graph
shown in Figure 8.37(c), in which the direction of the arrow is reversed. This is the
simplest example of an inference problem for a graphical model.

8.4.1 Inference on a chain
Now consider a more complex problem involving the chain of nodes of the form
shown in Figure 8.32. This example will lay the foundation for a discussion of exact
inference in more general graphs later in this section.

Speciﬁcally, we shall consider the undirected graph in Figure 8.32(b). We have
already seen that the directed chain can be transformed into an equivalent undirected
chain. Because the directed graph does not have any nodes with more than one
parent, this does not require the addition of any extra links, and the directed and
undirected versions of this graph express exactly the same set of conditional inde-
pendence statements.










x1

xN

8.4. Inference in Graphical Models

The joint distribution for this graph takes the form

p(x) =

1
Z

ψ1,2(x1, x2)ψ2,3(x2, x3)··· ψN−1,N (xN−1, xN ).

We shall consider the speciﬁc case in which the N nodes represent discrete vari-
ables each having K states, in which case each potential function ψn−1,n(xn−1, xn)
comprises an K × K table, and so the joint distribution has (N − 1)K 2 parameters.
Let us consider the inference problem of ﬁnding the marginal distribution p(xn)
for a speciﬁc node xn that is part way along the chain. Note that, for the moment,
there are no observed nodes. By deﬁnition, the required marginal is obtained by
summing the joint distribution over all variables except xn, so that

395

(8.49)

p(xn) =

···

xn−1

xn+1

p(x).

···

xN

(8.50)

In a naive implementation, we would ﬁrst evaluate the joint distribution and
then perform the summations explicitly. The joint distribution can be represented as
a set of numbers, one for each possible value for x. Because there are N variables
each with K states, there are K N values for x and so evaluation and storage of the
joint distribution, as well as marginalization to obtain p(xn), all involve storage and
computation that scale exponentially with the length N of the chain.

We can, however, obtain a much more efﬁcient algorithm by exploiting the con-
ditional independence properties of the graphical model. If we substitute the factor-
ized expression (8.49) for the joint distribution into (8.50), then we can rearrange the
order of the summations and the multiplications to allow the required marginal to be
evaluated much more efﬁciently. Consider for instance the summation over xN . The
potential ψN−1,N (xN−1, xN ) is the only one that depends on xN , and so we can
perform the summation
(8.51)

ψN−1,N (xN−1, xN )

ﬁrst to give a function of xN−1. We can then use this to perform the summation
over xN−1, which will involve only this new function together with the potential
ψN−2,N−1(xN−2, xN−1), because this is the only other place that xN−1 appears.
Similarly, the summation over x1 involves only the potential ψ1,2(x1, x2) and so
can be performed separately to give a function of x2, and so on. Because each
summation effectively removes a variable from the distribution, this can be viewed
as the removal of a node from the graph.

If we group the potentials and summations together in this way, we can express

  

⎤⎦
+

···



x1

)*

ψ2,3(x2, x3)



)*

x2

xN

 

⎤⎦
+

⎡⎣
(
⎡⎣
(

xn+1

xn−1

396

8. GRAPHICAL MODELS

the desired marginal in the form

p(xn) =

1
Z

ψ1,2(x1, x2)

ψn−1,n(xn−1, xn)···

ψn,n+1(xn, xn+1)···

µα(xn)

ψN−1,N (xN−1, xN )

···

.

(8.52)

µβ(xn)

The reader is encouraged to study this re-ordering carefully as the underlying idea
forms the basis for the later discussion of the general sum-product algorithm. Here
the key concept that we are exploiting is that multiplication is distributive over addi-
tion, so that

ab + ac = a(b + c)

(8.53)

in which the left-hand side involves three arithmetic operations whereas the right-
hand side reduces this to two operations.

Let us work out the computational cost of evaluating the required marginal using
this re-ordered expression. We have to perform N − 1 summations each of which is
over K states and each of which involves a function of two variables. For instance,
the summation over x1 involves only the function ψ1,2(x1, x2), which is a table of
K × K numbers. We have to sum this table over x1 for each value of x2 and so this
has O(K 2) cost. The resulting vector of K numbers is multiplied by the matrix of
numbers ψ2,3(x2, x3) and so is again O(K 2). Because there are N − 1 summations
and multiplications of this kind, the total cost of evaluating the marginal p(xn) is
O(N K 2). This is linear in the length of the chain, in contrast to the exponential cost
of a naive approach. We have therefore been able to exploit the many conditional
independence properties of this simple graph in order to obtain an efﬁcient calcula-
tion. If the graph had been fully connected, there would have been no conditional
independence properties, and we would have been forced to work directly with the
full joint distribution.

We now give a powerful interpretation of this calculation in terms of the passing
of local messages around on the graph. From (8.52) we see that the expression for the
marginal p(xn) decomposes into the product of two factors times the normalization
constant

p(xn) =

µα(xn)µβ(xn).

(8.54)

1
Z

We shall interpret µα(xn) as a message passed forwards along the chain from node
xn−1 to node xn. Similarly, µβ(xn) can be viewed as a message passed backwards

⎡⎣
⎡⎣

⎤⎦
⎤⎦



x1






xn−1

xn−1

Figure 8.38 The marginal distribution
p(xn) for a node xn along the chain is ob-
tained by multiplying the two messages
µα(xn) and µβ(xn), and then normaliz-
ing. These messages can themselves
be evaluated recursively by passing mes-
sages from both ends of the chain to-
wards node xn.

x1

8.4. Inference in Graphical Models

µα(xn−1)

µα(xn)

µβ(xn)

µβ(xn+1)

xn−1

xn

xn+1

397

xN

(8.55)

(8.56)

along the chain to node xn from node xn+1. Note that each of the messages com-
prises a set of K values, one for each choice of xn, and so the product of two mes-
sages should be interpreted as the point-wise multiplication of the elements of the
two messages to give another set of K values.

The message µα(xn) can be evaluated recursively because

µα(xn) =

=

We therefore ﬁrst evaluate

ψn−1,n(xn−1, xn)

···

xn−2

ψn−1,n(xn−1, xn)µα(xn−1).

µα(x2) =

ψ1,2(x1, x2)

and then apply (8.55) repeatedly until we reach the desired node. Note carefully the
structure of the message passing equation. The outgoing message µα(xn) in (8.55)
is obtained by multiplying the incoming message µα(xn−1) by the local potential
involving the node variable and the outgoing variable and then summing over the
node variable.

Similarly, the message µβ(xn) can be evaluated recursively by starting with

node xN and using

µβ(xn) =

ψn+1,n(xn+1, xn)

···

xn+2

xn+1

=

ψn+1,n(xn+1, xn)µβ(xn+1).

(8.57)

xn+1

This recursive message passing is illustrated in Figure 8.38. The normalization con-
stant Z is easily evaluated by summing the right-hand side of (8.54) over all states
of xn, an operation that requires only O(K) computation.

Graphs of the form shown in Figure 8.38 are called Markov chains, and the
corresponding message passing equations represent an example of the Chapman-
Kolmogorov equations for Markov processes (Papoulis, 1984).








398

8. GRAPHICAL MODELS

Now suppose we wish to evaluate the marginals p(xn) for every node n ∈
{1, . . . , N} in the chain. Simply applying the above procedure separately for each
node will have computational cost that is O(N 2M 2). However, such an approach
would be very wasteful of computation. For instance, to ﬁnd p(x1) we need to prop-
agate a message µβ(·) from node xN back to node x2. Similarly, to evaluate p(x2)
we need to propagate a messages µβ(·) from node xN back to node x3. This will
involve much duplicated computation because most of the messages will be identical
in the two cases.

Suppose instead we ﬁrst launch a message µβ(xN−1) starting from node xN
and propagate corresponding messages all the way back to node x1, and suppose we
similarly launch a message µα(x2) starting from node x1 and propagate the corre-
sponding messages all the way forward to node xN . Provided we store all of the
intermediate messages along the way, then any node can evaluate its marginal sim-
ply by applying (8.54). The computational cost is only twice that for ﬁnding the
marginal of a single node, rather than N times as much. Observe that a message
has passed once in each direction across each link in the graph. Note also that the
normalization constant Z need be evaluated only once, using any convenient node.
If some of the nodes in the graph are observed, then the corresponding variables
are simply clamped to their observed values and there is no summation. To see
this, note that the effect of clamping a variable xn to an observed value
xn can
be expressed by multiplying the joint distribution by (one or more copies of) an
additional function I(xn,
xn and the value
0 otherwise. One such function can then be absorbed into each of the potentials that
contain xn. Summations over xn then contain only one term in which xn =

xn), which takes the value 1 when xn =

xn.

Exercise 8.15

Chapter 9

Now suppose we wish to calculate the joint distribution p(xn−1, xn) for two
neighbouring nodes on the chain. This is similar to the evaluation of the marginal
for a single node, except that there are now two variables that are not summed out.
A few moments thought will show that the required joint distribution can be written
in the form

p(xn−1, xn) =

1
Z

µα(xn−1)ψn−1,n(xn−1, xn)µβ(xn).

(8.58)

Thus we can obtain the joint distributions over all of the sets of variables in each
of the potentials directly once we have completed the message passing required to
obtain the marginals.

This is a useful result because in practice we may wish to use parametric forms
for the clique potentials, or equivalently for the conditional distributions if we started
from a directed graph. In order to learn the parameters of these potentials in situa-
tions where not all of the variables are observed, we can employ the EM algorithm,
and it turns out that the local joint distributions of the cliques, conditioned on any
observed data, is precisely what is needed in the E step. We shall consider some
examples of this in detail in Chapter 13.

8.4.2 Trees
We have seen that exact inference on a graph comprising a chain of nodes can be
performed efﬁciently in time that is linear in the number of nodes, using an algorithm

8.4. Inference in Graphical Models

399

Figure 8.39 Examples
tree-
structured graphs, showing (a) an
undirected tree, (b) a directed tree,
and (c) a directed polytree.

of

(a)

(b)

(c)



Exercise 8.18

that can be interpreted in terms of messages passed along the chain. More generally,
inference can be performed efﬁciently using local message passing on a broader
class of graphs called trees. In particular, we shall shortly generalize the message
passing formalism derived above for chains to give the sum-product algorithm, which
provides an efﬁcient framework for exact inference in tree-structured graphs.

In the case of an undirected graph, a tree is deﬁned as a graph in which there
is one, and only one, path between any pair of nodes. Such graphs therefore do not
have loops. In the case of directed graphs, a tree is deﬁned such that there is a single
node, called the root, which has no parents, and all other nodes have one parent. If
we convert a directed tree into an undirected graph, we see that the moralization step
will not add any links as all nodes have at most one parent, and as a consequence the
corresponding moralized graph will be an undirected tree. Examples of undirected
and directed trees are shown in Figure 8.39(a) and 8.39(b). Note that a distribution
represented as a directed tree can easily be converted into one represented by an
undirected tree, and vice versa.

If there are nodes in a directed graph that have more than one parent, but there is
still only one path (ignoring the direction of the arrows) between any two nodes, then
the graph is a called a polytree, as illustrated in Figure 8.39(c). Such a graph will
have more than one node with the property of having no parents, and furthermore,
the corresponding moralized undirected graph will have loops.

8.4.3 Factor graphs
The sum-product algorithm that we derive in the next section is applicable to
undirected and directed trees and to polytrees. It can be cast in a particularly simple
and general form if we ﬁrst introduce a new graphical construction called a factor
graph (Frey, 1998; Kschischnang et al., 2001).

Both directed and undirected graphs allow a global function of several vari-
ables to be expressed as a product of factors over subsets of those variables. Factor
graphs make this decomposition explicit by introducing additional nodes for the fac-
tors themselves in addition to the nodes representing the variables. They also allow
us to be more explicit about the details of the factorization, as we shall see.

Let us write the joint distribution over a set of variables in the form of a product

of factors

where xs denotes a subset of the variables. For convenience, we shall denote the

p(x) =

fs(xs)

s

(8.59)

400

8. GRAPHICAL MODELS

Figure 8.40 Example of a factor graph, which corresponds

to the factorization (8.60).

x1

x2

x3

fa

fb

fc

fd

individual variables by xi, however, as in earlier discussions, these can comprise
groups of variables (such as vectors or matrices). Each factor fs is a function of a
corresponding set of variables xs.

Directed graphs, whose factorization is deﬁned by (8.5), represent special cases
of (8.59) in which the factors fs(xs) are local conditional distributions. Similarly,
undirected graphs, given by (8.39), are a special case in which the factors are po-
tential functions over the maximal cliques (the normalizing coefﬁcient 1/Z can be
viewed as a factor deﬁned over the empty set of variables).

In a factor graph, there is a node (depicted as usual by a circle) for every variable
in the distribution, as was the case for directed and undirected graphs. There are also
additional nodes (depicted by small squares) for each factor fs(xs) in the joint dis-
tribution. Finally, there are undirected links connecting each factor node to all of the
variables nodes on which that factor depends. Consider, for example, a distribution
that is expressed in terms of the factorization

p(x) = fa(x1, x2)fb(x1, x2)fc(x2, x3)fd(x3).

(8.60)

This can be expressed by the factor graph shown in Figure 8.40. Note that there are
two factors fa(x1, x2) and fb(x1, x2) that are deﬁned over the same set of variables.
In an undirected graph, the product of two such factors would simply be lumped
together into the same clique potential. Similarly, fc(x2, x3) and fd(x3) could be
combined into a single potential over x2 and x3. The factor graph, however, keeps
such factors explicit and so is able to convey more detailed information about the
underlying factorization.

x1

x2

x1

x2

x1

f

x3

(b)

x3

(a)

x2

fb

fa

x3

(c)

Figure 8.41 (a) An undirected graph with a single clique potential ψ(x1, x2, x3). (b) A factor graph with factor
f (x1, x2, x3) = ψ(x1, x2, x3) representing the same distribution as the undirected graph. (c) A different factor
graph representing the same distribution, whose factors satisfy fa(x1, x2, x3)fb(x1, x2) = ψ(x1, x2, x3).

x1

x2

x1

x2

x1

x2

8.4. Inference in Graphical Models

401

f

x3

(b)

fc

fa

fb

x3

(c)

x3

(a)

Figure 8.42 (a) A directed graph with the factorization p(x1)p(x2)p(x3|x1, x2). (b) A factor graph representing
the same distribution as the directed graph, whose factor satisﬁes f (x1, x2, x3) = p(x1)p(x2)p(x3|x1, x2). (c)
A different factor graph representing the same distribution with factors fa(x1) = p(x1), fb(x2) = p(x2) and
fc(x1, x2, x3) = p(x3|x1, x2).

Factor graphs are said to be bipartite because they consist of two distinct kinds
of nodes, and all links go between nodes of opposite type. In general, factor graphs
can therefore always be drawn as two rows of nodes (variable nodes at the top and
factor nodes at the bottom) with links between the rows, as shown in the example in
Figure 8.40. In some situations, however, other ways of laying out the graph may
be more intuitive, for example when the factor graph is derived from a directed or
undirected graph, as we shall see.

If we are given a distribution that is expressed in terms of an undirected graph,
then we can readily convert it to a factor graph. To do this, we create variable nodes
corresponding to the nodes in the original undirected graph, and then create addi-
tional factor nodes corresponding to the maximal cliques xs. The factors fs(xs) are
then set equal to the clique potentials. Note that there may be several different factor
graphs that correspond to the same undirected graph. These concepts are illustrated
in Figure 8.41.

Similarly, to convert a directed graph to a factor graph, we simply create variable
nodes in the factor graph corresponding to the nodes of the directed graph, and then
create factor nodes corresponding to the conditional distributions, and then ﬁnally
add the appropriate links. Again, there can be multiple factor graphs all of which
correspond to the same directed graph. The conversion of a directed graph to a
factor graph is illustrated in Figure 8.42.

We have already noted the importance of tree-structured graphs for performing
efﬁcient inference. If we take a directed or undirected tree and convert it into a factor
graph, then the result will again be a tree (in other words, the factor graph will have
no loops, and there will be one and only one path connecting any two nodes). In
the case of a directed polytree, conversion to an undirected graph results in loops
due to the moralization step, whereas conversion to a factor graph again results in a
tree, as illustrated in Figure 8.43.
In fact, local cycles in a directed graph due to
links connecting parents of a node can be removed on conversion to a factor graph
by deﬁning the appropriate factor function, as shown in Figure 8.44.

We have seen that multiple different factor graphs can represent the same di-
rected or undirected graph. This allows factor graphs to be more speciﬁc about the

402

8. GRAPHICAL MODELS

(a)

(b)

(c)

Figure 8.43 (a) A directed polytree. (b) The result of converting the polytree into an undirected graph showing
the creation of loops. (c) The result of converting the polytree into a factor graph, which retains the tree structure.

precise form of the factorization. Figure 8.45 shows an example of a fully connected
undirected graph along with two different factor graphs.
In (b), the joint distri-
bution is given by a general form p(x) = f(x1, x2, x3), whereas in (c), it is given
by the more speciﬁc factorization p(x) = fa(x1, x2)fb(x1, x3)fc(x2, x3). It should
be emphasized that the factorization in (c) does not correspond to any conditional
independence properties.

8.4.4 The sum-product algorithm
We shall now make use of the factor graph framework to derive a powerful class
of efﬁcient, exact inference algorithms that are applicable to tree-structured graphs.
Here we shall focus on the problem of evaluating local marginals over nodes or
subsets of nodes, which will lead us to the sum-product algorithm. Later we shall
modify the technique to allow the most probable state to be found, giving rise to the
max-sum algorithm.

Also we shall suppose that all of the variables in the model are discrete, and
so marginalization corresponds to performing sums. The framework, however, is
equally applicable to linear-Gaussian models in which case marginalization involves
integration, and we shall consider an example of this in detail when we discuss linear
dynamical systems.

Section 13.3

Figure 8.44 (a) A fragment of a di-
rected graph having a lo-
cal cycle.
(b) Conversion
to a fragment of a factor
graph having a tree struc-
ture, in which f (x1, x2, x3) =
p(x1)p(x2|x1)p(x3|x1, x2).

x1

x2

x1

x2

f(x1, x2, x3)

x3

(a)

x3
(b)

x1

x2

x1

x2

f(x1, x2, x3)

x3

(a)

x3
(b)

Figure 8.45 (a) A fully connected undirected graph. (b) and (c) Two factor graphs each of which corresponds
to the undirected graph in (a).




x\x

8.4. Inference in Graphical Models

403

x1

fa

x2

fb

fc

x3

(c)

There is an algorithm for exact inference on directed graphs without loops known
as belief propagation (Pearl, 1988; Lauritzen and Spiegelhalter, 1988), and is equiv-
alent to a special case of the sum-product algorithm. Here we shall consider only the
sum-product algorithm because it is simpler to derive and to apply, as well as being
more general.

We shall assume that the original graph is an undirected tree or a directed tree or
polytree, so that the corresponding factor graph has a tree structure. We ﬁrst convert
the original graph into a factor graph so that we can deal with both directed and
undirected models using the same framework. Our goal is to exploit the structure of
the graph to achieve two things: (i) to obtain an efﬁcient, exact inference algorithm
for ﬁnding marginals; (ii) in situations where several marginals are required to allow
computations to be shared efﬁciently.

We begin by considering the problem of ﬁnding the marginal p(x) for partic-
ular variable node x. For the moment, we shall suppose that all of the variables
are hidden. Later we shall see how to modify the algorithm to incorporate evidence
corresponding to observed variables. By deﬁnition, the marginal is obtained by sum-
ming the joint distribution over all variables except x so that

p(x) =

p(x)

(8.61)

where x \ x denotes the set of variables in x with variable x omitted. The idea is
to substitute for p(x) using the factor graph expression (8.59) and then interchange
summations and products in order to obtain an efﬁcient algorithm. Consider the
fragment of graph shown in Figure 8.46 in which we see that the tree structure of
the graph allows us to partition the factors in the joint distribution into groups, with
one group associated with each of the factor nodes that is a neighbour of the variable
node x. We see that the joint distribution can be written as a product of the form

p(x) =

Fs(x, Xs)

(8.62)

s∈ne(x)

ne(x) denotes the set of factor nodes that are neighbours of x, and Xs denotes the
set of all variables in the subtree connected to the variable node x via the factor node







x
(
s
F

)
s

X

Xs

,

 








x1




xM

 

(8.63)

(8.64)

404

8. GRAPHICAL MODELS

Figure 8.46 A fragment of a factor graph illustrating the

evaluation of the marginal p(x).

µfs→x(x)

fs

x

fs, and Fs(x, Xs) represents the product of all the factors in the group associated
with factor fs.

Substituting (8.62) into (8.61) and interchanging the sums and products, we ob-

tain

p(x) =

Fs(x, Xs)

=

s∈ne(x)

s∈ne(x)

µfs→x(x).

µfs→x(x) ≡

Xs

Fs(x, Xs)

Here we have introduced a set of functions µfs→x(x), deﬁned by

which can be viewed as messages from the factor nodes fs to the variable node x.
We see that the required marginal p(x) is given by the product of all the incoming
messages arriving at node x.

In order to evaluate these messages, we again turn to Figure 8.46 and note that
each factor Fs(x, Xs) is described by a factor (sub-)graph and so can itself be fac-
torized. In particular, we can write

Fs(x, Xs) = fs(x, x1, . . . , xM )G1 (x1, Xs1) . . . GM (xM , XsM )

(8.65)

where, for convenience, we have denoted the variables associated with factor fx, in
addition to x, by x1, . . . , xM . This factorization is illustrated in Figure 8.47. Note
that the set of variables {x, x1, . . . , xM} is the set of variables on which the factor
fs depends, and so it can also be denoted xs, using the notation of (8.59).

Substituting (8.65) into (8.64) we obtain

µfs→x(x) =

=

. . .

. . .

fs(x, x1, . . . , xM )

fs(x, x1, . . . , xM )

m∈ne(fs)\x

Gm(xm, Xsm)

Xxm

µxm→fs(xm)

(8.66)

x1

xM

m∈ne(fs)\x




Xsm

where ne(fs) denotes the set of variable nodes that are neighbours of the factor node
fs, and ne(fs) \ x denotes the same set but with node x removed. Here we have
deﬁned the following messages from variable nodes to factor nodes

µxm→fs(xm) ≡

Gm(xm, Xsm).

(8.67)

We have therefore introduced two distinct kinds of message, those that go from factor
nodes to variable nodes denoted µf→x(x), and those that go from variable nodes to
factor nodes denoted µx→f (x). In each case, we see that messages passed along a
link are always a function of the variable associated with the variable node that link
connects to.

The result (8.66) says that to evaluate the message sent by a factor node to a vari-
able node along the link connecting them, take the product of the incoming messages
along all other links coming into the factor node, multiply by the factor associated
with that node, and then marginalize over all of the variables associated with the
incoming messages. This is illustrated in Figure 8.47. It is important to note that
a factor node can send a message to a variable node once it has received incoming
messages from all other neighbouring variable nodes.

Finally, we derive an expression for evaluating the messages from variable nodes
to factor nodes, again by making use of the (sub-)graph factorization. From Fig-
ure 8.48, we see that term Gm(xm, Xsm) associated with node xm is given by a
product of terms Fl(xm, Xml) each associated with one of the factor nodes fl that is
linked to node xm (excluding node fs), so that

Gm(xm, Xsm) =

Fl(xm, Xml)

(8.68)

l∈ne(xm)\fs

where the product is taken over all neighbours of node xm except for node fs. Note
that each of the factors Fl(xm, Xml) represents a subtree of the original graph of
precisely the same kind as introduced in (8.62). Substituting (8.68) into (8.67), we

Figure 8.47 Illustration of the factorization of the subgraph as-

sociated with factor node fs.

8.4. Inference in Graphical Models

405

xM

µxM→fs(xM )

fs

µfs→x(x)

x

xm

Gm(xm, Xsm)



 

fL

fl

Fl(xm, Xml)

xm

fs




406

8. GRAPHICAL MODELS

Figure 8.48 Illustration of the evaluation of the message sent by a

variable node to an adjacent factor node.

then obtain

µxm→fs(xm) =

Fl(xm, Xml)

Xml

µfl→xm(xm)

(8.69)

l∈ne(xm)\fs

=

l∈ne(xm)\fs

where we have used the deﬁnition (8.64) of the messages passed from factor nodes to
variable nodes. Thus to evaluate the message sent by a variable node to an adjacent
factor node along the connecting link, we simply take the product of the incoming
messages along all of the other links. Note that any variable node that has only
two neighbours performs no computation but simply passes messages through un-
changed. Also, we note that a variable node can send a message to a factor node
once it has received incoming messages from all other neighbouring factor nodes.

Recall that our goal is to calculate the marginal for variable node x, and that this
marginal is given by the product of incoming messages along all of the links arriving
at that node. Each of these messages can be computed recursively in terms of other
messages. In order to start this recursion, we can view the node x as the root of the
tree and begin at the leaf nodes. From the deﬁnition (8.69), we see that if a leaf node
is a variable node, then the message that it sends along its one and only link is given
by

(8.70)
as illustrated in Figure 8.49(a). Similarly, if the leaf node is a factor node, we see
from (8.66) that the message sent should take the form

µx→f (x) = 1

µf→x(x) = f(x)

(8.71)

Figure 8.49 The sum-product algorithm
begins with messages sent
by the leaf nodes, which de-
pend on whether the leaf
node is (a) a variable node,
or (b) a factor node.

µx→f (x) = 1

µf→x(x) = f(x)

x

f

f

x

(a)

(b)

8.4. Inference in Graphical Models

407

as illustrated in Figure 8.49(b).

At this point, it is worth pausing to summarize the particular version of the sum-
product algorithm obtained so far for evaluating the marginal p(x). We start by
viewing the variable node x as the root of the factor graph and initiating messages
at the leaves of the graph using (8.70) and (8.71). The message passing steps (8.66)
and (8.69) are then applied recursively until messages have been propagated along
every link, and the root node has received messages from all of its neighbours. Each
node can send a message towards the root once it has received messages from all
of its other neighbours. Once the root node has received messages from all of its
neighbours, the required marginal can be evaluated using (8.63). We shall illustrate
this process shortly.

To see that each node will always receive enough messages to be able to send out
a message, we can use a simple inductive argument as follows. Clearly, for a graph
comprising a variable root node connected directly to several factor leaf nodes, the
algorithm trivially involves sending messages of the form (8.71) directly from the
leaves to the root. Now imagine building up a general graph by adding nodes one at
a time, and suppose that for some particular graph we have a valid algorithm. When
one more (variable or factor) node is added, it can be connected only by a single
link because the overall graph must remain a tree, and so the new node will be a leaf
node. It therefore sends a message to the node to which it is linked, which in turn
will therefore receive all the messages it requires in order to send its own message
towards the root, and so again we have a valid algorithm, thereby completing the
proof.

Now suppose we wish to ﬁnd the marginals for every variable node in the graph.
This could be done by simply running the above algorithm afresh for each such node.
However, this would be very wasteful as many of the required computations would
be repeated. We can obtain a much more efﬁcient procedure by ‘overlaying’ these
multiple message passing algorithms to obtain the general sum-product algorithm
as follows. Arbitrarily pick any (variable or factor) node and designate it as the
root. Propagate messages from the leaves to the root as before. At this point, the
root node will have received messages from all of its neighbours. It can therefore
send out messages to all of its neighbours. These in turn will then have received
messages from all of their neighbours and so can send out messages along the links
going away from the root, and so on. In this way, messages are passed outwards
from the root all the way to the leaves. By now, a message will have passed in
both directions across every link in the graph, and every node will have received
a message from all of its neighbours. Again a simple inductive argument can be
used to verify the validity of this message passing protocol. Because every variable
node will have received messages from all of its neighbours, we can readily calculate
the marginal distribution for every variable in the graph. The number of messages
that have to be computed is given by twice the number of links in the graph and
so involves only twice the computation involved in ﬁnding a single marginal. By
comparison, if we had run the sum-product algorithm separately for each node, the
amount of computation would grow quadratically with the size of the graph. Note
that this algorithm is in fact independent of which node was designated as the root,

Exercise 8.20











408

8. GRAPHICAL MODELS

Figure 8.50 The sum-product algorithm can be viewed
purely in terms of messages sent out by factor
nodes to other factor nodes.
In this example,
the outgoing message shown by the blue arrow
is obtained by taking the product of all the in-
coming messages shown by green arrows, mul-
tiplying by the factor fs, and marginalizing over
the variables x1 and x2.

x1

x2

fs

x3

and indeed the notion of one node having a special status was introduced only as a
convenient way to explain the message passing protocol.

Next suppose we wish to ﬁnd the marginal distributions p(xs) associated with
the sets of variables belonging to each of the factors. By a similar argument to that
used above, it is easy to see that the marginal associated with a factor is given by the
product of messages arriving at the factor node and the local factor at that node

Exercise 8.21

p(xs) = fs(xs)

i∈ne(fs)

µxi→fs(xi)

(8.72)

If the factors are
in complete analogy with the marginals at the variable nodes.
parameterized functions and we wish to learn the values of the parameters using
the EM algorithm, then these marginals are precisely the quantities we will need to
calculate in the E step, as we shall see in detail when we discuss the hidden Markov
model in Chapter 13.

The message sent by a variable node to a factor node, as we have seen, is simply
the product of the incoming messages on other links. We can if we wish view the
sum-product algorithm in a slightly different form by eliminating messages from
variable nodes to factor nodes and simply considering messages that are sent out by
factor nodes. This is most easily seen by considering the example in Figure 8.50.

So far, we have rather neglected the issue of normalization. If the factor graph
was derived from a directed graph, then the joint distribution is already correctly nor-
malized, and so the marginals obtained by the sum-product algorithm will similarly
be normalized correctly. However, if we started from an undirected graph, then in
general there will be an unknown normalization coefﬁcient 1/Z. As with the simple
chain example of Figure 8.38, this is easily handled by working with an unnormal-
p(x)/Z. We ﬁrst run the
ized version
p(xi). The
sum-product algorithm to ﬁnd the corresponding unnormalized marginals
coefﬁcient 1/Z is then easily obtained by normalizing any one of these marginals,
and this is computationally efﬁcient because the normalization is done over a single
variable rather than over the entire set of variables as would be required to normalize
p(x) directly.

p(x) of the joint distribution, where p(x) =

At this point, it may be helpful to consider a simple example to illustrate the
operation of the sum-product algorithm. Figure 8.51 shows a simple 4-node factor










x2

x1

x2

x2

409

x3

(8.73)

(8.74)

(8.75)

(8.76)

(8.77)

(8.78)

(8.79)

(8.80)

(8.81)

(8.82)

(8.83)

(8.84)

(8.85)

Figure 8.51 A simple factor graph used to illustrate the

x1

sum-product algorithm.

8.4. Inference in Graphical Models

x2

fa

fb

fc

x4

graph whose unnormalized joint distribution is given by

p(x) = fa(x1, x2)fb(x2, x3)fc(x2, x4).

In order to apply the sum-product algorithm to this graph, let us designate node x3
as the root, in which case there are two leaf nodes x1 and x4. Starting with the leaf
nodes, we then have the following sequence of six messages

The direction of ﬂow of these messages is illustrated in Figure 8.52. Once this mes-
sage propagation is complete, we can then propagate messages from the root node
out to the leaf nodes, and these are given by

fa(x1, x2)

µx1→fa(x1) = 1
µfa→x2(x2) =
µx4→fc(x4) = 1
µfc→x2(x2) =
µx2→fb(x2) = µfa→x2(x2)µfc→x2(x2)
µfb→x3(x3) =
fb(x2, x3)µx2→fb.

fc(x2, x4)

x4

x3

fb(x2, x3)

µx3→fb(x3) = 1
µfb→x2(x2) =
µx2→fa(x2) = µfb→x2(x2)µfc→x2(x2)
µfa→x1(x1) =
µx2→fc(x2) = µfa→x2(x2)µfb→x2(x2)
µfc→x4(x4) =

fa(x1, x2)µx2→fa(x2)

fc(x2, x4)µx2→fc(x2).




x4

x4



x3








x1

x3

x1

x1

x2

x1

 


x3









 

x4
(b)

x2

x3

 





410

8. GRAPHICAL MODELS

x1

x2

x4
(a)

Figure 8.52 Flow of messages for the sum-product algorithm applied to the example graph in Figure 8.51. (a)
From the leaf nodes x1 and x4 towards the root node x3. (b) From the root node towards the leaf nodes.

One message has now passed in each direction across each link, and we can now
evaluate the marginals. As a simple check, let us verify that the marginal p(x2) is
given by the correct expression. Using (8.63) and substituting for the messages using
the above results, we have

p(x2) = µfa→x2(x2)µfb→x2(x2)µfc→x2(x2)
fb(x2, x3)

fa(x1, x2)

=

fc(x2, x4)

x4

fa(x1, x2)fb(x2, x3)fc(x2, x4)

=

=

p(x)

(8.86)

as required.

So far, we have assumed that all of the variables in the graph are hidden. In most
practical applications, a subset of the variables will be observed, and we wish to cal-
culate posterior distributions conditioned on these observations. Observed nodes are
easily handled within the sum-product algorithm as follows. Suppose we partition x
into hidden variables h and observed variables v, and that the observed value of v
vi),
is denoted
where I(v,
v) = 0 otherwise. This product corresponds
v). By run-
to p(h, v =
ning the sum-product algorithm, we can efﬁciently calculate the posterior marginals
p(hi|v =
v) up to a normalization coefﬁcient whose value can be found efﬁciently
using a local computation. Any summations over variables in v then collapse into a
single term.

v. Then we simply multiply the joint distribution p(x) by
v) = 1 if v =
v) and hence is an unnormalized version of p(h|v =

v and I(v,

i I(vi,

We have assumed throughout this section that we are dealing with discrete vari-
ables. However, there is nothing speciﬁc to discrete variables either in the graphical
framework or in the probabilistic construction of the sum-product algorithm. For

8.4. Inference in Graphical Models

411

Table 8.1 Example of a joint distribution over two binary variables for
which the maximum of the joint distribution occurs for dif-
ferent variable values compared to the maxima of the two
marginals.

x = 0
0.3
0.3

x = 1
0.4
0.0

y = 0
y = 1

Section 13.3

continuous variables the summations are simply replaced by integrations. We shall
give an example of the sum-product algorithm applied to a graph of linear-Gaussian
variables when we consider linear dynamical systems.

8.4.5 The max-sum algorithm
The sum-product algorithm allows us to take a joint distribution p(x) expressed
as a factor graph and efﬁciently ﬁnd marginals over the component variables. Two
other common tasks are to ﬁnd a setting of the variables that has the largest prob-
ability and to ﬁnd the value of that probability. These can be addressed through a
closely related algorithm called max-sum, which can be viewed as an application of
dynamic programming in the context of graphical models (Cormen et al., 2001).

A simple approach to ﬁnding latent variable values having high probability
would be to run the sum-product algorithm to obtain the marginals p(xi) for ev-
ery variable, and then, for each marginal in turn, to ﬁnd the value x
i that maximizes
that marginal. However, this would give the set of values that are individually the
most probable. In practice, we typically wish to ﬁnd the set of values that jointly
have the largest probability, in other words the vector xmax that maximizes the joint
distribution, so that

xmax = arg max

p(x)

for which the corresponding value of the joint probability will be given by

x

p(xmax) = max

x

p(x).

(8.87)

(8.88)

In general, xmax is not the same as the set of x
i values, as we can easily show using
a simple example. Consider the joint distribution p(x, y) over two binary variables
x, y ∈ {0, 1} given in Table 8.1. The joint distribution is maximized by setting x =
1 and y = 0, corresponding the value 0.4. However, the marginal for p(x), obtained
by summing over both values of y, is given by p(x = 0) = 0.6 and p(x = 1) = 0.4,
and similarly the marginal for y is given by p(y = 0) = 0.7 and p(y = 1) = 0.3,
and so the marginals are maximized by x = 0 and y = 0, which corresponds to a
value of 0.3 for the joint distribution. In fact, it is not difﬁcult to construct examples
for which the set of individually most probable values has probability zero under the
joint distribution.

We therefore seek an efﬁcient algorithm for ﬁnding the value of x that maxi-
mizes the joint distribution p(x) and that will allow us to obtain the value of the
joint distribution at its maximum. To address the second of these problems, we shall
simply write out the max operator in terms of its components

max

x

p(x) = max
x1

. . . max
xM

p(x)

(8.89)

Exercise 8.27










412

8. GRAPHICAL MODELS

where M is the total number of variables, and then substitute for p(x) using its
expansion in terms of a product of factors. In deriving the sum-product algorithm,
we made use of the distributive law (8.53) for multiplication. Here we make use of
the analogous law for the max operator

max(ab, ac) = a max(b, c)

(8.90)

which holds if a � 0 (as will always be the case for the factors in a graphical model).
This allows us to exchange products with maximizations.

Consider ﬁrst the simple example of a chain of nodes described by (8.49). The

evaluation of the probability maximum can be written as

max

x

p(x) =

1
Z

=

1
Z

max
x1

max
x1

··· max
ψ1,2(x1, x2)

xN

[ψ1,2(x1, x2)··· ψN−1,N (xN−1, xN )]
··· max

ψN−1,N (xN−1, xN )

xN

.

As with the calculation of marginals, we see that exchanging the max and product
operators results in a much more efﬁcient computation, and one that is easily inter-
preted in terms of messages passed from node xN backwards along the chain to node
x1.

We can readily generalize this result to arbitrary tree-structured factor graphs
by substituting the expression (8.59) for the factor graph expansion into (8.89) and
again exchanging maximizations with products. The structure of this calculation is
identical to that of the sum-product algorithm, and so we can simply translate those
results into the present context. In particular, suppose that we designate a particular
variable node as the ‘root’ of the graph. Then we start a set of messages propagating
inwards from the leaves of the tree towards the root, with each node sending its
message towards the root once it has received all incoming messages from its other
neighbours. The ﬁnal maximization is performed over the product of all messages
arriving at the root node, and gives the maximum value for p(x). This could be called
the max-product algorithm and is identical to the sum-product algorithm except that
summations are replaced by maximizations. Note that at this stage, messages have
been sent from leaves to the root, but not in the other direction.

In practice, products of many small probabilities can lead to numerical under-
ﬂow problems, and so it is convenient to work with the logarithm of the joint distri-
bution. The logarithm is a monotonic function, so that if a > b then ln a > ln b, and
hence the max operator and the logarithm function can be interchanged, so that

ln

max

x

p(x)

= max

x

ln p(x).

The distributive property is preserved because

max(a + b, a + c) = a + max(b, c).

(8.91)

(8.92)

Thus taking the logarithm simply has the effect of replacing the products in the
max-product algorithm with sums, and so we obtain the max-sum algorithm. From

⎡⎣



⎤⎦

(8.93)

(8.94)

(8.95)
(8.96)

(8.97)


⎤⎦
⎤⎦

.

⎡⎣
⎡⎣

s∈ne(x)

8.4. Inference in Graphical Models

413

the results (8.66) and (8.69) derived earlier for the sum-product algorithm, we can
readily write down the max-sum algorithm in terms of message passing simply by
replacing ‘sum’ with ‘max’ and replacing products with sums of logarithms to give

µf→x(x) = max

x1,...,xM

ln f(x, x1, . . . , xM ) +

µxm→f (xm)

m∈ne(fs)\x

µx→f (x) =

l∈ne(x)\f

µfl→x(x).

The initial messages sent by the leaf nodes are obtained by analogy with (8.70) and
(8.71) and are given by

while at the root node the maximum probability can then be computed, by analogy
with (8.63), using

µx→f (x) = 0
µf→x(x) = ln f(x)

pmax = max

x

µfs→x(x)

So far, we have seen how to ﬁnd the maximum of the joint distribution by prop-
agating messages from the leaves to an arbitrarily chosen root node. The result will
be the same irrespective of which node is chosen as the root. Now we turn to the
second problem of ﬁnding the conﬁguration of the variables for which the joint dis-
tribution attains this maximum value. So far, we have sent messages from the leaves
to the root. The process of evaluating (8.97) will also give the value xmax for the
most probable value of the root node variable, deﬁned by

xmax = arg max

x

µfs→x(x)

.

s∈ne(x)

(8.98)

At this point, we might be tempted simply to continue with the message passing al-
gorithm and send messages from the root back out to the leaves, using (8.93) and
(8.94), then apply (8.98) to all of the remaining variable nodes. However, because
we are now maximizing rather than summing, it is possible that there may be mul-
tiple conﬁgurations of x all of which give rise to the maximum value for p(x). In
such cases, this strategy can fail because it is possible for the individual variable
values obtained by maximizing the product of messages at each node to belong to
different maximizing conﬁgurations, giving an overall conﬁguration that no longer
corresponds to a maximum.

The problem can be resolved by adopting a rather different kind of message
passing from the root node to the leaves. To see how this works, let us return once
again to the simple chain example of N variables x1, . . . , xN each having K states,







	

	

	

414

8. GRAPHICAL MODELS

Figure 8.53 A lattice, or trellis, diagram show-
ing explicitly the K possible states (one per row
of the diagram) for each of the variables xn in the
chain model.
In this illustration K = 3. The ar-
row shows the direction of message passing in the
max-product algorithm. For every state k of each
variable xn (corresponding to column n of the dia-
gram) the function φ(xn) deﬁnes a unique state at
the previous variable, indicated by the black lines.
The two paths through the lattice correspond to
conﬁgurations that give the global maximum of the
joint probability distribution, and either of these
can be found by tracing back along the black lines
in the opposite direction to the arrow.

k = 1

k = 2

k = 3

n − 2

n − 1

n

n + 1

corresponding to the graph shown in Figure 8.38. Suppose we take node xN to be
the root node. Then in the ﬁrst phase, we propagate messages from the leaf node x1
to the root node using

µxn→fn,n+1(xn) = µfn−1,n→xn(xn)
µfn−1,n→xn(xn) = max
xn−1

ln fn−1,n(xn−1, xn) + µxn−1→f n−1,n(xn)

which follow from applying (8.94) and (8.93) to this particular graph. The initial
message sent from the leaf node is simply

The most probable value for xN is then given by

µx1→f1,2(x1) = 0.

N = arg max
xmax

xN

µfN−1,N→xN (xN )

.

(8.99)

(8.100)

Now we need to determine the states of the previous variables that correspond to the
same maximizing conﬁguration. This can be done by keeping track of which values
of the variables gave rise to the maximum state of each variable, in other words by
storing quantities given by

φ(xn) = arg max

xn−1

ln fn−1,n(xn−1, xn) + µxn−1→f n−1,n(xn)

.

(8.101)

To understand better what is happening, it is helpful to represent the chain of vari-
ables in terms of a lattice or trellis diagram as shown in Figure 8.53. Note that this
is not a probabilistic graphical model because the nodes represent individual states
of variables, while each variable corresponds to a column of such states in the di-
agram. For each state of a given variable, there is a unique state of the previous
variable that maximizes the probability (ties are broken either systematically or at
random), corresponding to the function φ(xn) given by (8.101), and this is indicated

8.4. Inference in Graphical Models

415

by the lines connecting the nodes. Once we know the most probable value of the ﬁ-
nal node xN , we can then simply follow the link back to ﬁnd the most probable state
of node xN−1 and so on back to the initial node x1. This corresponds to propagating
a message back down the chain using

n−1 = φ(xmax
n )
xmax

(8.102)

and is known as back-tracking. Note that there could be several values of xn−1 all
of which give the maximum value in (8.101). Provided we chose one of these values
when we do the back-tracking, we are assured of a globally consistent maximizing
conﬁguration.

In Figure 8.53, we have indicated two paths, each of which we shall suppose
corresponds to a global maximum of the joint probability distribution. If k = 2
and k = 3 each represent possible values of xmax
N , then starting from either state
and tracing back along the black lines, which corresponds to iterating (8.102), we
obtain a valid global maximum conﬁguration. Note that if we had run a forward
pass of max-sum message passing followed by a backward pass and then applied
(8.98) at each node separately, we could end up selecting some states from one path
and some from the other path, giving an overall conﬁguration that is not a global
maximizer. We see that it is necessary instead to keep track of the maximizing states
during the forward pass using the functions φ(xn) and then use back-tracking to ﬁnd
a consistent solution.

The extension to a general tree-structured factor graph should now be clear. If
a message is sent from a factor node f to a variable node x, a maximization is
performed over all other variable nodes x1, . . . , xM that are neighbours of that fac-
tor node, using (8.93). When we perform this maximization, we keep a record of
which values of the variables x1, . . . , xM gave rise to the maximum. Then in the
back-tracking step, having found xmax, we can then use these stored values to as-
sign consistent maximizing states xmax
M . The max-sum algorithm, with
back-tracking, gives an exact maximizing conﬁguration for the variables provided
the factor graph is a tree. An important application of this technique is for ﬁnding
the most probable sequence of hidden states in a hidden Markov model, in which
case it is known as the Viterbi algorithm.

, . . . , xmax

1

As with the sum-product algorithm, the inclusion of evidence in the form of
observed variables is straightforward. The observed variables are clamped to their
observed values, and the maximization is performed over the remaining hidden vari-
ables. This can be shown formally by including identity functions for the observed
variables into the factor functions, as we did for the sum-product algorithm.

It is interesting to compare max-sum with the iterated conditional modes (ICM)
algorithm described on page 389. Each step in ICM is computationally simpler be-
cause the ‘messages’ that are passed from one node to the next comprise a single
value consisting of the new state of the node for which the conditional distribution
is maximized. The max-sum algorithm is more complex because the messages are
functions of node variables x and hence comprise a set of K values for each pos-
sible state of x. Unlike max-sum, however, ICM is not guaranteed to ﬁnd a global
maximum even for tree-structured graphs.

Section 13.2

416

8. GRAPHICAL MODELS

8.4.6 Exact inference in general graphs
The sum-product and max-sum algorithms provide efﬁcient and exact solutions
to inference problems in tree-structured graphs. For many practical applications,
however, we have to deal with graphs having loops.

The message passing framework can be generalized to arbitrary graph topolo-
gies, giving an exact inference procedure known as the junction tree algorithm (Lau-
ritzen and Spiegelhalter, 1988; Jordan, 2007). Here we give a brief outline of the
key steps involved. This is not intended to convey a detailed understanding of the
algorithm, but rather to give a ﬂavour of the various stages involved. If the starting
point is a directed graph, it is ﬁrst converted to an undirected graph by moraliza-
tion, whereas if starting from an undirected graph this step is not required. Next the
graph is triangulated, which involves ﬁnding chord-less cycles containing four or
more nodes and adding extra links to eliminate such chord-less cycles. For instance,
in the graph in Figure 8.36, the cycle A–C–B–D–A is chord-less a link could be
added between A and B or alternatively between C and D. Note that the joint dis-
tribution for the resulting triangulated graph is still deﬁned by a product of the same
potential functions, but these are now considered to be functions over expanded sets
of variables. Next the triangulated graph is used to construct a new tree-structured
undirected graph called a join tree, whose nodes correspond to the maximal cliques
of the triangulated graph, and whose links connect pairs of cliques that have vari-
ables in common. The selection of which pairs of cliques to connect in this way is
important and is done so as to give a maximal spanning tree deﬁned as follows. Of
all possible trees that link up the cliques, the one that is chosen is one for which the
weight of the tree is largest, where the weight for a link is the number of nodes shared
by the two cliques it connects, and the weight for the tree is the sum of the weights
for the links. If the tree is condensed, so that any clique that is a subset of another
clique is absorbed into the larger clique, this gives a junction tree. As a consequence
of the triangulation step, the resulting tree satisﬁes the running intersection property,
which means that if a variable is contained in two cliques, then it must also be con-
tained in every clique on the path that connects them. This ensures that inference
about variables will be consistent across the graph. Finally, a two-stage message
passing algorithm, essentially equivalent to the sum-product algorithm, can now be
applied to this junction tree in order to ﬁnd marginals and conditionals. Although
the junction tree algorithm sounds complicated, at its heart is the simple idea that
we have used already of exploiting the factorization properties of the distribution to
allow sums and products to be interchanged so that partial summations can be per-
formed, thereby avoiding having to work directly with the joint distribution. The
role of the junction tree is to provide a precise and efﬁcient way to organize these
computations. It is worth emphasizing that this is achieved using purely graphical
operations!

The junction tree is exact for arbitrary graphs and is efﬁcient in the sense that
for a given graph there does not in general exist a computationally cheaper approach.
Unfortunately, the algorithm must work with the joint distributions within each node
(each of which corresponds to a clique of the triangulated graph) and so the compu-
tational cost of the algorithm is determined by the number of variables in the largest

8.4. Inference in Graphical Models

417

clique and will grow exponentially with this number in the case of discrete variables.
An important concept is the treewidth of a graph (Bodlaender, 1993), which is de-
ﬁned in terms of the number of variables in the largest clique. In fact, it is deﬁned to
be as one less than the size of the largest clique, to ensure that a tree has a treewidth
of 1. Because there in general there can be multiple different junction trees that can
be constructed from a given starting graph, the treewidth is deﬁned by the junction
tree for which the largest clique has the fewest variables.
If the treewidth of the
original graph is high, the junction tree algorithm becomes impractical.

8.4.7 Loopy belief propagation
For many problems of practical interest, it will not be feasible to use exact in-
ference, and so we need to exploit effective approximation methods. An important
class of such approximations, that can broadly be called variational methods, will be
discussed in detail in Chapter 10. Complementing these deterministic approaches is
a wide range of sampling methods, also called Monte Carlo methods, that are based
on stochastic numerical sampling from distributions and that will be discussed at
length in Chapter 11.

Here we consider one simple approach to approximate inference in graphs with
loops, which builds directly on the previous discussion of exact inference in trees.
The idea is simply to apply the sum-product algorithm even though there is no guar-
antee that it will yield good results. This approach is known as loopy belief propa-
gation (Frey and MacKay, 1998) and is possible because the message passing rules
(8.66) and (8.69) for the sum-product algorithm are purely local. However, because
the graph now has cycles, information can ﬂow many times around the graph. For
some models, the algorithm will converge, whereas for others it will not.

In order to apply this approach, we need to deﬁne a message passing schedule.
Let us assume that one message is passed at a time on any given link and in any
given direction. Each message sent from a node replaces any previous message sent
in the same direction across the same link and will itself be a function only of the
most recent messages received by that node at previous steps of the algorithm.

We have seen that a message can only be sent across a link from a node when
all other messages have been received by that node across its other links. Because
there are loops in the graph, this raises the problem of how to initiate the message
passing algorithm. To resolve this, we suppose that an initial message given by the
unit function has been passed across every link in each direction. Every node is then
in a position to send a message.

There are now many possible ways to organize the message passing schedule.
For example, the ﬂooding schedule simultaneously passes a message across every
link in both directions at each time step, whereas schedules that pass one message at
a time are called serial schedules.

Following Kschischnang et al. (2001), we will say that a (variable or factor)
node a has a message pending on its link to a node b if node a has received any
message on any of its other links since the last time it send a message to b. Thus,
when a node receives a message on one of its links, this creates pending messages
on all of its other links. Only pending messages need to be transmitted because

418

8. GRAPHICAL MODELS

Exercise 8.29

other messages would simply duplicate the previous message on the same link. For
graphs that have a tree structure, any schedule that sends only pending messages
will eventually terminate once a message has passed in each direction across every
link. At this point, there are no pending messages, and the product of the received
messages at every variable give the exact marginal. In graphs having loops, however,
the algorithm may never terminate because there might always be pending messages,
although in practice it is generally found to converge within a reasonable time for
most applications. Once the algorithm has converged, or once it has been stopped
if convergence is not observed, the (approximate) local marginals can be computed
using the product of the most recently received incoming messages to each variable
node or factor node on every link.

In some applications, the loopy belief propagation algorithm can give poor re-
sults, whereas in other applications it has proven to be very effective. In particular,
state-of-the-art algorithms for decoding certain kinds of error-correcting codes are
equivalent to loopy belief propagation (Gallager, 1963; Berrou et al., 1993; McEliece
et al., 1998; MacKay and Neal, 1999; Frey, 1998).

8.4.8 Learning the graph structure
In our discussion of inference in graphical models, we have assumed that the
structure of the graph is known and ﬁxed. However, there is also interest in go-
ing beyond the inference problem and learning the graph structure itself from data
(Friedman and Koller, 2003). This requires that we deﬁne a space of possible struc-
tures as well as a measure that can be used to score each structure.

From a Bayesian viewpoint, we would ideally like to compute a posterior dis-
tribution over graph structures and to make predictions by averaging with respect
to this distribution. If we have a prior p(m) over graphs indexed by m, then the
posterior distribution is given by

p(m|D) ∝ p(m)p(D|m)

(8.103)
where D is the observed data set. The model evidence p(D|m) then provides the
score for each model. However, evaluation of the evidence involves marginalization
over the latent variables and presents a challenging computational problem for many
models.

Exploring the space of structures can also be problematic. Because the number
of different graph structures grows exponentially with the number of nodes, it is
often necessary to resort to heuristics to ﬁnd good candidates.

Exercises

8.1 () www By marginalizing out the variables in order, show that the representation
(8.5) for the joint distribution of a directed graph is correctly normalized, provided
each of the conditional distributions is normalized.

8.2 () www Show that the property of there being no directed cycles in a directed
graph follows from the statement that there exists an ordered numbering of the nodes
such that for each node there are no links going to a lower-numbered node.

Table 8.2 The joint distribution over three binary variables.

Exercises

419

a
0
0
0
0
1
1
1
1

b
0
0
1
1
0
0
1
1

c
0
1
0
1
0
1
0
1

p(a, b, c)

0.192
0.144
0.048
0.216
0.192
0.064
0.048
0.096



M

8.3 ( ) Consider three binary variables a, b, c ∈ {0, 1} having the joint distribution
given in Table 8.2. Show by direct evaluation that this distribution has the property
that a and b are marginally dependent, so that p(a, b) = p(a)p(b), but that they
become independent when conditioned on c, so that p(a, b|c) = p(a|c)p(b|c) for
both c = 0 and c = 1.

8.4 ( ) Evaluate the distributions p(a), p(b|c), and p(c|a) corresponding to the joint
distribution given in Table 8.2. Hence show by direct evaluation that p(a, b, c) =
p(a)p(c|a)p(b|c). Draw the corresponding directed graph.

8.5 () www Draw a directed probabilistic graphical model corresponding to the

relevance vector machine described by (7.79) and (7.80).

8.6 () For the model shown in Figure 8.13, we have seen that the number of parameters
required to specify the conditional distribution p(y|x1, . . . , xM ), where xi ∈ {0, 1},
could be reduced from 2M to M + 1 by making use of the logistic sigmoid represen-
tation (8.10). An alternative representation (Pearl, 1988) is given by

p(y = 1|x1, . . . , xM ) = 1 − (1 − µ0)

(1 − µi)xi

i=1

(8.104)

where the parameters µi represent the probabilities p(xi = 1), and µ0 is an additional
parameters satisfying 0 � µ0 � 1. The conditional distribution (8.104) is known as
the noisy-OR. Show that this can be interpreted as a ‘soft’ (probabilistic) form of the
logical OR function (i.e., the function that gives y = 1 whenever at least one of the
xi = 1). Discuss the interpretation of µ0.

8.7 ( ) Using the recursion relations (8.15) and (8.16), show that the mean and covari-
ance of the joint distribution for the graph shown in Figure 8.14 are given by (8.17)
and (8.18), respectively.

8.8 () www Show that a ⊥⊥ b, c | d implies a ⊥⊥ b | d.
8.9 () www Using the d-separation criterion, show that the conditional distribution
for a node x in a directed graph, conditioned on all of the nodes in the Markov
blanket, is independent of the remaining variables in the graph.

420

8. GRAPHICAL MODELS

Figure 8.54 Example of a graphical model used to explore the con-
ditional independence properties of the head-to-head
path a–c–b when a descendant of c, namely the node
d, is observed.

a

b

c

d

8.10 () Consider the directed graph shown in Figure 8.54 in which none of the variables
is observed. Show that a ⊥⊥ b | ∅. Suppose we now observe the variable d. Show
that in general a ⊥⊥ b | d.

8.11 ( ) Consider the example of the car fuel system shown in Figure 8.21, and suppose
that instead of observing the state of the fuel gauge G directly, the gauge is seen by
the driver D who reports to us the reading on the gauge. This report is either that the
gauge shows full D = 1 or that it shows empty D = 0. Our driver is a bit unreliable,
as expressed through the following probabilities

p(D = 1|G = 1) = 0.9
p(D = 0|G = 0) = 0.9.

(8.105)
(8.106)

Suppose that the driver tells us that the fuel gauge shows empty, in other words
that we observe D = 0. Evaluate the probability that the tank is empty given only
this observation. Similarly, evaluate the corresponding probability given also the
observation that the battery is ﬂat, and note that this second probability is lower.
Discuss the intuition behind this result, and relate the result to Figure 8.54.

8.12 () www Show that there are 2M (M−1)/2 distinct undirected graphs over a set of

M distinct random variables. Draw the 8 possibilities for the case of M = 3.

8.13 () Consider the use of iterated conditional modes (ICM) to minimize the energy
function given by (8.42). Write down an expression for the difference in the values
of the energy associated with the two states of a particular variable xj, with all other
variables held ﬁxed, and show that it depends only on quantities that are local to xj
in the graph.

8.14 () Consider a particular case of the energy function given by (8.42) in which the
coefﬁcients β = h = 0. Show that the most probable conﬁguration of the latent
variables is given by xi = yi for all i.

8.15 ( ) www Show that the joint distribution p(xn−1, xn) for two neighbouring
nodes in the graph shown in Figure 8.38 is given by an expression of the form (8.58).

Exercises

421

8.16 ( ) Consider the inference problem of evaluating p(xn|xN ) for the graph shown
in Figure 8.38, for all nodes n ∈ {1, . . . , N − 1}. Show that the message passing
algorithm discussed in Section 8.4.1 can be used to solve this efﬁciently, and discuss
which messages are modiﬁed and in what way.

8.17 ( ) Consider a graph of the form shown in Figure 8.38 having N = 5 nodes, in
which nodes x3 and x5 are observed. Use d-separation to show that x2 ⊥⊥ x5 | x3.
Show that if the message passing algorithm of Section 8.4.1 is applied to the evalu-
ation of p(x2|x3, x5), the result will be independent of the value of x5.

8.18 ( ) www Show that a distribution represented by a directed tree can trivially
be written as an equivalent distribution over the corresponding undirected tree. Also
show that a distribution expressed as an undirected tree can, by suitable normaliza-
tion of the clique potentials, be written as a directed tree. Calculate the number of
distinct directed trees that can be constructed from a given undirected tree.

8.19 ( ) Apply the sum-product algorithm derived in Section 8.4.4 to the chain-of-
nodes model discussed in Section 8.4.1 and show that the results (8.54), (8.55), and
(8.57) are recovered as a special case.

8.20 () www Consider the message passing protocol for the sum-product algorithm on
a tree-structured factor graph in which messages are ﬁrst propagated from the leaves
to an arbitrarily chosen root node and then from the root node out to the leaves. Use
proof by induction to show that the messages can be passed in such an order that
at every step, each node that must send a message has received all of the incoming
messages necessary to construct its outgoing messages.

8.21 ( ) www Show that the marginal distributions p(xs) over the sets of variables
xs associated with each of the factors fx(xs) in a factor graph can be found by ﬁrst
running the sum-product message passing algorithm and then evaluating the required
marginals using (8.72).

8.22 () Consider a tree-structured factor graph, in which a given subset of the variable
nodes form a connected subgraph (i.e., any variable node of the subset is connected
to at least one of the other variable nodes via a single factor node). Show how the
sum-product algorithm can be used to compute the marginal distribution over that
subset.

8.23 ( ) www In Section 8.4.4, we showed that the marginal distribution p(xi) for a
variable node xi in a factor graph is given by the product of the messages arriving at
this node from neighbouring factor nodes in the form (8.63). Show that the marginal
p(xi) can also be written as the product of the incoming message along any one of
the links with the outgoing message along the same link.

8.24 ( ) Show that the marginal distribution for the variables xs in a factor fs(xs) in
a tree-structured factor graph, after running the sum-product message passing algo-
rithm, can be written as the product of the message arriving at the factor node along
all its links, times the local factor f(xs), in the form (8.72).









422

8. GRAPHICAL MODELS

8.25 ( )

In (8.86), we veriﬁed that the sum-product algorithm run on the graph in
Figure 8.51 with node x3 designated as the root node gives the correct marginal for
x2. Show that the correct marginals are obtained also for x1 and x3. Similarly, show
that the use of the result (8.72) after running the sum-product algorithm on this graph
gives the correct joint distribution for x1, x2.

8.26 () Consider a tree-structured factor graph over discrete variables, and suppose we
wish to evaluate the joint distribution p(xa, xb) associated with two variables xa and
xb that do not belong to a common factor. Deﬁne a procedure for using the sum-
product algorithm to evaluate this joint distribution in which one of the variables is
successively clamped to each of its allowed values.

8.27 ( ) Consider two discrete variables x and y each having three possible states, for
example x, y ∈ {0, 1, 2}. Construct a joint distribution p(x, y) over these variables
x that maximizes the marginal p(x), along with
having the property that the value
y that maximizes the marginal p(y), together have probability zero under
the value
the joint distribution, so that p(

y) = 0.

x,

8.28 ( ) www The concept of a pending message in the sum-product algorithm for
a factor graph was deﬁned in Section 8.4.7. Show that if the graph has one or more
cycles, there will always be at least one pending message irrespective of how long
the algorithm runs.

8.29 ( ) www Show that if the sum-product algorithm is run on a factor graph with a
tree structure (no loops), then after a ﬁnite number of messages have been sent, there
will be no pending messages.

9

Mixture Models

and EM

If we deﬁne a joint distribution over observed and latent variables, the correspond-
ing distribution of the observed variables alone is obtained by marginalization. This
allows relatively complex marginal distributions over observed variables to be ex-
pressed in terms of more tractable joint distributions over the expanded space of
observed and latent variables. The introduction of latent variables thereby allows
complicated distributions to be formed from simpler components. In this chapter,
we shall see that mixture distributions, such as the Gaussian mixture discussed in
Section 2.3.9, can be interpreted in terms of discrete latent variables. Continuous
latent variables will form the subject of Chapter 12.

As well as providing a framework for building more complex probability dis-
tributions, mixture models can also be used to cluster data. We therefore begin our
discussion of mixture distributions by considering the problem of ﬁnding clusters
in a set of data points, which we approach ﬁrst using a nonprobabilistic technique
called the K-means algorithm (Lloyd, 1982). Then we introduce the latent variable

423

Section 9.1

424

9. MIXTURE MODELS AND EM

Section 9.2

Section 9.3

Section 9.4

view of mixture distributions in which the discrete latent variables can be interpreted
as deﬁning assignments of data points to speciﬁc components of the mixture. A gen-
eral technique for ﬁnding maximum likelihood estimators in latent variable models
is the expectation-maximization (EM) algorithm. We ﬁrst of all use the Gaussian
mixture distribution to motivate the EM algorithm in a fairly informal way, and then
we give a more careful treatment based on the latent variable viewpoint. We shall
see that the K-means algorithm corresponds to a particular nonprobabilistic limit of
EM applied to mixtures of Gaussians. Finally, we discuss EM in some generality.

Gaussian mixture models are widely used in data mining, pattern recognition,
machine learning, and statistical analysis. In many applications, their parameters are
determined by maximum likelihood, typically using the EM algorithm. However, as
we shall see there are some signiﬁcant limitations to the maximum likelihood ap-
proach, and in Chapter 10 we shall show that an elegant Bayesian treatment can be
given using the framework of variational inference. This requires little additional
computation compared with EM, and it resolves the principal difﬁculties of maxi-
mum likelihood while also allowing the number of components in the mixture to be
inferred automatically from the data.



N



K

9.1. K-means Clustering

We begin by considering the problem of identifying groups, or clusters, of data points
in a multidimensional space. Suppose we have a data set {x1, . . . , xN} consisting
of N observations of a random D-dimensional Euclidean variable x. Our goal is to
partition the data set into some number K of clusters, where we shall suppose for
the moment that the value of K is given. Intuitively, we might think of a cluster as
comprising a group of data points whose inter-point distances are small compared
with the distances to points outside of the cluster. We can formalize this notion by
ﬁrst introducing a set of D-dimensional vectors µk, where k = 1, . . . , K, in which
µk is a prototype associated with the kth cluster. As we shall see shortly, we can
think of the µk as representing the centres of the clusters. Our goal is then to ﬁnd
an assignment of data points to clusters, as well as a set of vectors {µk}, such that
the sum of the squares of the distances of each data point to its closest vector µk, is
a minimum.

It is convenient at this point to deﬁne some notation to describe the assignment
of data points to clusters. For each data point xn, we introduce a corresponding set
of binary indicator variables rnk ∈ {0, 1}, where k = 1, . . . , K describing which of
the K clusters the data point xn is assigned to, so that if data point xn is assigned to
cluster k then rnk = 1, and rnj = 0 for j = k. This is known as the 1-of-K coding
scheme. We can then deﬁne an objective function, sometimes called a distortion
measure, given by

J =

n=1

k=1

rnkxn − µk2

(9.1)

which represents the sum of the squares of the distances of each data point to its





N




which we can easily solve for µk to give

2

n=1

rnk(xn − µk) = 0

µk =

n rnkxn
n rnk

.

(9.3)

(9.4)

The denominator in this expression is equal to the number of points assigned to
cluster k, and so this result has a simple interpretation, namely set µk equal to the
mean of all of the data points xn assigned to cluster k. For this reason, the procedure
is known as the K-means algorithm.

The two phases of re-assigning data points to clusters and re-computing the clus-
ter means are repeated in turn until there is no further change in the assignments (or
until some maximum number of iterations is exceeded). Because each phase reduces
the value of the objective function J, convergence of the algorithm is assured. How-
ever, it may converge to a local rather than global minimum of J. The convergence
properties of the K-means algorithm were studied by MacQueen (1967).

The K-means algorithm is illustrated using the Old Faithful data set in Fig-
ure 9.1. For the purposes of this example, we have made a linear re-scaling of the
data, known as standardizing, such that each of the variables has zero mean and
unit standard deviation. For this example, we have chosen K = 2, and so in this

9.1. K-means Clustering

425

assigned vector µk. Our goal is to ﬁnd values for the {rnk} and the {µk} so as to
minimize J. We can do this through an iterative procedure in which each iteration
involves two successive steps corresponding to successive optimizations with respect
to the rnk and the µk. First we choose some initial values for the µk. Then in the ﬁrst
phase we minimize J with respect to the rnk, keeping the µk ﬁxed. In the second
phase we minimize J with respect to the µk, keeping rnk ﬁxed. This two-stage
optimization is then repeated until convergence. We shall see that these two stages
of updating rnk and updating µk correspond respectively to the E (expectation) and
M (maximization) steps of the EM algorithm, and to emphasize this we shall use the
terms E step and M step in the context of the K-means algorithm.

Consider ﬁrst the determination of the rnk. Because J in (9.1) is a linear func-
tion of rnk, this optimization can be performed easily to give a closed form solution.
The terms involving different n are independent and so we can optimize for each
n separately by choosing rnk to be 1 for whichever value of k gives the minimum
value of xn − µk2. In other words, we simply assign the nth data point to the
closest cluster centre. More formally, this can be expressed as
if k = arg minj xn − µj2
otherwise.

rnk =

(9.2)

1
0

Now consider the optimization of the µk with the rnk held ﬁxed. The objective
function J is a quadratic function of µk, and it can be minimized by setting its
derivative with respect to µk to zero giving

Section 9.4

Exercise 9.1

Appendix A

426

9. MIXTURE MODELS AND EM

(a)

−2

(d)

−2

(g)

2

0

−2

2

0

−2

2

0

−2

0

2

0

2

(b)

−2

(e)

−2

(h)

2

0

−2

2

0

−2

2

0

−2

0

2

(c)

−2

(f)

2

0

−2

2

0

−2

0

2

0

2

−2

0

2

2

(i)

0

−2

−2

0

2

−2

0

2

−2

0

2

Figure 9.1 Illustration of the K-means algorithm using the re-scaled Old Faithful data set. (a) Green points
denote the data set in a two-dimensional Euclidean space. The initial choices for centres µ1 and µ2 are shown
by the red and blue crosses, respectively. (b) In the initial E step, each data point is assigned either to the red
cluster or to the blue cluster, according to which cluster centre is nearer. This is equivalent to classifying the
points according to which side of the perpendicular bisector of the two cluster centres, shown by the magenta
line, they lie on. (c) In the subsequent M step, each cluster centre is re-computed to be the mean of the points
assigned to the corresponding cluster. (d)–(i) show successive E and M steps through to ﬁnal convergence of
the algorithm.

9.1. K-means Clustering

427

Figure 9.2 Plot of the cost function J given by
(9.1) after each E step (blue points)
and M step (red points) of the K-
means algorithm for the example
shown in Figure 9.1. The algo-
rithm has converged after the third
M step, and the ﬁnal EM cycle pro-
duces no changes in either the as-
signments or the prototype vectors.

1000

J

500

0

1

2

3

4

case, the assignment of each data point to the nearest cluster centre is equivalent to a
classiﬁcation of the data points according to which side they lie of the perpendicular
bisector of the two cluster centres. A plot of the cost function J given by (9.1) for
the Old Faithful example is shown in Figure 9.2.

Note that we have deliberately chosen poor initial values for the cluster centres
so that the algorithm takes several steps before convergence. In practice, a better
initialization procedure would be to choose the cluster centres µk to be equal to a
random subset of K data points. It is also worth noting that the K-means algorithm
itself is often used to initialize the parameters in a Gaussian mixture model before
applying the EM algorithm.

A direct implementation of the K-means algorithm as discussed here can be
relatively slow, because in each E step it is necessary to compute the Euclidean dis-
tance between every prototype vector and every data point. Various schemes have
been proposed for speeding up the K-means algorithm, some of which are based on
precomputing a data structure such as a tree such that nearby points are in the same
subtree (Ramasubramanian and Paliwal, 1990; Moore, 2000). Other approaches
make use of the triangle inequality for distances, thereby avoiding unnecessary dis-
tance calculations (Hodgson, 1998; Elkan, 2003).

So far, we have considered a batch version of K-means in which the whole data
set is used together to update the prototype vectors. We can also derive an on-line
stochastic algorithm (MacQueen, 1967) by applying the Robbins-Monro procedure
to the problem of ﬁnding the roots of the regression function given by the derivatives
of J in (9.1) with respect to µk. This leads to a sequential update in which, for each
data point xn in turn, we update the nearest prototype µk using

Section 9.2.2

Section 2.3.5

Exercise 9.2

µnew

k = µold

k + ηn(xn − µold
k )

(9.5)

where ηn is the learning rate parameter, which is typically made to decrease mono-
tonically as more data points are considered.

The K-means algorithm is based on the use of squared Euclidean distance as the
measure of dissimilarity between a data point and a prototype vector. Not only does
this limit the type of data variables that can be considered (it would be inappropriate
for cases where some or all of the variables represent categorical labels for instance),



N



K



428

9. MIXTURE MODELS AND EM

Section 2.3.7

but it can also make the determination of the cluster means nonrobust to outliers. We
can generalize the K-means algorithm by introducing a more general dissimilarity
measure V(x, x) between two vectors x and x and then minimizing the following
distortion measure

J =

n=1

k=1

rnkV(xn, µk)

(9.6)

which gives the K-medoids algorithm. The E step again involves, for given cluster
prototypes µk, assigning each data point to the cluster for which the dissimilarity to
the corresponding prototype is smallest. The computational cost of this is O(KN),
as is the case for the standard K-means algorithm. For a general choice of dissimi-
larity measure, the M step is potentially more complex than for K-means, and so it
is common to restrict each cluster prototype to be equal to one of the data vectors as-
signed to that cluster, as this allows the algorithm to be implemented for any choice
of dissimilarity measure V(·,·) so long as it can be readily evaluated. Thus the M
step involves, for each cluster k, a discrete search over the Nk points assigned to that
cluster, which requires O(N 2

One notable feature of the K-means algorithm is that at each iteration, every
data point is assigned uniquely to one, and only one, of the clusters. Whereas some
data points will be much closer to a particular centre µk than to any other centre,
there may be other data points that lie roughly midway between cluster centres. In
the latter case, it is not clear that the hard assignment to the nearest cluster is the
most appropriate. We shall see in the next section that by adopting a probabilistic
approach, we obtain ‘soft’ assignments of data points to clusters in a way that reﬂects
the level of uncertainty over the most appropriate assignment. This probabilistic
formulation brings with it numerous beneﬁts.

k) evaluations of V(·,·).

9.1.1 Image segmentation and compression
As an illustration of the application of the K-means algorithm, we consider
the related problems of image segmentation and image compression. The goal of
segmentation is to partition an image into regions each of which has a reasonably
homogeneous visual appearance or which corresponds to objects or parts of objects
(Forsyth and Ponce, 2003). Each pixel in an image is a point in a 3-dimensional space
comprising the intensities of the red, blue, and green channels, and our segmentation
algorithm simply treats each pixel in the image as a separate data point. Note that
strictly this space is not Euclidean because the channel intensities are bounded by
the interval [0, 1]. Nevertheless, we can apply the K-means algorithm without difﬁ-
culty. We illustrate the result of running K-means to convergence, for any particular
value of K, by re-drawing the image replacing each pixel vector with the {R, G, B}
intensity triplet given by the centre µk to which that pixel has been assigned. Results
for various values of K are shown in Figure 9.3. We see that for a given value of K,
the algorithm is representing the image using a palette of only K colours. It should
be emphasized that this use of K-means is not a particularly sophisticated approach
to image segmentation, not least because it takes no account of the spatial proximity
of different pixels. The image segmentation problem is in general extremely difﬁcult

K = 2

K = 3

K = 10

Original image

9.1. K-means Clustering

429

Figure 9.3 Two examples of the application of the K-means clustering algorithm to image segmentation show-
ing the initial images together with their K-means segmentations obtained using various values of K. This
also illustrates of the use of vector quantization for data compression, in which smaller values of K give higher
compression at the expense of poorer image quality.

and remains the subject of active research and is introduced here simply to illustrate
the behaviour of the K-means algorithm.

We can also use the result of a clustering algorithm to perform data compres-
sion.
It is important to distinguish between lossless data compression, in which
the goal is to be able to reconstruct the original data exactly from the compressed
representation, and lossy data compression, in which we accept some errors in the
reconstruction in return for higher levels of compression than can be achieved in the
lossless case. We can apply the K-means algorithm to the problem of lossy data
compression as follows. For each of the N data points, we store only the identity
k of the cluster to which it is assigned. We also store the values of the K clus-
ter centres µk, which typically requires signiﬁcantly less data, provided we choose
K  N. Each data point is then approximated by its nearest centre µk. New data
points can similarly be compressed by ﬁrst ﬁnding the nearest µk and then storing
the label k instead of the original data vector. This framework is often called vector
quantization, and the vectors µk are called code-book vectors.

430

9. MIXTURE MODELS AND EM



K



The image segmentation problem discussed above also provides an illustration
of the use of clustering for data compression. Suppose the original image has N
pixels comprising {R, G, B} values each of which is stored with 8 bits of precision.
Then to transmit the whole image directly would cost 24N bits. Now suppose we
ﬁrst run K-means on the image data, and then instead of transmitting the original
pixel intensity vectors we transmit the identity of the nearest vector µk. Because
there are K such vectors, this requires log2 K bits per pixel. We must also transmit
the K code book vectors µk, which requires 24K bits, and so the total number of
bits required to transmit the image is 24K + N log2 K (rounding up to the nearest
integer). The original image shown in Figure 9.3 has 240 × 180 = 43, 200 pixels
and so requires 24 × 43, 200 = 1, 036, 800 bits to transmit directly. By comparison,
the compressed images require 43, 248 bits (K = 2), 86, 472 bits (K = 3), and
173, 040 bits (K = 10), respectively, to transmit. These represent compression ratios
compared to the original image of 4.2%, 8.3%, and 16.7%, respectively. We see that
there is a trade-off between degree of compression and image quality. Note that our
aim in this example is to illustrate the K-means algorithm. If we had been aiming to
produce a good image compressor, then it would be more fruitful to consider small
blocks of adjacent pixels, for instance 5× 5, and thereby exploit the correlations that
exist in natural images between nearby pixels.

9.2. Mixtures of Gaussians

In Section 2.3.9 we motivated the Gaussian mixture model as a simple linear super-
position of Gaussian components, aimed at providing a richer class of density mod-
els than the single Gaussian. We now turn to a formulation of Gaussian mixtures in
terms of discrete latent variables. This will provide us with a deeper insight into this
important distribution, and will also serve to motivate the expectation-maximization
algorithm.

Recall from (2.188) that the Gaussian mixture distribution can be written as a

linear superposition of Gaussians in the form

p(x) =

πkN (x|µk, Σk).

k=1

(9.7)

Let us introduce a K-dimensional binary random variable z having a 1-of-K repre-
sentation in which a particular element zk is equal to 1 and all other elements are
equal to 0. The values of zk therefore satisfy zk ∈ {0, 1} and
k zk = 1, and we
see that there are K possible states for the vector z according to which element is
nonzero. We shall deﬁne the joint distribution p(x, z) in terms of a marginal dis-
tribution p(z) and a conditional distribution p(x|z), corresponding to the graphical
model in Figure 9.4. The marginal distribution over z is speciﬁed in terms of the
mixing coefﬁcients πk, such that

p(zk = 1) = πk




k=1

K

K

k=1




K

K

k=1





Figure 9.4 Graphical representation of a mixture model,

in which
the joint distribution is expressed in the form p(x, z) =
p(z)p(x|z).

9.2. Mixtures of Gaussians

z

x

431

(9.8)

(9.9)

where the parameters {πk} must satisfy

together with

0 � πk � 1

πk = 1

in order to be valid probabilities. Because z uses a 1-of-K representation, we can
also write this distribution in the form

p(z) =

πzk
k .

(9.10)

Similarly, the conditional distribution of x given a particular value for z is a Gaussian

which can also be written in the form

p(x|zk = 1) = N (x|µk, Σk)

p(x|z) =

N (x|µk, Σk)zk .

(9.11)

Exercise 9.3

The joint distribution is given by p(z)p(x|z), and the marginal distribution of x is
then obtained by summing the joint distribution over all possible states of z to give

p(x) =

p(z)p(x|z) =

z

k=1

πkN (x|µk, Σk)

(9.12)

where we have made use of (9.10) and (9.11). Thus the marginal distribution of x is
a Gaussian mixture of the form (9.7). If we have several observations x1, . . . , xN ,
then, because we have represented the marginal distribution in the form p(x) =
z p(x, z), it follows that for every observed data point xn there is a corresponding

latent variable zn.

We have therefore found an equivalent formulation of the Gaussian mixture in-
volving an explicit latent variable.
It might seem that we have not gained much
by doing so. However, we are now able to work with the joint distribution p(x, z)




j=1

K

K

j=1





432

9. MIXTURE MODELS AND EM

instead of the marginal distribution p(x), and this will lead to signiﬁcant simpliﬁca-
tions, most notably through the introduction of the expectation-maximization (EM)
algorithm.

Another quantity that will play an important role is the conditional probability
of z given x. We shall use γ(zk) to denote p(zk = 1|x), whose value can be found
using Bayes’ theorem

γ(zk) ≡ p(zk = 1|x) =

p(zk = 1)p(x|zk = 1)
p(zj = 1)p(x|zj = 1)
πkN (x|µk, Σk)
πjN (x|µj, Σj)

.

=

(9.13)

Section 8.1.2

We shall view πk as the prior probability of zk = 1, and the quantity γ(zk) as the
corresponding posterior probability once we have observed x. As we shall see later,
γ(zk) can also be viewed as the responsibility that component k takes for ‘explain-
ing’ the observation x.

We can use the technique of ancestral sampling to generate random samples
distributed according to the Gaussian mixture model. To do this, we ﬁrst generate a
z, from the marginal distribution p(z) and then generate
value for z, which we denote
a value for x from the conditional distribution p(x|
z). Techniques for sampling from
standard distributions are discussed in Chapter 11. We can depict samples from the
joint distribution p(x, z) by plotting points at the corresponding values of x and
then colouring them according to the value of z, in other words according to which
Gaussian component was responsible for generating them, as shown in Figure 9.5(a).
Similarly samples from the marginal distribution p(x) are obtained by taking the
samples from the joint distribution and ignoring the values of z. These are illustrated
in Figure 9.5(b) by plotting the x values without any coloured labels.

We can also use this synthetic data set to illustrate the ‘responsibilities’ by eval-
uating, for every data point, the posterior probability for each component in the
mixture distribution from which this data set was generated. In particular, we can
represent the value of the responsibilities γ(znk) associated with data point xn by
plotting the corresponding point using proportions of red, blue, and green ink given
by γ(znk) for k = 1, 2, 3, respectively, as shown in Figure 9.5(c). So, for instance,
a data point for which γ(zn1) = 1 will be coloured red, whereas one for which
γ(zn2) = γ(zn3) = 0.5 will be coloured with equal proportions of blue and green
ink and so will appear cyan. This should be compared with Figure 9.5(a) in which
the data points were labelled using the true identity of the component from which
they were generated.

9.2.1 Maximum likelihood
Suppose we have a data set of observations {x1, . . . , xN}, and we wish to model
this data using a mixture of Gaussians. We can represent this data set as an N × D



N



1



K



0.5

9.2. Mixtures of Gaussians

433

1

1

(a)

1

(b)

0.5

0

0.5

0

0

0.5

1

0

0.5

1

(c)

0.5

0

0

Figure 9.5 Example of 500 points drawn from the mixture of 3 Gaussians shown in Figure 2.23. (a) Samples
from the joint distribution p(z)p(x|z) in which the three states of z, corresponding to the three components of the
mixture, are depicted in red, green, and blue, and (b) the corresponding samples from the marginal distribution
p(x), which is obtained by simply ignoring the values of z and just plotting the x values. The data set in (a) is
said to be complete, whereas that in (b) is incomplete. (c) The same samples in which the colours represent the
value of the responsibilities γ(znk) associated with data point xn, obtained by plotting the corresponding point
using proportions of red, blue, and green ink given by γ(znk) for k = 1, 2, 3, respectively

matrix X in which the nth row is given by xT
n. Similarly, the corresponding latent
variables will be denoted by an N × K matrix Z with rows zT
n. If we assume that
the data points are drawn independently from the distribution, then we can express
the Gaussian mixture model for this i.i.d. data set using the graphical representation
shown in Figure 9.6. From (9.7) the log of the likelihood function is given by

ln p(X|π, µ, Σ) =

ln

n=1

k=1

πkN (xn|µk, Σk)

.

(9.14)

Before discussing how to maximize this function, it is worth emphasizing that
there is a signiﬁcant problem associated with the maximum likelihood framework
applied to Gaussian mixture models, due to the presence of singularities. For sim-
plicity, consider a Gaussian mixture whose components have covariance matrices
given by Σk = σ2
kI, where I is the unit matrix, although the conclusions will hold
for general covariance matrices. Suppose that one of the components of the mixture
model, let us say the jth component, has its mean µj exactly equal to one of the data

Figure 9.6 Graphical representation of a Gaussian mixture model
for a set of N i.i.d. data points {xn}, with corresponding
latent points {zn}, where n = 1, . . . , N.

zn

xn

π

µ

Σ

N

434

9. MIXTURE MODELS AND EM

Figure 9.7 Illustration of how singularities in the
likelihood function arise with mixtures
of Gaussians. This should be com-
pared with the case of a single Gaus-
sian shown in Figure 1.14 for which no
singularities arise.

p(x)

x

points so that µj = xn for some value of n. This data point will then contribute a
term in the likelihood function of the form

N (xn|xn, σ2

j I) =

1

(2π)1/2

1
σj

.

(9.15)

If we consider the limit σj → 0, then we see that this term goes to inﬁnity and
so the log likelihood function will also go to inﬁnity. Thus the maximization of
the log likelihood function is not a well posed problem because such singularities
will always be present and will occur whenever one of the Gaussian components
‘collapses’ onto a speciﬁc data point. Recall that this problem did not arise in the
case of a single Gaussian distribution. To understand the difference, note that if a
single Gaussian collapses onto a data point it will contribute multiplicative factors
to the likelihood function arising from the other data points and these factors will go
to zero exponentially fast, giving an overall likelihood that goes to zero rather than
inﬁnity. However, once we have (at least) two components in the mixture, one of
the components can have a ﬁnite variance and therefore assign ﬁnite probability to
all of the data points while the other component can shrink onto one speciﬁc data
point and thereby contribute an ever increasing additive value to the log likelihood.
This is illustrated in Figure 9.7. These singularities provide another example of the
severe over-ﬁtting that can occur in a maximum likelihood approach. We shall see
that this difﬁculty does not occur if we adopt a Bayesian approach. For the moment,
however, we simply note that in applying maximum likelihood to Gaussian mixture
models we must take steps to avoid ﬁnding such pathological solutions and instead
seek local maxima of the likelihood function that are well behaved. We can hope to
avoid the singularities by using suitable heuristics, for instance by detecting when a
Gaussian component is collapsing and resetting its mean to a randomly chosen value
while also resetting its covariance to some large value, and then continuing with the
optimization.

A further issue in ﬁnding maximum likelihood solutions arises from the fact
that for any given maximum likelihood solution, a K-component mixture will have
a total of K! equivalent solutions corresponding to the K! ways of assigning K
sets of parameters to K components. In other words, for any given (nondegenerate)
point in the space of parameter values there will be a further K!−1 additional points
all of which give rise to exactly the same distribution. This problem is known as

Section 10.1



N


(

+

)*



n=1

N

N

9.2. Mixtures of Gaussians

435

identiﬁability (Casella and Berger, 2002) and is an important issue when we wish to
interpret the parameter values discovered by a model. Identiﬁability will also arise
when we discuss models having continuous latent variables in Chapter 12. However,
for the purposes of ﬁnding a good density model, it is irrelevant because any of the
equivalent solutions is as good as any other.

Maximizing the log likelihood function (9.14) for a Gaussian mixture model
turns out to be a more complex problem than for the case of a single Gaussian. The
difﬁculty arises from the presence of the summation over k that appears inside the
logarithm in (9.14), so that the logarithm function no longer acts directly on the
Gaussian. If we set the derivatives of the log likelihood to zero, we will no longer
obtain a closed form solution, as we shall see shortly.

One approach is to apply gradient-based optimization techniques (Fletcher, 1987;
Nocedal and Wright, 1999; Bishop and Nabney, 2008). Although gradient-based
techniques are feasible, and indeed will play an important role when we discuss
mixture density networks in Chapter 5, we now consider an alternative approach
known as the EM algorithm which has broad applicability and which will lay the
foundations for a discussion of variational inference techniques in Chapter 10.

9.2.2 EM for Gaussian mixtures
An elegant and powerful method for ﬁnding maximum likelihood solutions for
models with latent variables is called the expectation-maximization algorithm, or EM
algorithm (Dempster et al., 1977; McLachlan and Krishnan, 1997). Later we shall
give a general treatment of EM, and we shall also show how EM can be generalized
to obtain the variational inference framework. Initially, we shall motivate the EM
algorithm by giving a relatively informal treatment in the context of the Gaussian
mixture model. We emphasize, however, that EM has broad applicability, and indeed
it will be encountered in the context of a variety of different models in this book.

Let us begin by writing down the conditions that must be satisﬁed at a maximum
of the likelihood function. Setting the derivatives of ln p(X|π, µ, Σ) in (9.14) with
respect to the means µk of the Gaussian components to zero, we obtain

0 = −

n=1

πkN (xn|µk, Σk)
j πjN (xn|µj, Σj)

γ(znk)

Σk(xn − µk)

(9.16)

where we have made use of the form (2.43) for the Gaussian distribution. Note that
the posterior probabilities, or responsibilities, given by (9.13) appear naturally on
the right-hand side. Multiplying by Σ−1
(which we assume to be nonsingular) and
k
rearranging we obtain

µk =

where we have deﬁned

1
Nk

γ(znk)xn

Nk =

γ(znk).

n=1

(9.17)

(9.18)

Section 10.1





n=1

N

N

n=1





K



436

9. MIXTURE MODELS AND EM

We can interpret Nk as the effective number of points assigned to cluster k. Note
carefully the form of this solution. We see that the mean µk for the kth Gaussian
component is obtained by taking a weighted mean of all of the points in the data set,
in which the weighting factor for data point xn is given by the posterior probability
γ(znk) that component k was responsible for generating xn.

If we set the derivative of ln p(X|π, µ, Σ) with respect to Σk to zero, and follow
a similar line of reasoning, making use of the result for the maximum likelihood
solution for the covariance matrix of a single Gaussian, we obtain

Σk =

1
Nk

γ(znk)(xn − µk)(xn − µk)T

(9.19)

which has the same form as the corresponding result for a single Gaussian ﬁtted to
the data set, but again with each data point weighted by the corresponding poste-
rior probability and with the denominator given by the effective number of points
associated with the corresponding component.

Finally, we maximize ln p(X|π, µ, Σ) with respect to the mixing coefﬁcients
πk. Here we must take account of the constraint (9.9), which requires the mixing
coefﬁcients to sum to one. This can be achieved using a Lagrange multiplier and
maximizing the following quantity

Section 2.3.4

Appendix E

ln p(X|π, µ, Σ) + λ

πk − 1

k=1

which gives

0 =

N (xn|µk, Σk)
j πjN (xn|µj, Σj)

+ λ

(9.20)

(9.21)

where again we see the appearance of the responsibilities. If we now multiply both
sides by πk and sum over k making use of the constraint (9.9), we ﬁnd λ = −N.
Using this to eliminate λ and rearranging we obtain

πk = Nk
N

(9.22)

so that the mixing coefﬁcient for the kth component is given by the average respon-
sibility which that component takes for explaining the data points.

It is worth emphasizing that the results (9.17), (9.19), and (9.22) do not con-
stitute a closed-form solution for the parameters of the mixture model because the
responsibilities γ(znk) depend on those parameters in a complex way through (9.13).
However, these results do suggest a simple iterative scheme for ﬁnding a solution to
the maximum likelihood problem, which as we shall see turns out to be an instance
of the EM algorithm for the particular case of the Gaussian mixture model. We
ﬁrst choose some initial values for the means, covariances, and mixing coefﬁcients.
Then we alternate between the following two updates that we shall call the E step

9.2. Mixtures of Gaussians

437

2

0

−2

L = 1

2

0

−2

−2

0

(a)

2

−2

0

(b)

2

−2

0

(c)

2

L = 2

L = 5

2

0

−2

L = 20

2

0

−2

2

0

−2

2

0

−2

−2

0

(d)

2

−2

0

(e)

2

−2

0

(f)

2

Figure 9.8 Illustration of the EM algorithm using the Old Faithful set as used for the illustration of the K-means
algorithm in Figure 9.1. See the text for details.

Section 9.4

and the M step, for reasons that will become apparent shortly. In the expectation
step, or E step, we use the current values for the parameters to evaluate the posterior
probabilities, or responsibilities, given by (9.13). We then use these probabilities in
the maximization step, or M step, to re-estimate the means, covariances, and mix-
ing coefﬁcients using the results (9.17), (9.19), and (9.22). Note that in so doing
we ﬁrst evaluate the new means using (9.17) and then use these new values to ﬁnd
the covariances using (9.19), in keeping with the corresponding result for a single
Gaussian distribution. We shall show that each update to the parameters resulting
from an E step followed by an M step is guaranteed to increase the log likelihood
function. In practice, the algorithm is deemed to have converged when the change
in the log likelihood function, or alternatively in the parameters, falls below some
threshold. We illustrate the EM algorithm for a mixture of two Gaussians applied to
the rescaled Old Faithful data set in Figure 9.8. Here a mixture of two Gaussians
is used, with centres initialized using the same values as for the K-means algorithm
in Figure 9.1, and with precision matrices initialized to be proportional to the unit
matrix. Plot (a) shows the data points in green, together with the initial conﬁgura-
tion of the mixture model in which the one standard-deviation contours for the two

438

9. MIXTURE MODELS AND EM



j=1

Gaussian components are shown as blue and red circles. Plot (b) shows the result
of the initial E step, in which each data point is depicted using a proportion of blue
ink equal to the posterior probability of having been generated from the blue com-
ponent, and a corresponding proportion of red ink given by the posterior probability
of having been generated by the red component. Thus, points that have a signiﬁcant
probability for belonging to either cluster appear purple. The situation after the ﬁrst
M step is shown in plot (c), in which the mean of the blue Gaussian has moved to
the mean of the data set, weighted by the probabilities of each data point belonging
to the blue cluster, in other words it has moved to the centre of mass of the blue ink.
Similarly, the covariance of the blue Gaussian is set equal to the covariance of the
blue ink. Analogous results hold for the red component. Plots (d), (e), and (f) show
the results after 2, 5, and 20 complete cycles of EM, respectively. In plot (f) the
algorithm is close to convergence.

Note that the EM algorithm takes many more iterations to reach (approximate)
convergence compared with the K-means algorithm, and that each cycle requires
signiﬁcantly more computation. It is therefore common to run the K-means algo-
rithm in order to ﬁnd a suitable initialization for a Gaussian mixture model that is
subsequently adapted using EM. The covariance matrices can conveniently be ini-
tialized to the sample covariances of the clusters found by the K-means algorithm,
and the mixing coefﬁcients can be set to the fractions of data points assigned to the
respective clusters. As with gradient-based approaches for maximizing the log like-
lihood, techniques must be employed to avoid singularities of the likelihood function
in which a Gaussian component collapses onto a particular data point. It should be
emphasized that there will generally be multiple local maxima of the log likelihood
function, and that EM is not guaranteed to ﬁnd the largest of these maxima. Because
the EM algorithm for Gaussian mixtures plays such an important role, we summarize
it below.

EM for Gaussian Mixtures

Given a Gaussian mixture model, the goal is to maximize the likelihood function
with respect to the parameters (comprising the means and covariances of the
components and the mixing coefﬁcients).

1. Initialize the means µk, covariances Σk and mixing coefﬁcients πk, and

evaluate the initial value of the log likelihood.

2. E step. Evaluate the responsibilities using the current parameter values

γ(znk) = πkN (xn|µk, Σk)
πjN (xn|µj, Σj)

K

.

(9.23)




n=1

N

N

n=1







γ(znk)xn

Nk =

n=1

n=1

ln

k=1

K

N

N





(9.24)

)T

(9.25)

(9.26)

(9.27)

(9.28)

9.3. An Alternative View of EM

439

3. M step. Re-estimate the parameters using the current responsibilities

µnew

k

=

1
Nk

Σnew

k

πnew
k

=

1
Nk
= Nk
N

where

γ(znk) (xn − µnew

k

) (xn − µnew

k

4. Evaluate the log likelihood

ln p(X|µ, Σ, π) =

γ(znk).

πkN (xn|µk, Σk)

and check for convergence of either the parameters or the log likelihood. If
the convergence criterion is not satisﬁed return to step 2.

9.3. An Alternative View of EM

In this section, we present a complementary view of the EM algorithm that recog-
nizes the key role played by latent variables. We discuss this approach ﬁrst of all
in an abstract setting, and then for illustration we consider once again the case of
Gaussian mixtures.

The goal of the EM algorithm is to ﬁnd maximum likelihood solutions for mod-
els having latent variables. We denote the set of all observed data by X, in which the
n, and similarly we denote the set of all latent variables by Z,
nth row represents xT
with a corresponding row zT
n. The set of all model parameters is denoted by θ, and
so the log likelihood function is given by

ln p(X|θ) = ln

p(X, Z|θ)

.

Z

(9.29)

Note that our discussion will apply equally well to continuous latent variables simply
by replacing the sum over Z with an integral.

A key observation is that the summation over the latent variables appears inside
the logarithm. Even if the joint distribution p(X, Z|θ) belongs to the exponential



Z

440

9. MIXTURE MODELS AND EM

family, the marginal distribution p(X|θ) typically does not as a result of this sum-
mation. The presence of the sum prevents the logarithm from acting directly on the
joint distribution, resulting in complicated expressions for the maximum likelihood
solution.

Now suppose that, for each observation in X, we were told the corresponding
value of the latent variable Z. We shall call {X, Z} the complete data set, and we
shall refer to the actual observed data X as incomplete, as illustrated in Figure 9.5.
The likelihood function for the complete data set simply takes the form ln p(X, Z|θ),
and we shall suppose that maximization of this complete-data log likelihood function
is straightforward.

In practice, however, we are not given the complete data set {X, Z}, but only
the incomplete data X. Our state of knowledge of the values of the latent variables
in Z is given only by the posterior distribution p(Z|X, θ). Because we cannot use
the complete-data log likelihood, we consider instead its expected value under the
posterior distribution of the latent variable, which corresponds (as we shall see) to the
E step of the EM algorithm. In the subsequent M step, we maximize this expectation.
If the current estimate for the parameters is denoted θold, then a pair of successive
E and M steps gives rise to a revised estimate θnew. The algorithm is initialized by
choosing some starting value for the parameters θ0. The use of the expectation may
seem somewhat arbitrary. However, we shall see the motivation for this choice when
we give a deeper treatment of EM in Section 9.4.

In the E step, we use the current parameter values θold to ﬁnd the posterior
distribution of the latent variables given by p(Z|X, θold). We then use this posterior
distribution to ﬁnd the expectation of the complete-data log likelihood evaluated for
some general parameter value θ. This expectation, denoted Q(θ, θold), is given by
(9.30)

Q(θ, θold) =

p(Z|X, θold) ln p(X, Z|θ).

In the M step, we determine the revised parameter estimate θnew by maximizing this
function

θnew = arg max

Q(θ, θold).

θ

(9.31)

Section 9.4

Note that in the deﬁnition of Q(θ, θold), the logarithm acts directly on the joint
distribution p(X, Z|θ), and so the corresponding M-step maximization will, by sup-
position, be tractable.
The general EM algorithm is summarized below. It has the property, as we shall
show later, that each cycle of EM will increase the incomplete-data log likelihood
(unless it is already at a local maximum).

The General EM Algorithm
Given a joint distribution p(X, Z|θ) over observed variables X and latent vari-
ables Z, governed by parameters θ, the goal is to maximize the likelihood func-
tion p(X|θ) with respect to θ.
1. Choose an initial setting for the parameters θold.



9.3. An Alternative View of EM

441

(9.32)

(9.33)

2. E step Evaluate p(Z|X, θold).
3. M step Evaluate θnew given by

θnew = arg max

Q(θ, θold)

θ

where

Q(θ, θold) =

Z

p(Z|X, θold) ln p(X, Z|θ).

Exercise 9.4

4. Check for convergence of either the log likelihood or the parameter values.

If the convergence criterion is not satisﬁed, then let

θold ← θnew

(9.34)

and return to step 2.

The EM algorithm can also be used to ﬁnd MAP (maximum posterior) solutions
for models in which a prior p(θ) is deﬁned over the parameters. In this case the E
step remains the same as in the maximum likelihood case, whereas in the M step the
quantity to be maximized is given by Q(θ, θold) + ln p(θ). Suitable choices for the
prior will remove the singularities of the kind illustrated in Figure 9.7.
Here we have considered the use of the EM algorithm to maximize a likelihood
function when there are discrete latent variables. However, it can also be applied
when the unobserved variables correspond to missing values in the data set. The
distribution of the observed values is obtained by taking the joint distribution of all
the variables and then marginalizing over the missing ones. EM can then be used
to maximize the corresponding likelihood function. We shall show an example of
the application of this technique in the context of principal component analysis in
Figure 12.11. This will be a valid procedure if the data values are missing at random,
meaning that the mechanism causing values to be missing does not depend on the
unobserved values. In many situations this will not be the case, for instance if a
sensor fails to return a value whenever the quantity it is measuring exceeds some
threshold.

9.3.1 Gaussian mixtures revisited
We now consider the application of this latent variable view of EM to the spe-
ciﬁc case of a Gaussian mixture model. Recall that our goal is to maximize the log
likelihood function (9.14), which is computed using the observed data set X, and we
saw that this was more difﬁcult than for the case of a single Gaussian distribution
due to the presence of the summation over k that occurs inside the logarithm. Sup-
pose then that in addition to the observed data set X, we were also given the values
of the corresponding discrete variables Z. Recall that Figure 9.5(a) shows a ‘com-
plete’ data set (i.e., one that includes labels showing which component generated
each data point) while Figure 9.5(b) shows the corresponding ‘incomplete’ data set.
The graphical model for the complete data is shown in Figure 9.9.




N

K



N




K

N

(9.35)

(9.36)

Now consider the problem of maximizing the likelihood for the complete data

set {X, Z}. From (9.10) and (9.11), this likelihood function takes the form

p(X, Z|µ, Σ, π) =

πznk
k N (xn|µk, Σk)znk

n=1

k=1

where znk denotes the kth component of zn. Taking the logarithm, we obtain

ln p(X, Z|µ, Σ, π) =

n=1

k=1

znk {ln πk + lnN (xn|µk, Σk)} .

Comparison with the log likelihood function (9.14) for the incomplete data shows
that the summation over k and the logarithm have been interchanged. The loga-
rithm now acts directly on the Gaussian distribution, which itself is a member of
the exponential family. Not surprisingly, this leads to a much simpler solution to
the maximum likelihood problem, as we now show. Consider ﬁrst the maximization
with respect to the means and covariances. Because zn is a K-dimensional vec-
tor with all elements equal to 0 except for a single element having the value 1, the
complete-data log likelihood function is simply a sum of K independent contribu-
tions, one for each mixture component. Thus the maximization with respect to a
mean or a covariance is exactly as for a single Gaussian, except that it involves only
the subset of data points that are ‘assigned’ to that component. For the maximization
with respect to the mixing coefﬁcients, we note that these are coupled for different
values of k by virtue of the summation constraint (9.9). Again, this can be enforced
using a Lagrange multiplier as before, and leads to the result

πk =

1
N

znk

n=1

(9.37)

so that the mixing coefﬁcients are equal to the fractions of data points assigned to
the corresponding components.

Thus we see that the complete-data log likelihood function can be maximized
trivially in closed form. In practice, however, we do not have values for the latent
variables so, as discussed earlier, we consider the expectation, with respect to the
posterior distribution of the latent variables, of the complete-data log likelihood.

442

9. MIXTURE MODELS AND EM

Figure 9.9 This shows the same graph as in Figure 9.6 except that
we now suppose that the discrete variables zn are ob-
served, as well as the data variables xn.

zn

xn

π

µ

Σ

N



K









n=1

j=1

znk

znj

N

N

K

	

9.3. An Alternative View of EM

443

Using (9.10) and (9.11) together with Bayes’ theorem, we see that this posterior
distribution takes the form

p(Z|X, µ, Σ, π) ∝

[πkN (xn|µk, Σk)]znk .

k=1

(9.38)

and hence factorizes over n so that under the posterior distribution the {zn} are
independent. This is easily veriﬁed by inspection of the directed graph in Figure 9.6
and making use of the d-separation criterion. The expected value of the indicator
variable znk under this posterior distribution is then given by

E[znk] =

znk [πkN (xn|µk, Σk)]znk

πjN (xn|µj, Σj)

znj

=

K

πkN (xn|µk, Σk)
πjN (xn|µj, Σj)

= γ(znk)

(9.39)

which is just the responsibility of component k for data point xn. The expected value
of the complete-data log likelihood function is therefore given by

EZ[ln p(X, Z|µ, Σ, π)] =

n=1

k=1

γ(znk){ln πk + lnN (xn|µk, Σk)} .

(9.40)

We can now proceed as follows. First we choose some initial values for the param-
eters µold, Σold and πold, and use these to evaluate the responsibilities (the E step).
We then keep the responsibilities ﬁxed and maximize (9.40) with respect to µk, Σk
and πk (the M step). This leads to closed form solutions for µnew, Σnew and πnew
given by (9.17), (9.19), and (9.22) as before. This is precisely the EM algorithm for
Gaussian mixtures as derived earlier. We shall gain more insight into the role of the
expected complete-data log likelihood function when we give a proof of convergence
of the EM algorithm in Section 9.4.

9.3.2 Relation to K-means
Comparison of the K-means algorithm with the EM algorithm for Gaussian
mixtures shows that there is a close similarity. Whereas the K-means algorithm
performs a hard assignment of data points to clusters, in which each data point is
associated uniquely with one cluster, the EM algorithm makes a soft assignment
based on the posterior probabilities. In fact, we can derive the K-means algorithm
as a particular limit of EM for Gaussian mixtures as follows.

Consider a Gaussian mixture model in which the covariance matrices of the
mixture components are given by 	I, where 	 is a variance parameter that is shared

Exercise 9.5
Section 8.2

Exercise 9.8



.

(9.41)










K



N

444

9. MIXTURE MODELS AND EM

by all of the components, and I is the identity matrix, so that

p(x|µk, Σk) =

(2π	)1/2

1

exp

1
2	x − µk2

−

We now consider the EM algorithm for a mixture of K Gaussians of this form in
which we treat 	 as a ﬁxed constant, instead of a parameter to be re-estimated. From
(9.13) the posterior probabilities, or responsibilities, for a particular data point xn,
are given by

γ(znk) = πk exp{−xn − µk2/2	}
−xn − µj2/2	

j πj exp

.

(9.42)

If we consider the limit 	 → 0, we see that in the denominator the term for which
xn − µj2 is smallest will go to zero most slowly, and hence the responsibilities
γ(znk) for the data point xn all go to zero except for term j, for which the responsi-
bility γ(znj) will go to unity. Note that this holds independently of the values of the
πk so long as none of the πk is zero. Thus, in this limit, we obtain a hard assignment
of data points to clusters, just as in the K-means algorithm, so that γ(znk) → rnk
where rnk is deﬁned by (9.2). Each data point is thereby assigned to the cluster
having the closest mean.

The EM re-estimation equation for the µk, given by (9.17), then reduces to the
K-means result (9.4). Note that the re-estimation formula for the mixing coefﬁcients
(9.22) simply re-sets the value of πk to be equal to the fraction of data points assigned
to cluster k, although these parameters no longer play an active role in the algorithm.
Finally, in the limit 	 → 0 the expected complete-data log likelihood, given by

(9.40), becomes

EZ[ln p(X, Z|µ, Σ, π)] → −

1
2

rnkxn − µk2 + const.

(9.43)

n=1

k=1

Thus we see that in this limit, maximizing the expected complete-data log likelihood
is equivalent to minimizing the distortion measure J for the K-means algorithm
given by (9.1).

Note that the K-means algorithm does not estimate the covariances of the clus-
ters but only the cluster means. A hard-assignment version of the Gaussian mixture
model with general covariance matrices, known as the elliptical K-means algorithm,
has been considered by Sung and Poggio (1994).

9.3.3 Mixtures of Bernoulli distributions
So far in this chapter, we have focussed on distributions over continuous vari-
ables described by mixtures of Gaussians. As a further example of mixture mod-
elling, and to illustrate the EM algorithm in a different context, we now discuss mix-
tures of discrete binary variables described by Bernoulli distributions. This model
is also known as latent class analysis (Lazarsfeld and Henry, 1968; McLachlan and
Peel, 2000). As well as being of practical importance in its own right, our discus-
sion of Bernoulli mixtures will also lay the foundation for a consideration of hidden
Markov models over discrete variables.

Exercise 9.11

Section 13.2






i=1

i=1

D

D

N





k=1

K

K




k=1

K

K





(9.44)

(9.45)
(9.46)

(9.47)

(9.48)

(9.49)

(9.50)

9.3. An Alternative View of EM

445

Consider a set of D binary variables xi, where i = 1, . . . , D, each of which is

governed by a Bernoulli distribution with parameter µi, so that

where x = (x1, . . . , xD)T and µ = (µ1, . . . , µD)T. We see that the individual
variables xi are independent, given µ. The mean and covariance of this distribution
are easily seen to be

Now let us consider a ﬁnite mixture of these distributions given by

p(x|µ) =

µxi
i (1 − µi)(1−xi)

E[x] = µ
cov[x] = diag{µi(1 − µi)}.

p(x|µ, π) =

πkp(x|µk)

where µ = {µ1, . . . , µK}, π = {π1, . . . , πK}, and

p(x|µk) =

µxi
ki(1 − µki)(1−xi).

Exercise 9.12

The mean and covariance of this mixture distribution are given by

E[x] =

πkµk

cov[x] =

k=1

πk

Σk + µkµT
k

− E[x]E[x]T

where Σk = diag {µki(1 − µki)}. Because the covariance matrix cov[x] is no
longer diagonal, the mixture distribution can capture correlations between the vari-
ables, unlike a single Bernoulli distribution.

If we are given a data set X = {x1, . . . , xN} then the log likelihood function

for this model is given by

ln p(X|µ, π) =

ln

n=1

k=1

πkp(xn|µk)

.

(9.51)

Again we see the appearance of the summation inside the logarithm, so that the
maximum likelihood solution no longer has closed form.

We now derive the EM algorithm for maximizing the likelihood function for
the mixture of Bernoulli distributions. To do this, we ﬁrst introduce an explicit latent



N












znk

n=1

k=1

k=1

k=1

znk

znj

N

K

K

K

K






	




i=1

D

D

446

9. MIXTURE MODELS AND EM

variable z associated with each instance of x. As in the case of the Gaussian mixture,
z = (z1, . . . , zK)T is a binary K-dimensional variable having a single component
equal to 1, with all other components equal to 0. We can then write the conditional
distribution of x, given the latent variable, as

while the prior distribution for the latent variables is the same as for the mixture of
Gaussians model, so that

p(x|z, µ) =

p(x|µk)zk

p(z|π) =

πzk
k .

(9.52)

(9.53)

Exercise 9.14

If we form the product of p(x|z, µ) and p(z|π) and then marginalize over z, then we
recover (9.47).
In order to derive the EM algorithm, we ﬁrst write down the complete-data log

likelihood function, which is given by

ln p(X, Z|µ, π) =

n=1

k=1

ln πk

+

[xni ln µki + (1 − xni) ln(1 − µki)]

(9.54)

where X = {xn} and Z = {zn}. Next we take the expectation of the complete-data
log likelihood with respect to the posterior distribution of the latent variables to give

EZ[ln p(X, Z|µ, π)] =

γ(znk)

ln πk

+

i=1

[xni ln µki + (1 − xni) ln(1 − µki)]

(9.55)

where γ(znk) = E[znk] is the posterior probability, or responsibility, of component
k given data point xn. In the E step, these responsibilities are evaluated using Bayes’
theorem, which takes the form

γ(znk) = E[znk] =

=

znk [πkp(xn|µk)]znk

πjp(xn|µj)

znj

K

πkp(xn|µk)
πjp(xn|µj)

j=1

.

(9.56)



N



N

n=1





9.3. An Alternative View of EM

447

If we consider the sum over n in (9.55), we see that the responsibilities enter

only through two terms, which can be written as

γ(znk)

Nk =

xk =

n=1
1
Nk

γ(znk)xn

(9.57)

(9.58)

where Nk is the effective number of data points associated with component k. In the
M step, we maximize the expected complete-data log likelihood with respect to the
parameters µk and π. If we set the derivative of (9.55) with respect to µk equal to
zero and rearrange the terms, we obtain

µk = xk.

(9.59)

We see that this sets the mean of component k equal to a weighted mean of the
data, with weighting coefﬁcients given by the responsibilities that component k takes
for data points. For the maximization with respect to πk, we need to introduce a
k πk = 1. Following analogous
Lagrange multiplier to enforce the constraint
steps to those used for the mixture of Gaussians, we then obtain

πk = Nk
N

(9.60)

which represents the intuitively reasonable result that the mixing coefﬁcient for com-
ponent k is given by the effective fraction of points in the data set explained by that
component.

Note that in contrast to the mixture of Gaussians, there are no singularities in
which the likelihood function goes to inﬁnity. This can be seen by noting that the
likelihood function is bounded above because 0 � p(xn|µk) � 1. There exist
singularities at which the likelihood function goes to zero, but these will not be
found by EM provided it is not initialized to a pathological starting point, because
the EM algorithm always increases the value of the likelihood function, until a local
maximum is found. We illustrate the Bernoulli mixture model in Figure 9.10 by
using it to model handwritten digits. Here the digit images have been turned into
binary vectors by setting all elements whose values exceed 0.5 to 1 and setting the
remaining elements to 0. We now ﬁt a data set of N = 600 such digits, comprising
the digits ‘2’, ‘3’, and ‘4’, with a mixture of K = 3 Bernoulli distributions by
running 10 iterations of the EM algorithm. The mixing coefﬁcients were initialized
to πk = 1/K, and the parameters µkj were set to random values chosen uniformly in
j µkj = 1.
the range (0.25, 0.75) and then normalized to satisfy the constraint that
We see that a mixture of 3 Bernoulli distributions is able to ﬁnd the three clusters in
the data set corresponding to the different digits.

The conjugate prior for the parameters of a Bernoulli distribution is given by
the beta distribution, and we have seen that a beta prior is equivalent to introducing

Exercise 9.15

Exercise 9.16

Exercise 9.17

Section 9.4

448

9. MIXTURE MODELS AND EM

Figure 9.10 Illustration of the Bernoulli mixture model in which the top row shows examples from the digits data
set after converting the pixel values from grey scale to binary using a threshold of 0.5. On the bottom row the ﬁrst
three images show the parameters µki for each of the three components in the mixture model. As a comparison,
we also ﬁt the same data set using a single multivariate Bernoulli distribution, again using maximum likelihood.
This amounts to simply averaging the counts in each pixel and is shown by the right-most image on the bottom
row.

Section 2.1.1

Exercise 9.18

Exercise 9.19

additional effective observations of x. We can similarly introduce priors into the
Bernoulli mixture model, and use EM to maximize the posterior probability distri-
butions.

It is straightforward to extend the analysis of Bernoulli mixtures to the case of
multinomial binary variables having M > 2 states by making use of the discrete dis-
tribution (2.26). Again, we can introduce Dirichlet priors over the model parameters
if desired.

9.3.4 EM for Bayesian linear regression
As a third example of the application of EM, we return to the evidence ap-
proximation for Bayesian linear regression. In Section 3.5.2, we obtained the re-
estimation equations for the hyperparameters α and β by evaluation of the evidence
and then setting the derivatives of the resulting expression to zero. We now turn to
an alternative approach for ﬁnding α and β based on the EM algorithm. Recall that
our goal is to maximize the evidence function p(t|α, β) given by (3.77) with respect
to α and β. Because the parameter vector w is marginalized out, we can regard it as
a latent variable, and hence we can optimize this marginal likelihood function using
EM. In the E step, we compute the posterior distribution of w given the current set-
ting of the parameters α and β and then use this to ﬁnd the expected complete-data
log likelihood. In the M step, we maximize this quantity with respect to α and β. We
have already derived the posterior distribution of w because this is given by (3.49).
The complete-data log likelihood function is then given by

ln p(t, w|α, β) = ln p(t|w, β) + ln p(w|α)

(9.61)







	
	




N






β
2

M

9.3. An Alternative View of EM

449

where the likelihood p(t|w, β) and the prior p(w|α) are given by (3.10) and (3.52),
respectively, and y(x, w) is given by (3.3). Taking the expectation with respect to
the posterior distribution of w then gives

E [ln p(t, w|α, β)] = M
2

ln

α
2π

α
2 E

−

wTw

+ N
2

ln

β
2π

−

E

(tn − wTφn)2

.

n=1

(9.62)

Setting the derivatives with respect to α to zero, we obtain the M step re-estimation
equation

α = M

E [wTw]

=

M

N mN + Tr(SN ) .
mT

(9.63)

An analogous result holds for β.

Note that this re-estimation equation takes a slightly different form from the
corresponding result (3.92) derived by direct evaluation of the evidence function.
However, they each involve computation and inversion (or eigen decomposition) of
an M × M matrix and hence will have comparable computational cost per iteration.
These two approaches to determining α should of course converge to the same
result (assuming they ﬁnd the same local maximum of the evidence function). This
can be veriﬁed by ﬁrst noting that the quantity γ is deﬁned by

γ = M − α

= M − αTr(SN ).

(9.64)

1

λi + α

i=1

Exercise 9.20

Exercise 9.21

At a stationary point of the evidence function, the re-estimation equation (3.92) will
be self-consistently satisﬁed, and hence we can substitute for γ to give

αmT

N mN = γ = M − αTr(SN )

(9.65)

and solving for α we obtain (9.63), which is precisely the EM re-estimation equation.
As a ﬁnal example, we consider a closely related model, namely the relevance
vector machine for regression discussed in Section 7.2.1. There we used direct max-
imization of the marginal likelihood to derive re-estimation equations for the hyper-
parameters α and β. Here we consider an alternative approach in which we view the
weight vector w as a latent variable and apply the EM algorithm. The E step involves
ﬁnding the posterior distribution over the weights, and this is given by (7.81). In the
M step we maximize the expected complete-data log likelihood, which is deﬁned by

Exercise 9.22

where the expectation is taken with respect to the posterior distribution computed
using the ‘old’ parameter values. To compute the new parameter values we maximize
with respect to α and β to give

Ew [ln p(t|X, w, β)p(w|α)]

(9.66)






1

m2





Z

Z



N



The expectation maximization algorithm, or EM algorithm, is a general technique for
ﬁnding maximum likelihood solutions for probabilistic models having latent vari-
ables (Dempster et al., 1977; McLachlan and Krishnan, 1997). Here we give a very
general treatment of the EM algorithm and in the process provide a proof that the
EM algorithm derived heuristically in Sections 9.2 and 9.3 for Gaussian mixtures
does indeed maximize the likelihood function (Csisz`ar and Tusn`ady, 1984; Hath-
away, 1986; Neal and Hinton, 1999). Our discussion will also form the basis for the
derivation of the variational inference framework.

Consider a probabilistic model in which we collectively denote all of the ob-
served variables by X and all of the hidden variables by Z. The joint distribution
p(X, Z|θ) is governed by a set of parameters denoted θ. Our goal is to maximize
the likelihood function that is given by

p(X|θ) =

p(X, Z|θ).

(9.69)

Here we are assuming Z is discrete, although the discussion is identical if Z com-
prises continuous variables or a combination of discrete and continuous variables,
with summation replaced by integration as appropriate.

We shall suppose that direct optimization of p(X|θ) is difﬁcult, but that opti-
mization of the complete-data likelihood function p(X, Z|θ) is signiﬁcantly easier.
Next we introduce a distribution q(Z) deﬁned over the latent variables, and we ob-
serve that, for any choice of q(Z), the following decomposition holds

ln p(X|θ) = L(q, θ) + KL(qp)

where we have deﬁned

L(q, θ) =

q(Z) ln

p(X, Z|θ)

q(Z)
p(Z|X, θ)

q(Z)

KL(qp) = −

Z

q(Z) ln

(9.70)

(9.71)

(9.72)

.

Note that L(q, θ) is a functional (see Appendix D for a discussion of functionals)
of the distribution q(Z), and a function of the parameters θ. It is worth studying

450

9. MIXTURE MODELS AND EM

αnew

i

=

i + Σii

(βnew)−1 = t − ΦmN2 + β−1

i γi

(9.67)

(9.68)

These re-estimation equations are formally equivalent to those obtained by direct
maxmization.

9.4. The EM Algorithm in General

Exercise 9.23

Section 10.1

9.4. The EM Algorithm in General

451

Figure 9.11 Illustration of the decomposition given
by (9.70), which holds for any choice
of distribution q(Z).
Because the
Kullback-Leibler divergence satisﬁes
KL(qp) � 0, we see that the quan-
tity L(q, θ) is a lower bound on the log
likelihood function ln p(X|θ).

KL(q||p)

L(q, θ)

ln p(X|θ)

Exercise 9.24

Section 1.6.1

carefully the forms of the expressions (9.71) and (9.72), and in particular noting that
they differ in sign and also that L(q, θ) contains the joint distribution of X and Z
while KL(qp) contains the conditional distribution of Z given X. To verify the
decomposition (9.70), we ﬁrst make use of the product rule of probability to give

ln p(X, Z|θ) = ln p(Z|X, θ) + ln p(X|θ)

(9.73)
which we then substitute into the expression for L(q, θ). This gives rise to two terms,
one of which cancels KL(qp) while the other gives the required log likelihood
ln p(X|θ) after noting that q(Z) is a normalized distribution that sums to 1.
From (9.72), we see that KL(qp) is the Kullback-Leibler divergence between
q(Z) and the posterior distribution p(Z|X, θ). Recall that the Kullback-Leibler di-
vergence satisﬁes KL(qp) � 0, with equality if, and only if, q(Z) = p(Z|X, θ). It
therefore follows from (9.70) that L(q, θ) � ln p(X|θ), in other words that L(q, θ)
is a lower bound on ln p(X|θ). The decomposition (9.70) is illustrated in Fig-
ure 9.11.
The EM algorithm is a two-stage iterative optimization technique for ﬁnding
maximum likelihood solutions. We can use the decomposition (9.70) to deﬁne the
EM algorithm and to demonstrate that it does indeed maximize the log likelihood.
Suppose that the current value of the parameter vector is θold. In the E step, the
lower bound L(q, θold) is maximized with respect to q(Z) while holding θold ﬁxed.
The solution to this maximization problem is easily seen by noting that the value
of ln p(X|θold) does not depend on q(Z) and so the largest value of L(q, θold) will
occur when the Kullback-Leibler divergence vanishes, in other words when q(Z) is
equal to the posterior distribution p(Z|X, θold). In this case, the lower bound will
equal the log likelihood, as illustrated in Figure 9.12.
In the subsequent M step, the distribution q(Z) is held ﬁxed and the lower bound
L(q, θ) is maximized with respect to θ to give some new value θnew. This will
cause the lower bound L to increase (unless it is already at a maximum), which will
necessarily cause the corresponding log likelihood function to increase. Because the
distribution q is determined using the old parameter values rather than the new values
and is held ﬁxed during the M step, it will not equal the new posterior distribution
p(Z|X, θnew), and hence there will be a nonzero KL divergence. The increase in the
log likelihood function is therefore greater than the increase in the lower bound, as





452

9. MIXTURE MODELS AND EM

KL(q||p) = 0

Figure 9.12 Illustration of the E step of
the EM algorithm. The q
distribution is set equal to
the posterior distribution for
the current parameter val-
ues θold, causing the lower
bound to move up to the
same value as the log like-
lihood function, with the KL
divergence vanishing.

L(q, θold)

ln p(X|θold)

shown in Figure 9.13. If we substitute q(Z) = p(Z|X, θold) into (9.71), we see that,
after the E step, the lower bound takes the form

L(q, θ) =

Z

p(Z|X, θold) ln p(X, Z|θ) −

= Q(θ, θold) + const

Z

p(Z|X, θold) ln p(Z|X, θold)
(9.74)

where the constant is simply the negative entropy of the q distribution and is there-
fore independent of θ. Thus in the M step, the quantity that is being maximized is the
expectation of the complete-data log likelihood, as we saw earlier in the case of mix-
tures of Gaussians. Note that the variable θ over which we are optimizing appears
only inside the logarithm. If the joint distribution p(Z, X|θ) comprises a member of
the exponential family, or a product of such members, then we see that the logarithm
will cancel the exponential and lead to an M step that will be typically much simpler
than the maximization of the corresponding incomplete-data log likelihood function
p(X|θ).
The operation of the EM algorithm can also be viewed in the space of parame-
ters, as illustrated schematically in Figure 9.14. Here the red curve depicts the (in-

Figure 9.13 Illustration of the M step of the EM
algorithm.
The distribution q(Z)
is held ﬁxed and the lower bound
L(q, θ) is maximized with respect
to the parameter vector θ to give
a revised value θnew. Because the
KL divergence is nonnegative, this
causes the log likelihood ln p(X|θ)
to increase by at least as much as
the lower bound does.

KL(q||p)

L(q, θnew)

ln p(X|θnew)







Z

Figure 9.14 The EM algorithm involves alter-
nately computing a lower bound
on the log likelihood for the cur-
rent parameter values and then
maximizing this bound to obtain
the new parameter values. See
the text for a full discussion.

9.4. The EM Algorithm in General

453

ln p(X|θ)

L (q, θ)

θold θnew

Exercise 9.25

complete data) log likelihood function whose value we wish to maximize. We start
with some initial parameter value θold, and in the ﬁrst E step we evaluate the poste-
rior distribution over latent variables, which gives rise to a lower bound L(θ, θ(old))
whose value equals the log likelihood at θ(old), as shown by the blue curve. Note that
the bound makes a tangential contact with the log likelihood at θ(old), so that both
curves have the same gradient. This bound is a convex function having a unique
maximum (for mixture components from the exponential family). In the M step, the
bound is maximized giving the value θ(new), which gives a larger value of log likeli-
hood than θ(old). The subsequent E step then constructs a bound that is tangential at
θ(new) as shown by the green curve.

For the particular case of an independent, identically distributed data set, X
will comprise N data points {xn} while Z will comprise N corresponding latent
variables {zn}, where n = 1, . . . , N. From the independence assumption, we have
n p(xn, zn) and, by marginalizing over the {zn} we have p(X) =
p(X, Z) =
n p(xn). Using the sum and product rules, we see that the posterior probability

that is evaluated in the E step takes the form

p(Z|X, θ) = p(X, Z|θ)
p(X, Z|θ)

=

=

n=1

p(zn|xn, θ)

(9.75)

p(xn, zn|θ)

p(xn, zn|θ)

Z

n=1

In the case of
and so the posterior distribution also factorizes with respect to n.
the Gaussian mixture model this simply says that the responsibility that each of the
mixture components takes for a particular data point xn depends only on the value
of xn and on the parameters θ of the mixture components, not on the values of the
other data points.

We have seen that both the E and the M steps of the EM algorithm are increas-
ing the value of a well-deﬁned bound on the log likelihood function and that the





n=1

N

N



N

454

9. MIXTURE MODELS AND EM

complete EM cycle will change the model parameters in such a way as to cause
the log likelihood to increase (unless it is already at a maximum, in which case the
parameters remain unchanged).

We can also use the EM algorithm to maximize the posterior distribution p(θ|X)
for models in which we have introduced a prior p(θ) over the parameters. To see this,
we note that as a function of θ, we have p(θ|X) = p(θ, X)/p(X) and so

ln p(θ|X) = ln p(θ, X) − ln p(X).

Making use of the decomposition (9.70), we have

ln p(θ|X) = L(q, θ) + KL(qp) + ln p(θ) − ln p(X)

� L(q, θ) + ln p(θ) − ln p(X).

(9.76)

(9.77)

where ln p(X) is a constant. We can again optimize the right-hand side alternately
with respect to q and θ. The optimization with respect to q gives rise to the same E-
step equations as for the standard EM algorithm, because q only appears in L(q, θ).
The M-step equations are modiﬁed through the introduction of the prior term ln p(θ),
which typically requires only a small modiﬁcation to the standard maximum likeli-
hood M-step equations.

The EM algorithm breaks down the potentially difﬁcult problem of maximizing
the likelihood function into two stages, the E step and the M step, each of which will
often prove simpler to implement. Nevertheless, for complex models it may be the
case that either the E step or the M step, or indeed both, remain intractable. This
leads to two possible extensions of the EM algorithm, as follows.

The generalized EM, or GEM, algorithm addresses the problem of an intractable
M step. Instead of aiming to maximize L(q, θ) with respect to θ, it seeks instead
to change the parameters in such a way as to increase its value. Again, because
L(q, θ) is a lower bound on the log likelihood function, each complete EM cycle of
the GEM algorithm is guaranteed to increase the value of the log likelihood (unless
the parameters already correspond to a local maximum). One way to exploit the
GEM approach would be to use one of the nonlinear optimization strategies, such
as the conjugate gradients algorithm, during the M step. Another form of GEM
algorithm, known as the expectation conditional maximization, or ECM, algorithm,
involves making several constrained optimizations within each M step (Meng and
Rubin, 1993). For instance, the parameters might be partitioned into groups, and the
M step is broken down into multiple steps each of which involves optimizing one of
the subset with the remainder held ﬁxed.

We can similarly generalize the E step of the EM algorithm by performing a
partial, rather than complete, optimization of L(q, θ) with respect to q(Z) (Neal and
Hinton, 1999). As we have seen, for any given value of θ there is a unique maximum
of L(q, θ) with respect to q(Z) that corresponds to the posterior distribution qθ(Z) =
p(Z|X, θ) and that for this choice of q(Z) the bound L(q, θ) is equal to the log
likelihood function ln p(X|θ). It follows that any algorithm that converges to the
global maximum of L(q, θ) will ﬁnd a value of θ that is also a global maximum
of the log likelihood ln p(X|θ). Provided p(X, Z|θ) is a continuous function of θ








Exercises

455

then, by continuity, any local maximum of L(q, θ) will also be a local maximum of
ln p(X|θ).
Consider the case of N independent data points x1, . . . , xN with corresponding
latent variables z1, . . . , zN . The joint distribution p(X, Z|θ) factorizes over the data
points, and this structure can be exploited in an incremental form of EM in which
at each EM cycle only one data point is processed at a time. In the E step, instead
of recomputing the responsibilities for all of the data points, we just re-evaluate the
responsibilities for one data point. It might appear that the subsequent M step would
require computation involving the responsibilities for all of the data points. How-
ever, if the mixture components are members of the exponential family, then the
responsibilities enter only through simple sufﬁcient statistics, and these can be up-
dated efﬁciently. Consider, for instance, the case of a Gaussian mixture, and suppose
we perform an update for data point m in which the corresponding old and new
values of the responsibilities are denoted γold(zmk) and γnew(zmk). In the M step,
the required sufﬁcient statistics can be updated incrementally. For instance, for the
means the sufﬁcient statistics are deﬁned by (9.17) and (9.18) from which we obtain

(9.78)

(9.79)

Exercise 9.26

Exercises

µnew

k = µold

k +

γnew(zmk) − γold(zmk)

N new

k

xm − µold

k

together with

N new

k = N old

k + γnew(zmk) − γold(zmk).

The corresponding results for the covariances and the mixing coefﬁcients are analo-
gous.

Thus both the E step and the M step take ﬁxed time that is independent of the
total number of data points. Because the parameters are revised after each data point,
rather than waiting until after the whole data set is processed, this incremental ver-
sion can converge faster than the batch version. Each E or M step in this incremental
algorithm is increasing the value of L(q, θ) and, as we have shown above, if the
algorithm converges to a local (or global) maximum of L(q, θ), this will correspond
to a local (or global) maximum of the log likelihood function ln p(X|θ).

9.1 () www Consider the K-means algorithm discussed in Section 9.1. Show that as
a consequence of there being a ﬁnite number of possible assignments for the set of
discrete indicator variables rnk, and that for each such assignment there is a unique
optimum for the {µk}, the K-means algorithm must converge after a ﬁnite number
of iterations.

9.2 () Apply the Robbins-Monro sequential estimation procedure described in Sec-
tion 2.3.5 to the problem of ﬁnding the roots of the regression function given by
the derivatives of J in (9.1) with respect to µk. Show that this leads to a stochastic
K-means algorithm in which, for each data point xn, the nearest prototype µk is
updated using (9.5).




N

K

456

9. MIXTURE MODELS AND EM

9.3 () www Consider a Gaussian mixture model in which the marginal distribution
p(z) for the latent variable is given by (9.10), and the conditional distribution p(x|z)
for the observed variable is given by (9.11). Show that the marginal distribution
p(x), obtained by summing p(z)p(x|z) over all possible values of z, is a Gaussian
mixture of the form (9.7).

9.4 () Suppose we wish to use the EM algorithm to maximize the posterior distri-
bution over parameters p(θ|X) for a model containing latent variables, where X is
the observed data set. Show that the E step remains the same as in the maximum
likelihood case, whereas in the M step the quantity to be maximized is given by
Q(θ, θold) + ln p(θ) where Q(θ, θold) is deﬁned by (9.30).

9.5 () Consider the directed graph for a Gaussian mixture model shown in Figure 9.6.
By making use of the d-separation criterion discussed in Section 8.2, show that the
posterior distribution of the latent variables factorizes with respect to the different
data points so that

p(Z|X, µ, Σ, π) =

p(zn|xn, µ, Σ, π).

(9.80)

n=1

9.6 ( ) Consider a special case of a Gaussian mixture model in which the covari-
ance matrices Σk of the components are all constrained to have a common value
Σ. Derive the EM equations for maximizing the likelihood function under such a
model.

9.7 () www Verify that maximization of the complete-data log likelihood (9.36) for
a Gaussian mixture model leads to the result that the means and covariances of each
component are ﬁtted independently to the corresponding group of data points, and
the mixing coefﬁcients are given by the fractions of points in each group.

9.8 () www Show that if we maximize (9.40) with respect to µk while keeping the

responsibilities γ(znk) ﬁxed, we obtain the closed form solution given by (9.17).

9.9 () Show that if we maximize (9.40) with respect to Σk and πk while keeping the
responsibilities γ(znk) ﬁxed, we obtain the closed form solutions given by (9.19)
and (9.22).

9.10 ( ) Consider a density model given by a mixture distribution

p(x) =

πkp(x|k)

k=1

(9.81)

and suppose that we partition the vector x into two parts so that x = (xa, xb).
Show that the conditional density p(xb|xa) is itself a mixture distribution and ﬁnd
expressions for the mixing coefﬁcients and for the component densities.




k=1

K



Exercises

457

9.11 ()

In Section 9.3.2, we obtained a relationship between K means and EM for
Gaussian mixtures by considering a mixture model in which all components have
covariance 	I. Show that in the limit 	 → 0, maximizing the expected complete-
data log likelihood for this model, given by (9.40), is equivalent to minimizing the
distortion measure J for the K-means algorithm given by (9.1).

9.12 () www Consider a mixture distribution of the form

p(x) =

πkp(x|k)

(9.82)

where the elements of x could be discrete or continuous or a combination of these.
Denote the mean and covariance of p(x|k) by µk and Σk, respectively. Show that
the mean and covariance of the mixture distribution are given by (9.49) and (9.50).

9.13 ( ) Using the re-estimation equations for the EM algorithm, show that a mix-
ture of Bernoulli distributions, with its parameters set to values corresponding to a
maximum of the likelihood function, has the property that

E[x] =

N

1
N

xn ≡ x.

n=1

(9.83)

Hence show that if the parameters of this model are initialized such that all compo-
nents have the same mean µk =
µ for k = 1, . . . , K, then the EM algorithm will
converge after one iteration, for any choice of the initial mixing coefﬁcients, and that
this solution has the property µk = x. Note that this represents a degenerate case of
the mixture model in which all of the components are identical, and in practice we
try to avoid such solutions by using an appropriate initialization.

9.14 () Consider the joint distribution of latent and observed variables for the Bernoulli
distribution obtained by forming the product of p(x|z, µ) given by (9.52) and p(z|π)
given by (9.53). Show that if we marginalize this joint distribution with respect to z,
then we obtain (9.47).

9.15 () www Show that if we maximize the expected complete-data log likelihood
function (9.55) for a mixture of Bernoulli distributions with respect to µk, we obtain
the M step equation (9.59).

9.16 () Show that if we maximize the expected complete-data log likelihood function
(9.55) for a mixture of Bernoulli distributions with respect to the mixing coefﬁcients
πk, using a Lagrange multiplier to enforce the summation constraint, we obtain the
M step equation (9.60).

9.17 () www Show that as a consequence of the constraint 0 � p(xn|µk) � 1 for
the discrete variable xn, the incomplete-data log likelihood function for a mixture
of Bernoulli distributions is bounded above, and hence that there are no singularities
for which the likelihood goes to inﬁnity.








k=1

i=1

j=1

K

p(x) =

where

πkp(x|µk)

p(x|µk) =

D

M

µxij
kij.

(9.84)

(9.85)

458

9. MIXTURE MODELS AND EM

9.18 ( ) Consider a Bernoulli mixture model as discussed in Section 9.3.3, together
with a prior distribution p(µk|ak, bk) over each of the parameter vectors µk given
by the beta distribution (2.13), and a Dirichlet prior p(π|α) given by (2.38). Derive
the EM algorithm for maximizing the posterior probability p(µ, π|X).

9.19 ( ) Consider a D-dimensional variable x each of whose components i is itself a
multinomial variable of degree M so that x is a binary vector with components xij
where i = 1, . . . , D and j = 1, . . . , M, subject to the constraint that
j xij = 1 for
all i. Suppose that the distribution of these variables is described by a mixture of the
discrete multinomial distributions considered in Section 2.2 so that

The parameters µkij represent the probabilities p(xij = 1|µk) and must satisfy
0 � µkij � 1 together with the constraint
j µkij = 1 for all values of k and i.
Given an observed data set {xn}, where n = 1, . . . , N, derive the E and M step
equations of the EM algorithm for optimizing the mixing coefﬁcients πk and the
component parameters µkij of this distribution by maximum likelihood.

9.20 () www Show that maximization of the expected complete-data log likelihood
function (9.62) for the Bayesian linear regression model leads to the M step re-
estimation result (9.63) for α.

9.21 ( ) Using the evidence framework of Section 3.5, derive the M-step re-estimation
equations for the parameter β in the Bayesian linear regression model, analogous to
the result (9.63) for α.

9.22 ( ) By maximization of the expected complete-data log likelihood deﬁned by
(9.66), derive the M step equations (9.67) and (9.68) for re-estimating the hyperpa-
rameters of the relevance vector machine for regression.

9.23 ( ) www In Section 7.2.1 we used direct maximization of the marginal like-
lihood to derive the re-estimation equations (7.87) and (7.88) for ﬁnding values of
the hyperparameters α and β for the regression RVM. Similarly, in Section 9.3.4
we used the EM algorithm to maximize the same marginal likelihood, giving the
re-estimation equations (9.67) and (9.68). Show that these two sets of re-estimation
equations are formally equivalent.

9.24 () Verify the relation (9.70) in which L(q, θ) and KL(qp) are deﬁned by (9.71)

and (9.72), respectively.

Exercises

459

9.25 () www Show that the lower bound L(q, θ) given by (9.71), with q(Z) =
p(Z|X, θ(old)), has the same gradient with respect to θ as the log likelihood function
ln p(X|θ) at the point θ = θ(old).

9.26 () www Consider the incremental form of the EM algorithm for a mixture of
Gaussians, in which the responsibilities are recomputed only for a speciﬁc data point
xm. Starting from the M-step formulae (9.17) and (9.18), derive the results (9.78)
and (9.79) for updating the component means.

9.27 ( ) Derive M-step formulae for updating the covariance matrices and mixing
coefﬁcients in a Gaussian mixture model when the responsibilities are updated in-
crementally, analogous to the result (9.78) for updating the means.

10

Approximate

Inference

A central task in the application of probabilistic models is the evaluation of the pos-
terior distribution p(Z|X) of the latent variables Z given the observed (visible) data
variables X, and the evaluation of expectations computed with respect to this dis-
tribution. The model might also contain some deterministic parameters, which we
will leave implicit for the moment, or it may be a fully Bayesian model in which any
unknown parameters are given prior distributions and are absorbed into the set of
latent variables denoted by the vector Z. For instance, in the EM algorithm we need
to evaluate the expectation of the complete-data log likelihood with respect to the
posterior distribution of the latent variables. For many models of practical interest, it
will be infeasible to evaluate the posterior distribution or indeed to compute expec-
tations with respect to this distribution. This could be because the dimensionality of
the latent space is too high to work with directly or because the posterior distribution
has a highly complex form for which expectations are not analytically tractable. In
the case of continuous variables, the required integrations may not have closed-form

461

462

10. APPROXIMATE INFERENCE

analytical solutions, while the dimensionality of the space and the complexity of the
integrand may prohibit numerical integration. For discrete variables, the marginal-
izations involve summing over all possible conﬁgurations of the hidden variables,
and though this is always possible in principle, we often ﬁnd in practice that there
may be exponentially many hidden states so that exact calculation is prohibitively
expensive.

In such situations, we need to resort to approximation schemes, and these fall
broadly into two classes, according to whether they rely on stochastic or determin-
istic approximations. Stochastic techniques such as Markov chain Monte Carlo, de-
scribed in Chapter 11, have enabled the widespread use of Bayesian methods across
many domains. They generally have the property that given inﬁnite computational
resource, they can generate exact results, and the approximation arises from the use
of a ﬁnite amount of processor time. In practice, sampling methods can be compu-
tationally demanding, often limiting their use to small-scale problems. Also, it can
be difﬁcult to know whether a sampling scheme is generating independent samples
from the required distribution.

In this chapter, we introduce a range of deterministic approximation schemes,
some of which scale well to large applications. These are based on analytical ap-
proximations to the posterior distribution, for example by assuming that it factorizes
in a particular way or that it has a speciﬁc parametric form such as a Gaussian. As
such, they can never generate exact results, and so their strengths and weaknesses
are complementary to those of sampling methods.

In Section 4.4, we discussed the Laplace approximation, which is based on a
local Gaussian approximation to a mode (i.e., a maximum) of the distribution. Here
we turn to a family of approximation techniques called variational inference or vari-
ational Bayes, which use more global criteria and which have been widely applied.
We conclude with a brief introduction to an alternative variational framework known
as expectation propagation.



10.1. Variational Inference

Variational methods have their origins in the 18th century with the work of Euler,
Lagrange, and others on the calculus of variations. Standard calculus is concerned
with ﬁnding derivatives of functions. We can think of a function as a mapping that
takes the value of a variable as the input and returns the value of the function as the
output. The derivative of the function then describes how the output value varies
as we make inﬁnitesimal changes to the input value. Similarly, we can deﬁne a
functional as a mapping that takes a function as the input and that returns the value
of the functional as the output. An example would be the entropy H[p], which takes
a probability distribution p(x) as the input and returns the quantity

H[p] =

p(x) ln p(x) dx

(10.1)












10.1. Variational Inference

463

as the output. We can the introduce the concept of a functional derivative, which ex-
presses how the value of the functional changes in response to inﬁnitesimal changes
to the input function (Feynman et al., 1964). The rules for the calculus of variations
mirror those of standard calculus and are discussed in Appendix D. Many problems
can be expressed in terms of an optimization problem in which the quantity being
optimized is a functional. The solution is obtained by exploring all possible input
functions to ﬁnd the one that maximizes, or minimizes, the functional. Variational
methods have broad applicability and include such areas as ﬁnite element methods
(Kapur, 1989) and maximum entropy (Schwarz, 1988).

Although there is nothing intrinsically approximate about variational methods,
they do naturally lend themselves to ﬁnding approximate solutions. This is done
by restricting the range of functions over which the optimization is performed, for
instance by considering only quadratic functions or by considering functions com-
posed of a linear combination of ﬁxed basis functions in which only the coefﬁcients
of the linear combination can vary. In the case of applications to probabilistic in-
ference, the restriction may for example take the form of factorization assumptions
(Jordan et al., 1999; Jaakkola, 2001).

Now let us consider in more detail how the concept of variational optimization
can be applied to the inference problem. Suppose we have a fully Bayesian model in
which all parameters are given prior distributions. The model may also have latent
variables as well as parameters, and we shall denote the set of all latent variables
and parameters by Z. Similarly, we denote the set of all observed variables by X.
For example, we might have a set of N independent, identically distributed data,
for which X = {x1, . . . , xN} and Z = {z1, . . . , zN}. Our probabilistic model
speciﬁes the joint distribution p(X, Z), and our goal is to ﬁnd an approximation for
the posterior distribution p(Z|X) as well as for the model evidence p(X). As in our
discussion of EM, we can decompose the log marginal probability using

where we have deﬁned

ln p(X) = L(q) + KL(qp)

L(q) =

q(Z) ln

KL(qp) = −

q(Z) ln

p(X, Z)

q(Z)
p(Z|X)
q(Z)

dZ

dZ.

(10.2)

(10.3)

(10.4)

This differs from our discussion of EM only in that the parameter vector θ no longer
appears, because the parameters are now stochastic variables and are absorbed into
Z. Since in this chapter we will mainly be interested in continuous variables we have
used integrations rather than summations in formulating this decomposition. How-
ever, the analysis goes through unchanged if some or all of the variables are discrete
simply by replacing the integrations with summations as required. As before, we
can maximize the lower bound L(q) by optimization with respect to the distribution
q(Z), which is equivalent to minimizing the KL divergence. If we allow any possible
choice for q(Z), then the maximum of the lower bound occurs when the KL diver-
gence vanishes, which occurs when q(Z) equals the posterior distribution p(Z|X).

464

10. APPROXIMATE INFERENCE

1

0.8

0.6

0.4

0.2

0
−2

−1

0

1

2

3

4

40

30

20

10

0
−2

Figure 10.1 Illustration of the variational approximation for the example considered earlier in Figure 4.14. The
left-hand plot shows the original distribution (yellow) along with the Laplace (red) and variational (green) approx-
imations, and the right-hand plot shows the negative logarithms of the corresponding curves.

−1

0

1

2

3

4



M

However, we shall suppose the model is such that working with the true posterior
distribution is intractable.

We therefore consider instead a restricted family of distributions q(Z) and then
seek the member of this family for which the KL divergence is minimized. Our goal
is to restrict the family sufﬁciently that they comprise only tractable distributions,
while at the same time allowing the family to be sufﬁciently rich and ﬂexible that it
can provide a good approximation to the true posterior distribution. It is important to
emphasize that the restriction is imposed purely to achieve tractability, and that sub-
ject to this requirement we should use as rich a family of approximating distributions
as possible. In particular, there is no ‘over-ﬁtting’ associated with highly ﬂexible dis-
tributions. Using more ﬂexible approximations simply allows us to approach the true
posterior distribution more closely.

One way to restrict the family of approximating distributions is to use a paramet-
ric distribution q(Z|ω) governed by a set of parameters ω. The lower bound L(q)
then becomes a function of ω, and we can exploit standard nonlinear optimization
techniques to determine the optimal values for the parameters. An example of this
approach, in which the variational distribution is a Gaussian and we have optimized
with respect to its mean and variance, is shown in Figure 10.1.

10.1.1 Factorized distributions
Here we consider an alternative way in which to restrict the family of distri-
butions q(Z). Suppose we partition the elements of Z into disjoint groups that we
denote by Zi where i = 1, . . . , M. We then assume that the q distribution factorizes
with respect to these groups, so that

q(Z) =

qi(Zi).

i=1

(10.5)









qj

qi

i










i




i=j



10.1. Variational Inference

465

It should be emphasized that we are making no further assumptions about the distri-
bution. In particular, we place no restriction on the functional forms of the individual
factors qi(Zi). This factorized form of variational inference corresponds to an ap-
proximation framework developed in physics called mean ﬁeld theory (Parisi, 1988).
Amongst all distributions q(Z) having the form (10.5), we now seek that distri-
bution for which the lower bound L(q) is largest. We therefore wish to make a free
form (variational) optimization of L(q) with respect to all of the distributions qi(Zi),
which we do by optimizing with respect to each of the factors in turn. To achieve
this, we ﬁrst substitute (10.5) into (10.3) and then dissect out the dependence on one
of the factors qj(Zj). Denoting qj(Zj) by simply qj to keep the notation uncluttered,
we then obtain

L(q) =

=

ln p(X, Z) −

ln qi

dZ

ln p(X, Z)

qi dZi

qj ln qj dZj + const

dZj −

i=j
p(X, Zj) dZj −
where we have deﬁned a new distribution

qj ln

=

qj ln qj dZj + const

(10.6)

p(X, Zj) by the relation

ln

p(X, Zj) = Ei=j[ln p(X, Z)] + const.

(10.7)
Here the notation Ei=j[··· ] denotes an expectation with respect to the q distributions
over all variables zi for i = j, so that
Ei=j[ln p(X, Z)] =

ln p(X, Z)

qi dZi.

(10.8)

Now suppose we keep the {qi=j} ﬁxed and maximize L(q) in (10.6) with re-
spect to all possible forms for the distribution qj(Zj). This is easily done by rec-
ognizing that (10.6) is a negative Kullback-Leibler divergence between qj(Zj) and
p(X, Zj). Thus maximizing (10.6) is equivalent to minimizing the Kullback-Leibler

Leonhard Euler
1707–1783

Euler was a Swiss mathematician
and physicist who worked in St.
Petersburg and Berlin and who is
widely considered to be one of the
greatest mathematicians of all time.
He is certainly the most proliﬁc, and
his collected works ﬁll 75 volumes. Amongst his many

contributions, he formulated the modern theory of the
function, he developed (together with Lagrange) the
calculus of variations, and he discovered the formula
eiπ = −1, which relates four of the most important
numbers in mathematics. During the last 17 years of
his life, he was almost totally blind, and yet he pro-
duced nearly half of his results during this period.












466

10. APPROXIMATE INFERENCE

divergence, and the minimum occurs when qj(Zj) =
general expression for the optimal solution q

j (Zj) given by
j (Zj) = Ei=j[ln p(X, Z)] + const.

ln q

p(X, Zj). Thus we obtain a

(10.9)

It is worth taking a few moments to study the form of this solution as it provides the
basis for applications of variational methods. It says that the log of the optimal so-
lution for factor qj is obtained simply by considering the log of the joint distribution
over all hidden and visible variables and then taking the expectation with respect to
all of the other factors {qi} for i = j.
Thus if we take the exponential of both sides and normalize, we have

The additive constant in (10.9) is set by normalizing the distribution q

j (Zj).

j (Zj) =
q

exp (Ei=j[ln p(X, Z)])
exp (Ei=j[ln p(X, Z)]) dZj

.

In practice, we shall ﬁnd it more convenient to work with the form (10.9) and then re-
instate the normalization constant (where required) by inspection. This will become
clear from subsequent examples.

The set of equations given by (10.9) for j = 1, . . . , M represent a set of con-
sistency conditions for the maximum of the lower bound subject to the factorization
constraint. However, they do not represent an explicit solution because the expres-
j (Zj) depends on expectations
sion on the right-hand side of (10.9) for the optimum q
computed with respect to the other factors qi(Zi) for i = j. We will therefore seek
a consistent solution by ﬁrst initializing all of the factors qi(Zi) appropriately and
then cycling through the factors and replacing each in turn with a revised estimate
given by the right-hand side of (10.9) evaluated using the current estimates for all of
the other factors. Convergence is guaranteed because bound is convex with respect
to each of the factors qi(Zi) (Boyd and Vandenberghe, 2004).

10.1.2 Properties of factorized approximations
Our approach to variational inference is based on a factorized approximation to
the true posterior distribution. Let us consider for a moment the problem of approx-
imating a general distribution by a factorized distribution. To begin with, we discuss
the problem of approximating a Gaussian distribution using a factorized Gaussian,
which will provide useful insight into the types of inaccuracy introduced in using
factorized approximations. Consider a Gaussian distribution p(z) = N (z|µ, Λ−1)
over two correlated variables z = (z1, z2) in which the mean and precision have
elements

µ =

µ1
µ2

,

Λ =

Λ11 Λ12
Λ21 Λ22

(10.10)

and Λ21 = Λ12 due to the symmetry of the precision matrix. Now suppose we
wish to approximate this distribution using a factorized Gaussian of the form q(z) =
q1(z1)q2(z2). We ﬁrst apply the general result (10.9) to ﬁnd an expression for the





1(z1). In doing so it is useful to note that on the right-hand side we
optimal factor q
only need to retain those terms that have some functional dependence on z1 because
all other terms can be absorbed into the normalization constant. Thus we have

ln q

1(z1) = Ez2[ln p(z)] + const

10.1. Variational Inference

467

1
2

(z1 − µ1)2Λ11 − (z1 − µ1)Λ12(z2 − µ2)

= Ez2
−
1
1Λ11 + z1µ1Λ11 − z1Λ12 (E[z2] − µ2) + const.
2 z2

= −

+ const

(10.11)

Next we observe that the right-hand side of this expression is a quadratic function of
z1, and so we can identify q(z1) as a Gaussian distribution. It is worth emphasizing
that we did not assume that q(zi) is Gaussian, but rather we derived this result by
variational optimization of the KL divergence over all possible distributions q(zi).
Note also that we do not need to consider the additive constant in (10.9) explicitly
because it represents the normalization constant that can be found at the end by
inspection if required. Using the technique of completing the square, we can identify
the mean and precision of this Gaussian, giving

Section 2.3.1

q(z1) = N (z1|m1, Λ−1
11 )

where

By symmetry, q

11 Λ12 (E[z2] − µ2) .
2(z2) is also Gaussian and can be written as

m1 = µ1 − Λ−1

2(z2) = N (z2|m2, Λ−1
22 )
q

(10.12)

(10.13)

(10.14)

in which

m2 = µ2 − Λ−1

22 Λ21 (E[z1] − µ1) .

(10.15)
Note that these solutions are coupled, so that q(z1) depends on expectations com-
puted with respect to q(z2) and vice versa. In general, we address this by treating
the variational solutions as re-estimation equations and cycling through the variables
in turn updating them until some convergence criterion is satisﬁed. We shall see
an example of this shortly. Here, however, we note that the problem is sufﬁciently
simple that a closed form solution can be found. In particular, because E[z1] = m1
and E[z2] = m2, we see that the two equations are satisﬁed if we take E[z1] = µ1
and E[z2] = µ2, and it is easily shown that this is the only solution provided the dis-
tribution is nonsingular. This result is illustrated in Figure 10.2(a). We see that the
mean is correctly captured but that the variance of q(z) is controlled by the direction
of smallest variance of p(z), and that the variance along the orthogonal direction is
signiﬁcantly under-estimated. It is a general result that a factorized variational ap-
proximation tends to give approximations to the posterior distribution that are too
compact.

By way of comparison, suppose instead that we had been minimizing the reverse
Kullback-Leibler divergence KL(pq). As we shall see, this form of KL divergence

Exercise 10.2

468

10. APPROXIMATE INFERENCE

1

z2

0.5

0

0

Figure 10.2 Comparison
of
the two alternative forms for
the
Kullback-Leibler divergence.
The
green contours corresponding to
1, 2, and 3 standard deviations for
a correlated Gaussian distribution
p(z) over two variables z1 and z2,
and the red contours represent
the corresponding levels for an
approximating
q(z)
over the same variables given by
the product of
two independent
univariate Gaussian distributions
whose parameters are obtained by
minimization of
the Kullback-
Leibler divergence KL(qp), and
the reverse Kullback-Leibler
(b)
divergence KL(pq).

distribution

(a)









i=1

z1

M

i=j



 

0



z2

0.5

1

0



0.5
(a)

1

0.5
(b)

z1

1

Section 10.7

Exercise 10.3

is used in an alternative approximate inference framework called expectation prop-
agation. We therefore consider the general problem of minimizing KL(pq) when
q(Z) is a factorized approximation of the form (10.5). The KL divergence can then
be written in the form

KL(pq) = −

p(Z)

ln qi(Zi)

dZ + const

(10.16)

where the constant term is simply the entropy of p(Z) and so does not depend on
q(Z). We can now optimize with respect to each of the factors qj(Zj), which is
easily done using a Lagrange multiplier to give

j (Zj) =
q

p(Z)

dZi = p(Zj).

(10.17)

In this case, we ﬁnd that the optimal solution for qj(Zj) is just given by the corre-
sponding marginal distribution of p(Z). Note that this is a closed-form solution and
so does not require iteration.

To apply this result to the illustrative example of a Gaussian distribution p(z)
over a vector z we can use (2.98), which gives the result shown in Figure 10.2(b).
We see that once again the mean of the approximation is correct, but that it places
signiﬁcant probability mass in regions of variable space that have very low probabil-
ity.

The difference between these two results can be understood by noting that there

is a large positive contribution to the Kullback-Leibler divergence

KL(qp) = −

q(Z) ln

p(Z)
q(Z)

dZ

(10.18)

10.1. Variational Inference

469

(a)

(b)

(c)

Figure 10.3 Another comparison of the two alternative forms for the Kullback-Leibler divergence. (a) The blue
contours show a bimodal distribution p(Z) given by a mixture of two Gaussians, and the red contours correspond
to the single Gaussian distribution q(Z) that best approximates p(Z) in the sense of minimizing the Kullback-
Leibler divergence KL(pq). (b) As in (a) but now the red contours correspond to a Gaussian distribution q(Z)
found by numerical minimization of the Kullback-Leibler divergence KL(qp). (c) As in (b) but showing a different
local minimum of the Kullback-Leibler divergence.

from regions of Z space in which p(Z) is near zero unless q(Z) is also close to
zero. Thus minimizing this form of KL divergence leads to distributions q(Z) that
avoid regions in which p(Z) is small. Conversely, the Kullback-Leibler divergence
KL(pq) is minimized by distributions q(Z) that are nonzero in regions where p(Z)
is nonzero.
We can gain further insight into the different behaviour of the two KL diver-
gences if we consider approximating a multimodal distribution by a unimodal one,
as illustrated in Figure 10.3.
In practical applications, the true posterior distri-
bution will often be multimodal, with most of the posterior mass concentrated in
some number of relatively small regions of parameter space. These multiple modes
may arise through nonidentiﬁability in the latent space or through complex nonlin-
ear dependence on the parameters. Both types of multimodality were encountered in
Chapter 9 in the context of Gaussian mixtures, where they manifested themselves as
multiple maxima in the likelihood function, and a variational treatment based on the
minimization of KL(qp) will tend to ﬁnd one of these modes. By contrast, if we
were to minimize KL(pq), the resulting approximations would average across all
of the modes and, in the context of the mixture model, would lead to poor predictive
distributions (because the average of two good parameter values is typically itself
not a good parameter value). It is possible to make use of KL(pq) to deﬁne a useful
inference procedure, but this requires a rather different approach to the one discussed
here, and will be considered in detail when we discuss expectation propagation.

The two forms of Kullback-Leibler divergence are members of the alpha family

Section 10.7

















N

n=1






470

10. APPROXIMATE INFERENCE

of divergences (Ali and Silvey, 1966; Amari, 1985; Minka, 2005) deﬁned by

Dα(pq) =

4

1 − α2

1 −

p(x)(1+α)/2q(x)(1−α)/2 dx

(10.19)

where −∞ < α < ∞ is a continuous parameter. The Kullback-Leibler divergence
KL(pq) corresponds to the limit α → 1, whereas KL(qp) corresponds to the limit
α → −1. For all values of α we have Dα(pq) � 0, with equality if, and only if,
p(x) = q(x). Suppose p(x) is a ﬁxed distribution, and we minimize Dα(pq) with
respect to some set of distributions q(x). Then for α � −1 the divergence is zero
forcing, so that any values of x for which p(x) = 0 will have q(x) = 0, and typically
q(x) will under-estimate the support of p(x) and will tend to seek the mode with the
largest mass. Conversely for α � 1 the divergence is zero-avoiding, so that values
of x for which p(x) > 0 will have q(x) > 0, and typically q(x) will stretch to cover
all of p(x), and will over-estimate the support of p(x). When α = 0 we obtain a
symmetric divergence that is linearly related to the Hellinger distance given by

DH(pq) =

p(x)1/2 − q(x)1/2

dx.

(10.20)

The square root of the Hellinger distance is a valid distance metric.

10.1.3 Example: The univariate Gaussian
We now illustrate the factorized variational approximation using a Gaussian dis-
tribution over a single variable x (MacKay, 2003). Our goal is to infer the posterior
distribution for the mean µ and precision τ , given a data set D = {x1, . . . , xN} of
observed values of x which are assumed to be drawn independently from the Gaus-
sian. The likelihood function is given by

p(D|µ, τ) =

N/2

τ
2π

exp

τ
2

−

(xn − µ)2

.

(10.21)

We now introduce conjugate prior distributions for µ and τ given by

µ|µ0, (λ0τ)−1

p(µ|τ) = N
p(τ) = Gam(τ|a0, b0)

(10.22)
(10.23)
where Gam(τ|a0, b0) is the gamma distribution deﬁned by (2.146). Together these
distributions constitute a Gaussian-Gamma conjugate prior distribution.
For this simple problem the posterior distribution can be found exactly, and again
takes the form of a Gaussian-gamma distribution. However, for tutorial purposes
we will consider a factorized variational approximation to the posterior distribution
given by

q(µ, τ) = qµ(µ)qτ (τ).

(10.24)

Exercise 10.6

Section 2.3.6

Exercise 2.44



n=1








n=1

N

N






 
 

10.1. Variational Inference

471

Note that the true posterior distribution does not factorize in this way. The optimum
factors qµ(µ) and qτ (τ) can be obtained from the general result (10.9) as follows.
For qµ(µ) we have

ln q

µ(µ) = Eτ [ln p(D|µ, τ) + ln p(µ|τ)] + const
(xn − µ)2

λ0(µ − µ0)2 +

= −

E[τ]
2

N

+ const. (10.25)

Exercise 10.7

Completing the square over µ we see that qµ(µ) is a Gaussian N
mean and precision given by

µ|µN , λ−1

N

with

µN = λ0µ0 + N x
λ0 + N
λN = (λ0 + N)E[τ].

(10.27)
Note that for N → ∞ this gives the maximum likelihood result in which µN = x
and the precision is inﬁnite.

Similarly, the optimal solution for the factor qτ (τ) is given by

ln q

τ (τ) = Eµ [ln p(D|µ, τ) + ln p(µ|τ)] + ln p(τ) + const

= (a0 − 1) ln τ − b0τ + N
2

ln τ

τ
2 Eµ

−

(xn − µ)2 + λ0(µ − µ0)2

+ const

and hence qτ (τ) is a gamma distribution Gam(τ|aN , bN ) with parameters

aN = a0 + N
2
1
2Eµ

bN = b0 +

(xn − µ)2 + λ0(µ − µ0)2

.

n=1

(10.26)

(10.28)

(10.29)

(10.30)

Exercise 10.8

Section 10.4.1

Again this exhibits the expected behaviour when N → ∞.
It should be emphasized that we did not assume these speciﬁc functional forms
for the optimal distributions qµ(µ) and qτ (τ). They arose naturally from the structure
of the likelihood function and the corresponding conjugate priors.

Thus we have expressions for the optimal distributions qµ(µ) and qτ (τ) each of
which depends on moments evaluated with respect to the other distribution. One ap-
proach to ﬁnding a solution is therefore to make an initial guess for, say, the moment
E[τ] and use this to re-compute the distribution qµ(µ). Given this revised distri-
bution we can then extract the required moments E[µ] and E[µ2], and use these to
recompute the distribution qτ (τ), and so on. Since the space of hidden variables for
this example is only two dimensional, we can illustrate the variational approxima-
tion to the posterior distribution by plotting contours of both the true posterior and
the factorized approximation, as illustrated in Figure 10.4.

10. APPROXIMATE INFERENCE

(a)

(c)

472

τ

2

1

0
−1
2

τ

1

0
−1



1



1

(b)

(d)

 

τ

2

1

0
−1
2

τ

1

0
−1

0

µ

0

µ

0

µ

1

0

µ

1

Figure 10.4 Illustration of variational inference for the mean µ and precision τ of a univariate Gaussian distribu-
tion. Contours of the true posterior distribution p(µ, τ|D) are shown in green. (a) Contours of the initial factorized
approximation qµ(µ)qτ (τ ) are shown in blue. (b) After re-estimating the factor qµ(µ). (c) After re-estimating the
factor qτ (τ ). (d) Contours of the optimal factorized approximation, to which the iterative scheme converges, are
shown in red.

In general, we will need to use an iterative approach such as this in order to
solve for the optimal factorized posterior distribution. For the very simple example
we are considering here, however, we can ﬁnd an explicit solution by solving the
simultaneous equations for the optimal factors qµ(µ) and qτ (τ). Before doing this,
we can simplify these expressions by considering broad, noninformative priors in
which µ0 = a0 = b0 = λ0 = 0. Although these parameter settings correspond to
improper priors, we see that the posterior distribution is still well deﬁned. Using the
standard result E[τ] = aN /bN for the mean of a gamma distribution, together with
(10.29) and (10.30), we have

Appendix B

N

1
E[τ]

= E

1
N

(xn − µ)2

= x2 − 2xE[µ] + E[µ2].

n=1

(10.31)

Then, using (10.26) and (10.27), we obtain the ﬁrst and second order moments of



N

n=1










1
E[τ]

m

Z

=

=





473

(10.32)

(10.33)

(10.34)

(10.35)

Exercise 10.9

Section 1.2.4

Exercise 10.10

Exercise 10.11

10.1. Variational Inference

1

N − 1

1

N − 1

(x2 − x2)

(xn − x)2.

qµ(µ) in the form

E[µ] = x,

E[µ2] = x2 +

1

NE[τ] .

We can now substitute these moments into (10.31) and then solve for E[τ] to give

We recognize the right-hand side as the familiar unbiased estimator for the variance
of a univariate Gaussian distribution, and so we see that the use of a Bayesian ap-
proach has avoided the bias of the maximum likelihood solution.

10.1.4 Model comparison
As well as performing inference over the hidden variables Z, we may also
wish to compare a set of candidate models, labelled by the index m, and having
prior probabilities p(m). Our goal is then to approximate the posterior probabilities
p(m|X), where X is the observed data. This is a slightly more complex situation
than that considered so far because different models may have different structure
and indeed different dimensionality for the hidden variables Z. We cannot there-
fore simply consider a factorized approximation q(Z)q(m), but must instead recog-
nize that the posterior over Z must be conditioned on m, and so we must consider
q(Z, m) = q(Z|m)q(m). We can readily verify the following decomposition based
on this variational distribution

ln p(X) = Lm −

q(Z|m)q(m) ln

p(Z, m|X)
q(Z|m)q(m)

where the Lm is a lower bound on ln p(X) and is given by

Lm =

m

Z

q(Z|m)q(m) ln

p(Z, X, m)
q(Z|m)q(m)

.

Here we are assuming discrete Z, but the same analysis applies to continuous latent
variables provided the summations are replaced with integrations. We can maximize
Lm with respect to the distribution q(m) using a Lagrange multiplier, with the result
(10.36)
However, if we maximize Lm with respect to the q(Z|m), we ﬁnd that the solutions
for different m are coupled, as we expect because they are conditioned on m. We
proceed instead by ﬁrst optimizing each of the q(Z|m) individually by optimization

q(m) ∝ p(m) exp{Lm}.

474

10. APPROXIMATE INFERENCE




N

K





K



N





.

K

10.2.

Section 10.4.1

of (10.35), and then subsequently determining the q(m) using (10.36). After nor-
malization the resulting values for q(m) can be used for model selection or model
averaging in the usual way.

Illustration: Variational Mixture of Gaussians

We now return to our discussion of the Gaussian mixture model and apply the vari-
ational inference machinery developed in the previous section. This will provide a
good illustration of the application of variational methods and will also demonstrate
how a Bayesian treatment elegantly resolves many of the difﬁculties associated with
the maximum likelihood approach (Attias, 1999b). The reader is encouraged to work
through this example in detail as it provides many insights into the practical appli-
cation of variational methods. Many Bayesian models, corresponding to much more
sophisticated distributions, can be solved by straightforward extensions and general-
izations of this analysis.

Our starting point is the likelihood function for the Gaussian mixture model, il-
lustrated by the graphical model in Figure 9.6. For each observation xn we have
a corresponding latent variable zn comprising a 1-of-K binary vector with ele-
ments znk for k = 1, . . . , K. As before we denote the observed data set by X =
{x1, . . . , xN}, and similarly we denote the latent variables by Z = {z1, . . . , zN}.
From (9.10) we can write down the conditional distribution of Z, given the mixing
coefﬁcients π, in the form

p(Z|π) =

πznk
k

n=1

k=1

(10.37)

Similarly, from (9.11), we can write down the conditional distribution of the ob-
served data vectors, given the latent variables and the component parameters

p(X|Z, µ, Λ) =

N

n=1

k=1

xn|µk, Λ−1

k

znk

(10.38)

where µ = {µk} and Λ = {Λk}. Note that we are working in terms of precision
matrices rather than covariance matrices as this somewhat simpliﬁes the mathemat-
ics.

Next we introduce priors over the parameters µ, Λ and π. The analysis is con-
siderably simpliﬁed if we use conjugate prior distributions. We therefore choose a
Dirichlet distribution over the mixing coefﬁcients π

p(π) = Dir(π|α0) = C(α0)

πα0−1
k

k=1

(10.39)

where by symmetry we have chosen the same parameter α0 for each of the compo-
nents, and C(α0) is the normalization constant for the Dirichlet distribution deﬁned



K






10.2. Illustration: Variational Mixture of Gaussians

Figure 10.5 Directed acyclic graph representing the Bayesian mix-
ture of Gaussians model, in which the box (plate) de-
notes a set of N i.i.d. observations. Here µ denotes
{µk} and Λ denotes {Λk}.

π

475

Λ

µ

zn

xn

N

Section 2.2.1

Section 2.3.6

by (B.23). As we have seen, the parameter α0 can be interpreted as the effective
prior number of observations associated with each component of the mixture. If the
value of α0 is small, then the posterior distribution will be inﬂuenced primarily by
the data rather than by the prior.

Similarly, we introduce an independent Gaussian-Wishart prior governing the

mean and precision of each Gaussian component, given by

p(µ, Λ) = p(µ|Λ)p(Λ)

=

k=1

N

µk|m0, (β0Λk)−1

W(Λk|W0, ν0)

(10.40)

because this represents the conjugate prior distribution when both the mean and pre-
cision are unknown. Typically we would choose m0 = 0 by symmetry.

The resulting model can be represented as a directed graph as shown in Fig-
ure 10.5. Note that there is a link from Λ to µ since the variance of the distribution
over µ in (10.40) is a function of Λ.

This example provides a nice illustration of the distinction between latent vari-
ables and parameters. Variables such as zn that appear inside the plate are regarded
as latent variables because the number of such variables grows with the size of the
data set. By contrast, variables such as µ that are outside the plate are ﬁxed in
number independently of the size of the data set, and so are regarded as parameters.
From the perspective of graphical models, however, there is really no fundamental
difference between them.

10.2.1 Variational distribution
In order to formulate a variational treatment of this model, we next write down

the joint distribution of all of the random variables, which is given by
p(X, Z, π, µ, Λ) = p(X|Z, µ, Λ)p(Z|π)p(π)p(µ|Λ)p(Λ)

(10.41)

in which the various factors are deﬁned above. The reader should take a moment to
verify that this decomposition does indeed correspond to the probabilistic graphical
model shown in Figure 10.5. Note that only the variables X = {x1, . . . , xN} are
observed.

	



N






n=1

k=1

K

N

N




K

K

k=1

476

10. APPROXIMATE INFERENCE

We now consider a variational distribution which factorizes between the latent

variables and the parameters so that

q(Z, π, µ, Λ) = q(Z)q(π, µ, Λ).

(10.42)

It is remarkable that this is the only assumption that we need to make in order to
obtain a tractable practical solution to our Bayesian mixture model. In particular, the
functional form of the factors q(Z) and q(π, µ, Λ) will be determined automatically
by optimization of the variational distribution. Note that we are omitting the sub-
scripts on the q distributions, much as we do with the p distributions in (10.41), and
are relying on the arguments to distinguish the different distributions.

The corresponding sequential update equations for these factors can be easily
derived by making use of the general result (10.9). Let us consider the derivation of
the update equation for the factor q(Z). The log of the optimized factor is given by

ln q(Z) = Eπ,µ,Λ[ln p(X, Z, π, µ, Λ)] + const.

(10.43)

We now make use of the decomposition (10.41). Note that we are only interested in
the functional dependence of the right-hand side on the variable Z. Thus any terms
that do not depend on Z can be absorbed into the additive normalization constant,
giving

ln q(Z) = Eπ[ln p(Z|π)] + Eµ,Λ[ln p(X|Z, µ, Λ)] + const.

(10.44)

Substituting for the two conditional distributions on the right-hand side, and again
absorbing any terms that are independent of Z into the additive constant, we have

ln q(Z) =

n=1

where we have deﬁned

znk ln ρnk + const

ln ρnk = E[ln πk] +

D
2

1
2E [ln|Λk| ] −
(xn − µk)TΛk(xn − µk)

ln(2π)

1
2Eµk,Λk

−

(10.45)

(10.46)

where D is the dimensionality of the data variable x. Taking the exponential of both
sides of (10.45) we obtain

q(Z) ∝

ρznk
nk .

(10.47)

Exercise 10.12

Requiring that this distribution be normalized, and noting that for each value of n
the quantities znk are binary and sum to 1 over all values of k, we obtain

q(Z) =

rznk
nk

n=1

k=1

(10.48)







k=1

j=1

K

K




n=1

N

rnk

N

n=1




n=1
1
Nk

1
Nk

N

N



K



10.2. Illustration: Variational Mixture of Gaussians

where

rnk = ρnk
ρnj

K

.

We see that the optimal solution for the factor q(Z) takes the same functional form
as the prior p(Z|π). Note that because ρnk is given by the exponential of a real
quantity, the quantities rnk will be nonnegative and will sum to one, as required.

For the discrete distribution q(Z) we have the standard result

E[znk] = rnk

(10.50)
from which we see that the quantities rnk are playing the role of responsibilities.
Note that the optimal solution for q(Z) depends on moments evaluated with respect
to the distributions of other variables, and so again the variational update equations
are coupled and must be solved iteratively.

At this point, we shall ﬁnd it convenient to deﬁne three statistics of the observed

data set evaluated with respect to the responsibilities, given by

477

(10.49)

(10.51)

(10.52)

(10.53)

Nk =

xk =

rnkxn

Sk =

rnk(xn − xk)(xn − xk)T.

Note that these are analogous to quantities evaluated in the maximum likelihood EM
algorithm for the Gaussian mixture model.

Now let us consider the factor q(π, µ, Λ) in the variational posterior distribu-

tion. Again using the general result (10.9) we have

ln q(π, µ, Λ) = ln p(π) +

ln p(µk, Λk) + EZ [ln p(Z|π)]

+

k=1

n=1

E[znk] lnN

xn|µk, Λ−1

k

+ const.

(10.54)

We observe that the right-hand side of this expression decomposes into a sum of
terms involving only π together with terms only involving µ and Λ, which implies
that the variational posterior q(π, µ, Λ) factorizes to give q(π)q(µ, Λ). Further-
more, the terms involving µ and Λ themselves comprise a sum over k of terms
involving µk and Λk leading to the further factorization

q(π, µ, Λ) = q(π)

q(µk, Λk).

(10.55)

k=1







k=1

K

D

i=1




	


n=1

k=1

K

N

2
α)











478

10. APPROXIMATE INFERENCE

Identifying the terms on the right-hand side of (10.54) that depend on π, we have

ln q(π) = (α0 − 1)

ln πk +

rnk ln πk + const

(10.56)

where we have used (10.50). Taking the exponential of both sides, we recognize
q(π) as a Dirichlet distribution

q(π) = Dir(π|α)

(10.57)

where α has components αk given by

αk = α0 + Nk.

(10.58)
Finally, the variational posterior distribution q(µk, Λk) does not factorize into
the product of the marginals, but we can always use the product rule to write it in the
form q(µk, Λk) = q(µk|Λk)q(Λk). The two factors can be found by inspecting
(10.54) and reading off those terms that involve µk and Λk. The result, as expected,
is a Gaussian-Wishart distribution and is given by
µk|mk, (βkΛk)−1

q(µk, Λk) = N

W(Λk|Wk, νk)

(10.59)

where we have deﬁned

βk = β0 + Nk
mk =

1
βk

= W−1

W−1
k
νk = ν0 + Nk.

(β0m0 + Nkxk)
0 + NkSk + β0Nk
β0 + Nk

(xk − m0)(xk − m0)T

(10.60)

(10.61)

(10.62)

(10.63)

These update equations are analogous to the M-step equations of the EM algorithm
for the maximum likelihood solution of the mixture of Gaussians. We see that the
computations that must be performed in order to update the variational posterior
distribution over the model parameters involve evaluation of the same sums over the
data set, as arose in the maximum likelihood treatment.

In order to perform this variational M step, we need the expectations E[znk] =
rnk representing the responsibilities. These are obtained by normalizing the ρnk that
are given by (10.46). We see that this expression involves expectations with respect
to the variational distributions of the parameters, and these are easily evaluated to
give

Eµk,Λk

(xn − µk)TΛk(xn − µk)

= Dβ−1

k + νk(xn − mk)TWk(xn − mk)
ψ

νk + 1 − i

(10.64)

+ D ln 2 + ln|Wk| (10.65)
(10.66)

ln

Λk ≡ E [ln|Λk| ] =
ln

πk ≡ E [ln πk] = ψ(αk) − ψ(

Exercise 10.13

Exercise 10.14



















Appendix B

Section 10.4.1

Section 3.4

Exercise 10.15

10.2. Illustration: Variational Mixture of Gaussians

479

where we have introduced deﬁnitions of
deﬁned by (B.25), with
the standard properties of the Wishart and Dirichlet distributions.

πk, and ψ(·) is the digamma function
k αk. The results (10.65) and (10.66) follow from

Λk and

α =

If we substitute (10.64), (10.65), and (10.66) into (10.46) and make use of

(10.49), we obtain the following result for the responsibilities

rnk ∝

πk

Λ1/2

k

exp

−

D
2βk −

νk
2

(xn − mk)TWk(xn − mk)

.

(10.67)

Notice the similarity to the corresponding result for the responsibilities in maximum
likelihood EM, which from (9.13) can be written in the form

rnk ∝ πk|Λk|1/2 exp

1
2

−

(xn − µk)TΛk(xn − µk)

(10.68)

where we have used the precision in place of the covariance to highlight the similarity
to (10.67).

Thus the optimization of the variational posterior distribution involves cycling
between two stages analogous to the E and M steps of the maximum likelihood EM
algorithm. In the variational equivalent of the E step, we use the current distributions
over the model parameters to evaluate the moments in (10.64), (10.65), and (10.66)
and hence evaluate E[znk] = rnk. Then in the subsequent variational equivalent
of the M step, we keep these responsibilities ﬁxed and use them to re-compute the
variational distribution over the parameters using (10.57) and (10.59). In each case,
we see that the variational posterior distribution has the same functional form as the
corresponding factor in the joint distribution (10.41). This is a general result and is
a consequence of the choice of conjugate distributions.

Figure 10.6 shows the results of applying this approach to the rescaled Old Faith-
ful data set for a Gaussian mixture model having K = 6 components. We see that
after convergence, there are only two components for which the expected values
of the mixing coefﬁcients are numerically distinguishable from their prior values.
This effect can be understood qualitatively in terms of the automatic trade-off in a
Bayesian model between ﬁtting the data and the complexity of the model, in which
the complexity penalty arises from components whose parameters are pushed away
from their prior values. Components that take essentially no responsibility for ex-
plaining the data points have rnk  0 and hence Nk  0. From (10.58), we see
that αk  α0 and from (10.60)–(10.63) we see that the other parameters revert to
their prior values. In principle such components are ﬁtted slightly to the data points,
but for broad priors this effect is too small to be seen numerically. For the varia-
tional Gaussian mixture model the expected values of the mixing coefﬁcients in the
posterior distribution are given by

E[πk] = αk + Nk
Kα0 + N

.

(10.69)

Consider a component for which Nk  0 and αk  α0. If the prior is broad so that
α0 → 0, then E[πk] → 0 and the component plays no role in the model, whereas if

480

10. APPROXIMATE INFERENCE

Figure 10.6 Variational Bayesian
mixture of K = 6 Gaussians ap-
plied to the Old Faithful data set, in
which the ellipses denote the one
standard-deviation density contours
for each of the components, and the
density of red ink inside each ellipse
corresponds to the mean value of
the mixing coefﬁcient for each com-
ponent. The number in the top left
of each diagram shows the num-
ber of iterations of variational infer-
ence. Components whose expected
mixing coefﬁcient are numerically in-
distinguishable from zero are not
plotted.

0

60

15

120

the prior tightly constrains the mixing coefﬁcients so that α0 → ∞, then E[πk] →
1/K.
In Figure 10.6, the prior over the mixing coefﬁcients is a Dirichlet of the form
(10.39). Recall from Figure 2.5 that for α0 < 1 the prior favours solutions in which
some of the mixing coefﬁcients are zero. Figure 10.6 was obtained using α0 = 10−3,
and resulted in two components having nonzero mixing coefﬁcients. If instead we
choose α0 = 1 we obtain three components with nonzero mixing coefﬁcients, and
for α = 10 all six components have nonzero mixing coefﬁcients.

As we have seen there is a close similarity between the variational solution for
the Bayesian mixture of Gaussians and the EM algorithm for maximum likelihood.
In fact if we consider the limit N → ∞ then the Bayesian treatment converges to the
maximum likelihood EM algorithm. For anything other than very small data sets,
the dominant computational cost of the variational algorithm for Gaussian mixtures
arises from the evaluation of the responsibilities, together with the evaluation and
inversion of the weighted data covariance matrices. These computations mirror pre-
cisely those that arise in the maximum likelihood EM algorithm, and so there is little
computational overhead in using this Bayesian approach as compared to the tradi-
tional maximum likelihood one. There are, however, some substantial advantages.
First of all, the singularities that arise in maximum likelihood when a Gaussian com-
ponent ‘collapses’ onto a speciﬁc data point are absent in the Bayesian treatment.















k=1

K

N

K





K





Section 10.2.4

Exercise 10.16

10.2. Illustration: Variational Mixture of Gaussians

481

Indeed, these singularities are removed if we simply introduce a prior and then use a
MAP estimate instead of maximum likelihood. Furthermore, there is no over-ﬁtting
if we choose a large number K of components in the mixture, as we saw in Fig-
ure 10.6. Finally, the variational treatment opens up the possibility of determining
the optimal number of components in the mixture without resorting to techniques
such as cross validation.

10.2.2 Variational lower bound
We can also straightforwardly evaluate the lower bound (10.3) for this model.
In practice, it is useful to be able to monitor the bound during the re-estimation in
order to test for convergence. It can also provide a valuable check on both the math-
ematical expressions for the solutions and their software implementation, because at
each step of the iterative re-estimation procedure the value of this bound should not
decrease. We can take this a stage further to provide a deeper test of the correctness
of both the mathematical derivation of the update equations and of their software im-
plementation by using ﬁnite differences to check that each update does indeed give
a (constrained) maximum of the bound (Svens´en and Bishop, 2004).

For the variational mixture of Gaussians, the lower bound (10.3) is given by

L =

q(Z, π, µ, Λ) ln

p(X, Z, π, µ, Λ)

dπ dµ dΛ

Z

q(Z, π, µ, Λ)
= E[ln p(X, Z, π, µ, Λ)] − E[ln q(Z, π, µ, Λ)]
= E[ln p(X|Z, µ, Λ)] + E[ln p(Z|π)] + E[ln p(π)] + E[ln p(µ, Λ)]

−E[ln q(Z)] − E[ln q(π)] − E[ln q(µ, Λ)]

(10.70)

where, to keep the notation uncluttered, we have omitted the  superscript on the
q distributions, along with the subscripts on the expectation operators because each
expectation is taken with respect to all of the random variables in its argument. The
various terms in the bound are easily evaluated to give the following results

E[ln p(X|Z, µ, Λ)] =

Nk

ln

Λk − Dβ−1

k − νkTr(SkWk)

1
2

−νk(xk − mk)TWk(xk − mk) − D ln(2π)

E[ln p(Z|π)] =

n=1

k=1

rnk ln

πk

E[ln p(π)] = ln C(α0) + (α0 − 1)

ln

k=1

πk

(10.71)

(10.72)

(10.73)



Λk −

Dβ0
βk




k=1

K














1
2

ln

k=1

k=1

k=1

K

K






n=1

k=1

k=1

2

K

K

N

b

z













482

10. APPROXIMATE INFERENCE

E[ln p(µ, Λ)] =

D ln(β0/2π) + ln

1
2

−β0νk(mk − m0)TWk(mk − m0)
(ν0 − D − 1)

ln

K

+

Λk −

1
2

νkTr(W−1

0 Wk)

+ K ln B(W0, ν0)

(10.74)

(10.75)

(10.76)

(10.77)

E[ln q(Z)] =

rnk ln rnk

E[ln q(π)] =

(αk − 1) ln

πk + ln C(α)

E[ln q(µ, Λ)] =

Λk + D
2

ln

βk
2π

D
2 − H [q(Λk)]

−

where D is the dimensionality of x, H[q(Λk)] is the entropy of the Wishart distribu-
tion given by (B.82), and the coefﬁcients C(α) and B(W, ν) are deﬁned by (B.23)
and (B.79), respectively. Note that the terms involving expectations of the logs of the
q distributions simply represent the negative entropies of those distributions. Some
simpliﬁcations and combination of terms can be performed when these expressions
are summed to give the lower bound. However, we have kept the expressions sepa-
rate for ease of understanding.

Finally, it is worth noting that the lower bound provides an alternative approach
for deriving the variational re-estimation equations obtained in Section 10.2.1. To do
this we use the fact that, since the model has conjugate priors, the functional form of
the factors in the variational posterior distribution is known, namely discrete for Z,
Dirichlet for π, and Gaussian-Wishart for (µk, Λk). By taking general parametric
forms for these distributions we can derive the form of the lower bound as a function
of the parameters of the distributions. Maximizing the bound with respect to these
parameters then gives the required re-estimation equations.

10.2.3 Predictive density
In applications of the Bayesian mixture of Gaussians model we will often be
x of the observed variable. As-
z, and the pre-

interested in the predictive density for a new value
sociated with this observation will be a corresponding latent variable
dictive density is then given by

p(

x|X) =

z, µ, Λ)p(

p(

x|

z|π)p(π, µ, Λ|X) dπ dµ dΛ

(10.78)

Exercise 10.18


















k=1

k=1

K

K






1

K

α

k=1

10.2. Illustration: Variational Mixture of Gaussians

483

where p(π, µ, Λ|X) is the (unknown) true posterior distribution of the parameters.
Using (10.37) and (10.38) we can ﬁrst perform the summation over

z to give

p(

x|X) =

πkN

x|µk, Λ−1

k

p(π, µ, Λ|X) dπ dµ dΛ.

(10.79)

Because the remaining integrations are intractable, we approximate the predictive
density by replacing the true posterior distribution p(π, µ, Λ|X) with its variational
approximation q(π)q(µ, Λ) to give

p(

x|X) =

πkN

x|µk, Λ−1

k

q(π)q(µk, Λk) dπ dµk dΛk

(10.80)

Exercise 10.19

where we have made use of the factorization (10.55) and in each term we have im-
plicitly integrated out all variables {µj, Λj} for j = k The remaining integrations
can now be evaluated analytically giving a mixture of Student’s t-distributions

p(

x|X) =

αkSt(

x|mk, Lk, νk + 1 − D)

in which the kth component has mean mk, and the precision is given by

Lk =

(νk + 1 − D)βk

(1 + βk)

Wk

(10.81)

(10.82)

Exercise 10.20

in which νk is given by (10.63). When the size N of the data set is large the predictive
distribution (10.81) reduces to a mixture of Gaussians.

Section 10.1.4

Exercise 10.21

10.2.4 Determining the number of components
We have seen that the variational lower bound can be used to determine a pos-
terior distribution over the number K of components in the mixture model. There
is, however, one subtlety that needs to be addressed. For any given setting of the
parameters in a Gaussian mixture model (except for speciﬁc degenerate settings),
there will exist other parameter settings for which the density over the observed vari-
ables will be identical. These parameter values differ only through a re-labelling of
the components. For instance, consider a mixture of two Gaussians and a single ob-
served variable x, in which the parameters have the values π1 = a, π2 = b, µ1 = c,
µ2 = d, σ1 = e, σ2 = f. Then the parameter values π1 = b, π2 = a, µ1 = d,
µ2 = c, σ1 = f, σ2 = e, in which the two components have been exchanged, will
by symmetry give rise to the same value of p(x). If we have a mixture model com-
prising K components, then each parameter setting will be a member of a family of
K! equivalent settings.

In the context of maximum likelihood, this redundancy is irrelevant because the
parameter optimization algorithm (for example EM) will, depending on the initial-
ization of the parameters, ﬁnd one speciﬁc solution, and the other equivalent solu-
tions play no role. In a Bayesian setting, however, we marginalize over all possible

484

10. APPROXIMATE INFERENCE

Figure 10.7 Plot of the variational lower bound
L versus the number K of com-
ponents in the Gaussian mixture
model, for the Old Faithful data,
showing a distinct peak at K =
2 components. For each value
of K, the model
is trained from
100 different random starts, and
the results shown as ‘+’ symbols
plotted with small random hori-
zontal perturbations so that they
can be distinguished. Note that
some solutions ﬁnd suboptimal
local maxima, but that this hap-
pens infrequently.

p(D|K)



N

1

2

3

4

5

6

K

parameter values. We have seen in Figure 10.2 that if the true posterior distribution
is multimodal, variational inference based on the minimization of KL(qp) will tend
to approximate the distribution in the neighbourhood of one of the modes and ignore
the others. Again, because equivalent modes have equivalent predictive densities,
this is of no concern provided we are considering a model having a speciﬁc number
K of components. If, however, we wish to compare different values of K, then we
need to take account of this multimodality. A simple approximate solution is to add
a term ln K! onto the lower bound when used for model comparison and averaging.
Figure 10.7 shows a plot of the lower bound, including the multimodality fac-
tor, versus the number K of components for the Old Faithful data set. It is worth
emphasizing once again that maximum likelihood would lead to values of the likeli-
hood function that increase monotonically with K (assuming the singular solutions
have been avoided, and discounting the effects of local maxima) and so cannot be
used to determine an appropriate model complexity. By contrast, Bayesian inference
automatically makes the trade-off between model complexity and ﬁtting the data.

This approach to the determination of K requires that a range of models having
different K values be trained and compared. An alternative approach to determining
a suitable value for K is to treat the mixing coefﬁcients π as parameters and make
point estimates of their values by maximizing the lower bound (Corduneanu and
Bishop, 2001) with respect to π instead of maintaining a probability distribution
over them as in the fully Bayesian approach. This leads to the re-estimation equation

Exercise 10.22

Section 3.4

Exercise 10.23

πk =

1
N

rnk

n=1

(10.83)

and this maximization is interleaved with the variational updates for the q distribution
over the remaining parameters. Components that provide insufﬁcient contribution

Section 7.2.2

10.2. Illustration: Variational Mixture of Gaussians

485

to explaining the data will have their mixing coefﬁcients driven to zero during the
optimization, and so they are effectively removed from the model through automatic
relevance determination. This allows us to make a single training run in which we
start with a relatively large initial value of K, and allow surplus components to be
pruned out of the model. The origins of the sparsity when optimizing with respect to
hyperparameters is discussed in detail in the context of the relevance vector machine.

10.2.5 Induced factorizations
In deriving these variational update equations for the Gaussian mixture model,
we assumed a particular factorization of the variational posterior distribution given
by (10.42). However, the optimal solutions for the various factors exhibit additional
factorizations. In particular, the solution for q(µ, Λ) is given by the product of an
independent distribution q(µk, Λk) over each of the components k of the mixture,
whereas the variational posterior distribution q(Z) over the latent variables, given
by (10.48), factorizes into an independent distribution q(zn) for each observation n
(note that it does not further factorize with respect to k because, for each value of n,
the znk are constrained to sum to one over k). These additional factorizations are a
consequence of the interaction between the assumed factorization and the conditional
independence properties of the true distribution, as characterized by the directed
graph in Figure 10.5.

We shall refer to these additional factorizations as induced factorizations be-
cause they arise from an interaction between the factorization assumed in the varia-
tional posterior distribution and the conditional independence properties of the true
joint distribution. In a numerical implementation of the variational approach it is
important to take account of such additional factorizations. For instance, it would
be very inefﬁcient to maintain a full precision matrix for the Gaussian distribution
over a set of variables if the optimal form for that distribution always had a diago-
nal precision matrix (corresponding to a factorization with respect to the individual
variables described by that Gaussian).

Such induced factorizations can easily be detected using a simple graphical test
based on d-separation as follows. We partition the latent variables into three disjoint
groups A, B, C and then let us suppose that we are assuming a factorization between
C and the remaining latent variables, so that

q(A, B, C) = q(A, B)q(C).

(10.84)

Using the general result (10.9), together with the product rule for probabilities, we
see that the optimal solution for q(A, B) is given by

ln q(A, B) = EC[ln p(X, A, B, C)] + const
= EC[ln p(A, B|X, C)] + const.

(10.85)

We now ask whether this resulting solution will factorize between A and B, in
other words whether q(A, B) = q(A)q(B). This will happen if, and only if,
ln p(A, B|X, C) = ln p(A|X, C) + ln p(B|X, C), that is, if the conditional inde-
pendence relation
(10.86)

A ⊥⊥ B | X, C

486

10. APPROXIMATE INFERENCE

is satisﬁed. We can test to see if this relation does hold, for any choice of A and B
by making use of the d-separation criterion.

To illustrate this, consider again the Bayesian mixture of Gaussians represented
by the directed graph in Figure 10.5, in which we are assuming a variational fac-
torization given by (10.42). We can see immediately that the variational posterior
distribution over the parameters must factorize between π and the remaining param-
eters µ and Λ because all paths connecting π to either µ or Λ must pass through
one of the nodes zn all of which are in the conditioning set for our conditional inde-
pendence test and all of which are head-to-tail with respect to such paths.

10.3. Variational Linear Regression



N

As a second illustration of variational inference, we return to the Bayesian linear
regression model of Section 3.3. In the evidence framework, we approximated the
integration over α and β by making point estimates obtained by maximizing the log
marginal likelihood. A fully Bayesian approach would integrate over the hyperpa-
rameters as well as over the parameters. Although exact integration is intractable,
we can use variational methods to ﬁnd a tractable approximation. In order to sim-
plify the discussion, we shall suppose that the noise precision parameter β is known,
and is ﬁxed to its true value, although the framework is easily extended to include
the distribution over β. For the linear regression model, the variational treatment
will turn out to be equivalent to the evidence framework. Nevertheless, it provides a
good exercise in the use of variational methods and will also lay the foundation for
variational treatment of Bayesian logistic regression in Section 10.6.

Recall that the likelihood function for w, and the prior over w, are given by

Exercise 10.26

N (tn|wTφn, β−1)

p(t|w) =
p(w|α) = N (w|0, α−1I)

n=1

(10.87)

(10.88)

where φn = φ(xn). We now introduce a prior distribution over α. From our dis-
cussion in Section 2.3.6, we know that the conjugate prior for the precision of a
Gaussian is given by a gamma distribution, and so we choose

(10.89)
where Gam(·|·,·) is deﬁned by (B.26). Thus the joint distribution of all the variables
is given by
(10.90)

p(α) = Gam(α|a0, b0)

p(t, w, α) = p(t|w)p(w|α)p(α).

This can be represented as a directed graphical model as shown in Figure 10.8.

10.3.1 Variational distribution
Our ﬁrst goal is to ﬁnd an approximation to the posterior distribution p(w, α|t).
To do this, we employ the variational framework of Section 10.1, with a variational

10.3. Variational Linear Regression

Figure 10.8 Probabilistic graphical model representing the joint dis-
regression

the Bayesian linear

for

tribution (10.90)
model.

φn

β

tn

N





n=1

N



487

α

w

(10.91)

(10.92)

(10.93)

(10.94)

(10.95)

posterior distribution given by the factorized expression

q(w, α) = q(w)q(α).

We can ﬁnd re-estimation equations for the factors in this distribution by making use
of the general result (10.9). Recall that for each factor, we take the log of the joint
distribution over all variables and then average with respect to those variables not in
that factor. Consider ﬁrst the distribution over α. Keeping only terms that have a
functional dependence on α, we have

ln q(α) = ln p(α) + Ew [ln p(w|α)] + const
= (a0 − 1) ln α − b0α + M
2

ln α −

α
2 E[wTw] + const.

We recognize this as the log of a gamma distribution, and so identifying the coefﬁ-
cients of α and ln α we obtain

where

q(α) = Gam(α|aN , bN )

aN = a0 + M
2
1
2E[wTw].

bN = b0 +

Similarly, we can ﬁnd the variational re-estimation equation for the posterior
distribution over w. Again, using the general result (10.9), and keeping only those
terms that have a functional dependence on w, we have
ln q(w) = ln p(t|w) + Eα [ln p(w|α)] + const

(10.96)

{wTφn − tn}2 −

1
2E[α]wTw + const

(10.97)

wT

E[α]I + βΦTΦ

w + βwTΦTt + const.

(10.98)

Because this is a quadratic form, the distribution q(w) is Gaussian, and so we can
complete the square in the usual way to identify the mean and covariance, giving

q(w) = N (w|mN , SN )

(10.99)

= −

= −

β
2
1
2



−1

.

(10.100)
(10.101)








488

10. APPROXIMATE INFERENCE

where

mN = βSN ΦTt
SN =

E[α]I + βΦTΦ

Note the close similarity to the posterior distribution (3.52) obtained when α was
treated as a ﬁxed parameter. The difference is that here α is replaced by its expecta-
tion E[α] under the variational distribution. Indeed, we have chosen to use the same
notation for the covariance matrix SN in both cases.

Using the standard results (B.27), (B.38), and (B.39), we can obtain the required

moments as follows

E[α] = aN /bN
E[wwT] = mN mT

N + SN .

(10.102)
(10.103)

The evaluation of the variational posterior distribution begins by initializing the pa-
rameters of one of the distributions q(w) or q(α), and then alternately re-estimates
these factors in turn until a suitable convergence criterion is satisﬁed (usually speci-
ﬁed in terms of the lower bound to be discussed shortly).

It is instructive to relate the variational solution to that found using the evidence
framework in Section 3.5. To do this consider the case a0 = b0 = 0, corresponding
to the limit of an inﬁnitely broad prior over α. The mean of the variational posterior
distribution q(α) is then given by

E[α] = aN
bN

= M/2

E[wTw]/2

=

M

N mN + Tr(SN ) .
mT

(10.104)

Comparison with (9.63) shows that in the case of this particularly simple model,
the variational approach gives precisely the same expression as that obtained by
maximizing the evidence function using EM except that the point estimate for α
is replaced by its expected value. Because the distribution q(w) depends on q(α)
only through the expectation E[α], we see that the two approaches will give identical
results for the case of an inﬁnitely broad prior.

10.3.2 Predictive distribution
The predictive distribution over t, given a new input x, is easily evaluated for

this model using the Gaussian variational posterior for the parameters

p(t|x, t) =

=
= N (t|mT

p(t|x, w)p(w|t) dw
p(t|x, w)q(w) dw
N (t|wTφ(x), β−1)N (w|mN , SN ) dw

N φ(x), σ2(x))

(10.105)







1
β

	

	

10.3. Variational Linear Regression

489

where we have evaluated the integral by making use of the result (2.115) for the
linear-Gaussian model. Here the input-dependent variance is given by

σ2(x) =

+ φ(x)TSN φ(x).

(10.106)

Note that this takes the same form as the result (3.59) obtained with ﬁxed α except
that now the expected value E[α] appears in the deﬁnition of SN .

10.3.3 Lower bound
Another quantity of importance is the lower bound L deﬁned by

L(q) = E[ln p(w, α, t)] − E[ln q(w, α)]
−Eα[ln q(w)]w − E[ln q(α)].

= Ew[ln p(t|w)] + Ew,α[ln p(w|α)] + Eα[ln p(α)]

(10.107)

Exercise 10.27

Evaluation of the various terms is straightforward, making use of results obtained in
previous chapters, and gives

E[ln p(t|w)]w = N
2
−
E[ln p(w|α)]w,α = −
−

ln

β
2
M
2
aN
2bN

β
2π

β
2

−

tTt + βmT

N ΦTt

ΦTΦ(mN mT

N + SN )

Tr
ln(2π) + M
2

(ψ(aN ) − ln bN )

mT

N mN + Tr(SN )

E[ln p(α)]α = a0 ln b0 + (a0 − 1) [ψ(aN ) − ln bN ]

(10.108)

(10.109)

(10.110)

aN
bN − ln Γ(aN )
−b0
1
ln|SN| + M
2
2

[1 + ln(2π)]

−E[ln q(w)]w =
(10.111)
−E[ln q(α)]α = ln Γ(aN ) − (aN − 1)ψ(aN ) − ln bN + aN . (10.112)
Figure 10.9 shows a plot of the lower bound L(q) versus the degree of a polynomial
model for a synthetic data set generated from a degree three polynomial. Here the
prior parameters have been set to a0 = b0 = 0, corresponding to the noninformative
prior p(α) ∝ 1/α, which is uniform over ln α as discussed in Section 2.3.6. As
we saw in Section 10.1, the quantity L represents lower bound on the log marginal
likelihood p(t|M) for the model. If we assign equal prior probabilities p(M) to the
different values of M, then we can interpret L as an approximation to the poste-
rior model probability p(M|t). Thus the variational framework assigns the highest
probability to the model with M = 3. This should be contrasted with the maximum
likelihood result, which assigns ever smaller residual error to models of increasing
complexity until the residual error is driven to zero, causing maximum likelihood to
favour severely over-ﬁtted models.



N








.

490

10. APPROXIMATE INFERENCE

Figure 10.9 Plot of

the lower bound L ver-
sus the order M of the polyno-
mial, for a polynomial model, in
which a set of 10 data points is
generated from a polynomial with
M = 3 sampled over the inter-
val (−5, 5) with additive Gaussian
noise of variance 0.09. The value
of the bound gives the log prob-
ability of the model, and we see
that the value of the bound peaks
at M = 3, corresponding to the
true model from which the data
set was generated.

1

3

5

7

9

10.4. Exponential Family Distributions

In Chapter 2, we discussed the important role played by the exponential family of
distributions and their conjugate priors. For many of the models discussed in this
book, the complete-data likelihood is drawn from the exponential family. However,
in general this will not be the case for the marginal likelihood function for the ob-
served data. For example, in a mixture of Gaussians, the joint distribution of obser-
vations xn and corresponding hidden variables zn is a member of the exponential
family, whereas the marginal distribution of xn is a mixture of Gaussians and hence
is not.

Up to now we have grouped the variables in the model into observed variables
and hidden variables. We now make a further distinction between latent variables,
denoted Z, and parameters, denoted θ, where parameters are intensive (ﬁxed in num-
ber independent of the size of the data set), whereas latent variables are extensive
(scale in number with the size of the data set). For example, in a Gaussian mixture
model, the indicator variables zkn (which specify which component k is responsible
for generating data point xn) represent the latent variables, whereas the means µk,
precisions Λk and mixing proportions πk represent the parameters.

Consider the case of independent identically distributed data. We denote the
data values by X = {xn}, where n = 1, . . . N, with corresponding latent variables
Z = {zn}. Now suppose that the joint distribution of observed and latent variables
is a member of the exponential family, parameterized by natural parameters η so that

p(X, Z|η) =

n=1

h(xn, zn)g(η) exp

ηTu(xn, zn)

(10.113)

We shall also use a conjugate prior for η, which can be written as

p(η|ν0, v0) = f(ν0, χ0)g(η)ν0 exp

νoηTχ0

.

(10.114)

Recall that the conjugate prior distribution can be interpreted as a prior number ν0
of observations all having the value χ0 for the u vector. Now consider a variational












n=1

N

n=1









10.4. Exponential Family Distributions

491

distribution that factorizes between the latent variables and the parameters, so that
q(Z, η) = q(Z)q(η). Using the general result (10.9), we can solve for the two
factors as follows

ln q(Z) = Eη[ln p(X, Z|η)] + const

=

ln h(xn, zn) + E[ηT]u(xn, zn)

+ const. (10.115)

Section 10.2.5

Thus we see that this decomposes into a sum of independent terms, one for each
value of n, and hence the solution for q(Z) will factorize over n so that q(Z) =
n q(zn). This is an example of an induced factorization. Taking the exponential

of both sides, we have

q(zn) = h(xn, zn)g (E[η]) exp

E[ηT]u(xn, zn)

(10.116)

where the normalization coefﬁcient has been re-instated by comparison with the
standard form for the exponential family.

Similarly, for the variational distribution over the parameters, we have

ln q(η) = ln p(η|ν0, χ0) + EZ[ln p(X, Z|η)] + const
ln g(η) + ηTEzn[u(xn, zn)]

= ν0 ln g(η) + ηTχ0 +

N

(10.117)

+ const. (10.118)

Again, taking the exponential of both sides, and re-instating the normalization coef-
ﬁcient by inspection, we have

q(η) = f(νN , χN )g(η)νN exp

ηTχN

(10.119)

where we have deﬁned

νN = ν0 + N
N

χN = χ0 +

Ezn[u(xn, zn)].

n=1

(10.120)

(10.121)

Note that the solutions for q(zn) and q(η) are coupled, and so we solve them iter-
atively in a two-stage procedure. In the variational E step, we evaluate the expected
sufﬁcient statistics E[u(xn, zn)] using the current posterior distribution q(zn) over
the latent variables and use this to compute a revised posterior distribution q(η) over
the parameters. Then in the subsequent variational M step, we use this revised pa-
rameter posterior distribution to ﬁnd the expected natural parameters E[ηT], which
gives rise to a revised variational distribution over the latent variables.

10.4.1 Variational message passing
We have illustrated the application of variational methods by considering a spe-
ciﬁc model, the Bayesian mixture of Gaussians, in some detail. This model can be





i

i

 

492

10. APPROXIMATE INFERENCE

described by the directed graph shown in Figure 10.5. Here we consider more gen-
erally the use of variational methods for models described by directed graphs and
derive a number of widely applicable results.

The joint distribution corresponding to a directed graph can be written using the

decomposition

p(x) =

p(xi|pai)

(10.122)

where xi denotes the variable(s) associated with node i, and pai denotes the parent
set corresponding to node i. Note that xi may be a latent variable or it may belong
to the set of observed variables. Now consider a variational approximation in which
the distribution q(x) is assumed to factorize with respect to the xi so that

q(x) =

qi(xi).

(10.123)

Note that for observed nodes, there is no factor q(xi) in the variational distribution.
We now substitute (10.122) into our general result (10.9) to give

ln q

j (xj) = Ei=j

ln p(xi|pai)

+ const.

i

(10.124)

Any terms on the right-hand side that do not depend on xj can be absorbed into
In fact, the only terms that do depend on xj are the con-
the additive constant.
ditional distribution for xj given by p(xj|paj) together with any other conditional
distributions that have xj in the conditioning set. By deﬁnition, these conditional
distributions correspond to the children of node j, and they therefore also depend on
the co-parents of the child nodes, i.e., the other parents of the child nodes besides
node xj itself. We see that the set of all nodes on which q(xj) depends corresponds
to the Markov blanket of node xj, as illustrated in Figure 8.26. Thus the update
of the factors in the variational posterior distribution represents a local calculation
on the graph. This makes possible the construction of general purpose software for
variational inference in which the form of the model does not need to be speciﬁed in
advance (Bishop et al., 2003).

If we now specialize to the case of a model in which all of the conditional dis-
tributions have a conjugate-exponential structure, then the variational update proce-
dure can be cast in terms of a local message passing algorithm (Winn and Bishop,
2005). In particular, the distribution associated with a particular node can be updated
once that node has received messages from all of its parents and all of its children.
This in turn requires that the children have already received messages from their co-
parents. The evaluation of the lower bound can also be simpliﬁed because many of
the required quantities are already evaluated as part of the message passing scheme.
This distributed message passing formulation has good scaling properties and is well
suited to large networks.

10.5. Local Variational Methods

493

10.5. Local Variational Methods

Section 1.6.1

The variational framework discussed in Sections 10.1 and 10.2 can be considered a
‘global’ method in the sense that it directly seeks an approximation to the full poste-
rior distribution over all random variables. An alternative ‘local’ approach involves
ﬁnding bounds on functions over individual variables or groups of variables within
a model. For instance, we might seek a bound on a conditional distribution p(y|x),
which is itself just one factor in a much larger probabilistic model speciﬁed by a
directed graph. The purpose of introducing the bound of course is to simplify the
resulting distribution. This local approximation can be applied to multiple variables
in turn until a tractable approximation is obtained, and in Section 10.6.1 we shall
give a practical example of this approach in the context of logistic regression. Here
we focus on developing the bounds themselves.

We have already seen in our discussion of the Kullback-Leibler divergence that
the convexity of the logarithm function played a key role in developing the lower
bound in the global variational approach. We have deﬁned a (strictly) convex func-
tion as one for which every chord lies above the function. Convexity also plays a
central role in the local variational framework. Note that our discussion will ap-
ply equally to concave functions with ‘min’ and ‘max’ interchanged and with lower
bounds replaced by upper bounds.

Let us begin by considering a simple example, namely the function f(x) =
exp(−x), which is a convex function of x, and which is shown in the left-hand plot
of Figure 10.10. Our goal is to approximate f(x) by a simpler function, in particular
a linear function of x. From Figure 10.10, we see that this linear function will be a
lower bound on f(x) if it corresponds to a tangent. We can obtain the tangent line
y(x) at a speciﬁc value of x, say x = ξ, by making a ﬁrst order Taylor expansion

so that y(x) � f(x) with equality when x = ξ. For our example function f(x) =

y(x) = f(ξ) + f(ξ)(x − ξ)

(10.125)

Figure 10.10 In the left-hand ﬁg-
ure the red curve shows the function
exp(−x), and the blue line shows
the tangent at x = ξ deﬁned by
(10.125) with ξ = 1. This line has
slope λ = f(ξ) = − exp(−ξ). Note
that any other tangent line, for ex-
ample the ones shown in green, will
have a smaller value of y at x =
ξ. The right-hand ﬁgure shows the
corresponding plot of
the function
λξ − g(λ), where g(λ) is given by
(10.131), versus λ for ξ = 1,
in
which the maximum corresponds to
λ = − exp(−ξ) = −1/e.

1

0.5

λξ − g(λ)

0.4

0.2

0

0

ξ

1.5

x

3

0
−1

−0.5

λ

0

494

10. APPROXIMATE INFERENCE

y

f(x)

λx

y

−g(λ)

f(x)

x

x

λx − g(λ)

Figure 10.11 In the left-hand plot the red curve shows a convex function f (x), and the blue line represents the
linear function λx, which is a lower bound on f (x) because f (x) > λx for all x. For the given value of slope λ the
contact point of the tangent line having the same slope is found by minimizing with respect to x the discrepancy
(shown by the green dashed lines) given by f (x) − λx. This deﬁnes the dual function g(λ), which corresponds
to the (negative of the) intercept of the tangent line having slope λ.

exp(−x), we therefore obtain the tangent line in the form

y(x) = exp(−ξ) − exp(−ξ)(x − ξ)

which is a linear function parameterized by ξ. For consistency with subsequent
discussion, let us deﬁne λ = − exp(−ξ) so that

(10.126)

(10.127)

y(x, λ) = λx − λ + λ ln(−λ).

Different values of λ correspond to different tangent lines, and because all such lines
are lower bounds on the function, we have f(x) � y(x, λ). Thus we can write the
function in the form

f(x) = max

λ {λx − λ + λ ln(−λ)} .

(10.128)

We have succeeded in approximating the convex function f(x) by a simpler, lin-
ear function y(x, λ). The price we have paid is that we have introduced a variational
parameter λ, and to obtain the tightest bound we must optimize with respect to λ.

We can formulate this approach more generally using the framework of convex
duality (Rockafellar, 1972; Jordan et al., 1999). Consider the illustration of a convex
function f(x) shown in the left-hand plot in Figure 10.11.
In this example, the
function λx is a lower bound on f(x) but it is not the best lower bound that can
be achieved by a linear function having slope λ, because the tightest bound is given
by the tangent line. Let us write the equation of the tangent line, having slope λ as
λx − g(λ) where the (negative) intercept g(λ) clearly depends on the slope λ of the
tangent. To determine the intercept, we note that the line must be moved vertically by
an amount equal to the smallest vertical distance between the line and the function,
as shown in Figure 10.11. Thus

g(λ) = − min
= max

x {f(x) − λx}
x {λx − f(x)} .

(10.129)

10.5. Local Variational Methods

495

Now, instead of ﬁxing λ and varying x, we can consider a particular x and then
adjust λ until the tangent plane is tangent at that particular x. Because the y value
of the tangent line at a particular x is maximized when that value coincides with its
contact point, we have

f(x) = max

λ {λx − g(λ)} .

(10.130)

We see that the functions f(x) and g(λ) play a dual role, and are related through
(10.129) and (10.130).

Let us apply these duality relations to our simple example f(x) = exp(−x).
From (10.129) we see that the maximizing value of x is given by ξ = − ln(−λ), and
back-substituting we obtain the conjugate function g(λ) in the form

g(λ) = λ − λ ln(−λ)

(10.131)
as obtained previously. The function λξ − g(λ) is shown, for ξ = 1 in the right-hand
plot in Figure 10.10. As a check, we can substitute (10.131) into (10.130), which
gives the maximizing value of λ = − exp(−x), and back-substituting then recovers
the original function f(x) = exp(−x).
For concave functions, we can follow a similar argument to obtain upper bounds,
in which max’ is replaced with ‘min’, so that

f(x) = min

g(λ) = min

λ {λx − g(λ)}
x {λx − f(x)} .

(10.132)

(10.133)

If the function of interest is not convex (or concave), then we cannot directly
apply the method above to obtain a bound. However, we can ﬁrst seek invertible
transformations either of the function or of its argument which change it into a con-
vex form. We then calculate the conjugate function and then transform back to the
original variables.

An important example, which arises frequently in pattern recognition, is the

logistic sigmoid function deﬁned by

σ(x) =

1

1 + e−x .

(10.134)

Exercise 10.30

As it stands this function is neither convex nor concave. However, if we take the
logarithm we obtain a function which is concave, as is easily veriﬁed by ﬁnding the
second derivative. From (10.133) the corresponding conjugate function then takes
the form

g(λ) = min

x {λx − f(x)} = −λ ln λ − (1 − λ) ln(1 − λ)

(10.135)

Appendix B

which we recognize as the binary entropy function for a variable whose probability
of having the value 1 is λ. Using (10.132), we then obtain an upper bound on the log
sigmoid

ln σ(x) � λx − g(λ)

(10.136)










.

0











496

10. APPROXIMATE INFERENCE

1

0.5

0
−6

λ = 0.2

λ = 0.7

0

6

1

0.5

ξ = 2.5

0
−6

−ξ

ξ

6

Figure 10.12 The left-hand plot shows the logistic sigmoid function σ(x) deﬁned by (10.134) in red, together
with two examples of the exponential upper bound (10.137) shown in blue. The right-hand plot shows the logistic
sigmoid again in red together with the Gaussian lower bound (10.144) shown in blue. Here the parameter
ξ = 2.5, and the bound is exact at x = ξ and x = −ξ, denoted by the dashed green lines.

Exercise 10.31

and taking the exponential, we obtain an upper bound on the logistic sigmoid itself
of the form

σ(x) � exp(λx − g(λ))

(10.137)

which is plotted for two values of λ on the left-hand plot in Figure 10.12.

We can also obtain a lower bound on the sigmoid having the functional form of
a Gaussian. To do this, we follow Jaakkola and Jordan (2000) and make transforma-
tions both of the input variable and of the function itself. First we take the log of the
logistic function and then decompose it so that
ln σ(x) = − ln(1 + e−x) = − ln

e−x/2(ex/2 + e−x/2)

= x/2 − ln(ex/2 + e−x/2).

(10.138)
We now note that the function f(x) = − ln(ex/2 + e−x/2) is a convex function of
the variable x2, as can again be veriﬁed by ﬁnding the second derivative. This leads
to a lower bound on f(x), which is a linear function of x2 whose conjugate function
is given by

g(λ) = max
x2

λx2 − f

√x2

The stationarity condition leads to

0 = λ −

dx
dx2

d
dx

f(x) = λ +

tanh

1
4x

x
2

.

(10.139)

(10.140)

If we denote this value of x, corresponding to the contact point of the tangent line
for this particular value of λ, by ξ, then we have

λ(ξ) = −

1
4ξ

tanh

ξ
2

1
2ξ

= −

σ(ξ) −

1
2

.

(10.141)



(10.142)

(10.143)

(10.144)






Section 4.5

Section 4.3

10.5. Local Variational Methods

497

Instead of thinking of λ as the variational parameter, we can let ξ play this role as
this leads to simpler expressions for the conjugate function, which is then given by

g(λ) = λ(ξ)ξ2 − f(ξ) = λ(ξ)ξ2 + ln(eξ/2 + e−ξ/2).

Hence the bound on f(x) can be written as

f(x) � λx2 − g(λ) = λx2 − λξ2 − ln(eξ/2 + e−ξ/2).

The bound on the sigmoid then becomes

σ(x) � σ(ξ) exp

(x − ξ)/2 − λ(ξ)(x2 − ξ2)

where λ(ξ) is deﬁned by (10.141). This bound is illustrated in the right-hand plot of
Figure 10.12. We see that the bound has the form of the exponential of a quadratic
function of x, which will prove useful when we seek Gaussian representations of
posterior distributions deﬁned through logistic sigmoid functions.

The logistic sigmoid arises frequently in probabilistic models over binary vari-
ables because it is the function that transforms a log odds ratio into a posterior prob-
ability. The corresponding transformation for a multiclass distribution is given by
the softmax function. Unfortunately, the lower bound derived here for the logistic
sigmoid does not directly extend to the softmax. Gibbs (1997) proposes a method
for constructing a Gaussian distribution that is conjectured to be a bound (although
no rigorous proof is given), which may be used to apply local variational methods to
multiclass problems.

We shall see an example of the use of local variational bounds in Sections 10.6.1.
For the moment, however, it is instructive to consider in general terms how these
bounds can be used. Suppose we wish to evaluate an integral of the form

I =

σ(a)p(a) da

(10.145)

where σ(a) is the logistic sigmoid, and p(a) is a Gaussian probability density. Such
integrals arise in Bayesian models when, for instance, we wish to evaluate the pre-
dictive distribution, in which case p(a) represents a posterior parameter distribution.
Because the integral is intractable, we employ the variational bound (10.144), which
we write in the form σ(a) � f(a, ξ) where ξ is a variational parameter. The inte-
gral now becomes the product of two exponential-quadratic functions and so can be
integrated analytically to give a bound on I

I �

f(a, ξ)p(a) da = F (ξ).

(10.146)

We now have the freedom to choose the variational parameter ξ, which we do by
ﬁnding the value ξ that maximizes the function F (ξ). The resulting value F (ξ)
represents the tightest bound within this family of bounds and can be used as an
approximation to I. This optimized bound, however, will in general not be exact.








n=1

N



 



498

10. APPROXIMATE INFERENCE

Although the bound σ(a) � f(a, ξ) on the logistic sigmoid can be optimized exactly,
the required choice for ξ depends on the value of a, so that the bound is exact for one
value of a only. Because the quantity F (ξ) is obtained by integrating over all values
of a, the value of ξ represents a compromise, weighted by the distribution p(a).

10.6. Variational Logistic Regression

We now illustrate the use of local variational methods by returning to the Bayesian
logistic regression model studied in Section 4.5. There we focussed on the use of
the Laplace approximation, while here we consider a variational treatment based on
the approach of Jaakkola and Jordan (2000). Like the Laplace method, this also
leads to a Gaussian approximation to the posterior distribution. However, the greater
ﬂexibility of the variational approximation leads to improved accuracy compared
to the Laplace method. Furthermore (unlike the Laplace method), the variational
approach is optimizing a well deﬁned objective function given by a rigourous bound
on the model evidence. Logistic regression has also been treated by Dybowski and
Roberts (2005) from a Bayesian perspective using Monte Carlo sampling techniques.

10.6.1 Variational posterior distribution
Here we shall make use of a variational approximation based on the local bounds
introduced in Section 10.5. This allows the likelihood function for logistic regres-
sion, which is governed by the logistic sigmoid, to be approximated by the expo-
nential of a quadratic form. It is therefore again convenient to choose a conjugate
Gaussian prior of the form (4.140). For the moment, we shall treat the hyperparam-
eters m0 and S0 as ﬁxed constants. In Section 10.6.3, we shall demonstrate how the
variational formalism can be extended to the case where there are unknown hyper-
parameters whose values are to be inferred from the data.

In the variational framework, we seek to maximize a lower bound on the marginal
likelihood. For the Bayesian logistic regression model, the marginal likelihood takes
the form

p(t) =

p(t|w)p(w) dw =

p(tn|w)

p(w) dw.

We ﬁrst note that the conditional distribution for t can be written as

p(t|w) = σ(a)t {1 − σ(a)}1−t
1 −

=

1

t

1 + e−a

1 + e−a
e−a
1 + e−a = eatσ(−a)

= eat

1

1−t

where a = wTφ. In order to obtain a lower bound on p(t), we make use of the
variational lower bound on the logistic sigmoid function given by (10.144), which

(10.147)

(10.148)










N







N

n=1










N



10.6. Variational Logistic Regression

we reproduce here for convenience

σ(z) � σ(ξ) exp

(z − ξ)/2 − λ(ξ)(z2 − ξ2)

where

We can therefore write

λ(ξ) =

1
2ξ

σ(ξ) −

1
2

.

499

(10.149)

(10.150)

.

(10.151)

p(t|w) = eatσ(−a) � eatσ(ξ) exp

−(a + ξ)/2 − λ(ξ)(a2 − ξ2)

Note that because this bound is applied to each of the terms in the likelihood function
separately, there is a variational parameter ξn corresponding to each training set
observation (φn, tn). Using a = wTφ, and multiplying by the prior distribution, we
obtain the following bound on the joint distribution of t and w
p(t, w) = p(t|w)p(w) � h(w, ξ)p(w)
where ξ denotes the set {ξn} of variational parameters, and

(10.152)

h(w, ξ) =

σ(ξn) exp

n=1

wTφntn − (wTφn + ξn)/2

− λ(ξn)([wTφn]2 − ξ2
n)

.

(10.153)

Evaluation of the exact posterior distribution would require normalization of the left-
hand side of this inequality. Because this is intractable, we work instead with the
right-hand side. Note that the function on the right-hand side cannot be interpreted
as a probability density because it is not normalized. Once it is normalized to give a
variational posterior distribution q(w), however, it no longer represents a bound.

Because the logarithm function is monotonically increasing, the inequality A �
B implies ln A � ln B. This gives a lower bound on the log of the joint distribution
of t and w of the form

ln{p(t|w)p(w)} � ln p(w) +

ln σ(ξn) + wTφntn

− (wTφn + ξn)/2 − λ(ξn)([wTφn]2 − ξ2
n)

.

(10.154)

Substituting for the prior p(w), the right-hand side of this inequality becomes, as a
function of w

1
2

(w − m0)TS−1

0 (w − m0)

−

+

n=1

wTφn(tn − 1/2) − λ(ξn)wT(φnφT

n)w

+ const.

(10.155)







N



N

n=1





500

10. APPROXIMATE INFERENCE

Exercise 10.32

(10.156)

(10.157)

(10.158)

This is a quadratic function of w, and so we can obtain the corresponding variational
approximation to the posterior distribution by identifying the linear and quadratic
terms in w, giving a Gaussian variational posterior of the form

where

q(w) = N (w|mN , SN )

mN = SN

S−1
0 m0 +

(tn − 1/2)φn

n=1

S−1
N = S−1

0 + 2

λ(ξn)φnφT
n.

As with the Laplace framework, we have again obtained a Gaussian approximation
to the posterior distribution. However, the additional ﬂexibility provided by the vari-
ational parameters {ξn} leads to improved accuracy in the approximation (Jaakkola
and Jordan, 2000).
Here we have considered a batch learning context in which all of the training
data is available at once. However, Bayesian methods are intrinsically well suited
to sequential learning in which the data points are processed one at a time and then
discarded. The formulation of this variational approach for the sequential case is
straightforward.

Note that the bound given by (10.149) applies only to the two-class problem and
so this approach does not directly generalize to classiﬁcation problems with K > 2
classes. An alternative bound for the multiclass case has been explored by Gibbs
(1997).

10.6.2 Optimizing the variational parameters
We now have a normalized Gaussian approximation to the posterior distribution,
which we shall use shortly to evaluate the predictive distribution for new data points.
First, however, we need to determine the variational parameters {ξn} by maximizing
the lower bound on the marginal likelihood.
To do this, we substitute the inequality (10.152) back into the marginal likeli-

hood to give

ln p(t) = ln

p(t|w)p(w) dw � ln

h(w, ξ)p(w) dw = L(ξ).

(10.159)

As with the optimization of the hyperparameter α in the linear regression model of
Section 3.5, there are two approaches to determining the ξn. In the ﬁrst approach, we
recognize that the function L(ξ) is deﬁned by an integration over w and so we can
view w as a latent variable and invoke the EM algorithm. In the second approach,
we integrate over w analytically and then perform a direct maximization over ξ. Let
us begin by considering the EM approach.

The EM algorithm starts by choosing some initial values for the parameters
In the E step of the EM algorithm,

{ξn}, which we denote collectively by ξold.












N

n=1

10.6. Variational Logistic Regression

501

we then use these parameter values to ﬁnd the posterior distribution over w, which
is given by (10.156). In the M step, we then maximize the expected complete-data
log likelihood which is given by

Q(ξ, ξold) = E [ln h(w, ξ)p(w)]

(10.160)

where the expectation is taken with respect to the posterior distribution q(w) evalu-
ated using ξold. Noting that p(w) does not depend on ξ, and substituting for h(w, ξ)
we obtain

Q(ξ, ξold) =

ln σ(ξn) − ξn/2 − λ(ξn)(φT

nE[wwT]φn − ξ2
n)

+ const

(10.161)
where ‘const’ denotes terms that are independent of ξ. We now set the derivative with
respect to ξn equal to zero. A few lines of algebra, making use of the deﬁnitions of
σ(ξ) and λ(ξ), then gives

0 = λ(ξn)(φT

n).
nE[wwT]φn − ξ2

(10.162)

We now note that λ(ξ) is a monotonic function of ξ for ξ � 0, and that we can
restrict attention to nonnegative values of ξ without loss of generality due to the
symmetry of the bound around ξ = 0. Thus λ(ξ) = 0, and hence we obtain the
following re-estimation equations

(ξnew

n )2 = φT

nE[wwT]φn = φT
n

SN + mN mT
N

φn

(10.163)

where we have used (10.156).

Let us summarize the EM algorithm for ﬁnding the variational posterior distri-
bution. We ﬁrst initialize the variational parameters ξold. In the E step, we evaluate
the posterior distribution over w given by (10.156), in which the mean and covari-
ance are deﬁned by (10.157) and (10.158). In the M step, we then use this variational
posterior to compute a new value for ξ given by (10.163). The E and M steps are
repeated until a suitable convergence criterion is satisﬁed, which in practice typically
requires only a few iterations.

An alternative approach to obtaining re-estimation equations for ξ is to note
that in the integral over w in the deﬁnition (10.159) of the lower bound L(ξ), the
integrand has a Gaussian-like form and so the integral can be evaluated analytically.
Having evaluated the integral, we can then differentiate with respect to ξn. It turns
out that this gives rise to exactly the same re-estimation equations as does the EM
approach given by (10.163).

As we have emphasized already, in the application of variational methods it is
useful to be able to evaluate the lower bound L(ξ) given by (10.159). The integration
over w can be performed analytically by noting that p(w) is Gaussian and h(w, ξ)
is the exponential of a quadratic function of w. Thus, by completing the square
and making use of the standard result for the normalization coefﬁcient of a Gaussian
distribution, we can obtain a closed form solution which takes the form

Exercise 10.33

Exercise 10.34

Exercise 10.35





N

6

4

2

0

−2

−4

−6

−4



10. APPROXIMATE INFERENCE

502

6

4

2

0

−2

−4

−6

−4

5
2
.
0

.

50
7
9
.
9
0

1
0.0

−2

0

2

4

−2

0

2

4

Figure 10.13 Illustration of the Bayesian approach to logistic regression for a simple linearly separable data
set. The plot on the left shows the predictive distribution obtained using variational inference. We see that
the decision boundary lies roughly mid way between the clusters of data points, and that the contours of the
predictive distribution splay out away from the data reﬂecting the greater uncertainty in the classiﬁcation of such
regions. The plot on the right shows the decision boundaries corresponding to ﬁve samples of the parameter
vector w drawn from the posterior distribution p(w|t).

L(ξ) =

1
2

+

ln |SN|
|S0| −

1
2

mT

N S−1

N mN +

1
2

mT

0 S−1

0 m0

ln σ(ξn) −

n=1

1
2 ξn − λ(ξn)ξ2

n

.

(10.164)

This variational framework can also be applied to situations in which the data
In this case we maintain a
is arriving sequentially (Jaakkola and Jordan, 2000).
Gaussian posterior distribution over w, which is initialized using the prior p(w). As
each data point arrives, the posterior is updated by making use of the bound (10.151)
and then normalized to give an updated posterior distribution.

The predictive distribution is obtained by marginalizing over the posterior dis-
tribution, and takes the same form as for the Laplace approximation discussed in
Section 4.5.2. Figure 10.13 shows the variational predictive distributions for a syn-
thetic data set. This example provides interesting insights into the concept of ‘large
margin’, which was discussed in Section 7.1 and which has qualitatively similar be-
haviour to the Bayesian solution.

10.6.3 Inference of hyperparameters
So far, we have treated the hyperparameter α in the prior distribution as a known
constant. We now extend the Bayesian logistic regression model to allow the value of
this parameter to be inferred from the data set. This can be achieved by combining
the global and local variational approximations into a single framework, so as to
maintain a lower bound on the marginal likelihood at each stage. Such a combined
approach was adopted by Bishop and Svens´en (2003) in the context of a Bayesian
treatment of the hierarchical mixture of experts model.




















(10.166)

(10.167)

(10.168)

10.6. Variational Logistic Regression

503

Speciﬁcally, we consider once again a simple isotropic Gaussian prior distribu-

tion of the form

p(w|α) = N (w|0, α−1I).

(10.165)
Our analysis is readily extended to more general Gaussian priors, for instance if we
wish to associate a different hyperparameter with different subsets of the parame-
ters wj. As usual, we consider a conjugate hyperprior over α given by a gamma
distribution

governed by the constants a0 and b0.

The marginal likelihood for this model now takes the form

p(α) = Gam(α|a0, b0)

p(t) =

p(w, α, t) dw dα

where the joint distribution is given by

p(w, α, t) = p(t|w)p(w|α)p(α).

We are now faced with an analytically intractable integration over w and α, which
we shall tackle by using both the local and global variational approaches in the same
model

To begin with, we introduce a variational distribution q(w, α), and then apply

the decomposition (10.2), which in this instance takes the form

(10.169)
where the lower bound L(q) and the Kullback-Leibler divergence KL(qp) are de-
ﬁned by

ln p(t) = L(q) + KL(qp)

L(q) =

q(w, α) ln

dw dα

KL(qp) = −

q(w, α) ln

dw dα.

p(w, α, t)
q(w, α)
p(w, α|t))
q(w, α)

(10.170)

(10.171)

At this point, the lower bound L(q) is still intractable due to the form of the
likelihood factor p(t|w). We therefore apply the local variational bound to each of
the logistic sigmoid factors as before. This allows us to use the inequality (10.152)
and place a lower bound on L(q), which will therefore also be a lower bound on the
log marginal likelihood

ln p(t) � L(q) �

L(q, ξ)
q(w, α) ln

=

h(w, ξ)p(w|α)p(α)

q(w, α)

dw dα.

(10.172)

Next we assume that the variational distribution factorizes between parameters and
hyperparameters so that

q(w, α) = q(w)q(α).

(10.173)







N

n=1




n=1

N

	




n=1

N

	

504

10. APPROXIMATE INFERENCE

With this factorization we can appeal to the general result (10.9) to ﬁnd expressions
for the optimal factors. Consider ﬁrst the distribution q(w). Discarding terms that
are independent of w, we have

ln q(w) = Eα [ln{h(w, ξ)p(w|α)p(α)}] + const
= ln h(w, ξ) + Eα [ln p(w|α)] + const.

We now substitute for ln h(w, ξ) using (10.153), and for ln p(w|α) using (10.165),
giving

ln q(w) = −

E[α]
2

wTw +

(tn − 1/2)wTφn − λ(ξn)wTφnφT
nw

+ const.

We see that this is a quadratic function of w and so the solution for q(w) will be
Gaussian. Completing the square in the usual way, we obtain

q(w) = N (w|µN , ΣN )

where we have deﬁned

Σ−1

N µN =

(tn − 1/2)φn

Σ−1

N = E[α]I + 2

λ(ξn)φnφT
n.

Similarly, the optimal solution for the factor q(α) is obtained from

ln q(α) = Ew [ln p(w|α)] + ln p(α) + const.

Substituting for ln p(w|α) using (10.165), and for ln p(α) using (10.166), we obtain

ln q(α) = M
2

ln α −

α
2 E

wTw

+ (a0 − 1) ln α − b0α + const.

We recognize this as the log of a gamma distribution, and so we obtain

q(α) = Gam(α|aN , bN ) =

where

1
Γ(a0) ab0

0 αa0−1e−b0α

aN = a0 + M
2
1
2Ew

bN = b0 +

wTw

.

(10.174)

(10.175)

(10.176)

(10.177)

(10.178)

(10.179)












	







10.7. Expectation Propagation

505

We also need to optimize the variational parameters ξn, and this is also done by
L(q, ξ). Omitting terms that are independent of ξ, and

maximizing the lower bound
integrating over α, we have

L(q, ξ) =

q(w) ln h(w, ξ) dw + const.

(10.180)

Note that this has precisely the same form as (10.159), and so we can again appeal
to our earlier result (10.163), which can be obtained by direct optimization of the
marginal likelihood function, leading to re-estimation equations of the form

(ξnew

n )2 = φT
n

ΣN + µN µT
N

φn.

(10.181)

Appendix B

We have obtained re-estimation equations for the three quantities q(w), q(α),
and ξ, and so after making suitable initializations, we can cycle through these quan-
tities, updating each in turn. The required moments are given by

E [α] = aN
bN
wTw

E

= ΣN + µT

N µN .

(10.182)

(10.183)

10.7. Expectation Propagation

We conclude this chapter by discussing an alternative form of deterministic approx-
imate inference, known as expectation propagation or EP (Minka, 2001a; Minka,
2001b). As with the variational Bayes methods discussed so far, this too is based
on the minimization of a Kullback-Leibler divergence but now of the reverse form,
which gives the approximation rather different properties.

Consider for a moment the problem of minimizing KL(pq) with respect to q(z)
when p(z) is a ﬁxed distribution and q(z) is a member of the exponential family and
so, from (2.194), can be written in the form

q(z) = h(z)g(η) exp

ηTu(z)

.

(10.184)

As a function of η, the Kullback-Leibler divergence then becomes
KL(pq) = − ln g(η) − ηTEp(z)[u(z)] + const

(10.185)

where the constant terms are independent of the natural parameters η. We can mini-
mize KL(pq) within this family of distributions by setting the gradient with respect
to η to zero, giving
(10.186)
However, we have already seen in (2.226) that the negative gradient of ln g(η) is
given by the expectation of u(z) under the distribution q(z). Equating these two
results, we obtain

−∇ ln g(η) = Ep(z)[u(z)].

Eq(z)[u(z)] = Ep(z)[u(z)].

(10.187)







1
p(D)

i

i

i





506

10. APPROXIMATE INFERENCE

We see that the optimum solution simply corresponds to matching the expected suf-
ﬁcient statistics. So, for instance, if q(z) is a Gaussian N (z|µ, Σ) then we minimize
the Kullback-Leibler divergence by setting the mean µ of q(z) equal to the mean of
the distribution p(z) and the covariance Σ equal to the covariance of p(z). This is
sometimes called moment matching. An example of this was seen in Figure 10.3(a).
Now let us exploit this result to obtain a practical algorithm for approximate
inference. For many probabilistic models, the joint distribution of data D and hidden
variables (including parameters) θ comprises a product of factors in the form

p(D, θ) =

fi(θ).

(10.188)

This would arise, for example, in a model for independent, identically distributed
data in which there is one factor fn(θ) = p(xn|θ) for each data point xn, along
with a factor f0(θ) = p(θ) corresponding to the prior. More generally, it would also
apply to any model deﬁned by a directed probabilistic graph in which each factor is a
conditional distribution corresponding to one of the nodes, or an undirected graph in
which each factor is a clique potential. We are interested in evaluating the posterior
distribution p(θ|D) for the purpose of making predictions, as well as the model
evidence p(D) for the purpose of model comparison. From (10.188) the posterior is
given by
(10.189)

fi(θ)

p(θ|D) =

and the model evidence is given by

p(D) =

fi(θ) dθ.

(10.190)

Here we are considering continuous variables, but the following discussion applies
equally to discrete variables with integrals replaced by summations. We shall sup-
pose that the marginalization over θ, along with the marginalizations with respect to
the posterior distribution required to make predictions, are intractable so that some
form of approximation is required.

Expectation propagation is based on an approximation to the posterior distribu-

tion which is also given by a product of factors

q(θ) =

1
Z

fi(θ)

i

(10.191)

fi(θ) in the approximation corresponds to one of the factors
in which each factor
fi(θ) in the true posterior (10.189), and the factor 1/Z is the normalizing constant
needed to ensure that the left-hand side of (10.191) integrates to unity. In order to
fi(θ) in some way,
obtain a practical algorithm, we need to constrain the factors
and in particular we shall assume that they come from the exponential family. The
product of the factors will therefore also be from the exponential family and so can









''''' 1










q\j(θ) = q(θ)

fj(θ)

fj(θ)

fi(θ)

fi(θ)

i=j

Z

i

i

1
p(D)

.

fj(θ)





507

fi(θ)

(10.193)

(10.194)

(10.195)







10.7. Expectation Propagation

be described by a ﬁnite set of sufﬁcient statistics. For example, if each of the
is a Gaussian, then the overall approximation q(θ) will also be Gaussian.

Ideally we would like to determine the

fi(θ) by minimizing the Kullback-Leibler

divergence between the true posterior and the approximation given by

KL (pq) = KL

fi(θ)

.

(10.192)

Note that this is the reverse form of KL divergence compared with that used in varia-
tional inference. In general, this minimization will be intractable because the KL di-
vergence involves averaging with respect to the true distribution. As a rough approx-
imation, we could instead minimize the KL divergences between the corresponding
pairs fi(θ) and
fi(θ) of factors. This represents a much simpler problem to solve,
and has the advantage that the algorithm is noniterative. However, because each fac-
tor is individually approximated, the product of the factors could well give a poor
approximation.

Expectation propagation makes a much better approximation by optimizing each
factor in turn in the context of all of the remaining factors. It starts by initializing
fi(θ), and then cycles through the factors reﬁning them one at a time.
the factors
This is similar in spirit to the update of factors in the variational Bayes framework
fj(θ). We ﬁrst remove this
considered earlier. Suppose we wish to reﬁne factor
fi(θ). Conceptually, we will now determine a
factor from the product to give
revised form of the factor

fj(θ) by ensuring that the product

i=j

qnew(θ) ∝

fi(θ)

is as close as possible to

i=j
fi(θ) for i = j. This ensures that the
in which we keep ﬁxed all of the factors
approximation is most accurate in the regions of high posterior probability as deﬁned
by the remaining factors. We shall see an example of this effect when we apply EP
fj(θ) from the
to the ‘clutter problem’. To achieve this, we ﬁrst remove the factor
current approximation to the posterior by deﬁning the unnormalized distribution

Section 10.7.1

Note that we could instead ﬁnd q\j(θ) from the product of factors i = j, although
in practice division is usually easier. This is now combined with the factor fj(θ) to
give a distribution

1
Zj

fj(θ)q\j(θ)

(10.196)




40

30

20

10

0
−2





''''





508

10. APPROXIMATE INFERENCE

1

0.8

0.6

0.4

0.2

0
−2

−1

0

1

2

3

4

Figure 10.14 Illustration of the expectation propagation approximation using a Gaussian distribution for the
example considered earlier in Figures 4.14 and 10.1. The left-hand plot shows the original distribution (yellow)
along with the Laplace (red), global variational (green), and EP (blue) approximations, and the right-hand plot
shows the corresponding negative logarithms of the distributions. Note that the EP distribution is broader than
that variational inference, as a consequence of the different form of KL divergence.

−1

0

1

2

3

4

where Zj is the normalization constant given by

Zj =

fj(θ)q\j(θ) dθ.

(10.197)

We now determine a revised factor
gence

fj(θ) by minimizing the Kullback-Leibler diver-

KL

fj(θ)q\j(θ)

Zj

qnew(θ)

.

(10.198)

This is easily solved because the approximating distribution qnew(θ) is from the ex-
ponential family, and so we can appeal to the result (10.187), which tells us that the
parameters of qnew(θ) are obtained by matching its expected sufﬁcient statistics to
the corresponding moments of (10.196). We shall assume that this is a tractable oper-
ation. For example, if we choose q(θ) to be a Gaussian distribution N (θ|µ, Σ), then
µ is set equal to the mean of the (unnormalized) distribution fj(θ)q\j(θ), and Σ is
set to its covariance. More generally, it is straightforward to obtain the required ex-
pectations for any member of the exponential family, provided it can be normalized,
because the expected statistics can be related to the derivatives of the normalization
coefﬁcient, as given by (2.226). The EP approximation is illustrated in Figure 10.14.

From (10.193), we see that the revised factor

qnew(θ) and dividing out the remaining factors so that

fj(θ) can be found by taking

fj(θ) = K

qnew(θ)
q\j(θ)

(10.199)

where we have used (10.195). The coefﬁcient K is determined by multiplying both










q\j(θ) = q(θ)

fj(θ)q\j(θ) dθ

q(θ) ∝

fi(θ).

fi(θ).

fi(θ)

1
Z

i

i

i






sides of (10.199) by q\i(θ) and integrating to give

K =

10.7. Expectation Propagation

509

(10.200)

where we have used the fact that qnew(θ) is normalized. The value of K can therefore
be found by matching zeroth-order moments

fj(θ)q\j(θ) dθ =

fj(θ)q\j(θ) dθ.

(10.201)

Combining this with (10.197), we then see that K = Zj and so can be found by
evaluating the integral in (10.197).

In practice, several passes are made through the set of factors, revising each
factor in turn. The posterior distribution p(θ|D) is then approximated using (10.191),
and the model evidence p(D) can be approximated by using (10.190) with the factors
fi(θ) replaced by their approximations

fi(θ).

Expectation Propagation

We are given a joint distribution over observed data D and stochastic variables
θ in the form of a product of factors

p(D, θ) =

(10.202)

and we wish to approximate the posterior distribution p(θ|D) by a distribution
of the form
(10.203)

q(θ) =

We also wish to approximate the model evidence p(D).
fi(θ).
1. Initialize all of the approximating factors
2. Initialize the posterior approximation by setting

3. Until convergence:

(a) Choose a factor
(b) Remove

fj(θ) to reﬁne.

fj(θ) from the posterior by division

(10.204)

.

(10.205)

fj(θ)








Zj =

q\j(θ)fj(θ) dθ.

(d) Evaluate and store the new factor

qnew(θ)
q\j(θ) .
4. Evaluate the approximation to the model evidence

fj(θ) = Zj

p(D) 

fi(θ) dθ.

i

(10.206)

(10.207)

(10.208)

510

10. APPROXIMATE INFERENCE

(c) Evaluate the new posterior by setting the sufﬁcient statistics (moments)
of qnew(θ) equal to those of q\j(θ)fj(θ), including evaluation of the
normalization constant

A special case of EP, known as assumed density ﬁltering (ADF) or moment
matching (Maybeck, 1982; Lauritzen, 1992; Boyen and Koller, 1998; Opper and
Winther, 1999), is obtained by initializing all of the approximating factors except
the ﬁrst to unity and then making one pass through the factors updating each of them
once. Assumed density ﬁltering can be appropriate for on-line learning in which data
points are arriving in a sequence and we need to learn from each data point and then
discard it before considering the next point. However, in a batch setting we have the
opportunity to re-use the data points many times in order to achieve improved ac-
curacy, and it is this idea that is exploited in expectation propagation. Furthermore,
if we apply ADF to batch data, the results will have an undesirable dependence on
the (arbitrary) order in which the data points are considered, which again EP can
overcome.

One disadvantage of expectation propagation is that there is no guarantee that
the iterations will converge. However, for approximations q(θ) in the exponential
family, if the iterations do converge, the resulting solution will be a stationary point
of a particular energy function (Minka, 2001a), although each iteration of EP does
not necessarily decrease the value of this energy function. This is in contrast to
variational Bayes, which iteratively maximizes a lower bound on the log marginal
likelihood, in which each iteration is guaranteed not to decrease the bound. It is
possible to optimize the EP cost function directly, in which case it is guaranteed
to converge, although the resulting algorithms can be slower and more complex to
implement.

Another difference between variational Bayes and EP arises from the form of
KL divergence that is minimized by the two algorithms, because the former mini-
mizes KL(qp) whereas the latter minimizes KL(pq). As we saw in Figure 10.3,
for distributions p(θ) which are multimodal, minimizing KL(pq) can lead to poor
approximations. In particular, if EP is applied to mixtures the results are not sen-
sible because the approximation tries to capture all of the modes of the posterior
distribution. Conversely, in logistic-type models, EP often out-performs both local
variational methods and the Laplace approximation (Kuss and Rasmussen, 2006).

10.7. Expectation Propagation

511

Figure 10.15 Illustration of the clutter problem
for a data space dimensionality of
D = 1. Training data points, de-
noted by the crosses, are drawn
from a mixture of two Gaussians
with components shown in red
and green. The goal is to infer the
mean of the green Gaussian from
the observed data.

−5



N

0

θ

5

x

10

10.7.1 Example: The clutter problem
Following Minka (2001b), we illustrate the EP algorithm using a simple exam-
ple in which the goal is to infer the mean θ of a multivariate Gaussian distribution
over a variable x given a set of observations drawn from that distribution. To make
the problem more interesting, the observations are embedded in background clutter,
which itself is also Gaussian distributed, as illustrated in Figure 10.15. The distribu-
tion of observed values x is therefore a mixture of Gaussians, which we take to be
of the form

p(x|θ) = (1 − w)N (x|θ, I) + wN (x|0, aI)

(10.209)

where w is the proportion of background clutter and is assumed to be known. The
prior over θ is taken to be Gaussian

p(θ) = N (θ|0, bI)

(10.210)

and Minka (2001a) chooses the parameter values a = 10, b = 100 and w = 0.5.
The joint distribution of N observations D = {x1, . . . , xN} and θ is given by

p(D, θ) = p(θ)

p(xn|θ)

n=1

(10.211)

and so the posterior distribution comprises a mixture of 2N Gaussians. Thus the
computational cost of solving this problem exactly would grow exponentially with
the size of the data set, and so an exact solution is intractable for moderately large
N.

To apply EP to the clutter problem, we ﬁrst identify the factors f0(θ) = p(θ)
and fn(θ) = p(xn|θ). Next we select an approximating distribution from the expo-
nential family, and for this example it is convenient to choose a spherical Gaussian

q(θ) = N (θ|m, vI).

(10.212)











512

10. APPROXIMATE INFERENCE

The factor approximations will therefore take the form of exponential-quadratic
functions of the form

fn(θ) = snN (θ|mn, vnI)

(10.213)

where n = 1, . . . , N, and we set
f0(θ) equal to the prior p(θ). Note that the use of
N (θ|·,·) does not imply that the right-hand side is a well-deﬁned Gaussian density
(in fact, as we shall see, the variance parameter vn can be negative) but is simply a
fn(θ), for n = 1, . . . , N, can
convenient shorthand notation. The approximations
be initialized to unity, corresponding to sn = (2πvn)D/2, vn → ∞ and mn = 0,
where D is the dimensionality of x and hence of θ. The initial q(θ), deﬁned by
(10.191), is therefore equal to the prior.

We then iteratively reﬁne the factors by taking one factor fn(θ) at a time and
applying (10.205), (10.206), and (10.207). Note that we do not need to revise the
term f0(θ) because an EP update will leave this term unchanged. Here we state the
results and leave the reader to ﬁll in the details.

First we remove the current estimate

fn(θ) from q(θ) by division using (10.205)

to give q\n(θ), which has mean and inverse variance given by
m\n = m + v\nv−1
n (m − mn)
(v\n)−1 = v−1 − v−1
n .

Next we evaluate the normalization constant Zn using (10.206) to give

Zn = (1 − w)N (xn|m\n, (v\n + 1)I) + wN (xn|0, aI).

Similarly, we compute the mean and variance of qnew(θ) by ﬁnding the mean and
variance of q\n(θ)fn(θ) to give

Exercise 10.37

Exercise 10.38

Exercise 10.39

m = m\n + ρn

v = v\n − ρn

where the quantity

v\n

v\n + 1
(v\n)2
v\n + 1

(xn − m\n)
+ ρn(1 − ρn)

(v\n)2xn − m\n2

D(v\n + 1)2

ρn = 1 −

w
ZnN (xn|0, aI)

has a simple interpretation as the probability of the point xn not being clutter. Then
fn(θ) whose parameters are given by
we use (10.207) to compute the reﬁned factor
n = (vnew)−1 − (v\n)−1
v−1
mn = m\n + (vn + v\n)(v\n)−1(mnew − m\n)
sn =

(10.220)
(10.221)

(10.222)

Zn

(2πvn)D/2N (mn|m\n, (vn + v\n)I) .

This reﬁnement process is repeated until a suitable termination criterion is satisﬁed,
for instance that the maximum change in parameter values resulting from a complete

(10.214)
(10.215)

(10.216)

(10.217)

(10.218)

(10.219)



5



N




0

N

10.7. Expectation Propagation

513

θ

10

(10.223)

(10.224)

e

e



−5

0

5

θ

10

−5

Figure 10.16 Examples of the approximation of speciﬁc factors for a one-dimensional version of the clutter
fn(θ) in red, and q\n(θ) in green. Notice that the current form for q\n(θ) controls
problem, showing fn(θ) in blue,
the range of θ over which

fn(θ) will be a good approximation to fn(θ).

pass through all factors is less than some threshold. Finally, we use (10.208) to
evaluate the approximation to the model evidence, given by

p(D)  (2πvnew)D/2 exp(B/2)

n=1

sn(2πvn)−D/2

where

B =

(mnew)Tmnew

v

nmn
mT
vn

.

−

n=1

Examples factor approximations for the clutter problem with a one-dimensional pa-
rameter space θ are shown in Figure 10.16. Note that the factor approximations can
have inﬁnite or even negative values for the ‘variance’ parameter vn. This simply
corresponds to approximations that curve upwards instead of downwards and are not
necessarily problematic provided the overall approximate posterior q(θ) has posi-
tive variance. Figure 10.17 compares the performance of EP with variational Bayes
(mean ﬁeld theory) and the Laplace approximation on the clutter problem.

10.7.2 Expectation propagation on graphs
So far in our general discussion of EP, we have allowed the factors fi(θ) in the
distribution p(θ) to be functions of all of the components of θ, and similarly for the
f(θ) in the approximating distribution q(θ). We now consider
approximating factors
situations in which the factors depend only on subsets of the variables. Such restric-
tions can be conveniently expressed using the framework of probabilistic graphical
models, as discussed in Chapter 8. Here we use a factor graph representation because
this encompasses both directed and undirected graphs.

















10. APPROXIMATE INFERENCE

514

100

r
o
r
r

E

10−5

Posterior mean

laplace

vb

104

ep

106

FLOPS

10−200

r
o
r
r

E

10−202

laplace

10−204

104

Evidence

vb

ep

106

FLOPS

Figure 10.17 Comparison of expectation propagation, variational inference, and the Laplace approximation on
the clutter problem. The left-hand plot shows the error in the predicted posterior mean versus the number of
ﬂoating point operations, and the right-hand plot shows the corresponding results for the model evidence.

Section 8.4.4

We shall focus on the case in which the approximating distribution is fully fac-
torized, and we shall show that in this case expectation propagation reduces to loopy
belief propagation (Minka, 2001a). To start with, we show this in the context of a
simple example, and then we shall explore the general case.

First of all, recall from (10.17) that if we minimize the Kullback-Leibler diver-
gence KL(pq) with respect to a factorized distribution q, then the optimal solution
for each factor is simply the corresponding marginal of p.
Now consider the factor graph shown on the left in Figure 10.18, which was
introduced earlier in the context of the sum-product algorithm. The joint distribution
is given by

p(x) = fa(x1, x2)fb(x2, x3)fc(x2, x4).

We seek an approximation q(x) that has the same factorization, so that

(10.225)

(10.226)

q(x) ∝

fa(x1, x2)

fb(x2, x3)

fc(x2, x4).

Note that normalization constants have been omitted, and these can be re-instated at
the end by local normalization, as is generally done in belief propagation. Now sup-
pose we restrict attention to approximations in which the factors themselves factorize
with respect to the individual variables so that

q(x) ∝

fa1(x1)

fa2(x2)

fb2(x2)

fb3(x3)

fc2(x2)

fc4(x4)

(10.227)

which corresponds to the factor graph shown on the right in Figure 10.18. Because
the individual factors are factorized, the overall distribution q(x) is itself fully fac-
torized.

Now we apply the EP algorithm using the fully factorized approximation. Sup-
pose that we have initialized all of the factors and that we choose to reﬁne factor



































x3





















10.7. Expectation Propagation

515

x2

x3

x1

x2

x3

x1

fa

fb

fc

x4

˜fa1

˜fa2

˜fb2

˜fb3

˜fc2

˜fc4

x4

Figure 10.18 On the left is a simple factor graph from Figure 8.51 and reproduced here for convenience. On
the right is the corresponding factorized approximation.

fb(x2, x3) =
distribution to give

fb2(x2)

fb3(x3). We ﬁrst remove this factor from the approximating

q\b(x) =

fa1(x1)

fa2(x2)

fc2(x2)

fc4(x4)

(10.228)

and we then multiply this by the exact factor fb(x2, x3) to give

fa1(x1)

fa2(x2)

p(x) = q\b(x)fb(x2, x3) =

(10.229)
pqnew).
We now ﬁnd qnew(x) by minimizing the Kullback-Leibler divergence KL(
The result, as noted above, is that qnew(z) comprises the product of factors, one for
each variable xi, in which each factor is given by the corresponding marginal of
p(x). These four marginals are given by

fc4(x4)fb(x2, x3).

fc2(x2)

p(x1) ∝
p(x2) ∝

p(x3) ∝
p(x4) ∝

fa1(x1)
fa2(x2)

x2
fc4(x4)

fc2(x2)

fb(x2, x3)

fb(x2, x3)

fc2(x2)

x3
fa2(x2)

and qnew(x) is obtained by multiplying these marginals together. We see that the
fb(x2, x3) are those that involve
only factors in q(x) that change when we update
fb(x2, x3) =
the variables in fb namely x2 and x3. To obtain the reﬁned factor
fb2(x2)

fb3(x3) we simply divide qnew(x) by q\b(x), which gives

fb2(x2) ∝

fb(x2, x3)

fb3(x3) ∝

x2

fb(x2, x3)

fa2(x2)

fc2(x2)

.

(10.235)

(10.230)

(10.231)

(10.232)

(10.233)

(10.234)



















i=j

k

i

k

i
















516

10. APPROXIMATE INFERENCE

Section 8.4.4

These are precisely the messages obtained using belief propagation in which mes-
sages from variable nodes to factor nodes have been folded into the messages from
fb2(x2) corresponds to the message
factor nodes to variable nodes. In particular,
µfb→x2(x2) sent by factor node fb to variable node x2 and is given by (8.81). Simi-
fa2(x2) corre-
larly, if we substitute (8.78) into (8.79), we obtain (10.235) in which
sponds to µfa→x2(x2) and
fc2(x2) corresponds to µfc→x2(x2), giving the message
fb3(x3) which corresponds to µfb→x3(x3).
This result differs slightly from standard belief propagation in that messages are
passed in both directions at the same time. We can easily modify the EP procedure
to give the standard form of the sum-product algorithm by updating just one of the
fb2(x2) is unchanged
factors at a time, for instance if we reﬁne only
fb3(x3) is again given by (10.235). If
by deﬁnition, while the reﬁned version of
we are reﬁning only one term at a time, then we can choose the order in which the
reﬁnements are done as we wish. In particular, for a tree-structured graph we can
follow a two-pass update scheme, corresponding to the standard belief propagation
schedule, which will result in exact inference of the variable and factor marginals.
The initialization of the approximation factors in this case is unimportant.

fb3(x3), then

Now let us consider a general factor graph corresponding to the distribution

p(θ) =

fi(θi)

(10.236)

where θi represents the subset of variables associated with factor fi. We approximate
this using a fully factorized distribution of the form

q(θ) ∝

fik(θk)

(10.237)

where θk corresponds to an individual variable node. Suppose that we wish to reﬁne
fjl(θl) keeping all other terms ﬁxed. We ﬁrst remove the term
the particular term
fj(θj) from q(θ) to give

q\j(θ) ∝

fik(θk)

(10.238)

fjl(θl),
and then multiply by the exact factor fj(θj). To determine the reﬁned term
we need only consider the functional dependence on θl, and so we simply ﬁnd the
corresponding marginal of

q\j(θ)fj(θj).

(10.239)
Up to a multiplicative constant, this involves taking the marginal of fj(θj) multiplied
by any terms from q\j(θ) that are functions of any of the variables in θj. Terms that
fi(θi) for i = j will cancel between numerator and
correspond to other factors
denominator when we subsequently divide by q\j(θ). We therefore obtain

fjl(θl) ∝

fj(θj)

fkm(θm).

(10.240)

θm=l∈θj

k

m=l



Exercises

517

We recognize this as the sum-product rule in the form in which messages from vari-
able nodes to factor nodes have been eliminated, as illustrated by the example shown
fjm(θm) corresponds to the message µfj→θm(θm),
in Figure 8.50. The quantity
which factor node j sends to variable node m, and the product over k in (10.240)
is over all factors that depend on the variables θm that have variables (other than
variable θl) in common with factor fj(θj). In other words, to compute the outgoing
message from a factor node, we take the product of all the incoming messages from
other factor nodes, multiply by the local factor, and then marginalize.

Thus, the sum-product algorithm arises as a special case of expectation propa-
gation if we use an approximating distribution that is fully factorized. This suggests
that more ﬂexible approximating distributions, corresponding to partially discon-
nected graphs, could be used to achieve higher accuracy. Another generalization is
to group factors fi(θi) together into sets and to reﬁne all the factors in a set together
at each iteration. Both of these approaches can lead to improvements in accuracy
(Minka, 2001b). In general, the problem of choosing the best combination of group-
ing and disconnection is an open research issue.

We have seen that variational message passing and expectation propagation op-
timize two different forms of the Kullback-Leibler divergence. Minka (2005) has
shown that a broad range of message passing algorithms can be derived from a com-
mon framework involving minimization of members of the alpha family of diver-
gences, given by (10.19). These include variational message passing, loopy belief
propagation, and expectation propagation, as well as a range of other algorithms,
which we do not have space to discuss here, such as tree-reweighted message pass-
ing (Wainwright et al., 2005), fractional belief propagation (Wiegerinck and Heskes,
2003), and power EP (Minka, 2004).

Exercises

10.1 () www Verify that the log marginal distribution of the observed data ln p(X)
can be decomposed into two terms in the form (10.2) where L(q) is given by (10.3)
and KL(qp) is given by (10.4).

10.2 () Use the properties E[z1] = m1 and E[z2] = m2 to solve the simultaneous equa-
tions (10.13) and (10.15), and hence show that, provided the original distribution
p(z) is nonsingular, the unique solution for the means of the factors in the approxi-
mation distribution is given by E[z1] = µ1 and E[z2] = µ2.

10.3 ( ) www Consider a factorized variational distribution q(Z) of the form (10.5).
By using the technique of Lagrange multipliers, verify that minimization of the
Kullback-Leibler divergence KL(pq) with respect to one of the factors qi(Zi),
keeping all other factors ﬁxed, leads to the solution (10.17).

10.4 ( ) Suppose that p(x) is some ﬁxed distribution and that we wish to approximate
it using a Gaussian distribution q(x) = N (x|µ, Σ). By writing down the form of
the KL divergence KL(pq) for a Gaussian q(x) and then differentiating, show that

518

10. APPROXIMATE INFERENCE

minimization of KL(pq) with respect to µ and Σ leads to the result that µ is given
by the expectation of x under p(x) and that Σ is given by the covariance.

10.5 ( ) www Consider a model in which the set of all hidden stochastic variables, de-
noted collectively by Z, comprises some latent variables z together with some model
parameters θ. Suppose we use a variational distribution that factorizes between la-
tent variables and parameters so that q(z, θ) = qz(z)qθ(θ), in which the distribution
qθ(θ) is approximated by a point estimate of the form qθ(θ) = δ(θ − θ0) where θ0
is a vector of free parameters. Show that variational optimization of this factorized
distribution is equivalent to an EM algorithm, in which the E step optimizes qz(z),
and the M step maximizes the expected complete-data log posterior distribution of θ
with respect to θ0.

10.6 ( ) The alpha family of divergences is deﬁned by (10.19). Show that the Kullback-
Leibler divergence KL(pq) corresponds to α → 1. This can be done by writing
p = exp(	 ln p) = 1 + 	 ln p + O(	2) and then taking 	 → 0. Similarly show that
KL(qp) corresponds to α → −1.

10.7 ( ) Consider the problem of inferring the mean and precision of a univariate Gaus-
sian using a factorized variational approximation, as considered in Section 10.1.3.
Show that the factor qµ(µ) is a Gaussian of the form N (µ|µN , λ−1
N ) with mean and
precision given by (10.26) and (10.27), respectively. Similarly show that the factor
qτ (τ) is a gamma distribution of the form Gam(τ|aN , bN ) with parameters given by
(10.29) and (10.30).

10.8 () Consider the variational posterior distribution for the precision of a univariate
Gaussian whose parameters are given by (10.29) and (10.30). By using the standard
results for the mean and variance of the gamma distribution given by (B.27) and
(B.28), show that if we let N → ∞, this variational posterior distribution has a
mean given by the inverse of the maximum likelihood estimator for the variance of
the data, and a variance that goes to zero.

10.9 ( ) By making use of the standard result E[τ] = aN /bN for the mean of a gamma
distribution, together with (10.26), (10.27), (10.29), and (10.30), derive the result
(10.33) for the reciprocal of the expected precision in the factorized variational treat-
ment of a univariate Gaussian.

10.10 () www Derive the decomposition given by (10.34) that is used to ﬁnd approxi-

mate posterior distributions over models using variational inference.

10.11 ( ) www By using a Lagrange multiplier to enforce the normalization constraint
on the distribution q(m), show that the maximum of the lower bound (10.35) is given
by (10.36).

10.12 ( ) Starting from the joint distribution (10.41), and applying the general result
(10.9), show that the optimal variational distribution q(Z) over the latent variables
for the Bayesian mixture of Gaussians is given by (10.48) by verifying the steps
given in the text.

Exercises

519

10.13 ( ) www Starting from (10.54), derive the result (10.59) for the optimum vari-
ational posterior distribution over µk and Λk in the Bayesian mixture of Gaussians,
and hence verify the expressions for the parameters of this distribution given by
(10.60)–(10.63).

10.14 ( ) Using the distribution (10.59), verify the result (10.64).

10.15 () Using the result (B.17), show that the expected value of the mixing coefﬁcients

in the variational mixture of Gaussians is given by (10.69).

10.16 ( ) www Verify the results (10.71) and (10.72) for the ﬁrst two terms in the

lower bound for the variational Gaussian mixture model given by (10.70).

10.17 (  ) Verify the results (10.73)–(10.77) for the remaining terms in the lower bound

for the variational Gaussian mixture model given by (10.70).

10.18 (  )

In this exercise, we shall derive the variational re-estimation equations for
the Gaussian mixture model by direct differentiation of the lower bound. To do this
we assume that the variational distribution has the factorization deﬁned by (10.42)
and (10.55) with factors given by (10.48), (10.57), and (10.59). Substitute these into
(10.70) and hence obtain the lower bound as a function of the parameters of the varia-
tional distribution. Then, by maximizing the bound with respect to these parameters,
derive the re-estimation equations for the factors in the variational distribution, and
show that these are the same as those obtained in Section 10.2.1.

10.19 ( ) Derive the result (10.81) for the predictive distribution in the variational treat-

ment of the Bayesian mixture of Gaussians model.

10.20 ( ) www This exercise explores the variational Bayes solution for the mixture of
Gaussians model when the size N of the data set is large and shows that it reduces (as
we would expect) to the maximum likelihood solution based on EM derived in Chap-
ter 9. Note that results from Appendix B may be used to help answer this exercise.
First show that the posterior distribution q(Λk) of the precisions becomes sharply
peaked around the maximum likelihood solution. Do the same for the posterior dis-
tribution of the means q(µk|Λk). Next consider the posterior distribution q(π)
for the mixing coefﬁcients and show that this too becomes sharply peaked around
the maximum likelihood solution. Similarly, show that the responsibilities become
equal to the corresponding maximum likelihood values for large N, by making use
of the following asymptotic result for the digamma function for large x

ψ(x) = ln x + O (1/x) .

(10.241)

Finally, by making use of (10.80), show that for large N, the predictive distribution
becomes a mixture of Gaussians.

10.21 () Show that the number of equivalent parameter settings due to interchange sym-

metries in a mixture model with K components is K!.

520

10. APPROXIMATE INFERENCE

10.22 ( ) We have seen that each mode of the posterior distribution in a Gaussian mix-
ture model is a member of a family of K! equivalent modes. Suppose that the result
of running the variational inference algorithm is an approximate posterior distribu-
tion q that is localized in the neighbourhood of one of the modes. We can then
approximate the full posterior distribution as a mixture of K! such q distributions,
once centred on each mode and having equal mixing coefﬁcients. Show that if we
assume negligible overlap between the components of the q mixture, the resulting
lower bound differs from that for a single component q distribution through the ad-
dition of an extra term ln K!.

10.23 ( ) www Consider a variational Gaussian mixture model in which there is no
prior distribution over mixing coefﬁcients {πk}. Instead, the mixing coefﬁcients are
treated as parameters, whose values are to be found by maximizing the variational
lower bound on the log marginal likelihood. Show that maximizing this lower bound
with respect to the mixing coefﬁcients, using a Lagrange multiplier to enforce the
constraint that the mixing coefﬁcients sum to one, leads to the re-estimation result
(10.83). Note that there is no need to consider all of the terms in the lower bound but
only the dependence of the bound on the {πk}.

10.24 ( ) www We have seen in Section 10.2 that the singularities arising in the max-
imum likelihood treatment of Gaussian mixture models do not arise in a Bayesian
treatment. Discuss whether such singularities would arise if the Bayesian model
were solved using maximum posterior (MAP) estimation.

10.25 ( ) The variational treatment of the Bayesian mixture of Gaussians, discussed in
Section 10.2, made use of a factorized approximation (10.5) to the posterior distribu-
tion. As we saw in Figure 10.2, the factorized assumption causes the variance of the
posterior distribution to be under-estimated for certain directions in parameter space.
Discuss qualitatively the effect this will have on the variational approximation to the
model evidence, and how this effect will vary with the number of components in
the mixture. Hence explain whether the variational Gaussian mixture will tend to
under-estimate or over-estimate the optimal number of components.

10.26 (  ) Extend the variational treatment of Bayesian linear regression to include
a gamma hyperprior Gam(β|c0, d0) over β and solve variationally, by assuming a
factorized variational distribution of the form q(w)q(α)q(β). Derive the variational
update equations for the three factors in the variational distribution and also obtain
an expression for the lower bound and for the predictive distribution.

10.27 ( ) By making use of the formulae given in Appendix B show that the variational
lower bound for the linear basis function regression model, deﬁned by (10.107), can
be written in the form (10.107) with the various terms deﬁned by (10.108)–(10.112).

10.28 (  ) Rewrite the model for the Bayesian mixture of Gaussians, introduced in
Section 10.2, as a conjugate model from the exponential family, as discussed in
Section 10.4. Hence use the general results (10.115) and (10.119) to derive the
speciﬁc results (10.48), (10.57), and (10.59).

Exercises

521

10.29 () www Show that the function f(x) = ln(x) is concave for 0 < x < ∞
by computing its second derivative. Determine the form of the dual function g(λ)
deﬁned by (10.133), and verify that minimization of λx − g(λ) with respect to λ
according to (10.132) indeed recovers the function ln(x).

10.30 () By evaluating the second derivative, show that the log logistic function f(x) =
− ln(1 + e−x) is concave. Derive the variational upper bound (10.137) directly by
making a second order Taylor expansion of the log logistic function around a point
x = ξ.

10.31 ( ) By ﬁnding the second derivative with respect to x, show that the function
f(x) = − ln(ex/2 + e−x/2) is a concave function of x. Now consider the second
derivatives with respect to the variable x2 and hence show that it is a convex function
of x2. Plot graphs of f(x) against x and against x2. Derive the lower bound (10.144)
on the logistic sigmoid function directly by making a ﬁrst order Taylor series expan-
sion of the function f(x) in the variable x2 centred on the value ξ2.

10.32 ( ) www Consider the variational treatment of logistic regression with sequen-
tial learning in which data points are arriving one at a time and each must be pro-
cessed and discarded before the next data point arrives. Show that a Gaussian ap-
proximation to the posterior distribution can be maintained through the use of the
lower bound (10.151), in which the distribution is initialized using the prior, and as
each data point is absorbed its corresponding variational parameter ξn is optimized.
10.33 () By differentiating the quantity Q(ξ, ξold) deﬁned by (10.161) with respect to
the variational parameter ξn show that the update equation for ξn for the Bayesian
logistic regression model is given by (10.163).

10.34 ( )

In this exercise we derive re-estimation equations for the variational parame-
ters ξ in the Bayesian logistic regression model of Section 4.5 by direct maximization
of the lower bound given by (10.164). To do this set the derivative of L(ξ) with re-
spect to ξn equal to zero, making use of the result (3.117) for the derivative of the log
of a determinant, together with the expressions (10.157) and (10.158) which deﬁne
the mean and covariance of the variational posterior distribution q(w).

10.35 ( ) Derive the result (10.164) for the lower bound L(ξ) in the variational logistic
regression model. This is most easily done by substituting the expressions for the
Gaussian prior q(w) = N (w|m0, S0), together with the lower bound h(w, ξ) on
the likelihood function, into the integral (10.159) which deﬁnes L(ξ). Next gather
together the terms which depend on w in the exponential and complete the square
to give a Gaussian integral, which can then be evaluated by invoking the standard
result for the normalization coefﬁcient of a multivariate Gaussian. Finally take the
logarithm to obtain (10.164).

10.36 ( ) Consider the ADF approximation scheme discussed in Section 10.7, and show
that inclusion of the factor fj(θ) leads to an update of the model evidence of the
form

pj(D)  pj−1(D)Zj

(10.242)









522

10. APPROXIMATE INFERENCE

where Zj is the normalization constant deﬁned by (10.197). By applying this result
recursively, and initializing with p0(D) = 1, derive the result

p(D) 

Zj.

j

(10.243)

10.37 () www Consider the expectation propagation algorithm from Section 10.7, and
suppose that one of the factors f0(θ) in the deﬁnition (10.188) has the same expo-
nential family functional form as the approximating distribution q(θ). Show that if
f0(θ) leaves
the factor
f0(θ) unchanged. This situation typically arises when one of the factors is the prior
p(θ), and so we see that the prior factor can be incorporated once exactly and does
not need to be reﬁned.

f0(θ) is initialized to be f0(θ), then an EP update to reﬁne

10.38 (  )

In this exercise and the next, we shall verify the results (10.214)–(10.224)
for the expectation propagation algorithm applied to the clutter problem. Begin by
using the division formula (10.205) to derive the expressions (10.214) and (10.215)
by completing the square inside the exponential to identify the mean and variance.
Also, show that the normalization constant Zn, deﬁned by (10.206), is given for the
clutter problem by (10.216). This can be done by making use of the general result
(2.115).

10.39 (  ) Show that the mean and variance of qnew(θ) for EP applied to the clutter
problem are given by (10.217) and (10.218). To do this, ﬁrst prove the following
results for the expectations of θ and θθT under qnew(θ)

E[θ] = m\n + v\n∇m\n ln Zn

E[θTθ] = 2(v\n)2∇v\n ln Zn + 2E[θ]Tm\n − m\n2

(10.244)
(10.245)

and then make use of the result (10.216) for Zn. Next, prove the results (10.220)–
(10.222) by using (10.207) and completing the square in the exponential. Finally,
use (10.208) to derive the result (10.223).

11

Sampling
Methods

For most probabilistic models of practical interest, exact inference is intractable, and
so we have to resort to some form of approximation. In Chapter 10, we discussed
inference algorithms based on deterministic approximations, which include methods
such as variational Bayes and expectation propagation. Here we consider approxi-
mate inference methods based on numerical sampling, also known as Monte Carlo
techniques.

Although for some applications the posterior distribution over unobserved vari-
ables will be of direct interest in itself, for most situations the posterior distribution
is required primarily for the purpose of evaluating expectations, for example in order
to make predictions. The fundamental problem that we therefore wish to address in
this chapter involves ﬁnding the expectation of some function f(z) with respect to a
probability distribution p(z). Here, the components of z might comprise discrete or
continuous variables or some combination of the two. Thus in the case of continuous

523






l=1

L






	



f(z)

z

(11.1)

(11.2)

524

11. SAMPLING METHODS

Figure 11.1 Schematic illustration of a function f (z)
whose expectation is to be evaluated with
respect to a distribution p(z).

p(z)

variables, we wish to evaluate the expectation

E[f] =

f(z)p(z) dz

where the integral is replaced by summation in the case of discrete variables. This
is illustrated schematically for a single continuous variable in Figure 11.1. We shall
suppose that such expectations are too complex to be evaluated exactly using analyt-
ical techniques.

The general idea behind sampling methods is to obtain a set of samples z(l)
(where l = 1, . . . , L) drawn independently from the distribution p(z). This allows
the expectation (11.1) to be approximated by a ﬁnite sum

f =

1
L

f(z(l)).

Exercise 11.1

As long as the samples z(l) are drawn from the distribution p(z), then E[
and so the estimator
by

f] = E[f]
f has the correct mean. The variance of the estimator is given

var[

f] =

E

(f − E[f])2

(11.3)

1
L

is the variance of the function f(z) under the distribution p(z). It is worth emphasiz-
ing that the accuracy of the estimator therefore does not depend on the dimension-
ality of z, and that, in principle, high accuracy may be achievable with a relatively
small number of samples z(l). In practice, ten or twenty independent samples may
sufﬁce to estimate an expectation to sufﬁcient accuracy.

The problem, however, is that the samples {z(l)} might not be independent, and
so the effective sample size might be much smaller than the apparent sample size.
Also, referring back to Figure 11.1, we note that if f(z) is small in regions where
p(z) is large, and vice versa, then the expectation may be dominated by regions
of small probability, implying that relatively large sample sizes will be required to
achieve sufﬁcient accuracy.

For many models, the joint distribution p(z) is conveniently speciﬁed in terms
of a graphical model. In the case of a directed graph with no observed variables, it is



M

11. SAMPLING METHODS

525

straightforward to sample from the joint distribution (assuming that it is possible to
sample from the conditional distributions at each node) using the following ances-
tral sampling approach, discussed brieﬂy in Section 8.1.2. The joint distribution is
speciﬁed by

p(z) =

p(zi|pai)

i=1

(11.4)

where zi are the set of variables associated with node i, and pai denotes the set of
variables associated with the parents of node i. To obtain a sample from the joint
distribution, we make one pass through the set of variables in the order z1, . . . , zM
sampling from the conditional distributions p(zi|pai). This is always possible be-
cause at each step all of the parent values will have been instantiated. After one pass
through the graph, we will have obtained a sample from the joint distribution.

Now consider the case of a directed graph in which some of the nodes are in-
stantiated with observed values. We can in principle extend the above procedure, at
least in the case of nodes representing discrete variables, to give the following logic
sampling approach (Henrion, 1988), which can be seen as a special case of impor-
tance sampling discussed in Section 11.1.4. At each step, when a sampled value is
obtained for a variable zi whose value is observed, the sampled value is compared
to the observed value, and if they agree then the sample value is retained and the al-
gorithm proceeds to the next variable in turn. However, if the sampled value and the
observed value disagree, then the whole sample so far is discarded and the algorithm
starts again with the ﬁrst node in the graph. This algorithm samples correctly from
the posterior distribution because it corresponds simply to drawing samples from the
joint distribution of hidden variables and data variables and then discarding those
samples that disagree with the observed data (with the slight saving of not continu-
ing with the sampling from the joint distribution as soon as one contradictory value is
observed). However, the overall probability of accepting a sample from the posterior
decreases rapidly as the number of observed variables increases and as the number
of states that those variables can take increases, and so this approach is rarely used
in practice.

In the case of probability distributions deﬁned by an undirected graph, there is
no one-pass sampling strategy that will sample even from the prior distribution with
no observed variables. Instead, computationally more expensive techniques must be
employed, such as Gibbs sampling, which is discussed in Section 11.3.

As well as sampling from conditional distributions, we may also require samples
from a marginal distribution. If we already have a strategy for sampling from a joint
distribution p(u, v), then it is straightforward to obtain samples from the marginal
distribution p(u) simply by ignoring the values for v in each sample.

There are numerous texts dealing with Monte Carlo methods. Those of partic-
ular interest from the statistical inference perspective include Chen et al. (2001),
Gamerman (1997), Gilks et al. (1996), Liu (2001), Neal (1996), and Robert and
Casella (1999). Also there are review articles by Besag et al. (1995), Brooks (1998),
Diaconis and Saloff-Coste (1998), Jerrum and Sinclair (1996), Neal (1993), Tierney
(1994), and Andrieu et al. (2003) that provide additional information on sampling

 dz



dy





y

11.1. Basic Sampling Algorithms

In this section, we consider some simple strategies for generating random samples
from a given distribution. Because the samples will be generated by a computer
algorithm they will in fact be pseudo-random numbers, that is, they will be deter-
ministically calculated, but must nevertheless pass appropriate tests for randomness.
Generating such numbers raises several subtleties (Press et al., 1992) that lie outside
the scope of this book. Here we shall assume that an algorithm has been provided
that generates pseudo-random numbers distributed uniformly over (0, 1), and indeed
most software environments have such a facility built in.

11.1.1 Standard distributions
We ﬁrst consider how to generate random numbers from simple nonuniform dis-
tributions, assuming that we already have available a source of uniformly distributed
random numbers. Suppose that z is uniformly distributed over the interval (0, 1),
and that we transform the values of z using some function f(·) so that y = f(z).
The distribution of y will be governed by

where, in this case, p(z) = 1. Our goal is to choose the function f(z) such that the
resulting values of y have some speciﬁc desired distribution p(y). Integrating (11.5)
we obtain

p(y) = p(z)

z = h(y) ≡

p(

y) d

y

−∞

(11.5)

(11.6)

526

11. SAMPLING METHODS

methods for statistical inference.

Diagnostic tests for convergence of Markov chain Monte Carlo algorithms are
summarized in Robert and Casella (1999), and some practical guidance on the use of
sampling methods in the context of machine learning is given in Bishop and Nabney
(2008).

Exercise 11.2

which is the indeﬁnite integral of p(y). Thus, y = h−1(z), and so we have to
transform the uniformly distributed random numbers using a function which is the
inverse of the indeﬁnite integral of the desired distribution. This is illustrated in
Figure 11.2.

Consider for example the exponential distribution
p(y) = λ exp(−λy)

(11.7)
where 0 � y < ∞. In this case the lower limit of the integral in (11.6) is 0, and so
h(y) = 1 − exp(−λy). Thus, if we transform our uniformly distributed variable z
using y = −λ−1 ln(1 − z), then y will have an exponential distribution.

 ∂(z1, . . . , zM )

y

∂(y1, . . . , yM )



p(y)

11.1. Basic Sampling Algorithms

527

h(y)

Figure 11.2 Geometrical

interpretation of

the trans-
formation method for generating nonuni-
formly distributed random numbers. h(y)
is the indeﬁnite integral of the desired dis-
tribution p(y).
If a uniformly distributed
random variable z is transformed using
y = h−1(z), then y will be distributed ac-
cording to p(y).

1

0

Another example of a distribution to which the transformation method can be

applied is given by the Cauchy distribution

p(y) =

1
π

1

1 + y2 .

(11.8)

Exercise 11.3

In this case, the inverse of the indeﬁnite integral can be expressed in terms of the
‘tan’ function.

The generalization to multiple variables is straightforward and involves the Ja-

cobian of the change of variables, so that

p(y1, . . . , yM ) = p(z1, . . . , zM )

.

(11.9)

As a ﬁnal example of the transformation method we consider the Box-Muller
method for generating samples from a Gaussian distribution. First, suppose we gen-
erate pairs of uniformly distributed random numbers z1, z2 ∈ (−1, 1), which we can
do by transforming a variable distributed uniformly over (0, 1) using z → 2z − 1.
2 � 1. This leads to a uniform
Next we discard each pair unless it satisﬁes z2
distribution of points inside the unit circle with p(z1, z2) = 1/π, as illustrated in
Figure 11.3. Then, for each pair z1, z2 we evaluate the quantities

1 + z2

Figure 11.3 The Box-Muller method for generating Gaussian dis-
tributed random numbers starts by generating samples
from a uniform distribution inside the unit circle.

1

z2

−1−1

z1

1












 ∂(z1, z2)


∂(y1, y2)

r2

r2

1
Zp



528

11. SAMPLING METHODS

Exercise 11.4

Exercise 11.5

y1 = z1

y2 = z2

1/2

1/2

−2 ln z1

−2 ln z2

(11.10)

(11.11)

where r2 = z2

1 + z2

2. Then the joint distribution of y1 and y2 is given by

p(y1, y2) = p(z1, z2)

=

1
√2π

exp(−y2

1/2)

1
√2π

exp(−y2

2/2)

(11.12)

and so y1 and y2 are independent and each has a Gaussian distribution with zero
mean and unit variance.

If y has a Gaussian distribution with zero mean and unit variance, then σy + µ
will have a Gaussian distribution with mean µ and variance σ2. To generate vector-
valued variables having a multivariate Gaussian distribution with mean µ and co-
variance Σ, we can make use of the Cholesky decomposition, which takes the form
Σ = LLT (Press et al., 1992). Then, if z is a vector valued random variable whose
components are independent and Gaussian distributed with zero mean and unit vari-
ance, then y = µ + Lz will have mean µ and covariance Σ.

Obviously, the transformation technique depends for its success on the ability
to calculate and then invert the indeﬁnite integral of the required distribution. Such
operations will only be feasible for a limited number of simple distributions, and so
we must turn to alternative approaches in search of a more general strategy. Here
we consider two techniques called rejection sampling and importance sampling. Al-
though mainly limited to univariate distributions and thus not directly applicable to
complex problems in many dimensions, they do form important components in more
general strategies.

11.1.2 Rejection sampling
The rejection sampling framework allows us to sample from relatively complex
distributions, subject to certain constraints. We begin by considering univariate dis-
tributions and discuss the extension to multiple dimensions subsequently.

Suppose we wish to sample from a distribution p(z) that is not one of the simple,
standard distributions considered so far, and that sampling directly from p(z) is dif-
ﬁcult. Furthermore suppose, as is often the case, that we are easily able to evaluate
p(z) for any given value of z, up to some normalizing constant Z, so that

p(z) =

p(z)

(11.13)

where

p(z) can readily be evaluated, but Zp is unknown.

In order to apply rejection sampling, we need some simpler distribution q(z),
sometimes called a proposal distribution, from which we can readily draw samples.

e

e





z












11.1. Basic Sampling Algorithms

529

Figure 11.4 In the rejection sampling method,
samples are drawn from a sim-
ple distribution q(z) and rejected
if they fall
in the grey area be-
tween the unnormalized distribu-
tion
p(z) and the scaled distribu-
tion kq(z). The resulting samples
are distributed according to p(z),
which is the normalized version of
p(z).

kq(z)

˜p(z)

kq(z0)

u0

z0

Exercise 11.6

We next introduce a constant k whose value is chosen such that kq(z) �
p(z) for
all values of z. The function kq(z) is called the comparison function and is illus-
trated for a univariate distribution in Figure 11.4. Each step of the rejection sampler
involves generating two random numbers. First, we generate a number z0 from the
distribution q(z). Next, we generate a number u0 from the uniform distribution over
[0, kq(z0)]. This pair of random numbers has uniform distribution under the curve
p(z0) then the sample is rejected, otherwise
of the function kq(z). Finally, if u0 >
u0 is retained. Thus the pair is rejected if it lies in the grey shaded region in Fig-
p(z),
ure 11.4. The remaining pairs then have uniform distribution under the curve of
and hence the corresponding z values are distributed according to p(z), as desired.
The original values of z are generated from the distribution q(z), and these sam-
p(z)/kq(z), and so the probability that a

ples are then accepted with probability
sample will be accepted is given by

p(accept) =

=

1
k

p(z)/kq(z)} q(z) dz
{
p(z) dz.

(11.14)

Thus the fraction of points that are rejected by this method depends on the ratio of
p(z) to the area under the curve kq(z).
the area under the unnormalized distribution
We therefore see that the constant k should be as small as possible subject to the
limitation that kq(z) must be nowhere less than

p(z).

As an illustration of the use of rejection sampling, consider the task of sampling

from the gamma distribution

Gam(z|a, b) = baza−1 exp(−bz)

Γ(a)

(11.15)

which, for a > 1, has a bell-shaped form, as shown in Figure 11.5. A suitable
proposal distribution is therefore the Cauchy (11.8) because this too is bell-shaped
and because we can use the transformation method, discussed earlier, to sample from
it. We need to generalize the Cauchy slightly to ensure that it nowhere has a smaller
value than the gamma distribution. This can be achieved by transforming a uniform
random variable y using z = b tan y + c, which gives random numbers distributed
according to.

Exercise 11.7



10

20

30

z

(11.16)

q(z) =

k

1 + (z − c)2/b2 .

The minimum reject rate is obtained by setting c = a − 1, b2 = 2a − 1 and choos-
ing the constant k to be as small as possible while still satisfying the requirement
kq(z) �
p(z). The resulting comparison function is also illustrated in Figure 11.5.

11.1.3 Adaptive rejection sampling
In many instances where we might wish to apply rejection sampling, it proves
difﬁcult to determine a suitable analytic form for the envelope distribution q(z). An
alternative approach is to construct the envelope function on the ﬂy based on mea-
sured values of the distribution p(z) (Gilks and Wild, 1992). Construction of an
envelope function is particularly straightforward for cases in which p(z) is log con-
cave, in other words when ln p(z) has derivatives that are nonincreasing functions
of z. The construction of a suitable envelope function is illustrated graphically in
Figure 11.6.

The function ln p(z) and its gradient are evaluated at some initial set of grid
points, and the intersections of the resulting tangent lines are used to construct the
envelope function. Next a sample value is drawn from the envelope distribution.
This is straightforward because the log of the envelope distribution is a succession

Exercise 11.9

Figure 11.6 In the case of distributions that are
log concave, an envelope function
for use in rejection sampling can be
constructed using the tangent lines
computed at a set of grid points. If a
sample point is rejected, it is added
to the set of grid points and used to
reﬁne the envelope distribution.

ln p(z)

z1

z2

z3

z

530

11. SAMPLING METHODS

Figure 11.5 Plot showing the gamma distribu-
tion given by (11.15) as the green
curve, with a scaled Cauchy pro-
posal distribution shown by the red
curve. Samples from the gamma
distribution can be obtained by
sampling from the Cauchy and
then applying the rejection sam-
pling criterion.

p(z)

0.15

0.1

0.05

0

0

11.1. Basic Sampling Algorithms

531

Figure 11.7 Illustrative example of

rejection
sampling involving sampling from a
Gaussian distribution p(z) shown by
the green curve, by using rejection
sampling from a proposal distri-
bution q(z) that
is also Gaussian
and whose scaled version kq(z) is
shown by the red curve.

0.5

p(z)

0.25

0
−5

0

z

5

of linear functions, and hence the envelope distribution itself comprises a piecewise
exponential distribution of the form

q(z) = kiλi exp{−λi(z − zi−1)}

zi−1 < z � zi.

(11.17)

Once a sample has been drawn, the usual rejection criterion can be applied. If the
sample is accepted, then it will be a draw from the desired distribution. If, however,
the sample is rejected, then it is incorporated into the set of grid points, a new tangent
line is computed, and the envelope function is thereby reﬁned. As the number of
grid points increases, so the envelope function becomes a better approximation of
the desired distribution p(z) and the probability of rejection decreases.

A variant of the algorithm exists that avoids the evaluation of derivatives (Gilks,
1992). The adaptive rejection sampling framework can also be extended to distri-
butions that are not log concave, simply by following each rejection sampling step
with a Metropolis-Hastings step (to be discussed in Section 11.2.2), giving rise to
adaptive rejection Metropolis sampling (Gilks et al., 1995).

Clearly for rejection sampling to be of practical value, we require that the com-
parison function be close to the required distribution so that the rate of rejection is
kept to a minimum. Now let us examine what happens when we try to use rejection
sampling in spaces of high dimensionality. Consider, for the sake of illustration,
a somewhat artiﬁcial problem in which we wish to sample from a zero-mean mul-
pI, where I is the unit matrix, by
tivariate Gaussian distribution with covariance σ2
rejection sampling from a proposal distribution that is itself a zero-mean Gaussian
p in order that
distribution having covariance σ2
there exists a k such that kq(z) � p(z). In D-dimensions the optimum value of k
is given by k = (σq/σp)D, as illustrated for D = 1 in Figure 11.7. The acceptance
rate will be the ratio of volumes under p(z) and kq(z), which, because both distribu-
tions are normalized, is just 1/k. Thus the acceptance rate diminishes exponentially
with dimensionality. Even if σq exceeds σp by just one percent, for D = 1, 000 the
acceptance ratio will be approximately 1/20, 000. In this illustrative example the
comparison function is close to the required distribution. For more practical exam-
ples, where the desired distribution may be multimodal and sharply peaked, it will
be extremely difﬁcult to ﬁnd a good proposal distribution and comparison function.

qI. Obviously, we must have σ2

q � σ2

532

11. SAMPLING METHODS

Figure 11.8 Importance sampling addresses the prob-
lem of evaluating the expectation of a func-
tion f (z) with respect to a distribution p(z)
from which it is difﬁcult to draw samples di-
Instead, samples {z(l)} are drawn
rectly.
from a simpler distribution q(z), and the
corresponding terms in the summation are
weighted by the ratios p(z(l))/q(z(l)).

p(z)

q(z)

f(z)

z



L

Furthermore, the exponential decrease of acceptance rate with dimensionality is a
generic feature of rejection sampling. Although rejection can be a useful technique
in one or two dimensions it is unsuited to problems of high dimensionality. It can,
however, play a role as a subroutine in more sophisticated algorithms for sampling
in high dimensional spaces.

11.1.4 Importance sampling
One of the principal reasons for wishing to sample from complicated probability
distributions is to be able to evaluate expectations of the form (11.1). The technique
of importance sampling provides a framework for approximating expectations di-
rectly but does not itself provide a mechanism for drawing samples from distribution
p(z).

The ﬁnite sum approximation to the expectation, given by (11.2), depends on
being able to draw samples from the distribution p(z). Suppose, however, that it is
impractical to sample directly from p(z) but that we can evaluate p(z) easily for any
given value of z. One simplistic strategy for evaluating expectations would be to
discretize z-space into a uniform grid and to evaluate the integrand as a sum of the
form

E[f] 

l=1

p(z(l))f(z(l)).

(11.18)

An obvious problem with this approach is that the number of terms in the summation
grows exponentially with the dimensionality of z. Furthermore, as we have already
noted, the kinds of probability distributions of interest will often have much of their
mass conﬁned to relatively small regions of z space and so uniform sampling will be
very inefﬁcient because in high-dimensional problems, only a very small proportion
of the samples will make a signiﬁcant contribution to the sum. We would really like
to choose the sample points to fall in regions where p(z) is large, or ideally where
the product p(z)f(z) is large.

As in the case of rejection sampling, importance sampling is based on the use
of a proposal distribution q(z) from which it is easy to draw samples, as illustrated
in Figure 11.8. We can then express the expectation in the form of a ﬁnite sum over
















Zq
Zp

l=1

L

=

1
L

=



rl











f(z)

1
L

l=1

l=1

l=1

L

L

L







1
Zq
1
L



p(z)

p(z)

E[f] =

f(z)p(z) dz

= Zq
Zp

q(z) q(z) dz

rlf(z(l)).

samples {z(l)} drawn from q(z)
E[f] =

11.1. Basic Sampling Algorithms

f(z)p(z) dz
f(z) p(z)

q(z) q(z) dz
p(z(l))
q(z(l)) f(z(l)).

The quantities rl = p(z(l))/q(z(l)) are known as importance weights, and they cor-
rect the bias introduced by sampling from the wrong distribution. Note that, unlike
rejection sampling, all of the samples generated are retained.

It will often be the case that the distribution p(z) can only be evaluated up to a
normalization constant, so that p(z) =
p(z) can be evaluated easily,
whereas Zp is unknown. Similarly, we may wish to use an importance sampling
distribution q(z) =

q(z)/Zq, which has the same property. We then have

p(z)/Zp where

rl =

p(z(l))/
where
Zp/Zq with the result

q(z(l)). We can use the same sample set to evaluate the ratio

p(z) dz =

q(z) q(z) dz

Zp
Zq

=



and hence

where we have deﬁned

wl =

E[f] 

wlf(z(l))

rl

rm

m

p(z(l))/q(z(l))
p(z(m))/q(z(m)) .

m

As with rejection sampling, the success of the importance sampling approach
depends crucially on how well the sampling distribution q(z) matches the desired

533

(11.19)

(11.20)

(11.21)

(11.22)

(11.23)












534

11. SAMPLING METHODS

distribution p(z). If, as is often the case, p(z)f(z) is strongly varying and has a sig-
niﬁcant proportion of its mass concentrated over relatively small regions of z space,
then the set of importance weights {rl} may be dominated by a few weights hav-
ing large values, with the remaining weights being relatively insigniﬁcant. Thus the
effective sample size can be much smaller than the apparent sample size L. The prob-
lem is even more severe if none of the samples falls in the regions where p(z)f(z)
is large. In that case, the apparent variances of rl and rlf(z(l)) may be small even
though the estimate of the expectation may be severely wrong. Hence a major draw-
back of the importance sampling method is the potential to produce results that are
arbitrarily in error and with no diagnostic indication. This also highlights a key re-
quirement for the sampling distribution q(z), namely that it should not be small or
zero in regions where p(z) may be signiﬁcant.

For distributions deﬁned in terms of a graphical model, we can apply the impor-
tance sampling technique in various ways. For discrete variables, a simple approach
is called uniform sampling. The joint distribution for a directed graph is deﬁned
by (11.4). Each sample from the joint distribution is obtained by ﬁrst setting those
variables zi that are in the evidence set equal to their observed values. Each of the
remaining variables is then sampled independently from a uniform distribution over
the space of possible instantiations. To determine the corresponding weight associ-
q(z) is uniform over
ated with a sample z(l), we note that the sampling distribution
p(z), where x denotes the subset of
the possible choices for z, and that
variables that are observed, and the equality follows from the fact that every sample
z that is generated is necessarily consistent with the evidence. Thus the weights rl
are simply proportional to p(z). Note that the variables can be sampled in any order.
This approach can yield poor results if the posterior distribution is far from uniform,
as is often the case in practice.

p(z|x) =

An improvement on this approach is called likelihood weighted sampling (Fung
and Chang, 1990; Shachter and Peot, 1990) and is based on ancestral sampling of
the variables. For each variable in turn, if that variable is in the evidence set, then it
is just set to its instantiated value. If it is not in the evidence set, then it is sampled
from the conditional distribution p(zi|pai) in which the conditioning variables are
set to their currently sampled values. The weighting associated with the resulting
sample z is then given by

r(z) =

p(zi|pai)
p(zi|pai)

zi∈e

zi∈e

p(zi|pai)

1

=

zi∈e

p(zi|pai).

(11.24)

This method can be further extended using self-importance sampling (Shachter and
Peot, 1990) in which the importance sampling distribution is continually updated to
reﬂect the current estimated posterior distribution.

11.1.5 Sampling-importance-resampling
The rejection sampling method discussed in Section 11.1.2 depends in part for
its success on the determination of a suitable value for the constant k. For many
pairs of distributions p(z) and q(z), it will be impractical to determine a suitable










{










11.1. Basic Sampling Algorithms

535

value for k in that any value that is sufﬁciently large to guarantee a bound on the
desired distribution will lead to impractically small acceptance rates.

As in the case of rejection sampling, the sampling-importance-resampling (SIR)
approach also makes use of a sampling distribution q(z) but avoids having to de-
termine the constant k. There are two stages to the scheme.
In the ﬁrst stage,
L samples z(1), . . . , z(L) are drawn from q(z). Then in the second stage, weights
w1, . . . , wL are constructed using (11.23). Finally, a second set of L samples is
drawn from the discrete distribution (z(1), . . . , z(L)) with probabilities given by the
weights (w1, . . . , wL).

The resulting L samples are only approximately distributed according to p(z),
but the distribution becomes correct in the limit L → ∞. To see this, consider the
univariate case, and note that the cumulative distribution of the resampled values is
given by

p(z � a) =

=

wl

l:z(l)�a

l I(z(l) � a)

p(z(l))/q(z(l))

p(z(l))/q(z(l))

l

(11.25)

where I(.) is the indicator function (which equals 1 if its argument is true and 0
otherwise). Taking the limit L → ∞, and assuming suitable regularity of the dis-
tributions, we can replace the sums by integrals weighted according to the original
sampling distribution q(z)

I(z � a){

p(z)/q(z)} q(z) dz

p(z)/q(z)} q(z) dz

I(z � a)

p(z) dz

p(z) dz

p(z � a) =

=

=

I(z � a)p(z) dz

(11.26)

which is the cumulative distribution function of p(z). Again, we see that the normal-
ization of p(z) is not required.

For a ﬁnite value of L, and a given initial sample set, the resampled values will
only approximately be drawn from the desired distribution. As with rejection sam-
pling, the approximation improves as the sampling distribution q(z) gets closer to
the desired distribution p(z). When q(z) = p(z), the initial samples (z(1), . . . , z(L))
have the desired distribution, and the weights wn = 1/L so that the resampled values
also have the desired distribution.

If moments with respect to the distribution p(z) are required, then they can be






L

l=1






[

L



=



536

11. SAMPLING METHODS

evaluated directly using the original samples together with the weights, because

E[f(z)] =

f(z)p(z) dz

f(z)[

p(z)/q(z)]q(z) dz

p(z)/q(z)]q(z) dz

wlf(zl).

(11.27)

11.1.6 Sampling and the EM algorithm
In addition to providing a mechanism for direct implementation of the Bayesian
framework, Monte Carlo methods can also play a role in the frequentist paradigm,
for example to ﬁnd maximum likelihood solutions. In particular, sampling methods
can be used to approximate the E step of the EM algorithm for models in which the
E step cannot be performed analytically. Consider a model with hidden variables
Z, visible (observed) variables X, and parameters θ. The function that is optimized
with respect to θ in the M step is the expected complete-data log likelihood, given
by

Q(θ, θold) =

p(Z|X, θold) ln p(Z, X|θ) dZ.

(11.28)

We can use sampling methods to approximate this integral by a ﬁnite sum over sam-
ples {Z(l)}, which are drawn from the current estimate for the posterior distribution
p(Z|X, θold), so that

Q(θ, θold) 

1
L

ln p(Z(l), X|θ).

l=1

(11.29)

The Q function is then optimized in the usual way in the M step. This procedure is
called the Monte Carlo EM algorithm.

It is straightforward to extend this to the problem of ﬁnding the mode of the
posterior distribution over θ (the MAP estimate) when a prior distribution p(θ) has
been deﬁned, simply by adding ln p(θ) to the function Q(θ, θold) before performing
the M step.

A particular instance of the Monte Carlo EM algorithm, called stochastic EM,
arises if we consider a ﬁnite mixture model, and draw just one sample at each E step.
Here the latent variable Z characterizes which of the K components of the mixture
is responsible for generating each data point. In the E step, a sample of Z is taken
from the posterior distribution p(Z|X, θold) where X is the data set. This effectively
makes a hard assignment of each data point to one of the components in the mixture.
In the M step, this sampled approximation to the posterior distribution is used to
update the model parameters in the usual way.






L

11.2. Markov Chain Monte Carlo

537

Now suppose we move from a maximum likelihood approach to a full Bayesian
treatment in which we wish to sample from the posterior distribution over the param-
eter vector θ. In principle, we would like to draw samples from the joint posterior
p(θ, Z|X), but we shall suppose that this is computationally difﬁcult. Suppose fur-
ther that it is relatively straightforward to sample from the complete-data parameter
posterior p(θ|Z, X). This inspires the data augmentation algorithm, which alter-
nates between two steps known as the I-step (imputation step, analogous to an E
step) and the P-step (posterior step, analogous to an M step).

IP Algorithm

I-step. We wish to sample from p(Z|X) but we cannot do this directly. We

therefore note the relation

p(Z|X) =

p(Z|θ, X)p(θ|X) dθ

(11.30)

and hence for l = 1, . . . , L we ﬁrst draw a sample θ(l) from the current esti-
mate for p(θ|X), and then use this to draw a sample Z(l) from p(Z|θ(l), X).

P-step. Given the relation

p(θ|X) =

p(θ|Z, X)p(Z|X) dZ

(11.31)

we use the samples {Z(l)} obtained from the I-step to compute a revised
estimate of the posterior distribution over θ given by

p(θ|X) 

1
L

p(θ|Z(l), X).

l=1

(11.32)

By assumption, it will be feasible to sample from this approximation in the
I-step.

Note that we are making a (somewhat artiﬁcial) distinction between parameters θ
and hidden variables Z. From now on, we blur this distinction and focus simply on
the problem of drawing samples from a given posterior distribution.

11.2. Markov Chain Monte Carlo

In the previous section, we discussed the rejection sampling and importance sam-
pling strategies for evaluating expectations of functions, and we saw that they suffer
from severe limitations particularly in spaces of high dimensionality. We therefore
turn in this section to a very general and powerful framework called Markov chain
Monte Carlo (MCMC), which allows sampling from a large class of distributions,












538

11. SAMPLING METHODS

Section 11.2.1

A(z, z(τ )) = min

1,

.

(11.33)

p(z)
p(z(τ ))

and which scales well with the dimensionality of the sample space. Markov chain
Monte Carlo methods have their origins in physics (Metropolis and Ulam, 1949),
and it was only towards the end of the 1980s that they started to have a signiﬁcant
impact in the ﬁeld of statistics.

As with rejection and importance sampling, we again sample from a proposal
distribution. This time, however, we maintain a record of the current state z(τ ), and
the proposal distribution q(z|z(τ )) depends on this current state, and so the sequence
of samples z(1), z(2), . . . forms a Markov chain. Again, if we write p(z) =
p(z)/Zp,
p(z) can readily be evaluated for any given value of z, although
we will assume that
the value of Zp may be unknown. The proposal distribution itself is chosen to be
sufﬁciently simple that it is straightforward to draw samples from it directly. At
each cycle of the algorithm, we generate a candidate sample z from the proposal
distribution and then accept the sample according to an appropriate criterion.

In the basic Metropolis algorithm (Metropolis et al., 1953), we assume that the
proposal distribution is symmetric, that is q(zA|zB) = q(zB|zA) for all values of
zA and zB. The candidate sample is then accepted with probability

This can be achieved by choosing a random number u with uniform distribution over
the unit interval (0, 1) and then accepting the sample if A(z, z(τ )) > u. Note that
if the step from z(τ ) to z causes an increase in the value of p(z), then the candidate
point is certain to be kept.

If the candidate sample is accepted, then z(τ +1) = z, otherwise the candidate
point z is discarded, z(τ +1) is set to z(τ ) and another candidate sample is drawn
from the distribution q(z|z(τ +1)). This is in contrast to rejection sampling, where re-
jected samples are simply discarded. In the Metropolis algorithm when a candidate
point is rejected, the previous sample is included instead in the ﬁnal list of samples,
leading to multiple copies of samples. Of course, in a practical implementation,
only a single copy of each retained sample would be kept, along with an integer
weighting factor recording how many times that state appears. As we shall see, as
long as q(zA|zB) is positive for any values of zA and zB (this is a sufﬁcient but
not necessary condition), the distribution of z(τ ) tends to p(z) as τ → ∞. It should
be emphasized, however, that the sequence z(1), z(2), . . . is not a set of independent
samples from p(z) because successive samples are highly correlated. If we wish to
obtain independent samples, then we can discard most of the sequence and just re-
tain every M th sample. For M sufﬁciently large, the retained samples will for all
practical purposes be independent. Figure 11.9 shows a simple illustrative exam-
ple of sampling from a two-dimensional Gaussian distribution using the Metropolis
algorithm in which the proposal distribution is an isotropic Gaussian.

Further insight into the nature of Markov chain Monte Carlo algorithms can be
gleaned by looking at the properties of a speciﬁc example, namely a simple random

11.2. Markov Chain Monte Carlo

539

Figure 11.9 A simple illustration using Metropo-
lis algorithm to sample from a
Gaussian distribution whose one
standard-deviation contour is shown
by the ellipse. The proposal distribu-
tion is an isotropic Gaussian distri-
bution whose standard deviation is
0.2. Steps that are accepted are
shown as green lines, and rejected
steps are shown in red. A total of
150 candidate samples are gener-
ated, of which 43 are rejected.

3

2.5

2

1.5

1

0.5

0

0

0.5

1

1.5

2

2.5

3

Exercise 11.10

walk. Consider a state space z consisting of the integers, with probabilities

p(z(τ +1) = z(τ )) = 0.5
p(z(τ +1) = z(τ ) + 1) = 0.25
p(z(τ +1) = z(τ ) − 1) = 0.25

(11.34)
(11.35)
(11.36)

where z(τ ) denotes the state at step τ . If the initial state is z(1) = 0, then by sym-
metry the expected state at time τ will also be zero E[z(τ )] = 0, and similarly it is
easily seen that E[(z(τ ))2] = τ /2. Thus after τ steps, the random walk has only trav-
elled a distance that on average is proportional to the square root of τ . This square
root dependence is typical of random walk behaviour and shows that random walks
are very inefﬁcient in exploring the state space. As we shall see, a central goal in
designing Markov chain Monte Carlo methods is to avoid random walk behaviour.

11.2.1 Markov chains
Before discussing Markov chain Monte Carlo methods in more detail, it is use-
ful to study some general properties of Markov chains in more detail. In particular,
we ask under what circumstances will a Markov chain converge to the desired dis-
tribution. A ﬁrst-order Markov chain is deﬁned to be a series of random variables
z(1), . . . , z(M ) such that the following conditional independence property holds for
m ∈ {1, . . . , M − 1}

p(z(m+1)|z(1), . . . , z(m)) = p(z(m+1)|z(m)).

(11.37)

This of course can be represented as a directed graph in the form of a chain, an ex-
ample of which is shown in Figure 8.38. We can then specify the Markov chain by
giving the probability distribution for the initial variable p(z(0)) together with the





z

z(m)

K







540

11. SAMPLING METHODS

conditional probabilities for subsequent variables in the form of transition probabil-
ities Tm(z(m), z(m+1)) ≡ p(z(m+1)|z(m)). A Markov chain is called homogeneous
if the transition probabilities are the same for all m.
The marginal probability for a particular variable can be expressed in terms of

the marginal probability for the previous variable in the chain in the form

p(z(m+1)) =

p(z(m+1)|z(m))p(z(m)).

(11.38)

A distribution is said to be invariant, or stationary, with respect to a Markov chain
if each step in the chain leaves that distribution invariant. Thus, for a homogeneous
Markov chain with transition probabilities T (z, z), the distribution p(z) is invariant
if

p(z) =

T (z, z)p(z).

(11.39)

Note that a given Markov chain may have more than one invariant distribution. For
instance, if the transition probabilities are given by the identity transformation, then
any distribution will be invariant.

A sufﬁcient (but not necessary) condition for ensuring that the required distribu-
tion p(z) is invariant is to choose the transition probabilities to satisfy the property
of detailed balance, deﬁned by

p(z)T (z, z) = p(z)T (z, z)

(11.40)

for the particular distribution p(z).
It is easily seen that a transition probability
that satisﬁes detailed balance with respect to a particular distribution will leave that
distribution invariant, because

p(z)T (z, z) =

z

z

p(z)T (z, z) = p(z)

p(z|z) = p(z).

(11.41)

z

A Markov chain that respects detailed balance is said to be reversible.

Our goal is to use Markov chains to sample from a given distribution. We can
achieve this if we set up a Markov chain such that the desired distribution is invariant.
However, we must also require that for m → ∞, the distribution p(z(m)) converges
to the required invariant distribution p(z), irrespective of the choice of initial dis-
tribution p(z(0)). This property is called ergodicity, and the invariant distribution
is then called the equilibrium distribution. Clearly, an ergodic Markov chain can
have only one equilibrium distribution. It can be shown that a homogeneous Markov
chain will be ergodic, subject only to weak restrictions on the invariant distribution
and the transition probabilities (Neal, 1993).

In practice we often construct the transition probabilities from a set of ‘base’
transitions B1, . . . , BK. This can be achieved through a mixture distribution of the
form

T (z, z) =

αkBk(z, z)

k=1

(11.42)
















11.2. Markov Chain Monte Carlo

541

for some set of mixing coefﬁcients α1, . . . , αK satisfying αk � 0 and
k αk = 1.
Alternatively, the base transitions may be combined through successive application,
so that

T (z, z) =

. . .

z1

zn−1

B1(z, z1) . . . BK−1(zK−2, zK−1)BK(zK−1, z).

(11.43)

If a distribution is invariant with respect to each of the base transitions, then obvi-
ously it will also be invariant with respect to either of the T (z, z) given by (11.42)
or (11.43). For the case of the mixture (11.42), if each of the base transitions sat-
isﬁes detailed balance, then the mixture transition T will also satisfy detailed bal-
ance. This does not hold for the transition probability constructed using (11.43), al-
though by symmetrizing the order of application of the base transitions, in the form
B1, B2, . . . , BK, BK, . . . , B2, B1, detailed balance can be restored. A common ex-
ample of the use of composite transition probabilities is where each base transition
changes only a subset of the variables.

11.2.2 The Metropolis-Hastings algorithm
Earlier we introduced the basic Metropolis algorithm, without actually demon-
strating that it samples from the required distribution. Before giving a proof, we
ﬁrst discuss a generalization, known as the Metropolis-Hastings algorithm (Hast-
ings, 1970), to the case where the proposal distribution is no longer a symmetric
function of its arguments. In particular at step τ of the algorithm, in which the cur-
rent state is z(τ ), we draw a sample z from the distribution qk(z|z(τ )) and then
accept it with probability Ak(z, zτ ) where

Ak(z, z(τ )) = min

1,

.

(11.44)

p(z)qk(z(τ )|z)
p(z(τ ))qk(z|z(τ ))

Here k labels the members of the set of possible transitions being considered. Again,
the evaluation of the acceptance criterion does not require knowledge of the normal-
izing constant Zp in the probability distribution p(z) =
p(z)/Zp. For a symmetric
proposal distribution the Metropolis-Hastings criterion (11.44) reduces to the stan-
dard Metropolis criterion given by (11.33).

We can show that p(z) is an invariant distribution of the Markov chain deﬁned
by the Metropolis-Hastings algorithm by showing that detailed balance, deﬁned by
(11.40), is satisﬁed. Using (11.44) we have

p(z)qk(z|z)Ak(z, z) = min (p(z)qk(z|z), p(z)qk(z|z))
= min (p(z)qk(z|z), p(z)qk(z|z))
= p(z)qk(z|z)Ak(z, z)

(11.45)

as required.

The speciﬁc choice of proposal distribution can have a marked effect on the
performance of the algorithm. For continuous state spaces, a common choice is a
Gaussian centred on the current state, leading to an important trade-off in determin-
ing the variance parameter of this distribution.
If the variance is small, then the

542

11. SAMPLING METHODS

Figure 11.10 Schematic illustration of the use of an isotropic
Gaussian proposal distribution (blue circle) to
sample from a correlated multivariate Gaussian
distribution (red ellipse) having very different stan-
dard deviations in different directions, using the
Metropolis-Hastings algorithm.
In order to keep
the rejection rate low, the scale ρ of the proposal
distribution should be on the order of the smallest
standard deviation σmin, which leads to random
walk behaviour in which the number of steps sep-
arating states that are approximately independent
is of order (σmax/σmin)2 where σmax is the largest
standard deviation.

σmin

σmax

ρ

proportion of accepted transitions will be high, but progress through the state space
takes the form of a slow random walk leading to long correlation times. However,
if the variance parameter is large, then the rejection rate will be high because, in the
kind of complex problems we are considering, many of the proposed steps will be
to states for which the probability p(z) is low. Consider a multivariate distribution
p(z) having strong correlations between the components of z, as illustrated in Fig-
ure 11.10. The scale ρ of the proposal distribution should be as large as possible
without incurring high rejection rates. This suggests that ρ should be of the same
order as the smallest length scale σmin. The system then explores the distribution
along the more extended direction by means of a random walk, and so the number
of steps to arrive at a state that is more or less independent of the original state is
of order (σmax/σmin)2. In fact in two dimensions, the increase in rejection rate as ρ
increases is offset by the larger steps sizes of those transitions that are accepted, and
more generally for a multivariate Gaussian the number of steps required to obtain
independent samples scales like (σmax/σ2)2 where σ2 is the second-smallest stan-
dard deviation (Neal, 1993). These details aside, it remains the case that if the length
scales over which the distributions vary are very different in different directions, then
the Metropolis Hastings algorithm can have very slow convergence.

11.3. Gibbs Sampling

Gibbs sampling (Geman and Geman, 1984) is a simple and widely applicable Markov
chain Monte Carlo algorithm and can be seen as a special case of the Metropolis-
Hastings algorithm.

Consider the distribution p(z) = p(z1, . . . , zM ) from which we wish to sample,
and suppose that we have chosen some initial state for the Markov chain. Each step
of the Gibbs sampling procedure involves replacing the value of one of the variables
by a value drawn from the distribution of that variable conditioned on the values of
the remaining variables. Thus we replace zi by a value drawn from the distribution
p(zi|z\i), where zi denotes the ith component of z, and z\i denotes z1, . . . , zM but
with zi omitted. This procedure is repeated either by cycling through the variables

11.3. Gibbs Sampling

543

in some particular order or by choosing the variable to be updated at each step at
random from some distribution.

For example, suppose we have a distribution p(z1, z2, z3) over three variables,
3 . We ﬁrst
obtained by sampling from the conditional distri-

and at step τ of the algorithm we have selected values z(τ )
replace z(τ )
bution

1 by a new value z(τ +1)

1 , z(τ )

and z(τ )

2

1

p(z1|z(τ )

2 , z(τ )
3 ).
obtained by sampling from the conditional

(11.46)

Next we replace z(τ )
distribution

2

by a value z(τ +1)

2

(11.47)
so that the new value for z1 is used straight away in subsequent sampling steps. Then
we update z3 with a sample z(τ +1)

drawn from

p(z2|z(τ +1)

, z(τ )
3 )

1

3

p(z3|z(τ +1)

1

, z(τ +1)

2

)

(11.48)

and so on, cycling through the three variables in turn.

Gibbs Sampling

1. Initialize {zi : i = 1, . . . , M}
2. For τ = 1, . . . , T :
– Sample z(τ +1)
– Sample z(τ +1)

∼ p(z1|z(τ )
∼ p(z2|z(τ +1)

1

2

1

2 , z(τ )

3 , . . . , z(τ )
M ).
3 , . . . , z(τ )
, z(τ )
M ).

– Sample z(τ +1)

j

∼ p(zj|z(τ +1)

1

, . . . , z(τ +1)

j−1 , z(τ )

j+1, . . . , z(τ )
M ).

...

...

– Sample z(τ +1)

M

∼ p(zM|z(τ +1)

1

, z(τ +1)

2

, . . . , z(τ +1)
M−1 ).

Josiah Willard Gibbs
1839–1903

Gibbs spent almost his entire life liv-
ing in a house built by his father in
New Haven, Connecticut.
In 1863,
Gibbs was granted the ﬁrst PhD in
engineering in the United States,
and in 1871 he was appointed to
the ﬁrst chair of mathematical physics in the United

States at Yale, a post for which he received no salary
because at the time he had no publications. He de-
veloped the ﬁeld of vector analysis and made contri-
butions to crystallography and planetary orbits. His
most famous work, entitled OntheEquilibriumofHet-
erogeneous Substances, laid the foundations for the
science of physical chemistry.

544

11. SAMPLING METHODS

To show that this procedure samples from the required distribution, we ﬁrst of
all note that the distribution p(z) is an invariant of each of the Gibbs sampling steps
individually and hence of the whole Markov chain. This follows from the fact that
when we sample from p(zi|{z\i), the marginal distribution p(z\i) is clearly invariant
because the value of z\i is unchanged. Also, each step by deﬁnition samples from the
correct conditional distribution p(zi|z\i). Because these conditional and marginal
distributions together specify the joint distribution, we see that the joint distribution
is itself invariant.

The second requirement to be satisﬁed in order that the Gibbs sampling proce-
dure samples from the correct distribution is that it be ergodic. A sufﬁcient condition
for ergodicity is that none of the conditional distributions be anywhere zero. If this
is the case, then any point in z space can be reached from any other point in a ﬁnite
number of steps involving one update of each of the component variables. If this
requirement is not satisﬁed, so that some of the conditional distributions have zeros,
then ergodicity, if it applies, must be proven explicitly.

The distribution of initial states must also be speciﬁed in order to complete the
algorithm, although samples drawn after many iterations will effectively become
independent of this distribution. Of course, successive samples from the Markov
chain will be highly correlated, and so to obtain samples that are nearly independent
it will be necessary to subsample the sequence.

We can obtain the Gibbs sampling procedure as a particular instance of the
Metropolis-Hastings algorithm as follows. Consider a Metropolis-Hastings sampling
step involving the variable zk in which the remaining variables z\k remain ﬁxed, and
for which the transition probability from z to z is given by qk(z|z) = p(z
k|z\k).
\k = z\k because these components are unchanged by the sampling
We note that z
step. Also, p(z) = p(zk|z\k)p(z\k). Thus the factor that determines the acceptance
probability in the Metropolis-Hastings (11.44) is given by

k|z

\k)p(z

p(z
p(zk|z\k)p(z\k)p(z

\k)p(zk|z
\k)
k|z\k)

= 1

(11.49)

A(z, z) = p(z)qk(z|z)
p(z)qk(z|z)

=

where we have used z
accepted.

\k = z\k. Thus the Metropolis-Hastings steps are always
As with the Metropolis algorithm, we can gain some insight into the behaviour of
Gibbs sampling by investigating its application to a Gaussian distribution. Consider
a correlated Gaussian in two variables, as illustrated in Figure 11.11, having con-
ditional distributions of width l and marginal distributions of width L. The typical
step size is governed by the conditional distributions and will be of order l. Because
the state evolves according to a random walk, the number of steps needed to obtain
independent samples from the distribution will be of order (L/l)2. Of course if the
Gaussian distribution were uncorrelated, then the Gibbs sampling procedure would
be optimally efﬁcient. For this simple problem, we could rotate the coordinate sys-
tem in order to decorrelate the variables. However, in practical applications it will
generally be infeasible to ﬁnd such transformations.

One approach to reducing random walk behaviour in Gibbs sampling is called
over-relaxation (Adler, 1981). In its original form, this applies to problems for which

z2

Figure 11.11 Illustration of Gibbs sampling by alter-
nate updates of two variables whose
distribution is a correlated Gaussian.
The step size is governed by the stan-
dard deviation of the conditional distri-
bution (green curve), and is O(l), lead-
ing to slow progress in the direction of
elongation of the joint distribution (red
ellipse). The number of steps needed
to obtain an independent sample from
the distribution is O((L/l)2).

11.3. Gibbs Sampling

545

L

l

z1

the conditional distributions are Gaussian, which represents a more general class of
distributions than the multivariate Gaussian because, for example, the non-Gaussian
distribution p(z, y) ∝ exp(−z2y2) has Gaussian conditional distributions. At each
step of the Gibbs sampling algorithm, the conditional distribution for a particular
component zi has some mean µi and some variance σ2
i . In the over-relaxation frame-
work, the value of zi is replaced with

zi = µi + α(zi − µi) + σi(1 − α2

i )1/2ν

(11.50)

where ν is a Gaussian random variable with zero mean and unit variance, and α
is a parameter such that −1 < α < 1. For α = 0, the method is equivalent to
standard Gibbs sampling, and for α < 0 the step is biased to the opposite side of the
mean. This step leaves the desired distribution invariant because if zi has mean µi
i , then so too does zi. The effect of over-relaxation is to encourage
and variance σ2
directed motion through state space when the variables are highly correlated. The
framework of ordered over-relaxation (Neal, 1999) generalizes this approach to non-
Gaussian distributions.

The practical applicability of Gibbs sampling depends on the ease with which
samples can be drawn from the conditional distributions p(zk|z\k). In the case of
probability distributions speciﬁed using graphical models, the conditional distribu-
tions for individual nodes depend only on the variables in the corresponding Markov
blankets, as illustrated in Figure 11.12. For directed graphs, a wide choice of condi-
tional distributions for the individual nodes conditioned on their parents will lead to
conditional distributions for Gibbs sampling that are log concave. The adaptive re-
jection sampling methods discussed in Section 11.1.3 therefore provide a framework
for Monte Carlo sampling from directed graphs with broad applicability.

If the graph is constructed using distributions from the exponential family, and
if the parent-child relationships preserve conjugacy, then the full conditional distri-
butions arising in Gibbs sampling will have the same functional form as the orig-

546

11. SAMPLING METHODS

Figure 11.12 The Gibbs sampling method requires samples
to be drawn from the conditional distribution of a variable condi-
tioned on the remaining variables. For graphical models, this
conditional distribution is a function only of the states of the
nodes in the Markov blanket. For an undirected graph this com-
prises the set of neighbours, as shown on the left, while for a
directed graph the Markov blanket comprises the parents, the
children, and the co-parents, as shown on the right.

inal conditional distributions (conditioned on the parents) deﬁning each node, and
so standard sampling techniques can be employed. In general, the full conditional
distributions will be of a complex form that does not permit the use of standard sam-
pling algorithms. However, if these conditionals are log concave, then sampling can
be done efﬁciently using adaptive rejection sampling (assuming the corresponding
variable is a scalar).

If, at each stage of the Gibbs sampling algorithm, instead of drawing a sample
from the corresponding conditional distribution, we make a point estimate of the
variable given by the maximum of the conditional distribution, then we obtain the
iterated conditional modes (ICM) algorithm discussed in Section 8.3.3. Thus ICM
can be seen as a greedy approximation to Gibbs sampling.

Because the basic Gibbs sampling technique considers one variable at a time,
there are strong dependencies between successive samples. At the opposite extreme,
if we could draw samples directly from the joint distribution (an operation that we
are supposing is intractable), then successive samples would be independent. We can
hope to improve on the simple Gibbs sampler by adopting an intermediate strategy in
which we sample successively from groups of variables rather than individual vari-
ables. This is achieved in the blocking Gibbs sampling algorithm by choosing blocks
of variables, not necessarily disjoint, and then sampling jointly from the variables in
each block in turn, conditioned on the remaining variables (Jensen et al., 1995).



11.4. Slice Sampling

We have seen that one of the difﬁculties with the Metropolis algorithm is the sensi-
tivity to step size. If this is too small, the result is slow decorrelation due to random
walk behaviour, whereas if it is too large the result is inefﬁciency due to a high rejec-
tion rate. The technique of slice sampling (Neal, 2003) provides an adaptive step size
that is automatically adjusted to match the characteristics of the distribution. Again
it requires that we are able to evaluate the unnormalized distribution

p(z).

Consider ﬁrst the univariate case. Slice sampling involves augmenting z with
an additional variable u and then drawing samples from the joint (z, u) space. We
shall see another example of this approach when we discuss hybrid Monte Carlo in
Section 11.5. The goal is to sample uniformly from the area under the distribution

e

˜p(z)

u

z(τ )

(a)

given by


e







z













547

z

(11.51)

(11.52)

11.4. Slice Sampling

˜p(z)

zmin

zmax

u

z(τ )

(b)

(a) For a given value z(τ ), a value of u is chosen uniformly in
p(z(τ )), which then deﬁnes a ‘slice’ through the distribution, shown by the solid horizontal
(b) Because it is infeasible to sample directly from a slice, a new sample of z is drawn from a region

Figure 11.13 Illustration of slice sampling.
the region 0 � u �
lines.
zmin � z � zmax, which contains the previous value z(τ ).

p(z, u) =

1/Zp
0

if 0 � u �
otherwise

p(z)

where Zp =

p(z) dz. The marginal distribution over z is given by

p(z, u) du =

p(z)

1
Zp

0

du =

p(z)
Zp

= p(z)

p(z) and then sample u uniformly in the range 0 � u �

p(z) > u}. This is illustrated in Figure 11.13(a).

and so we can sample from p(z) by sampling from
p(z, u) and then ignoring the u
values. This can be achieved by alternately sampling z and u. Given the value of z
p(z), which is
we evaluate
straightforward. Then we ﬁx u and sample z uniformly from the ‘slice’ through the
distribution deﬁned by {z :
In practice, it can be difﬁcult to sample directly from a slice through the distribu-
tion and so instead we deﬁne a sampling scheme that leaves the uniform distribution
p(z, u) invariant, which can be achieved by ensuring that detailed balance is
under
satisﬁed. Suppose the current value of z is denoted z(τ ) and that we have obtained
a corresponding sample u. The next value of z is obtained by considering a region
zmin � z � zmax that contains z(τ ). It is in the choice of this region that the adap-
tation to the characteristic length scales of the distribution takes place. We want the
region to encompass as much of the slice as possible so as to allow large moves in z
space while having as little as possible of this region lying outside the slice, because
this makes the sampling less efﬁcient.

One approach to the choice of region involves starting with a region containing
z(τ ) having some width w and then testing each of the end points to see if they lie
within the slice.
If either end point does not, then the region is extended in that
direction by increments of value w until the end point lies outside the region. A
candidate value z is then chosen uniformly from this region, and if it lies within the
slice, then it forms z(τ +1). If it lies outside the slice, then the region is shrunk such
that z forms an end point and such that the region still contains z(τ ). Then another

548

11. SAMPLING METHODS

candidate point is drawn uniformly from this reduced region and so on, until a value
of z is found that lies within the slice.

Slice sampling can be applied to multivariate distributions by repeatedly sam-
pling each variable in turn, in the manner of Gibbs sampling. This requires that
we are able to compute, for each component zi, a function that is proportional to
p(zi|z\i).

11.5. The Hybrid Monte Carlo Algorithm

As we have already noted, one of the major limitations of the Metropolis algorithm
is that it can exhibit random walk behaviour whereby the distance traversed through
the state space grows only as the square root of the number of steps. The problem
cannot be resolved simply by taking bigger steps as this leads to a high rejection rate.
In this section, we introduce a more sophisticated class of transitions based on an
analogy with physical systems and that has the property of being able to make large
changes to the system state while keeping the rejection probability small. It is ap-
plicable to distributions over continuous variables for which we can readily evaluate
the gradient of the log probability with respect to the state variables. We will discuss
the dynamical systems framework in Section 11.5.1, and then in Section 11.5.2 we
explain how this may be combined with the Metropolis algorithm to yield the pow-
erful hybrid Monte Carlo algorithm. A background in physics is not required as this
section is self-contained and the key results are all derived from ﬁrst principles.

11.5.1 Dynamical systems
The dynamical approach to stochastic sampling has its origins in algorithms for
simulating the behaviour of physical systems evolving under Hamiltonian dynam-
ics. In a Markov chain Monte Carlo simulation, the goal is to sample from a given
probability distribution p(z). The framework of Hamiltonian dynamics is exploited
by casting the probabilistic simulation in the form of a Hamiltonian system. In order
to remain in keeping with the literature in this area, we make use of the relevant
dynamical systems terminology where appropriate, which will be deﬁned as we go
along.

The dynamics that we consider corresponds to the evolution of the state variable
z = {zi} under continuous time, which we denote by τ . Classical dynamics is de-
scribed by Newton’s second law of motion in which the acceleration of an object is
proportional to the applied force, corresponding to a second-order differential equa-
tion over time. We can decompose a second-order equation into two coupled ﬁrst-
order equations by introducing intermediate momentum variables r, corresponding
to the rate of change of the state variables z, having components

ri =

dzi
dτ

(11.53)

where the zi can be regarded as position variables in this dynamics perspective. Thus



.

where E(z) is interpreted as the potential energy of the system when in state z. The
system acceleration is the rate of change of momentum and is given by the applied
force, which itself is the negative gradient of the potential energy

dri
dτ

= −

∂E(z)

∂zi

(11.54)

(11.55)

11.5. The Hybrid Monte Carlo Algorithm

549

for each position variable there is a corresponding momentum variable, and the joint
space of position and momentum variables is called phase space.

Without loss of generality, we can write the probability distribution p(z) in the

form

p(z) =

1
Zp

exp (−E(z))

It is convenient to reformulate this dynamical system using the Hamiltonian

framework. To do this, we ﬁrst deﬁne the kinetic energy by

K(r) =

1
2r2 =

1
2

r2
i .

i

(11.56)

The total energy of the system is then the sum of its potential and kinetic energies

H(z, r) = E(z) + K(r)

(11.57)

Exercise 11.15

where H is the Hamiltonian function. Using (11.53), (11.55), (11.56), and (11.57),
we can now express the dynamics of the system in terms of the Hamiltonian equa-
tions given by

dzi
dτ
dri
dτ

= ∂H
∂ri
= −

∂H
∂zi

.

(11.58)

(11.59)

William Hamilton
1805–1865

William Rowan Hamilton was an
Irish mathematician and physicist,
and child prodigy, who was ap-
pointed Professor of Astronomy at
Trinity College, Dublin, in 1827, be-
fore he had even graduated. One
of Hamilton’s most important contributions was a new
formulation of dynamics, which played a signiﬁcant
role in the later development of quantum mechanics.

His other great achievement was the development of
quaternions, which generalize the concept of complex
numbers by introducing three distinct square roots of
minus one, which satisfy i2 = j2 = k2 = ijk = −1.
It is said that these equations occurred to him while
walking along the Royal Canal in Dublin with his wife,
on 16 October 1843, and he promptly carved the
equations into the side of Broome bridge. Although
there is no longer any evidence of the carving, there is
now a stone plaque on the bridge commemorating the
discovery and displaying the quaternion equations.











i

i

i










550

11. SAMPLING METHODS

During the evolution of this dynamical system, the value of the Hamiltonian H is
constant, as is easily seen by differentiation

∂H
∂zi

dzi
dτ

+ ∂H
∂ri

dri
dτ

dH
dτ

=

=

∂H
∂zi

∂H
∂ri −

∂H
∂ri

∂H
∂zi

= 0.

(11.60)

A second important property of Hamiltonian dynamical systems, known as Li-
ouville’s Theorem, is that they preserve volume in phase space. In other words, if
we consider a region within the space of variables (z, r), then as this region evolves
under the equations of Hamiltonian dynamics, its shape may change but its volume
will not. This can be seen by noting that the ﬂow ﬁeld (rate of change of location in
phase space) is given by

dz
dτ
and that the divergence of this ﬁeld vanishes

V =

,

dr
dτ

(11.61)

div V =

∂
∂zi

dzi
dτ

dri
dτ

+ ∂
∂ri
+ ∂
∂ri

=

i

∂
∂zi

−

∂H
∂ri

∂H
∂zi

= 0.

(11.62)

Now consider the joint distribution over phase space whose total energy is the

Hamiltonian, i.e., the distribution given by

p(z, r) =

1
ZH

exp(−H(z, r)).

(11.63)

Using the two results of conservation of volume and conservation of H, it follows
that the Hamiltonian dynamics will leave p(z, r) invariant. This can be seen by
considering a small region of phase space over which H is approximately constant.
If we follow the evolution of the Hamiltonian equations for a ﬁnite time, then the
volume of this region will remain unchanged as will the value of H in this region, and
hence the probability density, which is a function only of H, will also be unchanged.
Although H is invariant, the values of z and r will vary, and so by integrating
the Hamiltonian dynamics over a ﬁnite time duration it becomes possible to make
large changes to z in a systematic way that avoids random walk behaviour.

Evolution under the Hamiltonian dynamics will not, however, sample ergodi-
cally from p(z, r) because the value of H is constant. In order to arrive at an ergodic
sampling scheme, we can introduce additional moves in phase space that change
the value of H while also leaving the distribution p(z, r) invariant. The simplest
way to achieve this is to replace the value of r with one drawn from its distribution
conditioned on z. This can be regarded as a Gibbs sampling step, and hence from



















Exercise 11.16

11.5. The Hybrid Monte Carlo Algorithm

551

Section 11.3 we see that this also leaves the desired distribution invariant. Noting
that z and r are independent in the distribution p(z, r), we see that the conditional
distribution p(r|z) is a Gaussian from which it is straightforward to sample.
In a practical application of this approach, we have to address the problem of
performing a numerical integration of the Hamiltonian equations. This will neces-
sarily introduce numerical errors and so we should devise a scheme that minimizes
the impact of such errors. In fact, it turns out that integration schemes can be devised
for which Liouville’s theorem still holds exactly. This property will be important in
the hybrid Monte Carlo algorithm, which is discussed in Section 11.5.2. One scheme
for achieving this is called the leapfrog discretization and involves alternately updat-
r to the position and momentum variables
ing discrete-time approximations
using

z and

ri(τ + 	/2) =

zi(τ + 	) =

ri(τ + 	) =

(

z(τ))

∂E
	
ri(τ) −
2
∂zi
ri(τ + 	/2)
zi(τ) + 	
∂E
ri(τ + 	/2) −
∂zi

	
2

(

z(τ + 	)).

(11.64)

(11.65)

(11.66)

We see that this takes the form of a half-step update of the momentum variables with
step size 	/2, followed by a full-step update of the position variables with step size 	,
followed by a second half-step update of the momentum variables. If several leapfrog
steps are applied in succession, it can be seen that half-step updates to the momentum
variables can be combined into full-step updates with step size 	. The successive
updates to position and momentum variables then leapfrog over each other. In order
to advance the dynamics by a time interval τ , we need to take τ /	 steps. The error
involved in the discretized approximation to the continuous time dynamics will go to
zero, assuming a smooth function E(z), in the limit 	 → 0. However, for a nonzero
	 as used in practice, some residual error will remain. We shall see in Section 11.5.2
how the effects of such errors can be eliminated in the hybrid Monte Carlo algorithm.
In summary then, the Hamiltonian dynamical approach involves alternating be-
tween a series of leapfrog updates and a resampling of the momentum variables from
their marginal distribution.

Note that the Hamiltonian dynamics method, unlike the basic Metropolis algo-
rithm, is able to make use of information about the gradient of the log probability
distribution as well as about the distribution itself. An analogous situation is familiar
from the domain of function optimization. In most cases where gradient informa-
tion is available, it is highly advantageous to make use of it. Informally, this follows
from the fact that in a space of dimension D, the additional computational cost of
evaluating a gradient compared with evaluating the function itself will typically be a
ﬁxed factor independent of D, whereas the D-dimensional gradient vector conveys
D pieces of information compared with the one piece of information given by the
function itself.

552

11. SAMPLING METHODS

11.5.2 Hybrid Monte Carlo
As we discussed in the previous section, for a nonzero step size 	, the discretiza-
tion of the leapfrog algorithm will introduce errors into the integration of the Hamil-
tonian dynamical equations. Hybrid Monte Carlo (Duane et al., 1987; Neal, 1996)
combines Hamiltonian dynamics with the Metropolis algorithm and thereby removes
any bias associated with the discretization.

Speciﬁcally, the algorithm uses a Markov chain consisting of alternate stochastic
updates of the momentum variable r and Hamiltonian dynamical updates using the
leapfrog algorithm. After each application of the leapfrog algorithm, the resulting
candidate state is accepted or rejected according to the Metropolis criterion based
on the value of the Hamiltonian H. Thus if (z, r) is the initial state and (z, r)
is the state after the leapfrog integration, then this candidate state is accepted with
probability

min (1, exp{H(z, r) − H(z, r)}) .

(11.67)

If the leapfrog integration were to simulate the Hamiltonian dynamics perfectly,
then every such candidate step would automatically be accepted because the value
of H would be unchanged. Due to numerical errors, the value of H may sometimes
decrease, and we would like the Metropolis criterion to remove any bias due to this
effect and ensure that the resulting samples are indeed drawn from the required dis-
tribution. In order for this to be the case, we need to ensure that the update equations
corresponding to the leapfrog integration satisfy detailed balance (11.40). This is
easily achieved by modifying the leapfrog scheme as follows.

Before the start of each leapfrog integration sequence, we choose at random,
with equal probability, whether to integrate forwards in time (using step size 	) or
backwards in time (using step size −	). We ﬁrst note that the leapfrog integration
scheme (11.64), (11.65), and (11.66) is time-reversible, so that integration for L steps
using step size −	 will exactly undo the effect of integration for L steps using step
size 	. Next we show that the leapfrog integration preserves phase-space volume
exactly. This follows from the fact that each step in the leapfrog scheme updates
either a zi variable or an ri variable by an amount that is a function only of the other
variable. As shown in Figure 11.14, this has the effect of shearing a region of phase
space while not altering its volume.

Finally, we use these results to show that detailed balance holds. Consider a
small region R of phase space that, under a sequence of L leapfrog iterations of
step size 	, maps to a region R. Using conservation of volume under the leapfrog
iteration, we see that if R has volume δV then so too will R. If we choose an initial
point from the distribution (11.63) and then update it using L leapfrog interactions,
the probability of the transition going from R to R is given by

1
ZH

exp(−H(R))δV

min{1, exp(−H(R) + H(R))} .

(11.68)

1
2

where the factor of 1/2 arises from the probability of choosing to integrate with a
positive step size rather than a negative one. Similarly, the probability of starting in

11.5. The Hybrid Monte Carlo Algorithm

553

ri

zi

1
ZH

Exercise 11.17

ri





Figure 11.14 Each step of the leapfrog algorithm (11.64)–(11.66) modiﬁes either a position variable zi or a
momentum variable ri. Because the change to one variable is a function only of the other, any region in phase
space will be sheared without change of volume.

zi

region R and integrating backwards in time to end up in region R is given by

exp(−H(R))δV

min{1, exp(−H(R) + H(R))} .

(11.69)

1
2

It is easily seen that the two probabilities (11.68) and (11.69) are equal, and hence
detailed balance holds. Note that this proof ignores any overlap between the regions
R and R but is easily generalized to allow for such overlap.
It is not difﬁcult to construct examples for which the leapfrog algorithm returns
to its starting position after a ﬁnite number of iterations. In such cases, the random
replacement of the momentum values before each leapfrog integration will not be
sufﬁcient to ensure ergodicity because the position variables will never be updated.
Such phenomena are easily avoided by choosing the magnitude of the step size at
random from some small interval, before each leapfrog integration.

We can gain some insight into the behaviour of the hybrid Monte Carlo algo-
rithm by considering its application to a multivariate Gaussian. For convenience,
consider a Gaussian distribution p(z) with independent components, for which the
Hamiltonian is given by

H(z, r) =

1
2

i

1
σ2
i

i +
z2

1
2

r2
i .

i

(11.70)

Our conclusions will be equally valid for a Gaussian distribution having correlated
components because the hybrid Monte Carlo algorithm exhibits rotational isotropy.
During the leapfrog integration, each pair of phase-space variables zi, ri evolves in-
dependently. However, the acceptance or rejection of the candidate point is based
on the value of H, which depends on the values of all of the variables. Thus, a
signiﬁcant integration error in any one of the variables could lead to a high prob-
ability of rejection. In order that the discrete leapfrog integration be a reasonably

554

11. SAMPLING METHODS

good approximation to the true continuous-time dynamics, it is necessary for the
leapfrog integration scale 	 to be smaller than the shortest length-scale over which
the potential is varying signiﬁcantly. This is governed by the smallest value of σi,
which we denote by σmin. Recall that the goal of the leapfrog integration in hybrid
Monte Carlo is to move a substantial distance through phase space to a new state
that is relatively independent of the initial state and still achieve a high probability of
acceptance. In order to achieve this, the leapfrog integration must be continued for a
number of iterations of order σmax/σmin.

By contrast, consider the behaviour of a simple Metropolis algorithm with an
isotropic Gaussian proposal distribution of variance s2, considered earlier. In order
to avoid high rejection rates, the value of s must be of order σmin. The exploration of
state space then proceeds by a random walk and takes of order (σmax/σmin)2 steps
to arrive at a roughly independent state.








l

11.6. Estimating the Partition Function

As we have seen, most of the sampling algorithms considered in this chapter re-
quire only the functional form of the probability distribution up to a multiplicative
constant. Thus if we write

pE(z) =

1
ZE

exp(−E(z))

(11.71)

then the value of the normalization constant ZE, also known as the partition func-
tion, is not needed in order to draw samples from p(z). However, knowledge of the
value of ZE can be useful for Bayesian model comparison since it represents the
model evidence (i.e., the probability of the observed data given the model), and so
it is of interest to consider how its value might be obtained. We assume that direct
evaluation by summing, or integrating, the function exp(−E(z)) over the state space
of z is intractable.
For model comparison, it is actually the ratio of the partition functions for two
models that is required. Multiplication of this ratio by the ratio of prior probabilities
gives the ratio of posterior probabilities, which can then be used for model selection
or model averaging.

One way to estimate a ratio of partition functions is to use importance sampling

from a distribution with energy function G(z)
z exp(−E(z))
z exp(−G(z))
z exp(−E(z) + G(z)) exp(−G(z))

ZE
ZG

=

z exp(−G(z))

=
= EG(z)[exp(−E + G)]


exp(−E(z(l)) + G(z(l)))

(11.72)



L

where {z(l)} are samples drawn from the distribution deﬁned by pG(z). If the dis-
tribution pG is one for which the partition function can be evaluated analytically, for
example a Gaussian, then the absolute value of ZE can be obtained.

This approach will only yield accurate results if the importance sampling distri-
bution pG is closely matched to the distribution pE, so that the ratio pE/pG does not
have wide variations. In practice, suitable analytically speciﬁed importance sampling
distributions cannot readily be found for the kinds of complex models considered in
this book.

An alternative approach is therefore to use the samples obtained from a Markov
chain to deﬁne the importance-sampling distribution. If the transition probability for
the Markov chain is given by T (z, z), and the sample set is given by z(1), . . . , z(L),
then the sampling distribution can be written as

1
ZG

exp (−G(z)) =

which can be used directly in (11.72).

T (z(l), z)

l=1

(11.73)

11.6. Estimating the Partition Function

555

Methods for estimating the ratio of two partition functions require for their suc-
cess that the two corresponding distributions be reasonably closely matched. This is
especially problematic if we wish to ﬁnd the absolute value of the partition function
for a complex distribution because it is only for relatively simple distributions that
the partition function can be evaluated directly, and so attempting to estimate the
ratio of partition functions directly is unlikely to be successful. This problem can be
tackled using a technique known as chaining (Neal, 1993; Barber and Bishop, 1997),
which involves introducing a succession of intermediate distributions p2, . . . , pM−1
that interpolate between a simple distribution p1(z) for which we can evaluate the
normalization coefﬁcient Z1 and the desired complex distribution pM (z). We then
have

ZM
Z1

= Z2
Z1

Z3
Z2 ···

ZM
ZM−1

(11.74)

in which the intermediate ratios can be determined using Monte Carlo methods as
discussed above. One way to construct such a sequence of intermediate systems
is to use an energy function containing a continuous parameter 0 � α � 1 that
interpolates between the two distributions

Eα(z) = (1 − α)E1(z) + αEM (z).

(11.75)

If the intermediate ratios in (11.74) are to be found using Monte Carlo, it may be
more efﬁcient to use a single Markov chain run than to restart the Markov chain for
each ratio. In this case, the Markov chain is run initially for the system p1 and then
after some suitable number of steps moves on to the next distribution in the sequence.
Note, however, that the system must remain close to the equilibrium distribution at
each stage.









556

11. SAMPLING METHODS

Exercises

11.1 () www Show that the ﬁnite sample estimator

f deﬁned by (11.2) has mean

equal to E[f] and variance given by (11.3).

11.2 () Suppose that z is a random variable with uniform distribution over (0, 1) and
that we transform z using y = h−1(z) where h(y) is given by (11.6). Show that y
has the distribution p(y).

11.3 () Given a random variable z that is uniformly distributed over (0, 1), ﬁnd a trans-

formation y = f(z) such that y has a Cauchy distribution given by (11.8).

11.4 ( )

Suppose that z1 and z2 are uniformly distributed over the unit circle, as
shown in Figure 11.3, and that we make the change of variables given by (11.10)
and (11.11). Show that (y1, y2) will be distributed according to (11.12).

11.5 () www Let z be a D-dimensional random variable having a Gaussian distribu-
tion with zero mean and unit covariance matrix, and suppose that the positive deﬁnite
symmetric matrix Σ has the Cholesky decomposition Σ = LLT where L is a lower-
triangular matrix (i.e., one with zeros above the leading diagonal). Show that the
variable y = µ + Lz has a Gaussian distribution with mean µ and covariance Σ.
This provides a technique for generating samples from a general multivariate Gaus-
sian using samples from a univariate Gaussian having zero mean and unit variance.

p(z)/kq(z) where

11.6 ( ) www In this exercise, we show more carefully that rejection sampling does
indeed draw samples from the desired distribution p(z). Suppose the proposal dis-
tribution is q(z) and show that the probability of a sample value z being accepted is
given by
p is any unnormalized distribution that is proportional to
p(z), and the constant k is set to the smallest value that ensures kq(z) �
p(z) for all
values of z. Note that the probability of drawing a value z is given by the probability
of drawing that value from q(z) times the probability of accepting that value given
that it has been drawn. Make use of this, along with the sum and product rules of
probability, to write down the normalized form for the distribution over z, and show
that it equals p(z).

11.7 () Suppose that z has a uniform distribution over the interval [0, 1]. Show that the

variable y = b tan z + c has a Cauchy distribution given by (11.16).

11.8 ( ) Determine expressions for the coefﬁcients ki in the envelope distribution
(11.17) for adaptive rejection sampling using the requirements of continuity and nor-
malization.

11.9 ( ) By making use of the technique discussed in Section 11.1.1 for sampling
from a single exponential distribution, devise an algorithm for sampling from the
piecewise exponential distribution deﬁned by (11.17).

11.10 () Show that the simple random walk over the integers deﬁned by (11.34), (11.35),
and (11.36) has the property that E[(z(τ ))2] = E[(z(τ−1))2] + 1/2 and hence by
induction that E[(z(τ ))2] = τ /2.

Figure 11.15 A probability distribution over two variables z1
and z2 that is uniform over the shaded regions
and that is zero everywhere else.

z2

Exercises

557

z1

11.11 ( ) www Show that the Gibbs sampling algorithm, discussed in Section 11.3,

satisﬁes detailed balance as deﬁned by (11.40).

11.12 () Consider the distribution shown in Figure 11.15. Discuss whether the standard
Gibbs sampling procedure for this distribution is ergodic, and therefore whether it
would sample correctly from this distribution

11.13 ( ) Consider the simple 3-node graph shown in Figure 11.16 in which the observed
node x is given by a Gaussian distribution N (x|µ, τ−1) with mean µ and precision
τ . Suppose that the marginal distributions over the mean and precision are given
by N (µ|µ0, s0) and Gam(τ|a, b), where Gam(·|·,·) denotes a gamma distribution.
Write down expressions for the conditional distributions p(µ|x, τ) and p(τ|x, µ) that
would be required in order to apply Gibbs sampling to the posterior distribution
p(µ, τ|x).

11.14 () Verify that the over-relaxation update (11.50), in which zi has mean µi and
variance σi, and where ν has zero mean and unit variance, gives a value zi with
mean µi and variance σ2
i .

11.15 () www Using (11.56) and (11.57), show that the Hamiltonian equation (11.58)
is equivalent to (11.53). Similarly, using (11.57) show that (11.59) is equivalent to
(11.55).

11.16 () By making use of (11.56), (11.57), and (11.63), show that the conditional dis-

tribution p(r|z) is a Gaussian.

Figure 11.16 A graph involving an observed Gaussian variable x with

prior distributions over its mean µ and precision τ.

µ

τ

x

558

11. SAMPLING METHODS

11.17 () www Verify that the two probabilities (11.68) and (11.69) are equal, and hence

that detailed balance holds for the hybrid Monte Carlo algorithm.

13

Sequential

Data

So far in this book, we have focussed primarily on sets of data points that were as-
sumed to be independent and identically distributed (i.i.d.). This assumption allowed
us to express the likelihood function as the product over all data points of the prob-
ability distribution evaluated at each data point. For many applications, however,
the i.i.d. assumption will be a poor one. Here we consider a particularly important
class of such data sets, namely those that describe sequential data. These often arise
through measurement of time series, for example the rainfall measurements on suc-
cessive days at a particular location, or the daily values of a currency exchange rate,
or the acoustic features at successive time frames used for speech recognition. An
example involving speech data is shown in Figure 13.1. Sequential data can also
arise in contexts other than time series, for example the sequence of nucleotide base
pairs along a strand of DNA or the sequence of characters in an English sentence.
For convenience, we shall sometimes refer to ‘past’ and ‘future’ observations in a
sequence. However, the models explored in this chapter are equally applicable to all

605

606

13. SEQUENTIAL DATA

Figure 13.1 Example of a spectro-
gram of the spoken words “Bayes’ theo-
rem” showing a plot of the intensity of the
spectral coefﬁcients versus time index.

forms of sequential data, not just temporal sequences.

It is useful to distinguish between stationary and nonstationary sequential dis-
tributions. In the stationary case, the data evolves in time, but the distribution from
which it is generated remains the same. For the more complex nonstationary situa-
tion, the generative distribution itself is evolving with time. Here we shall focus on
the stationary case.

For many applications, such as ﬁnancial forecasting, we wish to be able to pre-
dict the next value in a time series given observations of the previous values. In-
tuitively, we expect that recent observations are likely to be more informative than
more historical observations in predicting future values. The example in Figure 13.1
shows that successive observations of the speech spectrum are indeed highly cor-
related. Furthermore, it would be impractical to consider a general dependence of
future observations on all previous observations because the complexity of such a
model would grow without limit as the number of observations increases. This leads
us to consider Markov models in which we assume that future predictions are inde-



N

pendent of all but the most recent observations.

Although such models are tractable, they are also severely limited. We can ob-
tain a more general framework, while still retaining tractability, by the introduction
of latent variables, leading to state space models. As in Chapters 9 and 12, we shall
see that complex models can thereby be constructed from simpler components (in
particular, from distributions belonging to the exponential family) and can be read-
ily characterized using the framework of probabilistic graphical models. Here we
focus on the two most important examples of state space models, namely the hid-
den Markov model, in which the latent variables are discrete, and linear dynamical
systems, in which the latent variables are Gaussian. Both models are described by di-
rected graphs having a tree structure (no loops) for which inference can be performed
efﬁciently using the sum-product algorithm.

13.1. Markov Models

Figure 13.2 The simplest approach to
modelling a sequence of ob-
servations is to treat
them
as independent, correspond-
ing to a graph without links.

x1

13.1. Markov Models

607

x3

x4

x2

The easiest way to treat sequential data would be simply to ignore the sequential
aspects and treat the observations as i.i.d., corresponding to the graph in Figure 13.2.
Such an approach, however, would fail to exploit the sequential patterns in the data,
such as correlations between observations that are close in the sequence. Suppose,
for instance, that we observe a binary variable denoting whether on a particular day
it rained or not. Given a time series of recent observations of this variable, we wish
to predict whether it will rain on the next day. If we treat the data as i.i.d., then the
only information we can glean from the data is the relative frequency of rainy days.
However, we know in practice that the weather often exhibits trends that may last for
several days. Observing whether or not it rains today is therefore of signiﬁcant help
in predicting if it will rain tomorrow.

To express such effects in a probabilistic model, we need to relax the i.i.d. as-
sumption, and one of the simplest ways to do this is to consider a Markov model.
First of all we note that, without loss of generality, we can use the product rule to
express the joint distribution for a sequence of observations in the form

p(x1, . . . , xN ) =

p(xn|x1, . . . , xn−1).

n=1

(13.1)

If we now assume that each of the conditional distributions on the right-hand side
is independent of all previous observations except the most recent, we obtain the
ﬁrst-order Markov chain, which is depicted as a graphical model in Figure 13.3. The




N

N

n=2

608

13. SEQUENTIAL DATA

Figure 13.3 A ﬁrst-order Markov chain of ob-
servations {xn} in which the dis-
tribution p(xn|xn−1) of a particu-
lar observation xn is conditioned
on the value of the previous ob-
servation xn−1.

x1

x2

x3

x4

Section 8.2

Exercise 13.1

joint distribution for a sequence of N observations under this model is given by

p(x1, . . . , xN ) = p(x1)

p(xn|xn−1).

(13.2)

From the d-separation property, we see that the conditional distribution for observa-
tion xn, given all of the observations up to time n, is given by
p(xn|x1, . . . , xn−1) = p(xn|xn−1)

(13.3)

which is easily veriﬁed by direct evaluation starting from (13.2) and using the prod-
uct rule of probability. Thus if we use such a model to predict the next observation
in a sequence, the distribution of predictions will depend only on the value of the im-
mediately preceding observation and will be independent of all earlier observations.
In most applications of such models, the conditional distributions p(xn|xn−1)
that deﬁne the model will be constrained to be equal, corresponding to the assump-
tion of a stationary time series. The model is then known as a homogeneous Markov
chain. For instance, if the conditional distributions depend on adjustable parameters
(whose values might be inferred from a set of training data), then all of the condi-
tional distributions in the chain will share the same values of those parameters.

Although this is more general than the independence model, it is still very re-
strictive. For many sequential observations, we anticipate that the trends in the data
over several successive observations will provide important information in predict-
ing the next value. One way to allow earlier observations to have an inﬂuence is to
move to higher-order Markov chains. If we allow the predictions to depend also on
the previous-but-one value, we obtain a second-order Markov chain, represented by
the graph in Figure 13.4. The joint distribution is now given by

p(x1, . . . , xN ) = p(x1)p(x2|x1)

p(xn|xn−1, xn−2).

n=3

(13.4)

Again, using d-separation or by direct evaluation, we see that the conditional distri-
bution of xn given xn−1 and xn−2 is independent of all observations x1, . . . xn−3.

Figure 13.4 A second-order Markov chain, in
which the conditional distribution
of a particular observation xn
depends on the values of the two
previous observations xn−1 and
xn−2.

x1

x2

x3

x4

Figure 13.5 We can represent sequen-
tial data using a Markov chain of latent
variables, with each observation condi-
tioned on the state of the corresponding
latent variable. This important graphical
structure forms the foundation both for the
hidden Markov model and for linear dy-
namical systems.

z1

x1

z2

x2

13.1. Markov Models

609

zn−1

xn−1

zn

xn

zn+1

xn+1

Each observation is now inﬂuenced by two previous observations. We can similarly
consider extensions to an M th order Markov chain in which the conditional distri-
bution for a particular variable depends on the previous M variables. However, we
have paid a price for this increased ﬂexibility because the number of parameters in
the model is now much larger. Suppose the observations are discrete variables hav-
ing K states. Then the conditional distribution p(xn|xn−1) in a ﬁrst-order Markov
chain will be speciﬁed by a set of K − 1 parameters for each of the K states of xn−1
giving a total of K(K − 1) parameters. Now suppose we extend the model to an
M th order Markov chain, so that the joint distribution is built up from conditionals
p(xn|xn−M , . . . , xn−1). If the variables are discrete, and if the conditional distri-
butions are represented by general conditional probability tables, then the number
of parameters in such a model will have KM−1(K − 1) parameters. Because this
grows exponentially with M, it will often render this approach impractical for larger
values of M.

For continuous variables, we can use linear-Gaussian conditional distributions
in which each node has a Gaussian distribution whose mean is a linear function
of its parents. This is known as an autoregressive or AR model (Box et al., 1994;
Thiesson et al., 2004). An alternative approach is to use a parametric model for
p(xn|xn−M , . . . , xn−1) such as a neural network. This technique is sometimes
called a tapped delay line because it corresponds to storing (delaying) the previous
M values of the observed variable in order to predict the next value. The number
of parameters can then be much smaller than in a completely general model (for ex-
ample it may grow linearly with M), although this is achieved at the expense of a
restricted family of conditional distributions.

Suppose we wish to build a model for sequences that is not limited by the
Markov assumption to any order and yet that can be speciﬁed using a limited number
of free parameters. We can achieve this by introducing additional latent variables to
permit a rich class of models to be constructed out of simple components, as we did
with mixture distributions in Chapter 9 and with continuous latent variable models in
Chapter 12. For each observation xn, we introduce a corresponding latent variable
zn (which may be of different type or dimensionality to the observed variable). We
now assume that it is the latent variables that form a Markov chain, giving rise to the
graphical structure known as a state space model, which is shown in Figure 13.5. It
satisﬁes the key conditional independence property that zn−1 and zn+1 are indepen-
dent given zn, so that
(13.5)

zn+1 ⊥⊥ zn−1 | zn.

 



N

p(xn|zn).

n=1

(13.6)





N



610

13. SEQUENTIAL DATA

The joint distribution for this model is given by

p(x1, . . . , xN , z1, . . . , zN ) = p(z1)

n=2

p(zn|zn−1)

Using the d-separation criterion, we see that there is always a path connecting any
two observed variables xn and xm via the latent variables, and that this path is never
blocked. Thus the predictive distribution p(xn+1|x1, . . . , xn) for observation xn+1
given all previous observations does not exhibit any conditional independence prop-
erties, and so our predictions for xn+1 depends on all previous observations. The
observed variables, however, do not satisfy the Markov property at any order. We
shall discuss how to evaluate the predictive distribution in later sections of this chap-
ter.

There are two important models for sequential data that are described by this
graph. If the latent variables are discrete, then we obtain the hidden Markov model,
or HMM (Elliott et al., 1995). Note that the observed variables in an HMM may
be discrete or continuous, and a variety of different conditional distributions can be
used to model them. If both the latent and the observed variables are Gaussian (with
a linear-Gaussian dependence of the conditional distributions on their parents), then
we obtain the linear dynamical system.

Section 13.2

Section 13.3

13.2. Hidden Markov Models

The hidden Markov model can be viewed as a speciﬁc instance of the state space
model of Figure 13.5 in which the latent variables are discrete. However, if we
examine a single time slice of the model, we see that it corresponds to a mixture
distribution, with component densities given by p(x|z).
It can therefore also be
interpreted as an extension of a mixture model in which the choice of mixture com-
ponent for each observation is not selected independently but depends on the choice
of component for the previous observation. The HMM is widely used in speech
recognition (Jelinek, 1997; Rabiner and Juang, 1993), natural language modelling
(Manning and Sch¨utze, 1999), on-line handwriting recognition (Nag et al., 1986),
and for the analysis of biological sequences such as proteins and DNA (Krogh et al.,
1994; Durbin et al., 1998; Baldi and Brunak, 2001).

As in the case of a standard mixture model, the latent variables are the discrete
multinomial variables zn describing which component of the mixture is responsible
for generating the corresponding observation xn. Again, it is convenient to use a
1-of-K coding scheme, as used for mixture models in Chapter 9. We now allow the
probability distribution of zn to depend on the state of the previous latent variable
zn−1 through a conditional distribution p(zn|zn−1). Because the latent variables are
K-dimensional binary variables, this conditional distribution corresponds to a table
of numbers that we denote by A, the elements of which are known as transition
probabilities. They are given by Ajk ≡ p(znk = 1|zn−1,j = 1), and because they
are probabilities, they satisfy 0 � Ajk � 1 with
k Ajk = 1, so that the matrix A





j=1

K

K

K

k=1



Figure 13.6 Transition diagram showing a model whose la-
tent variables have three possible states corre-
sponding to the three boxes. The black lines
denote the elements of the transition matrix
Ajk.

13.2. Hidden Markov Models

611

A32

A23

k = 1

A11

A22

A21

A12

k = 2

k = 3

A31

A13

A33

has K(K−1) independent parameters. We can then write the conditional distribution
explicitly in the form

p(zn|zn−1,A) =

Azn−1,j znk

jk

.

(13.7)

The initial latent node z1 is special in that it does not have a parent node, and so
it has a marginal distribution p(z1) represented by a vector of probabilities π with
elements πk ≡ p(z1k = 1), so that

p(z1|π) =

πz1k
k

k=1

(13.8)

where

k πk = 1.

The transition matrix is sometimes illustrated diagrammatically by drawing the
states as nodes in a state transition diagram as shown in Figure 13.6 for the case of
K = 3. Note that this does not represent a probabilistic graphical model, because
the nodes are not separate variables but rather states of a single variable, and so we
have shown the states as boxes rather than circles.

Section 8.4.5

It is sometimes useful to take a state transition diagram, of the kind shown in
Figure 13.6, and unfold it over time. This gives an alternative representation of the
transitions between latent states, known as a lattice or trellis diagram, and which is
shown for the case of the hidden Markov model in Figure 13.7.

The speciﬁcation of the probabilistic model is completed by deﬁning the con-
ditional distributions of the observed variables p(xn|zn, φ), where φ is a set of pa-
rameters governing the distribution. These are known as emission probabilities, and
might for example be given by Gaussians of the form (9.11) if the elements of x are
continuous variables, or by conditional probability tables if x is discrete. Because
xn is observed, the distribution p(xn|zn, φ) consists, for a given value of φ, of a
vector of K numbers corresponding to the K possible states of the binary vector zn.



K

 



N





N

k = 1

k = 2

612

13. SEQUENTIAL DATA

Figure 13.7 If we unfold the state transition dia-
gram of Figure 13.6 over time, we obtain a lattice,
or trellis, representation of the latent states. Each
column of this diagram corresponds to one of the
latent variables zn.

A11

A11

A11

A33

n + 1

n

(13.9)

We can represent the emission probabilities in the form

k = 3

A33

A33

n − 2

n − 1

p(xn|zn, φ) =

p(xn|φk)znk .

k=1

We shall focuss attention on homogeneous models for which all of the condi-
tional distributions governing the latent variables share the same parameters A, and
similarly all of the emission distributions share the same parameters φ (the extension
to more general cases is straightforward). Note that a mixture model for an i.i.d. data
set corresponds to the special case in which the parameters Ajk are the same for all
values of j, so that the conditional distribution p(zn|zn−1) is independent of zn−1.
This corresponds to deleting the horizontal links in the graphical model shown in
Figure 13.5.

The joint probability distribution over both latent and observed variables is then

given by

p(X, Z|θ) = p(z1|π)

p(zn|zn−1, A)

n=2

m=1

p(xm|zm, φ)

(13.10)

where X = {x1, . . . , xN}, Z = {z1, . . . , zN}, and θ = {π, A, φ} denotes the set
of parameters governing the model. Most of our discussion of the hidden Markov
model will be independent of the particular choice of the emission probabilities.
Indeed, the model is tractable for a wide range of emission distributions including
discrete tables, Gaussians, and mixtures of Gaussians. It is also possible to exploit
discriminative models such as neural networks. These can be used to model the
emission density p(x|z) directly, or to provide a representation for p(z|x) that can
be converted into the required emission density p(x|z) using Bayes’ theorem (Bishop
et al., 2004).
We can gain a better understanding of the hidden Markov model by considering
it from a generative point of view. Recall that to generate samples from a mixture of

Exercise 13.4

1

0.5

k = 1

0

0

k = 3

k = 2

0.5

13.2. Hidden Markov Models

613

1

0.5

1

0

0

0.5

1

Figure 13.8 Illustration of sampling from a hidden Markov model having a 3-state latent variable z and a
Gaussian emission model p(x|z) where x is 2-dimensional. (a) Contours of constant probability density for the
emission distributions corresponding to each of the three states of the latent variable. (b) A sample of 50 points
drawn from the hidden Markov model, colour coded according to the component that generated them and with
lines connecting the successive observations. Here the transition matrix was ﬁxed so that in any state there is a
5% probability of making a transition to each of the other states, and consequently a 90% probability of remaining
in the same state.

Gaussians, we ﬁrst chose one of the components at random with probability given by
the mixing coefﬁcients πk and then generate a sample vector x from the correspond-
ing Gaussian component. This process is repeated N times to generate a data set of
N independent samples. In the case of the hidden Markov model, this procedure is
modiﬁed as follows. We ﬁrst choose the initial latent variable z1 with probabilities
governed by the parameters πk and then sample the corresponding observation x1.
Now we choose the state of the variable z2 according to the transition probabilities
p(z2|z1) using the already instantiated value of z1. Thus suppose that the sample for
z1 corresponds to state j. Then we choose the state k of z2 with probabilities Ajk
for k = 1, . . . , K. Once we know z2 we can draw a sample for x2 and also sample
the next latent variable z3 and so on. This is an example of ancestral sampling for
a directed graphical model. If, for instance, we have a model in which the diago-
nal transition elements Akk are much larger than the off-diagonal elements, then a
typical data sequence will have long runs of points generated from a single compo-
nent, with infrequent transitions from one component to another. The generation of
samples from a hidden Markov model is illustrated in Figure 13.8.

There are many variants of the standard HMM model, obtained for instance by
imposing constraints on the form of the transition matrix A (Rabiner, 1989). Here we
mention one of particular practical importance called the left-to-right HMM, which
is obtained by setting the elements Ajk of A to zero if k < j, as illustrated in the

Section 8.1.2

614

13. SEQUENTIAL DATA

Figure 13.9 Example of the state transition diagram for a 3-state
left-to-right hidden Markov model. Note that once a
state has been vacated, it cannot later be re-entered.

A11

A22

A33

A12

A23

k = 1

A13

k = 2

k = 3

state transition diagram for a 3-state HMM in Figure 13.9. Typically for such models
the initial state probabilities for p(z1) are modiﬁed so that p(z11) = 1 and p(z1j) = 0
for j = 1, in other words every sequence is constrained to start in state j = 1. The
transition matrix may be further constrained to ensure that large changes in the state
index do not occur, so that Ajk = 0 if k > j + ∆. This type of model is illustrated
using a lattice diagram in Figure 13.10.

Many applications of hidden Markov models, for example speech recognition,
or on-line character recognition, make use of left-to-right architectures. As an illus-
tration of the left-to-right hidden Markov model, we consider an example involving
handwritten digits. This uses on-line data, meaning that each digit is represented
by the trajectory of the pen as a function of time in the form of a sequence of pen
coordinates, in contrast to the off-line digits data, discussed in Appendix A, which
comprises static two-dimensional pixellated images of the ink. Examples of the on-
line digits are shown in Figure 13.11. Here we train a hidden Markov model on a
subset of data comprising 45 examples of the digit ‘2’. There are K = 16 states,
each of which can generate a line segment of ﬁxed length having one of 16 possible
angles, and so the emission distribution is simply a 16 × 16 table of probabilities
associated with the allowed angle values for each state index value. Transition prob-
abilities are all set to zero except for those that keep the state index k the same or
that increment it by 1, and the model parameters are optimized using 25 iterations of
EM. We can gain some insight into the resulting model by running it generatively, as
shown in Figure 13.11.

Figure 13.10 Lattice diagram for a 3-state left-
to-right HMM in which the state index k is allowed
to increase by at most 1 at each transition.

k = 1

k = 2

k = 3

A11

A11

A11

A33

n − 2

n − 1

A33

A33

n + 1

n

13.2. Hidden Markov Models

615

Figure 13.11 Top row: examples of on-line handwritten
digits. Bottom row: synthetic digits sam-
pled generatively from a left-to-right hid-
den Markov model that has been trained
on a data set of 45 handwritten digits.



One of the most powerful properties of hidden Markov models is their ability to
exhibit some degree of invariance to local warping (compression and stretching) of
the time axis. To understand this, consider the way in which the digit ‘2’ is written
in the on-line handwritten digits example. A typical digit comprises two distinct
sections joined at a cusp. The ﬁrst part of the digit, which starts at the top left, has a
sweeping arc down to the cusp or loop at the bottom left, followed by a second more-
or-less straight sweep ending at the bottom right. Natural variations in writing style
will cause the relative sizes of the two sections to vary, and hence the location of the
cusp or loop within the temporal sequence will vary. From a generative perspective
such variations can be accommodated by the hidden Markov model through changes
in the number of transitions to the same state versus the number of transitions to the
successive state. Note, however, that if a digit ‘2’ is written in the reverse order, that
is, starting at the bottom right and ending at the top left, then even though the pen tip
coordinates may be identical to an example from the training set, the probability of
the observations under the model will be extremely small. In the speech recognition
context, warping of the time axis is associated with natural variations in the speed of
speech, and again the hidden Markov model can accommodate such a distortion and
not penalize it too heavily.

13.2.1 Maximum likelihood for the HMM
If we have observed a data set X = {x1, . . . , xN}, we can determine the param-
eters of an HMM using maximum likelihood. The likelihood function is obtained
from the joint distribution (13.10) by marginalizing over the latent variables

p(X|θ) =

p(X, Z|θ).

Z

(13.11)

Because the joint distribution p(X, Z|θ) does not factorize over n (in contrast to the
mixture distribution considered in Chapter 9), we cannot simply treat each of the
summations over zn independently. Nor can we perform the summations explicitly
because there are N variables to be summed over, each of which has K states, re-
sulting in a total of K N terms. Thus the number of terms in the summation grows







616

13. SEQUENTIAL DATA

Section 9.2

exponentially with the length of the chain. In fact, the summation in (13.11) cor-
responds to summing over exponentially many paths through the lattice diagram in
Figure 13.7.

We have already encountered a similar difﬁculty when we considered the infer-
ence problem for the simple chain of variables in Figure 8.32. There we were able
to make use of the conditional independence properties of the graph to re-order the
summations in order to obtain an algorithm whose cost scales linearly, instead of
exponentially, with the length of the chain. We shall apply a similar technique to the
hidden Markov model.

A further difﬁculty with the expression (13.11) for the likelihood function is that,
because it corresponds to a generalization of a mixture distribution, it represents a
summation over the emission models for different settings of the latent variables.
Direct maximization of the likelihood function will therefore lead to complex ex-
pressions with no closed-form solutions, as was the case for simple mixture models
(recall that a mixture model for i.i.d. data is a special case of the HMM).

We therefore turn to the expectation maximization algorithm to ﬁnd an efﬁcient
framework for maximizing the likelihood function in hidden Markov models. The
EM algorithm starts with some initial selection for the model parameters, which we
denote by θold. In the E step, we take these parameter values and ﬁnd the posterior
distribution of the latent variables p(Z|X, θold). We then use this posterior distri-
bution to evaluate the expectation of the logarithm of the complete-data likelihood
function, as a function of the parameters θ, to give the function Q(θ, θold) deﬁned
by

Q(θ, θold) =

Z

p(Z|X, θold) ln p(X, Z|θ).

At this point, it is convenient to introduce some notation. We shall use γ(zn) to
denote the marginal posterior distribution of a latent variable zn, and ξ(zn−1, zn) to
denote the joint posterior distribution of two successive latent variables, so that

γ(zn) = p(zn|X, θold)

ξ(zn−1, zn) = p(zn−1, zn|X, θold).

For each value of n, we can store γ(zn) using a set of K nonnegative numbers
that sum to unity, and similarly we can store ξ(zn−1, zn) using a K × K matrix of
nonnegative numbers that again sum to unity. We shall also use γ(znk) to denote the
conditional probability of znk = 1, with a similar use of notation for ξ(zn−1,j, znk)
and for other probabilistic variables introduced later. Because the expectation of a
binary random variable is just the probability that it takes the value 1, we have

γ(znk) = E[znk] =

z

γ(z)znk

ξ(zn−1,j, znk) = E[zn−1,jznk] =

γ(z)zn−1,jznk.

z

If we substitute the joint distribution p(X, Z|θ) given by (13.10) into (13.12),

(13.12)

(13.13)
(13.14)

(13.15)

(13.16)




k=1

K

N



K



K



K







γ(z1k)
K

γ(z1j)

n=2

n=2

j=1

N

N

K

N

13.2. Hidden Markov Models

617

and make use of the deﬁnitions of γ and ξ , we obtain

Q(θ, θold) =

γ(z1k) ln πk +

ξ(zn−1,j, znk) ln Ajk

j=1

k=1

+

n=1

k=1

γ(znk) ln p(xn|φk).

(13.17)

The goal of the E step will be to evaluate the quantities γ(zn) and ξ(zn−1, zn) efﬁ-
ciently, and we shall discuss this in detail shortly.
In the M step, we maximize Q(θ, θold) with respect to the parameters θ =
{π, A, φ} in which we treat γ(zn) and ξ(zn−1, zn) as constant. Maximization with
respect to π and A is easily achieved using appropriate Lagrange multipliers with
the results

πk =

Ajk =

(13.18)

.

(13.19)

ξ(zn−1,j, znk)

ξ(zn−1,j, znl)

l=1

n=2

The EM algorithm must be initialized by choosing starting values for π and A, which
should of course respect the summation constraints associated with their probabilis-
tic interpretation. Note that any elements of π or A that are set to zero initially will
remain zero in subsequent EM updates. A typical initialization procedure would
involve selecting random starting values for these parameters subject to the summa-
tion and non-negativity constraints. Note that no particular modiﬁcation to the EM
results are required for the case of left-to-right models beyond choosing initial values
for the elements Ajk in which the appropriate elements are set to zero, because these
will remain zero throughout.

To maximize Q(θ, θold) with respect to φk, we notice that only the ﬁnal term
in (13.17) depends on φk, and furthermore this term has exactly the same form as
the data-dependent term in the corresponding function for a standard mixture dis-
tribution for i.i.d. data, as can be seen by comparison with (9.40) for the case of a
Gaussian mixture. Here the quantities γ(znk) are playing the role of the responsibil-
ities. If the parameters φk are independent for the different components, then this
term decouples into a sum of terms one for each value of k, each of which can be
maximized independently. We are then simply maximizing the weighted log likeli-
hood function for the emission density p(x|φk) with weights γ(znk). Here we shall
suppose that this maximization can be done efﬁciently. For instance, in the case of

Exercise 13.5

Exercise 13.6





n=1
N

n=1

N

N







n=1

i=1

K

D

N

k=1

µk =

γ(znk)xn

γ(znk)

γ(znk)(xn − µk)(xn − µk)T

Σk =

n=1

γ(znk)

(13.20)

.

(13.21)

For the case of discrete multinomial observed variables, the conditional distribution
of the observations takes the form

p(x|z) =

µxizk
ik

(13.22)

and the corresponding M-step equations are given by

N

γ(znk)xni

µik =

n=1
N

.

(13.23)

γ(znk)

n=1

An analogous result holds for Bernoulli observed variables.

The EM algorithm requires initial values for the parameters of the emission dis-
tribution. One way to set these is ﬁrst to treat the data initially as i.i.d. and ﬁt the
emission density by maximum likelihood, and then use the resulting values to ini-
tialize the parameters for EM.

13.2.2 The forward-backward algorithm
Next we seek an efﬁcient procedure for evaluating the quantities γ(znk) and
ξ(zn−1,j, znk), corresponding to the E step of the EM algorithm. The graph for the
hidden Markov model, shown in Figure 13.5, is a tree, and so we know that the
posterior distribution of the latent variables can be obtained efﬁciently using a two-
stage message passing algorithm. In the particular context of the hidden Markov
model, this is known as the forward-backward algorithm (Rabiner, 1989), or the
Baum-Welch algorithm (Baum, 1972). There are in fact several variants of the basic
algorithm, all of which lead to the exact marginals, according to the precise form of

618

13. SEQUENTIAL DATA

Gaussian emission densities we have p(x|φk) = N (x|µk, Σk), and maximization
of the function Q(θ, θold) then gives

Exercise 13.8

Section 8.4

13.2. Hidden Markov Models

619

the messages that are propagated along the chain (Jordan, 2007). We shall focus on
the most widely used of these, known as the alpha-beta algorithm.

As well as being of great practical importance in its own right, the forward-
backward algorithm provides us with a nice illustration of many of the concepts
introduced in earlier chapters. We shall therefore begin in this section with a ‘con-
ventional’ derivation of the forward-backward equations, making use of the sum
and product rules of probability, and exploiting conditional independence properties
which we shall obtain from the corresponding graphical model using d-separation.
Then in Section 13.2.3, we shall see how the forward-backward algorithm can be
obtained very simply as a speciﬁc example of the sum-product algorithm introduced
in Section 8.4.4.

It is worth emphasizing that evaluation of the posterior distributions of the latent
variables is independent of the form of the emission density p(x|z) or indeed of
whether the observed variables are continuous or discrete. All we require is the
values of the quantities p(xn|zn) for each value of zn for every n. Also, in this
section and the next we shall omit the explicit dependence on the model parameters
θold because these ﬁxed throughout.

We therefore begin by writing down the following conditional independence

properties (Jordan, 2007)

p(X|zn) = p(x1, . . . , xn|zn)

p(xn+1, . . . , xN|zn)
p(x1, . . . , xn−1|xn, zn) = p(x1, . . . , xn−1|zn)
p(x1, . . . , xn−1|zn−1, zn) = p(x1, . . . , xn−1|zn−1)
p(xn+1, . . . , xN|zn, zn+1) = p(xn+1, . . . , xN|zn+1)
p(xn+2, . . . , xN|zn+1, xn+1) = p(xn+2, . . . , xN|zn+1)
p(X|zn−1, zn) = p(x1, . . . , xn−1|zn−1)

(13.24)
(13.25)
(13.26)
(13.27)
(13.28)

p(zN +1|zN , X) = p(zN +1|zN )

p(xN +1|X, zN +1) = p(xN +1|zN +1)

p(xn|zn)p(xn+1, . . . , xN|zn) (13.29)
(13.30)
(13.31)
where X = {x1, . . . , xN}. These relations are most easily proved using d-separation.
For instance in the ﬁrst of these results, we note that every path from any one of the
nodes x1, . . . , xn−1 to the node xn passes through the node zn, which is observed.
Because all such paths are head-to-tail, it follows that the conditional independence
property must hold. The reader should take a few moments to verify each of these
properties in turn, as an exercise in the application of d-separation. These relations
can also be proved directly, though with signiﬁcantly greater effort, from the joint
distribution for the hidden Markov model using the sum and product rules of proba-
bility.

Let us begin by evaluating γ(znk). Recall that for a discrete multinomial ran-
dom variable the expected value of one of its components is just the probability of
that component having the value 1. Thus we are interested in ﬁnding the posterior
distribution p(zn|x1, . . . , xN ) of zn given the observed data set x1, . . . , xN . This

Exercise 13.10

620

13. SEQUENTIAL DATA

represents a vector of length K whose entries correspond to the expected values of
znk. Using Bayes’ theorem, we have

γ(zn) = p(zn|X) = p(X|zn)p(zn)

p(X)

.

(13.32)






zn−1

zn−1

zn−1

zn−1



zn−1

Note that the denominator p(X) is implicitly conditioned on the parameters θold
of the HMM and hence represents the likelihood function. Using the conditional
independence property (13.24), together with the product rule of probability, we
obtain

γ(zn) = p(x1, . . . , xn, zn)p(xn+1, . . . , xN|zn)

p(X)

= α(zn)β(zn)

p(X)

(13.33)

where we have deﬁned

α(zn) ≡ p(x1, . . . , xn, zn)
β(zn) ≡ p(xn+1, . . . , xN|zn).

(13.34)
(13.35)
The quantity α(zn) represents the joint probability of observing all of the given
data up to time n and the value of zn, whereas β(zn) represents the conditional
probability of all future data from time n + 1 up to N given the value of zn. Again,
α(zn) and β(zn) each represent set of K numbers, one for each of the possible
settings of the 1-of-K coded binary vector zn. We shall use the notation α(znk) to
denote the value of α(zn) when znk = 1, with an analogous interpretation of β(znk).
We now derive recursion relations that allow α(zn) and β(zn) to be evaluated
efﬁciently. Again, we shall make use of conditional independence properties, in
particular (13.25) and (13.26), together with the sum and product rules, allowing us
to express α(zn) in terms of α(zn−1) as follows

α(zn) = p(x1, . . . , xn, zn)

= p(x1, . . . , xn|zn)p(zn)
= p(xn|zn)p(x1, . . . , xn−1|zn)p(zn)
= p(xn|zn)p(x1, . . . , xn−1, zn)
= p(xn|zn)

p(x1, . . . , xn−1, zn−1, zn)

= p(xn|zn)

= p(xn|zn)

= p(xn|zn)

p(x1, . . . , xn−1, zn|zn−1)p(zn−1)

p(x1, . . . , xn−1|zn−1)p(zn|zn−1)p(zn−1)

p(x1, . . . , xn−1, zn−1)p(zn|zn−1)

Making use of the deﬁnition (13.34) for α(zn), we then obtain
α(zn−1)p(zn|zn−1).

α(zn) = p(xn|zn)

(13.36)



K






zn+1

zn+1

zn+1

Figure 13.12 Illustration of the forward recursion (13.36) for
evaluation of the α variables.
In this fragment
of the lattice, we see that the quantity α(zn1)
is obtained by taking the elements α(zn−1,j) of
α(zn−1) at step n−1 and summing them up with
weights given by Aj1, corresponding to the val-
ues of p(zn|zn−1), and then multiplying by the
data contribution p(xn|zn1).

621

p(xn|zn,1)

13.2. Hidden Markov Models

α(zn−1,1)

α(zn,1)

A11

k = 1

A21

α(zn−1,2)

k = 2

A31

α(zn−1,3)

k = 3

n − 1

n

It is worth taking a moment to study this recursion relation in some detail. Note
that there are K terms in the summation, and the right-hand side has to be evaluated
for each of the K values of zn so each step of the α recursion has computational
cost that scaled like O(K 2). The forward recursion equation for α(zn) is illustrated
using a lattice diagram in Figure 13.12.

In order to start this recursion, we need an initial condition that is given by

α(z1) = p(x1, z1) = p(z1)p(x1|z1) =

{πkp(x1|φk)}z1k

k=1

(13.37)

which tells us that α(z1k), for k = 1, . . . , K, takes the value πkp(x1|φk). Starting
at the ﬁrst node of the chain, we can then work along the chain and evaluate α(zn)
for every latent node. Because each step of the recursion involves multiplying by a
K × K matrix, the overall cost of evaluating these quantities for the whole chain is
of O(K 2N).
We can similarly ﬁnd a recursion relation for the quantities β(zn) by making

use of the conditional independence properties (13.27) and (13.28) giving

β(zn) = p(xn+1, . . . , xN|zn)

p(xn+1, . . . , xN , zn+1|zn)

=

=

=

=

p(xn+1, . . . , xN|zn, zn+1)p(zn+1|zn)

p(xn+1, . . . , xN|zn+1)p(zn+1|zn)

p(xn+2, . . . , xN|zn+1)p(xn+1|zn+1)p(zn+1|zn).

zn+1

622

13. SEQUENTIAL DATA

Figure 13.13 Illustration of

β(zn,1)

β(zn+1,1)





n=1
n

n

zn+1

n=1

k = 1




n=1
n

n

k = 2

k = 3



the backward recursion
(13.38) for evaluation of the β variables. In
this fragment of the lattice, we see that the
quantity β(zn1) is obtained by taking the
components β(zn+1,k) of β(zn+1) at step
n + 1 and summing them up with weights
given by the products of A1k, correspond-
ing to the values of p(zn+1|zn) and the cor-
responding values of the emission density
p(xn|zn+1,k).

A11

A12

p(xn|zn+1,1)

β(zn+1,2)

A13

p(xn|zn+1,2)

β(zn+1,3)

n

n + 1

p(xn|zn+1,3)

Making use of the deﬁnition (13.35) for β(zn), we then obtain

β(zn) =

β(zn+1)p(xn+1|zn+1)p(zn+1|zn).

(13.38)

Note that in this case we have a backward message passing algorithm that evaluates
β(zn) in terms of β(zn+1). At each step, we absorb the effect of observation xn+1
through the emission probability p(xn+1|zn+1), multiply by the transition matrix
p(zn+1|zn), and then marginalize out zn+1. This is illustrated in Figure 13.13.
Again we need a starting condition for the recursion, namely a value for β(zN ).
This can be obtained by setting n = N in (13.33) and replacing α(zN ) with its
deﬁnition (13.34) to give

p(zN|X) = p(X, zN )β(zN )

p(X)

(13.39)

which we see will be correct provided we take β(zN ) = 1 for all settings of zN .

In the M step equations, the quantity p(X) will cancel out, as can be seen, for

instance, in the M-step equation for µk given by (13.20), which takes the form

γ(znk)xn

α(znk)β(znk)xn

.

(13.40)

µk =

=

γ(znk)

α(znk)β(znk)

n=1

However, the quantity p(X) represents the likelihood function whose value we typ-
ically wish to monitor during the EM optimization, and so it is useful to be able to
evaluate it. If we sum both sides of (13.33) over zn, and use the fact that the left-hand
side is a normalized distribution, we obtain

p(X) =

α(zn)β(zn).

zn

(13.41)



13.2. Hidden Markov Models

623

Thus we can evaluate the likelihood function by computing this sum, for any conve-
nient choice of n. For instance, if we only want to evaluate the likelihood function,
then we can do this by running the α recursion from the start to the end of the chain,
and then use this result for n = N, making use of the fact that β(zN ) is a vector of
1s. In this case no β recursion is required, and we simply have

p(X) =

α(zN ).

zN

(13.42)

Let us take a moment to interpret this result for p(X). Recall that to compute the
likelihood we should take the joint distribution p(X, Z) and sum over all possible
values of Z. Each such value represents a particular choice of hidden state for every
time step, in other words every term in the summation is a path through the lattice
diagram, and recall that there are exponentially many such paths. By expressing
the likelihood function in the form (13.42), we have reduced the computational cost
from being exponential in the length of the chain to being linear by swapping the
order of the summation and multiplications, so that at each time step n we sum
the contributions from all paths passing through each of the states znk to give the
intermediate quantities α(zn).

Next we consider the evaluation of the quantities ξ(zn−1, zn), which correspond
to the values of the conditional probabilities p(zn−1, zn|X) for each of the K × K
settings for (zn−1, zn). Using the deﬁnition of ξ(zn−1, zn), and applying Bayes’
theorem, we have
ξ(zn−1, zn) = p(zn−1, zn|X)
= p(X|zn−1, zn)p(zn−1, zn)
= p(x1, . . . , xn−1|zn−1)p(xn|zn)p(xn+1, . . . , xN|zn)p(zn|zn−1)p(zn−1)
= α(zn−1)p(xn|zn)p(zn|zn−1)β(zn)

(13.43)

p(X)

p(X)

p(X)

where we have made use of the conditional independence property (13.29) together
with the deﬁnitions of α(zn) and β(zn) given by (13.34) and (13.35). Thus we can
calculate the ξ(zn−1, zn) directly by using the results of the α and β recursions.
Let us summarize the steps required to train a hidden Markov model using
the EM algorithm. We ﬁrst make an initial selection of the parameters θold where
θ ≡ (π, A, φ). The A and π parameters are often initialized either uniformly or
randomly from a uniform distribution (respecting their non-negativity and summa-
tion constraints). Initialization of the parameters φ will depend on the form of the
distribution. For instance in the case of Gaussians, the parameters µk might be ini-
tialized by applying the K-means algorithm to the data, and Σk might be initialized
to the covariance matrix of the corresponding K means cluster. Then we run both
the forward α recursion and the backward β recursion and use the results to evaluate
γ(zn) and ξ(zn−1, zn). At this stage, we can also evaluate the likelihood function.







zN+1

zN+1

zN+1

zN+1

zN+1
1







zN

zN

zN



p(xN +1|X) =

=

=

=

=

=

p(xN +1, zN +1|X)

p(xN +1|zN +1)p(zN +1|X)

p(xN +1|zN +1)

p(zN +1, zN|X)

p(xN +1|zN +1)

p(xN +1|zN +1)

p(zN +1|zN )p(zN|X)
p(zN +1|zN ) p(zN , X)

p(X)

p(X)

zN+1

p(xN +1|zN +1)

zN

p(zN +1|zN )α(zN ) (13.44)

which can be evaluated by ﬁrst running a forward α recursion and then computing
the ﬁnal summations over zN and zN +1. The result of the ﬁrst summation over zN
can be stored and used once the value of xN +1 is observed in order to run the α
recursion forward to the next step in order to predict the subsequent value xN +2.

624

13. SEQUENTIAL DATA

This completes the E step, and we use the results to ﬁnd a revised set of parameters
θnew using the M-step equations from Section 13.2.1. We then continue to alternate
between E and M steps until some convergence criterion is satisﬁed, for instance
when the change in the likelihood function is below some threshold.

Note that in these recursion relations the observations enter through conditional
distributions of the form p(xn|zn). The recursions are therefore independent of
the type or dimensionality of the observed variables or the form of this conditional
distribution, so long as its value can be computed for each of the K possible states
of zn. Since the observed variables {xn} are ﬁxed, the quantities p(xn|zn) can be
pre-computed as functions of zn at the start of the EM algorithm, and remain ﬁxed
throughout.

We have seen in earlier chapters that the maximum likelihood approach is most
effective when the number of data points is large in relation to the number of parame-
ters. Here we note that a hidden Markov model can be trained effectively, using max-
imum likelihood, provided the training sequence is sufﬁciently long. Alternatively,
we can make use of multiple shorter sequences, which requires a straightforward
modiﬁcation of the hidden Markov model EM algorithm. In the case of left-to-right
models, this is particularly important because, in a given observation sequence, a
given state transition corresponding to a nondiagonal element of A will seen at most
once.

Another quantity of interest is the predictive distribution, in which the observed
data is X = {x1, . . . , xN} and we wish to predict xN +1, which would be important
for real-time applications such as ﬁnancial forecasting. Again we make use of the
sum and product rules together with the conditional independence properties (13.29)
and (13.31) giving

Exercise 13.12

Figure 13.14 A fragment of
the fac-
tor graph representation for the hidden
Markov model.

χ

13.2. Hidden Markov Models

625

zn−1

ψn

zn

g1

gn−1

gn

xn−1

xn

z1

x1

Section 10.1

Section 8.4.4

Note that in (13.44), the inﬂuence of all data from x1 to xN is summarized in the K
values of α(zN ). Thus the predictive distribution can be carried forward indeﬁnitely
using a ﬁxed amount of storage, as may be required for real-time applications.

Here we have discussed the estimation of the parameters of an HMM using max-
imum likelihood. This framework is easily extended to regularized maximum likeli-
hood by introducing priors over the model parameters π, A and φ whose values are
then estimated by maximizing their posterior probability. This can again be done us-
ing the EM algorithm in which the E step is the same as discussed above, and the M
step involves adding the log of the prior distribution p(θ) to the function Q(θ, θold)
before maximization and represents a straightforward application of the techniques
developed at various points in this book. Furthermore, we can use variational meth-
ods to give a fully Bayesian treatment of the HMM in which we marginalize over the
parameter distributions (MacKay, 1997). As with maximum likelihood, this leads to
a two-pass forward-backward recursion to compute posterior probabilities.

13.2.3 The sum-product algorithm for the HMM
The directed graph that represents the hidden Markov model, shown in Fig-
ure 13.5, is a tree and so we can solve the problem of ﬁnding local marginals for the
hidden variables using the sum-product algorithm. Not surprisingly, this turns out to
be equivalent to the forward-backward algorithm considered in the previous section,
and so the sum-product algorithm therefore provides us with a simple way to derive
the alpha-beta recursion formulae.

We begin by transforming the directed graph of Figure 13.5 into a factor graph,
of which a representative fragment is shown in Figure 13.14. This form of the fac-
tor graph shows all variables, both latent and observed, explicitly. However, for
the purpose of solving the inference problem, we shall always be conditioning on
the variables x1, . . . , xN , and so we can simplify the factor graph by absorbing the
emission probabilities into the transition probability factors. This leads to the sim-
pliﬁed factor graph representation in Figure 13.15, in which the factors are given
by

h(z1) = p(z1)p(x1|z1)

fn(zn−1, zn) = p(zn|zn−1)p(xn|zn).

(13.45)
(13.46)






z1

zn−1

626

13. SEQUENTIAL DATA

Figure 13.15 A simpliﬁed form of fac-
tor graph to describe the hidden Markov
model.

h

fn

zn−1

zn

To derive the alpha-beta algorithm, we denote the ﬁnal hidden variable zN as
the root node, and ﬁrst pass messages from the leaf node h to the root. From the
general results (8.66) and (8.69) for message propagation, we see that the messages
which are propagated in the hidden Markov model take the form

µzn−1→fn(zn−1) = µfn−1→zn−1(zn−1)

µfn→zn(zn) =

fn(zn−1, zn)µzn−1→fn(zn−1)

zn−1

(13.47)

(13.48)

These equations represent the propagation of messages forward along the chain and
are equivalent to the alpha recursions derived in the previous section, as we shall
now show. Note that because the variable nodes zn have only two neighbours, they
perform no computation.

We can eliminate µzn−1→fn(zn−1) from (13.48) using (13.47) to give a recur-

sion for the f → z messages of the form

µfn→zn(zn) =

fn(zn−1, zn)µfn−1→zn−1(zn−1).

(13.49)

If we now recall the deﬁnition (13.46), and if we deﬁne

α(zn) = µfn→zn(zn)

(13.50)

then we obtain the alpha recursion given by (13.36). We also need to verify that
the quantities α(zn) are themselves equivalent to those deﬁned previously. This
is easily done by using the initial condition (8.71) and noting that α(z1) is given
by h(z1) = p(z1)p(x1|z1) which is identical to (13.37). Because the initial α is
the same, and because they are iteratively computed using the same equation, all
subsequent α quantities must be the same.

Next we consider the messages that are propagated from the root node back to

the leaf node. These take the form

µfn+1→fn(zn) =

zn+1

fn+1(zn, zn+1)µfn+2→fn+1(zn+1)

(13.51)

where, as before, we have eliminated the messages of the type z → f since the
variable nodes perform no computation. Using the deﬁnition (13.46) to substitute
for fn+1(zn, zn+1), and deﬁning

β(zn) = µfn+1→zn(zn)

(13.52)

13.2. Hidden Markov Models

627

we obtain the beta recursion given by (13.38). Again, we can verify that the beta
variables themselves are equivalent by noting that (8.70) implies that the initial mes-
sage send by the root variable node is µzN→fN (zN ) = 1, which is identical to the
initialization of β(zN ) given in Section 13.2.2.
The sum-product algorithm also speciﬁes how to evaluate the marginals once all
the messages have been evaluated. In particular, the result (8.63) shows that the local
marginal at the node zn is given by the product of the incoming messages. Because
we have conditioned on the variables X = {x1, . . . , xN}, we are computing the
joint distribution

p(zn, X) = µfn→zn(zn)µfn+1→zn(zn) = α(zn)β(zn).

Dividing both sides by p(X), we then obtain
γ(zn) = p(zn, X)
p(X)

= α(zn)β(zn)

p(X)

(13.53)

(13.54)



Exercise 13.11

in agreement with (13.33). The result (13.43) can similarly be derived from (8.72).

13.2.4 Scaling factors
There is an important issue that must be addressed before we can make use of the
forward backward algorithm in practice. From the recursion relation (13.36), we note
that at each step the new value α(zn) is obtained from the previous value α(zn−1)
by multiplying by quantities p(zn|zn−1) and p(xn|zn). Because these probabilities
are often signiﬁcantly less than unity, as we work our way forward along the chain,
the values of α(zn) can go to zero exponentially quickly. For moderate lengths of
chain (say 100 or so), the calculation of the α(zn) will soon exceed the dynamic
range of the computer, even if double precision ﬂoating point is used.

In the case of i.i.d. data, we implicitly circumvented this problem with the eval-
uation of likelihood functions by taking logarithms. Unfortunately, this will not help
here because we are forming sums of products of small numbers (we are in fact im-
plicitly summing over all possible paths through the lattice diagram of Figure 13.7).
We therefore work with re-scaled versions of α(zn) and β(zn) whose values remain
of order unity. As we shall see, the corresponding scaling factors cancel out when
we use these re-scaled quantities in the EM algorithm.

In (13.34), we deﬁned α(zn) = p(x1, . . . , xn, zn) representing the joint distri-
bution of all the observations up to xn and the latent variable zn. Now we deﬁne a
normalized version of α given by

α(zn) = p(zn|x1, . . . , xn) =

α(zn)

p(x1, . . . , xn)

(13.55)

which we expect to be well behaved numerically because it is a probability distribu-
tion over K variables for any value of n. In order to relate the scaled and original al-
pha variables, we introduce scaling factors deﬁned by conditional distributions over
the observed variables

cn = p(xn|x1, . . . , xn−1).

(13.56)










n




n







zn−1

N

N

n=1

m=n+1







zn+1










628

13. SEQUENTIAL DATA

From the product rule, we then have

p(x1, . . . , xn) =

cm

m=1

(13.57)

and so

α(zn) = p(zn|x1, . . . , xn)p(x1, . . . , xn) =

cm

α(zn).

(13.58)

m=1

We can then turn the recursion equation (13.36) for α into one for

α given by

cn

α(zn) = p(xn|zn)

α(zn−1)p(zn|zn−1).

(13.59)

α(zn),
Note that at each stage of the forward message passing phase, used to evaluate
we have to evaluate and store cn, which is easily done because it is the coefﬁcient
that normalizes the right-hand side of (13.59) to give

We can similarly deﬁne re-scaled variables

α(zn).
β(zn) using

β(zn) =

cm

β(zn)

(13.60)

which will again remain within machine precision because, from (13.35), the quan-
tities

β(zn) are simply the ratio of two conditional probabilities

β(zn) =

p(xn+1, . . . , xN|zn)

p(xn+1, . . . , xN|x1, . . . , xn) .

(13.61)

The recursion result (13.38) for β then gives the following recursion for the re-scaled
variables

cn+1

β(zn) =

β(zn+1)p(xn+1|zn+1)p(zn+1|zn).

(13.62)

In applying this recursion relation, we make use of the scaling factors cn that were
previously computed in the α phase.

From (13.57), we see that the likelihood function can be found using

p(X) =

cn.

(13.63)

Exercise 13.15

Similarly, using (13.33) and (13.43), together with (13.63), we see that the required
marginals are given by

γ(zn) =

α(zn)

β(zn)

ξ(zn−1, zn) = cn

α(zn−1)p(xn|zn)p(zn|z−1)

β(zn).

(13.64)
(13.65)







Section 13.3

13.2. Hidden Markov Models

629

α(zn)

β(zn) instead of using

Finally, we note that there is an alternative formulation of the forward-backward
algorithm (Jordan, 2007) in which the backward pass is deﬁned by a recursion based
the quantities γ(zn) =
β(zn). This α–γ recursion
α(zn)
requires that the forward pass be completed ﬁrst so that all the quantities
are available for the backward pass, whereas the forward and backward passes of
the α–β algorithm can be done independently. Although these two algorithms have
comparable computational cost, the α–β version is the most commonly encountered
one in the case of hidden Markov models, whereas for linear dynamical systems a
recursion analogous to the α–γ form is more usual.

13.2.5 The Viterbi algorithm
In many applications of hidden Markov models, the latent variables have some
meaningful interpretation, and so it is often of interest to ﬁnd the most probable
sequence of hidden states for a given observation sequence. For instance in speech
recognition, we might wish to ﬁnd the most probable phoneme sequence for a given
series of acoustic observations. Because the graph for the hidden Markov model is
a directed tree, this problem can be solved exactly using the max-sum algorithm.
We recall from our discussion in Section 8.4.5 that the problem of ﬁnding the most
probable sequence of latent states is not the same as that of ﬁnding the set of states
that are individually the most probable. The latter problem can be solved by ﬁrst
running the forward-backward (sum-product) algorithm to ﬁnd the latent variable
marginals γ(zn) and then maximizing each of these individually (Duda et al., 2001).
However, the set of such states will not, in general, correspond to the most probable
sequence of states. In fact, this set of states might even represent a sequence having
zero probability, if it so happens that two successive states, which in isolation are
individually the most probable, are such that the transition matrix element connecting
them is zero.

In practice, we are usually interested in ﬁnding the most probable sequence of
states, and this can be solved efﬁciently using the max-sum algorithm, which in the
context of hidden Markov models is known as the Viterbi algorithm (Viterbi, 1967).
Note that the max-sum algorithm works with log probabilities and so there is no
need to use re-scaled variables as was done with the forward-backward algorithm.
Figure 13.16 shows a fragment of the hidden Markov model expanded as lattice
diagram. As we have already noted, the number of possible paths through the lattice
grows exponentially with the length of the chain. The Viterbi algorithm searches this
space of paths efﬁciently to ﬁnd the most probable path with a computational cost
that grows only linearly with the length of the chain.

As with the sum-product algorithm, we ﬁrst represent the hidden Markov model
as a factor graph, as shown in Figure 13.15. Again, we treat the variable node zN
as the root, and pass messages to the root starting with the leaf nodes. Using the
results (8.93) and (8.94), we see that the messages passed in the max-sum algorithm
are given by

µzn→fn+1(zn) = µfn→zn(zn)

µfn+1→zn+1(zn+1) = max

zn

ln fn+1(zn, zn+1) + µzn→fn+1(zn)

(13.66)
. (13.67)







630

13. SEQUENTIAL DATA

Figure 13.16 A fragment of
the HMM lattice
showing two possible paths. The Viterbi algorithm
efﬁciently determines the most probable path from
amongst the exponentially many possibilities. For
any given path, the corresponding probability is
given by the product of the elements of the tran-
sition matrix Ajk, corresponding to the probabil-
ities p(zn+1|zn) for each segment of the path,
along with the emission densities p(xn|k) asso-
ciated with each node on the path.

k = 1

k = 2

k = 3

n − 2

n − 1

n

n + 1

If we eliminate µzn→fn+1(zn) between these two equations, and make use of (13.46),
we obtain a recursion for the f → z messages of the form

ω(zn+1) = ln p(xn+1|zn+1) + max

zn {ln p(x+1|zn) + ω(zn)}

(13.68)

where we have introduced the notation ω(zn) ≡ µfn→zn(zn).
From (8.95) and (8.96), these messages are initialized using

ω(z1) = ln p(z1) + ln p(x1|z1).

(13.69)

where we have used (13.45). Note that to keep the notation uncluttered, we omit
the dependence on the model parameters θ that are held ﬁxed when ﬁnding the most
probable sequence.

Exercise 13.16

The Viterbi algorithm can also be derived directly from the deﬁnition (13.6) of
the joint distribution by taking the logarithm and then exchanging maximizations
and summations. It is easily seen that the quantities ω(zn) have the probabilistic
interpretation

ω(zn) = max

p(x1, . . . , xn, z1, . . . , zn).

(13.70)

z1,...,zn−1

Once we have completed the ﬁnal maximization over zN , we will obtain the
value of the joint distribution p(X, Z) corresponding to the most probable path. We
also wish to ﬁnd the sequence of latent variable values that corresponds to this path.
To do this, we simply make use of the back-tracking procedure discussed in Sec-
tion 8.4.5. Speciﬁcally, we note that the maximization over zn must be performed
for each of the K possible values of zn+1. Suppose we keep a record of the values
of zn that correspond to the maxima for each value of the K values of zn+1. Let us
denote this function by ψ(kn) where k ∈ {1, . . . , K}. Once we have passed mes-
sages to the end of the chain and found the most probable state of zN , we can then
use this function to backtrack along the chain by applying it recursively

n = ψ(kmax
kmax

n+1).

(13.71)





r=1

R



R



13.2. Hidden Markov Models

631

Intuitively, we can understand the Viterbi algorithm as follows. Naively, we
could consider explicitly all of the exponentially many paths through the lattice,
evaluate the probability for each, and then select the path having the highest proba-
bility. However, we notice that we can make a dramatic saving in computational cost
as follows. Suppose that for each path we evaluate its probability by summing up
products of transition and emission probabilities as we work our way forward along
each path through the lattice. Consider a particular time step n and a particular state
k at that time step. There will be many possible paths converging on the correspond-
ing node in the lattice diagram. However, we need only retain that particular path
that so far has the highest probability. Because there are K states at time step n, we
need to keep track of K such paths. At time step n + 1, there will be K 2 possible
paths to consider, comprising K possible paths leading out of each of the K current
states, but again we need only retain K of these corresponding to the best path for
each state at time n+1. When we reach the ﬁnal time step N we will discover which
state corresponds to the overall most probable path. Because there is a unique path
coming into that state we can trace the path back to step N − 1 to see what state it
occupied at that time, and so on back through the lattice to the state n = 1.

13.2.6 Extensions of the hidden Markov model
The basic hidden Markov model, along with the standard training algorithm
based on maximum likelihood, has been extended in numerous ways to meet the
requirements of particular applications. Here we discuss a few of the more important
examples.

We see from the digits example in Figure 13.11 that hidden Markov models can
be quite poor generative models for the data, because many of the synthetic digits
look quite unrepresentative of the training data. If the goal is sequence classiﬁca-
tion, there can be signiﬁcant beneﬁt in determining the parameters of hidden Markov
models using discriminative rather than maximum likelihood techniques. Suppose
we have a training set of R observation sequences Xr, where r = 1, . . . , R, each of
which is labelled according to its class m, where m = 1, . . . , M. For each class, we
have a separate hidden Markov model with its own parameters θm, and we treat the
problem of determining the parameter values as a standard classiﬁcation problem in
which we optimize the cross-entropy

where p(m) is the prior probability of class m. Optimization of this cost function
is more complex than for maximum likelihood (Kapadia, 1998), and in particular

Using Bayes’ theorem this can be expressed in terms of the sequence probabilities
associated with the hidden Markov models

ln p(mr|Xr).

ln

r=1

p(Xr|θr)p(mr)
l=1 p(Xr|θl)p(lr)

M

(13.72)

(13.73)

632

13. SEQUENTIAL DATA

Figure 13.17 Section of an autoregressive hidden
Markov model, in which the distribution
of the observation xn depends on a
subset of the previous observations as
well as on the hidden state zn.
In this
example, the distribution of xn depends
on the two previous observations xn−1
and xn−2.

zn−1

xn−1

zn

xn

zn+1

xn+1



requires that every training sequence be evaluated under each of the models in or-
der to compute the denominator in (13.73). Hidden Markov models, coupled with
discriminative training methods, are widely used in speech recognition (Kapadia,
1998).

A signiﬁcant weakness of the hidden Markov model is the way in which it rep-
resents the distribution of times for which the system remains in a given state. To see
the problem, note that the probability that a sequence sampled from a given hidden
Markov model will spend precisely T steps in state k and then make a transition to a
different state is given by

p(T ) = (Akk)T (1 − Akk) ∝ exp (−T ln Akk)

(13.74)

T

and so is an exponentially decaying function of T . For many applications, this will
be a very unrealistic model of state duration. The problem can be resolved by mod-
elling state duration directly in which the diagonal coefﬁcients Akk are all set to zero,
and each state k is explicitly associated with a probability distribution p(T|k) of pos-
sible duration times. From a generative point of view, when a state k is entered, a
value T representing the number of time steps that the system will remain in state k
is then drawn from p(T|k). The model then emits T values of the observed variable
xt, which are generally assumed to be independent so that the corresponding emis-
t=1 p(xt|k). This approach requires some straightforward
sion density is simply
modiﬁcations to the EM optimization procedure (Rabiner, 1989).
Another limitation of the standard HMM is that it is poor at capturing long-
range correlations between the observed variables (i.e., between variables that are
separated by many time steps) because these must be mediated via the ﬁrst-order
Markov chain of hidden states. Longer-range effects could in principle be included
by adding extra links to the graphical model of Figure 13.5. One way to address this
is to generalize the HMM to give the autoregressive hidden Markov model (Ephraim
et al., 1989), an example of which is shown in Figure 13.17. For discrete observa-
tions, this corresponds to expanded tables of conditional probabilities for the emis-
sion distributions. In the case of a Gaussian emission density, we can use the linear-
Gaussian framework in which the conditional distribution for xn given the values
of the previous observations, and the value of zn, is a Gaussian whose mean is a
linear combination of the values of the conditioning variables. Clearly the number
of additional links in the graph must be limited to avoid an excessive the number of
free parameters. In the example shown in Figure 13.17, each observation depends on

Figure 13.18 Example of an input-output hidden
Markov model.
In this case, both the
emission probabilities and the transition
probabilities depend on the values of a
sequence of observations u1, . . . , uN .

13.2. Hidden Markov Models

633

un−1

zn−1

xn−1

un

zn

xn

un+1

zn+1

xn+1

the two preceding observed variables as well as on the hidden state. Although this
graph looks messy, we can again appeal to d-separation to see that in fact it still has
a simple probabilistic structure. In particular, if we imagine conditioning on zn we
see that, as with the standard HMM, the values of zn−1 and zn+1 are independent,
corresponding to the conditional independence property (13.5). This is easily veri-
ﬁed by noting that every path from node zn−1 to node zn+1 passes through at least
one observed node that is head-to-tail with respect to that path. As a consequence,
we can again use a forward-backward recursion in the E step of the EM algorithm to
determine the posterior distributions of the latent variables in a computational time
that is linear in the length of the chain. Similarly, the M step involves only a minor
modiﬁcation of the standard M-step equations. In the case of Gaussian emission
densities this involves estimating the parameters using the standard linear regression
equations, discussed in Chapter 3.

We have seen that the autoregressive HMM appears as a natural extension of the
standard HMM when viewed as a graphical model. In fact the probabilistic graphical
modelling viewpoint motivates a plethora of different graphical structures based on
the HMM. Another example is the input-output hidden Markov model (Bengio and
Frasconi, 1995), in which we have a sequence of observed variables u1, . . . , uN , in
addition to the output variables x1, . . . , xN , whose values inﬂuence either the dis-
tribution of latent variables or output variables, or both. An example is shown in
Figure 13.18. This extends the HMM framework to the domain of supervised learn-
ing for sequential data. It is again easy to show, through the use of the d-separation
criterion, that the Markov property (13.5) for the chain of latent variables still holds.
To verify this, simply note that there is only one path from node zn−1 to node zn+1
and this is head-to-tail with respect to the observed node zn. This conditional inde-
pendence property again allows the formulation of a computationally efﬁcient learn-
ing algorithm. In particular, we can determine the parameters θ of the model by
maximizing the likelihood function L(θ) = p(X|U, θ) where U is a matrix whose
rows are given by uT
n. As a consequence of the conditional independence property
(13.5) this likelihood function can be maximized efﬁciently using an EM algorithm
in which the E step involves forward and backward recursions.

Another variant of the HMM worthy of mention is the factorial hidden Markov
model (Ghahramani and Jordan, 1997), in which there are multiple independent

Exercise 13.18

634

13. SEQUENTIAL DATA

Figure 13.19 A factorial hidden Markov model com-
prising two Markov chains of latent vari-
ables. For continuous observed variables
x, one possible choice of emission model
is a linear-Gaussian density in which the
mean of the Gaussian is a linear combi-
nation of the states of the corresponding
latent variables.

z(2)
n−1

z(1)
n−1

xn−1

z(2)
n

z(1)
n

z(2)
n+1

z(1)
n+1

xn

xn+1

Markov chains of latent variables, and the distribution of the observed variable at
a given time step is conditional on the states of all of the corresponding latent vari-
ables at that same time step. Figure 13.19 shows the corresponding graphical model.
The motivation for considering factorial HMM can be seen by noting that in order to
represent, say, 10 bits of information at a given time step, a standard HMM would
need K = 210 = 1024 latent states, whereas a factorial HMM could make use of 10
binary latent chains. The primary disadvantage of factorial HMMs, however, lies in
the additional complexity of training them. The M step for the factorial HMM model
is straightforward. However, observation of the x variables introduces dependencies
between the latent chains, leading to difﬁculties with the E step. This can be seen
by noting that in Figure 13.19, the variables z(1)
n are connected by a path
which is head-to-head at node xn and hence they are not d-separated. The exact E
step for this model does not correspond to running forward and backward recursions
along the M Markov chains independently. This is conﬁrmed by noting that the key
conditional independence property (13.5) is not satisﬁed for the individual Markov
chains in the factorial HMM model, as is shown using d-separation in Figure 13.20.
Now suppose that there are M chains of hidden nodes and for simplicity suppose
that all latent variables have the same number K of states. Then one approach would
be to note that there are KM combinations of latent variables at a given time step

n and z(2)

Figure 13.20 Example of a path, highlighted in green,
which is head-to-head at the observed
nodes xn−1 and xn+1, and head-to-tail
at the unobserved nodes z(2)
n and
z(2)
n+1. Thus the path is not blocked and
so the conditional independence property
(13.5) does not hold for the individual la-
tent chains of the factorial HMM model.
As a consequence, there is no efﬁcient
exact E step for this model.

n−1, z(2)

z(2)
n−1

z(1)
n−1

xn−1

z(2)
n

z(1)
n

z(2)
n+1

z(1)
n+1

xn

xn+1

Section 10.1

13.3. Linear Dynamical Systems

635

and so we can transform the model into an equivalent standard HMM having a single
chain of latent variables each of which has KM latent states. We can then run the
standard forward-backward recursions in the E step. This has computational com-
plexity O(N K 2M ) that is exponential in the number M of latent chains and so will
be intractable for anything other than small values of M. One solution would be
to use sampling methods (discussed in Chapter 11). As an elegant deterministic al-
ternative, Ghahramani and Jordan (1997) exploited variational inference techniques
to obtain a tractable algorithm for approximate inference. This can be done using
a simple variational posterior distribution that is fully factorized with respect to the
latent variables, or alternatively by using a more powerful approach in which the
variational distribution is described by independent Markov chains corresponding to
the chains of latent variables in the original model. In the latter case, the variational
inference algorithms involves running independent forward and backward recursions
along each chain, which is computationally efﬁcient and yet is also able to capture
correlations between variables within the same chain.

Clearly, there are many possible probabilistic structures that can be constructed
according to the needs of particular applications. Graphical models provide a general
technique for motivating, describing, and analysing such structures, and variational
methods provide a powerful framework for performing inference in those models for
which exact solution is intractable.

13.3. Linear Dynamical Systems

In order to motivate the concept of linear dynamical systems, let us consider the
following simple problem, which often arises in practical settings. Suppose we wish
to measure the value of an unknown quantity z using a noisy sensor that returns a
observation x representing the value of z plus zero-mean Gaussian noise. Given a
single measurement, our best guess for z is to assume that z = x. However, we
can improve our estimate for z by taking lots of measurements and averaging them,
because the random noise terms will tend to cancel each other. Now let’s make the
situation more complicated by assuming that we wish to measure a quantity z that
is changing over time. We can take regular measurements of x so that at some point
in time we have obtained x1, . . . , xN and we wish to ﬁnd the corresponding values
z1, . . . , xN . If we simply average the measurements, the error due to random noise
will be reduced, but unfortunately we will just obtain a single averaged estimate, in
which we have averaged over the changing value of z, thereby introducing a new
source of error.

Intuitively, we could imagine doing a bit better as follows. To estimate the value
of zN , we take only the most recent few measurements, say xN−L, . . . , xN and just
average these. If z is changing slowly, and the random noise level in the sensor is
high, it would make sense to choose a relatively long window of observations to
average. Conversely, if the signal is changing quickly, and the noise levels are small,
we might be better just to use xN directly as our estimate of zN . Perhaps we could
do even better if we take a weighted average, in which more recent measurements













636

13. SEQUENTIAL DATA

make a greater contribution than less recent ones.

Although this sort of intuitive argument seems plausible, it does not tell us how
to form a weighted average, and any sort of hand-crafted weighing is hardly likely
to be optimal. Fortunately, we can address problems such as this much more sys-
tematically by deﬁning a probabilistic model that captures the time evolution and
measurement processes and then applying the inference and learning methods devel-
oped in earlier chapters. Here we shall focus on a widely used model known as a
linear dynamical system.

As we have seen, the HMM corresponds to the state space model shown in
Figure 13.5 in which the latent variables are discrete but with arbitrary emission
probability distributions. This graph of course describes a much broader class of
probability distributions, all of which factorize according to (13.6). We now consider
extensions to other distributions for the latent variables. In particular, we consider
continuous latent variables in which the summations of the sum-product algorithm
become integrals. The general form of the inference algorithms will, however, be
the same as for the hidden Markov model. It is interesting to note that, historically,
hidden Markov models and linear dynamical systems were developed independently.
Once they are both expressed as graphical models, however, the deep relationship
between them immediately becomes apparent.

One key requirement is that we retain an efﬁcient algorithm for inference which
is linear in the length of the chain. This requires that, for instance, when we take
α(zn−1), representing the posterior probability of zn given observations
a quantity
x1, . . . , xn, and multiply by the transition probability p(zn|zn−1) and the emission
probability p(xn|zn) and then marginalize over zn−1, we obtain a distribution over
α(zn−1). That is to say, the
zn that is of the same functional form as that over
distribution must not become more complex at each stage, but must only change in
its parameter values. Not surprisingly, the only distributions that have this property
of being closed under multiplication are those belonging to the exponential family.
Here we consider the most important example from a practical perspective,
which is the Gaussian. In particular, we consider a linear-Gaussian state space model
so that the latent variables {zn}, as well as the observed variables {xn}, are multi-
variate Gaussian distributions whose means are linear functions of the states of their
parents in the graph. We have seen that a directed graph of linear-Gaussian units
is equivalent to a joint Gaussian distribution over all of the variables. Furthermore,
α(zn) are also Gaussian, so that the functional form of the mes-
marginals such as
sages is preserved and we will obtain an efﬁcient inference algorithm. By contrast,
suppose that the emission densities p(xn|zn) comprise a mixture of K Gaussians
α(z1) is Gaussian, the
each of which has a mean that is linear in zn. Then even if
α(z3) will be a mixture of K 2
quantity
Gaussians, and so on, and exact inference will not be of practical value.

α(z2) will be a mixture of K Gaussians,

We have seen that the hidden Markov model can be viewed as an extension of
the mixture models of Chapter 9 to allow for sequential correlations in the data.
In a similar way, we can view the linear dynamical system as a generalization of the
continuous latent variable models of Chapter 12 such as probabilistic PCA and factor
analysis. Each pair of nodes {zn, xn} represents a linear-Gaussian latent variable

13.3. Linear Dynamical Systems

637

model for that particular observation. However, the latent variables {zn} are no
longer treated as independent but now form a Markov chain.
Because the model is represented by a tree-structured directed graph, inference
problems can be solved efﬁciently using the sum-product algorithm. The forward re-
cursions, analogous to the α messages of the hidden Markov model, are known as the
Kalman ﬁlter equations (Kalman, 1960; Zarchan and Musoff, 2005), and the back-
ward recursions, analogous to the β messages, are known as the Kalman smoother
equations, or the Rauch-Tung-Striebel (RTS) equations (Rauch et al., 1965). The
Kalman ﬁlter is widely used in many real-time tracking applications.

Because the linear dynamical system is a linear-Gaussian model, the joint distri-
bution over all variables, as well as all marginals and conditionals, will be Gaussian.
It follows that the sequence of individually most probable latent variable values is
the same as the most probable latent sequence. There is thus no need to consider the
analogue of the Viterbi algorithm for the linear dynamical system.

Because the model has linear-Gaussian conditional distributions, we can write

the transition and emission distributions in the general form
p(zn|zn−1) = N (zn|Azn−1, Γ)
p(xn|zn) = N (xn|Czn, Σ).

The initial latent variable also has a Gaussian distribution which we write as

p(z1) = N (z1|µ0, V0).

(13.75)
(13.76)

(13.77)

Exercise 13.19

Exercise 13.24

Note that in order to simplify the notation, we have omitted additive constant terms
from the means of the Gaussians. In fact, it is straightforward to include them if
desired. Traditionally, these distributions are more commonly expressed in an equiv-
alent form in terms of noisy linear equations given by

zn = Azn−1 + wn
xn = Czn + vn
z1 = µ0 + u

(13.78)
(13.79)
(13.80)

where the noise terms have the distributions

w ∼ N (w|0, Γ)
v ∼ N (v|0, Σ)
u ∼ N (u|0, V0).

(13.81)
(13.82)
(13.83)
The parameters of the model, denoted by θ = {A, Γ, C, Σ, µ0, V0}, can be
determined using maximum likelihood through the EM algorithm. In the E step, we
need to solve the inference problem of determining the local posterior marginals for
the latent variables, which can be solved efﬁciently using the sum-product algorithm,
as we discuss in the next section.












638

13. SEQUENTIAL DATA

13.3.1 Inference in LDS
We now turn to the problem of ﬁnding the marginal distributions for the latent
variables conditional on the observation sequence. For given parameter settings, we
also wish to make predictions of the next latent state zn and of the next observation
xn conditioned on the observed data x1, . . . , xn−1 for use in real-time applications.
These inference problems can be solved efﬁciently using the sum-product algorithm,
which in the context of the linear dynamical system gives rise to the Kalman ﬁlter
and Kalman smoother equations.

It is worth emphasizing that because the linear dynamical system is a linear-
Gaussian model, the joint distribution over all latent and observed variables is simply
a Gaussian, and so in principle we could solve inference problems by using the
standard results derived in previous chapters for the marginals and conditionals of a
multivariate Gaussian. The role of the sum-product algorithm is to provide a more
efﬁcient way to perform such computations.

Linear dynamical systems have the identical factorization, given by (13.6), to
hidden Markov models, and are again described by the factor graphs in Figures 13.14
and 13.15. Inference algorithms therefore take precisely the same form except that
summations over latent variables are replaced by integrations. We begin by consid-
ering the forward equations in which we treat zN as the root node, and propagate
messages from the leaf node h(z1) to the root. From (13.77), the initial message will
be Gaussian, and because each of the factors is Gaussian, all subsequent messages
will also be Gaussian. By convention, we shall propagate messages that are nor-
malized marginal distributions corresponding to p(zn|x1, . . . , xn), which we denote
by
(13.84)
α(zn) given by
This is precisely analogous to the propagation of scaled variables
(13.59) in the discrete case of the hidden Markov model, and so the recursion equa-
tion now takes the form

α(zn) = N (zn|µn, Vn).

cn

α(zn) = p(xn|zn)

α(zn−1)p(zn|zn−1) dzn−1.

(13.85)

Substituting for the conditionals p(zn|zn−1) and p(xn|zn), using (13.75) and (13.76),
respectively, and making use of (13.84), we see that (13.85) becomes

cnN (zn|µn, Vn) = N (xn|Czn, Σ)

N (zn|Azn−1, Γ)N (zn−1|µn−1, Vn−1) dzn−1.

(13.86)

Here we are supposing that µn−1 and Vn−1 are known, and by evaluating the inte-
gral in (13.86), we wish to determine values for µn and Vn. The integral is easily
evaluated by making use of the result (2.115), from which it follows that

N (zn|Azn−1, Γ)N (zn−1|µn−1, Vn−1) dzn−1

= N (zn|Aµn−1, Pn−1)

(13.87)












13.3. Linear Dynamical Systems

639

(13.88)

(13.89)
(13.90)
(13.91)

where we have deﬁned

Pn−1 = AVn−1AT + Γ.

We can now combine this result with the ﬁrst factor on the right-hand side of (13.86)
by making use of (2.115) and (2.116) to give

µn = Aµn−1 + Kn(xn − CAµn−1)
Vn = (I − KnC)Pn−1
cn = N (xn|CAµn−1, CPn−1CT + Σ).

Here we have made use of the matrix inverse identities (C.5) and (C.7) and also
deﬁned the Kalman gain matrix

Kn = Pn−1CT

(13.92)
Thus, given the values of µn−1 and Vn−1, together with the new observation xn,
we can evaluate the Gaussian marginal for zn having mean µn and covariance Vn,
as well as the normalization coefﬁcient cn.

CPn−1CT + Σ

The initial conditions for these recursion equations are obtained from

−1

.

(13.93)
Because p(z1) is given by (13.77), and p(x1|z1) is given by (13.76), we can again
make use of (2.115) to calculate c1 and (2.116) to calculate µ1 and V1 giving

α(z1) = p(z1)p(x1|z1).

c1

µ1 = µ0 + K1(x1 − Cµ0)
V1 = (I − K1C)V0
c1 = N (x1|Cµ0, CV0CT + Σ)

K1 = V0CT

CV0CT + Σ

−1

.

(13.94)
(13.95)
(13.96)

(13.97)

where

Similarly, the likelihood function for the linear dynamical system is given by (13.63)
in which the factors cn are found using the Kalman ﬁltering equations.

We can interpret the steps involved in going from the posterior marginal over
zn−1 to the posterior marginal over zn as follows.
In (13.89), we can view the
quantity Aµn−1 as the prediction of the mean over zn obtained by simply taking the
mean over zn−1 and projecting it forward one step using the transition probability
matrix A. This predicted mean would give a predicted observation for xn given by
CAzn−1 obtained by applying the emission probability matrix C to the predicted
hidden state mean. We can view the update equation (13.89) for the mean of the
hidden variable distribution as taking the predicted mean Aµn−1 and then adding
a correction that is proportional to the error xn − CAzn−1 between the predicted
observation and the actual observation. The coefﬁcient of this correction is given by
the Kalman gain matrix. Thus we can view the Kalman ﬁlter as a process of making
successive predictions and then correcting these predictions in the light of the new
observations. This is illustrated graphically in Figure 13.21.

















640

13. SEQUENTIAL DATA

zn−1

zn

zn

Figure 13.21 The linear dynamical system can be viewed as a sequence of steps in which increasing un-
certainty in the state variable due to diffusion is compensated by the arrival of new data. In the left-hand plot,
the blue curve shows the distribution p(zn−1|x1, . . . , xn−1), which incorporates all the data up to step n − 1.
The diffusion arising from the nonzero variance of the transition probability p(zn|zn−1) gives the distribution
p(zn|x1, . . . , xn−1), shown in red in the centre plot. Note that this is broader and shifted relative to the blue curve
(which is shown dashed in the centre plot for comparison). The next data observation xn contributes through the
emission density p(xn|zn), which is shown as a function of zn in green on the right-hand plot. Note that this is not
a density with respect to zn and so is not normalized to one. Inclusion of this new data point leads to a revised
distribution p(zn|x1, . . . , xn) for the state density shown in blue. We see that observation of the data has shifted
and narrowed the distribution compared to p(zn|x1, . . . , xn−1) (which is shown in dashed in the right-hand plot
for comparison).

Exercise 13.27

Exercise 13.28

If we consider a situation in which the measurement noise is small compared
to the rate at which the latent variable is evolving, then we ﬁnd that the posterior
distribution for zn depends only on the current measurement xn, in accordance with
the intuition from our simple example at the start of the section. Similarly, if the
latent variable is evolving slowly relative to the observation noise level, we ﬁnd that
the posterior mean for zn is obtained by averaging all of the measurements obtained
up to that time.

One of the most important applications of the Kalman ﬁlter is to tracking, and
this is illustrated using a simple example of an object moving in two dimensions in
Figure 13.22.

So far, we have solved the inference problem of ﬁnding the posterior marginal
for a node zn given observations from x1 up to xn. Next we turn to the problem of
ﬁnding the marginal for a node zn given all observations x1 to xN . For temporal
data, this corresponds to the inclusion of future as well as past observations. Al-
though this cannot be used for real-time prediction, it plays a key role in learning the
parameters of the model. By analogy with the hidden Markov model, this problem
can be solved by propagating messages from node xN back to node x1 and com-
bining this information with that obtained during the forward message passing stage
used to compute the

α(zn).

In the LDS literature, it is usual to formulate this backward recursion in terms
β(zn). Because γ(zn) must also be

β(zn) rather than in terms of

of γ(zn) =
Gaussian, we write it in the form

α(zn)

γ(zn) =

α(zn)

β(zn) = N (zn|

µn,

Vn).

(13.98)

To derive the required recursion, we start from the backward recursion (13.62) for












13.3. Linear Dynamical Systems

641

Figure 13.22 An illustration of a linear dy-
namical system being used to
track a moving object. The blue
points indicate the true positions
of the object in a two-dimensional
space at successive time steps,
the green points denote noisy
measurements of the positions,
and the red crosses indicate the
means of the inferred posterior
distributions of the positions ob-
tained by running the Kalman ﬁl-
tering equations.
The covari-
ances of
the inferred positions
are indicated by the red ellipses,
which correspond to contours
having one standard deviation.

β(zn), which, for continuous latent variables, can be written in the form

cn+1

β(zn) =

β(zn+1)p(xn+1|zn+1)p(zn+1|zn) dzn+1.

(13.99)

α(zn) and substitute for p(xn+1|zn+1)
We now multiply both sides of (13.99) by
and p(zn+1|zn) using (13.75) and (13.76). Then we make use of (13.89), (13.90)
and (13.91), together with (13.98), and after some manipulation we obtain

µn = µn + Jn
Vn = Vn + Jn

µn+1 − AµN
Vn+1 − Pn

JT
n

(13.100)

(13.101)

where we have deﬁned

(13.102)
and we have made use of AVn = PnJT
n. Note that these recursions require that the
forward pass be completed ﬁrst so that the quantities µn and Vn will be available
for the backward pass.

Jn = VnAT (Pn)−1

For the EM algorithm, we also require the pairwise posterior marginals, which

can be obtained from (13.65) in the form
ξ(zn−1, zn) = (cn)−1
= N (zn−1|µn−1, Vn−1)N (zn|Azn−1, Γ)N (xn|Czn, Σ)N (zn|

α(zn−1)p(xn|zn)p(zn|z−1)

β(zn)

µn,

Vn)

.

cn

α(zn)

(13.103)
α(zn) using (13.84) and rearranging, we see that ξ(zn−1, zn) is a
Substituting for
Gaussian with mean given with components γ(zn−1) and γ(zn), and a covariance
between zn and zn−1 given by

cov[zn, zn−1] = Jn−1

Vn.

(13.104)

Exercise 13.29

Exercise 13.31



























N

n=2





	
	


N







642

13. SEQUENTIAL DATA

13.3.2 Learning in LDS
So far, we have considered the inference problem for linear dynamical systems,
assuming that the model parameters θ = {A, Γ, C, Σ, µ0, V0} are known. Next, we
consider the determination of these parameters using maximum likelihood (Ghahra-
mani and Hinton, 1996b). Because the model has latent variables, this can be ad-
dressed using the EM algorithm, which was discussed in general terms in Chapter 9.
We can derive the EM algorithm for the linear dynamical system as follows. Let
us denote the estimated parameter values at some particular cycle of the algorithm
by θold. For these parameter values, we can run the inference algorithm to determine
the posterior distribution of the latent variables p(Z|X, θold), or more precisely those
local posterior marginals that are required in the M step.
In particular, we shall
require the following expectations

E [zn] =

µn

E

znzT
E

n−1
znzT
n

= Jn−1
=
Vn +

Vn +
µT
n

µn

µT

µn

n−1

(13.105)
(13.106)

(13.107)

where we have used (13.104).

Now we consider the complete-data log likelihood function, which is obtained

by taking the logarithm of (13.6) and is therefore given by

ln p(X, Z|θ) = ln p(z1|µ0, V0) +

ln p(zn|zn−1, A, Γ)

+

n=1

ln p(xn|zn, C, Σ)

(13.108)

in which we have made the dependence on the parameters explicit. We now take the
expectation of the complete-data log likelihood with respect to the posterior distri-
bution p(Z|X, θold) which deﬁnes the function

Q(θ, θold) = EZ|θold [ln p(X, Z|θ)] .

(13.109)

In the M step, this function is maximized with respect to the components of θ.

Consider ﬁrst the parameters µ0 and V0. If we substitute for p(z1|µ0, V0) in
(13.108) using (13.77), and then take the expectation with respect to Z, we obtain

Q(θ, θold) = −

1
2

ln|V0| − EZ|θold

1
2

(z1 − µ0)TV−1

0 (z1 − µ0)

+ const

where all terms not dependent on µ0 or V0 have been absorbed into the additive
constant. Maximization with respect to µ0 and V0 is easily performed by making
use of the maximum likelihood solution for a Gaussian distribution discussed in
Section 2.3.4, giving

Exercise 13.32











	







	


µnew
Vnew

N − 1

ln|Σ|

n=2
1

xnE

N
2

1
2

n=2

n=1

n=1

E

2

N

N

N

N

N

N

1
N
−xnE

n=1

zT
n

N − 1
znzT
n−1



	

	

	


n=2

N

N



 
	
	
	

	
	


 


−1






13.3. Linear Dynamical Systems

643

0

(13.110)
(13.111)
Similarly, to optimize A and Γ, we substitute for p(zn|zn−1, A, Γ) in (13.108)

1 ] − E[z1]E[zT
1 ].

= E[z1]
= E[z1zT

0

using (13.75) giving

Q(θ, θold) = −

ln|Γ|

−EZ|θold

1
2

(zn − Azn−1)TΓ−1(zn − Azn−1)

+ const

(13.112)

in which the constant comprises terms that are independent of A and Γ. Maximizing
with respect to these parameters then gives

Anew =

Γnew =

−E

znzT

n−1

E

zn−1zT

n−1

E

znzT
n

− AnewE

zn−1zT

n

n=2
Anew + AnewE

zn−1zT

n−1

(Anew)T

(13.113)

.

(13.114)

Note that Anew must be evaluated ﬁrst, and the result can then be used to determine
Γnew.

Finally, in order to determine the new values of C and Σ, we substitute for

p(xn|zn, C, Σ) in (13.108) using (13.76) giving

Q(θ, θold) = −

−EZ|θold

(xn − Czn)TΣ−1(xn − Czn)

+ const.

Maximizing with respect to C and Σ then gives

Cnew =

Σnew =

zT
n

E

n=1

znzT
n

xnxT

n − CnewE [zn] xT
Cnew + CnewE
znzT
n

n

−1

(13.115)

Cnew

.

(13.116)

Exercise 13.33

Exercise 13.34




644

Chapter 10



13. SEQUENTIAL DATA

We have approached parameter learning in the linear dynamical system using
maximum likelihood. Inclusion of priors to give a MAP estimate is straightforward,
and a fully Bayesian treatment can be found by applying the analytical approxima-
tion techniques discussed in Chapter 10, though a detailed treatment is precluded
here due to lack of space.

13.3.3 Extensions of LDS
As with the hidden Markov model, there is considerable interest in extending
the basic linear dynamical system in order to increase its capabilities. Although the
assumption of a linear-Gaussian model leads to efﬁcient algorithms for inference
and learning, it also implies that the marginal distribution of the observed variables
is simply a Gaussian, which represents a signiﬁcant limitation. One simple extension
of the linear dynamical system is to use a Gaussian mixture as the initial distribution
for z1.
If this mixture has K components, then the forward recursion equations
(13.85) will lead to a mixture of K Gaussians over each hidden variable zn, and so
the model is again tractable.

For many applications, the Gaussian emission density is a poor approximation.
If instead we try to use a mixture of K Gaussians as the emission density, then the
α(z1) will also be a mixture of K Gaussians. However, from (13.85) the
posterior
α(zn)
α(z2) will comprise a mixture of K 2 Gaussians, and so on, with
posterior
being given by a mixture of K n Gaussians. Thus the number of components grows
exponentially with the length of the chain, and so this model is impractical.

More generally, introducing transition or emission models that depart from the
linear-Gaussian (or other exponential family) model leads to an intractable infer-
ence problem. We can make deterministic approximations such as assumed den-
sity ﬁltering or expectation propagation, or we can make use of sampling methods,
as discussed in Section 13.3.4. One widely used approach is to make a Gaussian
approximation by linearizing around the mean of the predicted distribution, which
gives rise to the extended Kalman ﬁlter (Zarchan and Musoff, 2005).

As with hidden Markov models, we can develop interesting extensions of the ba-
sic linear dynamical system by expanding its graphical representation. For example,
the switching state space model (Ghahramani and Hinton, 1998) can be viewed as
a combination of the hidden Markov model with a set of linear dynamical systems.
The model has multiple Markov chains of continuous linear-Gaussian latent vari-
ables, each of which is analogous to the latent chain of the linear dynamical system
discussed earlier, together with a Markov chain of discrete variables of the form used
in a hidden Markov model. The output at each time step is determined by stochas-
tically choosing one of the continuous latent chains, using the state of the discrete
latent variable as a switch, and then emitting an observation from the corresponding
conditional output distribution. Exact inference in this model is intractable, but vari-
ational methods lead to an efﬁcient inference scheme involving forward-backward
recursions along each of the continuous and discrete Markov chains independently.
Note that, if we consider multiple chains of discrete latent variables, and use one as
the switch to select from the remainder, we obtain an analogous model having only
discrete latent variables known as the switching hidden Markov model.








l=1

L



Chapter 11

13.3. Linear Dynamical Systems

645

13.3.4 Particle ﬁlters
For dynamical systems which do not have a linear-Gaussian, for example, if
they use a non-Gaussian emission density, we can turn to sampling methods in order
to ﬁnd a tractable inference algorithm. In particular, we can apply the sampling-
importance-resampling formalism of Section 11.1.5 to obtain a sequential Monte
Carlo algorithm known as the particle ﬁlter.

Consider the class of distributions represented by the graphical model in Fig-
ure 13.5, and suppose we are given the observed values Xn = (x1, . . . , xn) and
we wish to draw L samples from the posterior distribution p(zn|Xn). Using Bayes’
theorem, we have

f(zn)p(zn|Xn) dzn
f(zn)p(zn|xn, Xn−1) dzn
f(zn)p(xn|zn)p(zn|Xn−1) dzn

p(xn|zn)p(zn|Xn−1) dzn

E[f(zn)] =

=

=



w(l)

n f(z(l)
n )

(13.117)

where {z(l)
n } is a set of samples drawn from p(zn|Xn−1) and we have made use of
the conditional independence property p(xn|zn, Xn−1) = p(xn|zn), which follows
from the graph in Figure 13.5. The sampling weights {w(l)

n } are deﬁned by

w(l)

n =

p(xn|z(l)
n )
m=1 p(xn|z(m)
n )

L

(13.118)

l w(l)

where the same samples are used in the numerator as in the denominator. Thus the
posterior distribution p(zn|xn) is represented by the set of samples {z(l)
n } together
n }. Note that these weights satisfy 0 � w(l)
with the corresponding weights {w(l)
n 1
and
Because we wish to ﬁnd a sequential sampling scheme, we shall suppose that
a set of samples and weights have been obtained at time step n, and that we have
subsequently observed the value of xn+1, and we wish to ﬁnd the weights and sam-
ples at time step n + 1. We ﬁrst sample from the distribution p(zn+1|Xn). This is

n = 1.









p(zn+1|Xn) =
=

=

=

=

p(zn+1|zn, Xn)p(zn|Xn) dzn
p(zn+1|zn)p(zn|Xn) dzn
p(zn+1|zn)p(zn|xn, Xn−1) dzn
p(zn+1|zn)p(xn|zn)p(zn|Xn−1) dzn

p(xn|zn)p(zn|Xn−1) dzn

n p(zn+1|z(l)
n )
w(l)

l

(13.119)

(13.120)
(13.121)

646

13. SEQUENTIAL DATA

straightforward since, again using Bayes’ theorem

where we have made use of the conditional independence properties

p(zn+1|zn, Xn) = p(zn+1|zn)
p(xn|zn, Xn−1) = p(xn|zn)

which follow from the application of the d-separation criterion to the graph in Fig-
ure 13.5. The distribution given by (13.119) is a mixture distribution, and samples
can be drawn by choosing a component l with probability given by the mixing coef-
ﬁcients w(l) and then drawing a sample from the corresponding component.

In summary, we can view each step of the particle ﬁlter algorithm as comprising
two stages. At time step n, we have a sample representation of the posterior dis-
n } with corresponding weights {w(l)
tribution p(zn|Xn) expressed as samples {z(l)
n }.
This can be viewed as a mixture representation of the form (13.119). To obtain the
corresponding representation for the next time step, we ﬁrst draw L samples from
the mixture distribution (13.119), and then for each sample we use the new obser-
vation xn+1 to evaluate the corresponding weights w(l)
n+1). This is
illustrated, for the case of a single variable z, in Figure 13.23.

n+1 ∝ p(xn+1|z(l)

The particle ﬁltering, or sequential Monte Carlo, approach has appeared in the
literature under various names including the bootstrap ﬁlter (Gordon et al., 1993),
survival of the ﬁttest (Kanazawa et al., 1995), and the condensation algorithm (Isard
and Blake, 1998).

Exercises

13.1 () www Use the technique of d-separation, discussed in Section 8.2, to verify
that the Markov model shown in Figure 13.3 having N nodes in total satisﬁes the
conditional independence properties (13.3) for n = 2, . . . , N. Similarly, show that
a model described by the graph in Figure 13.4 in which there are N nodes in total

p(zn|Xn)

p(zn+1|Xn)

p(xn+1|zn+1)

p(zn+1|Xn+1)

Exercises

647

z

Figure 13.23 Schematic illustration of the operation of the particle ﬁlter for a one-dimensional latent
space. At time step n, the posterior p(zn|xn) is represented as a mixture distribution,
shown schematically as circles whose sizes are proportional to the weights w(l)
n . A set of
L samples is then drawn from this distribution and the new weights w(l)
n+1 evaluated using
p(xn+1|z(l)

n+1).

satisﬁes the conditional independence properties

p(xn|x1, . . . , xn−1) = p(xn|xn−1, xn−2)

(13.122)

for n = 3, . . . , N.

13.2 ( ) Consider the joint probability distribution (13.2) corresponding to the directed
graph of Figure 13.3. Using the sum and product rules of probability, verify that
this joint distribution satisﬁes the conditional independence property (13.3) for n =
2, . . . , N. Similarly, show that the second-order Markov model described by the
joint distribution (13.4) satisﬁes the conditional independence property

p(xn|x1, . . . , xn−1) = p(xn|xn−1, xn−2)

(13.123)

for n = 3, . . . , N.

13.3 () By using d-separation, show that the distribution p(x1, . . . , xN ) of the observed
data for the state space model represented by the directed graph in Figure 13.5 does
not satisfy any conditional independence properties and hence does not exhibit the
Markov property at any ﬁnite order.

13.4 ( ) www Consider a hidden Markov model in which the emission densities are
represented by a parametric model p(x|z, w), such as a linear regression model or
a neural network, in which w is a vector of adaptive parameters. Describe how the
parameters w can be learned from data using maximum likelihood.

648

13. SEQUENTIAL DATA

13.5 ( ) Verify the M-step equations (13.18) and (13.19) for the initial state probabili-
ties and transition probability parameters of the hidden Markov model by maximiza-
tion of the expected complete-data log likelihood function (13.17), using appropriate
Lagrange multipliers to enforce the summation constraints on the components of π
and A.

13.6 ()

Show that if any elements of the parameters π or A for a hidden Markov
model are initially set to zero, then those elements will remain zero in all subsequent
updates of the EM algorithm.

13.7 () Consider a hidden Markov model with Gaussian emission densities. Show that
maximization of the function Q(θ, θold) with respect to the mean and covariance
parameters of the Gaussians gives rise to the M-step equations (13.20) and (13.21).

13.8 ( ) www For a hidden Markov model having discrete observations governed by
a multinomial distribution, show that the conditional distribution of the observations
given the hidden variables is given by (13.22) and the corresponding M step equa-
tions are given by (13.23). Write down the analogous equations for the conditional
distribution and the M step equations for the case of a hidden Markov with multiple
binary output variables each of which is governed by a Bernoulli conditional dis-
tribution. Hint: refer to Sections 2.1 and 2.2 for a discussion of the corresponding
maximum likelihood solutions for i.i.d. data if required.

13.9 ( ) www Use the d-separation criterion to verify that the conditional indepen-
dence properties (13.24)–(13.31) are satisﬁed by the joint distribution for the hidden
Markov model deﬁned by (13.6).

13.10 (  ) By applying the sum and product rules of probability, verify that the condi-
tional independence properties (13.24)–(13.31) are satisﬁed by the joint distribution
for the hidden Markov model deﬁned by (13.6).

13.11 ( ) Starting from the expression (8.72) for the marginal distribution over the vari-
ables of a factor in a factor graph, together with the results for the messages in the
sum-product algorithm obtained in Section 13.2.3, derive the result (13.43) for the
joint posterior distribution over two successive latent variables in a hidden Markov
model.

13.12 ( ) Suppose we wish to train a hidden Markov model by maximum likelihood
using data that comprises R independent sequences of observations, which we de-
note by X(r) where r = 1, . . . , R. Show that in the E step of the EM algorithm,
we simply evaluate posterior probabilities for the latent variables by running the α
and β recursions independently for each of the sequences. Also show that in the
M step, the initial probability and transition probability parameters are re-estimated














n=1

n=2

n=2

r=1

r=1

r=1

j=1

r=1

l=1

K

N

R

K

R

N

r=1

R

N

R

using modiﬁed forms of (13.18 ) and (13.19) given by

Exercises

649

(13.124)

(13.125)

R

γ(z(r)
1k )

γ(z(r)
1j )

πk =

Ajk =

ξ(z(r)

n−1,j, z(r)
n,k)

ξ(z(r)

n−1,j, z(r)
n,l)

γ(z(r)

nk )x(r)

n

where, for notational convenience, we have assumed that the sequences are of the
same length (the generalization to sequences of different lengths is straightforward).
Similarly, show that the M-step equation for re-estimation of the means of Gaussian
emission models is given by

µk =

R

N

.

(13.126)

γ(z(r)
nk )

r=1

n=1

Note that the M-step equations for other emission model parameters and distributions
take an analogous form.

13.13 ( ) www Use the deﬁnition (8.64) of the messages passed from a factor node
to a variable node in a factor graph, together with the expression (13.6) for the joint
distribution in a hidden Markov model, to show that the deﬁnition (13.50) of the
alpha message is the same as the deﬁnition (13.34).

13.14 ( ) Use the deﬁnition (8.67) of the messages passed from a factor node to a
variable node in a factor graph, together with the expression (13.6) for the joint
distribution in a hidden Markov model, to show that the deﬁnition (13.52) of the
beta message is the same as the deﬁnition (13.35).

13.15 ( ) Use the expressions (13.33) and (13.43) for the marginals in a hidden Markov
model to derive the corresponding results (13.64) and (13.65) expressed in terms of
re-scaled variables.

13.16 (  )

In this exercise, we derive the forward message passing equation for the
Viterbi algorithm directly from the expression (13.6) for the joint distribution. This
involves maximizing over all of the hidden variables z1, . . . , zN . By taking the log-
arithm and then exchanging maximizations and summations, derive the recursion

650

13. SEQUENTIAL DATA

(13.68) where the quantities ω(zn) are deﬁned by (13.70). Show that the initial
condition for this recursion is given by (13.69).

13.17 () www Show that the directed graph for the input-output hidden Markov model,
given in Figure 13.18, can be expressed as a tree-structured factor graph of the form
shown in Figure 13.15 and write down expressions for the initial factor h(z1) and
for the general factor fn(zn−1, zn) where 2 � n � N.

13.18 (  ) Using the result of Exercise 13.17, derive the recursion equations, includ-
ing the initial conditions, for the forward-backward algorithm for the input-output
hidden Markov model shown in Figure 13.18.

13.19 () www The Kalman ﬁlter and smoother equations allow the posterior distribu-
tions over individual latent variables, conditioned on all of the observed variables,
to be found efﬁciently for linear dynamical systems. Show that the sequence of
latent variable values obtained by maximizing each of these posterior distributions
individually is the same as the most probable sequence of latent values. To do this,
simply note that the joint distribution of all latent and observed variables in a linear
dynamical system is Gaussian, and hence all conditionals and marginals will also be
Gaussian, and then make use of the result (2.98).

13.20 ( ) www Use the result (2.115) to prove (13.87).

13.21 ( ) Use the results (2.115) and (2.116), together with the matrix identities (C.5)
and (C.7), to derive the results (13.89), (13.90), and (13.91), where the Kalman gain
matrix Kn is deﬁned by (13.92).

13.22 ( ) www Using (13.93), together with the deﬁnitions (13.76) and (13.77) and

the result (2.115), derive (13.96).

13.23 ( ) Using (13.93), together with the deﬁnitions (13.76) and (13.77) and the result

(2.116), derive (13.94), (13.95) and (13.97).

13.24 ( ) www Consider a generalization of (13.75) and (13.76) in which we include

constant terms a and c in the Gaussian means, so that

p(zn|zn−1) = N (zn|Azn−1 + a, Γ)
p(xn|zn) = N (xn|Czn + c, Σ).

(13.127)
(13.128)

Show that this extension can be re-case in the framework discussed in this chapter by
deﬁning a state vector z with an additional component ﬁxed at unity, and then aug-
menting the matrices A and C using extra columns corresponding to the parameters
a and c.

13.25 ( )

In this exercise, we show that when the Kalman ﬁlter equations are applied
to independent observations, they reduce to the results given in Section 2.3 for the
maximum likelihood solution for a single Gaussian distribution. Consider the prob-
lem of ﬁnding the mean µ of a single Gaussian random variable x, in which we are
given a set of independent observations {x1, . . . , xN}. To model this we can use

Exercises

651

a linear dynamical system governed by (13.75) and (13.76), with latent variables
{z1, . . . , zN} in which C becomes the identity matrix and where the transition prob-
ability A = 0 because the observations are independent. Let the parameters m0
and V0 of the initial state be denoted by µ0 and σ2
0, respectively, and suppose that
Σ becomes σ2. Write down the corresponding Kalman ﬁlter equations starting from
the general results (13.89) and (13.90), together with (13.94) and (13.95). Show that
these are equivalent to the results (2.141) and (2.142) obtained directly by consider-
ing independent data.

13.26 (  ) Consider a special case of the linear dynamical system of Section 13.3 that is
equivalent to probabilistic PCA, so that the transition matrix A = 0, the covariance
Γ = I, and the noise covariance Σ = σ2I. By making use of the matrix inversion
identity (C.7) show that, if the emission density matrix C is denoted W, then the
posterior distribution over the hidden states deﬁned by (13.89) and (13.90) reduces
to the result (12.42) for probabilistic PCA.



13.27 () www Consider a linear dynamical system of the form discussed in Sec-
tion 13.3 in which the amplitude of the observation noise goes to zero, so that Σ = 0.
Show that the posterior distribution for zn has mean xn and zero variance. This
accords with our intuition that if there is no noise, we should just use the current
observation xn to estimate the state variable zn and ignore all previous observations.
13.28 (  ) Consider a special case of the linear dynamical system of Section 13.3 in
which the state variable zn is constrained to be equal to the previous state variable,
which corresponds to A = I and Γ = 0. For simplicity, assume also that V0 → ∞
so that the initial conditions for z are unimportant, and the predictions are determined
purely by the data. Use proof by induction to show that the posterior mean for state
zn is determined by the average of x1, . . . , xn. This corresponds to the intuitive
result that if the state variable is constant, our best estimate is obtained by averaging
the observations.

13.29 (  )

Starting from the backwards recursion equation (13.99), derive the RTS
smoothing equations (13.100) and (13.101) for the Gaussian linear dynamical sys-
tem.

13.30 ( ) Starting from the result (13.65) for the pairwise posterior marginal in a state
space model, derive the speciﬁc form (13.103) for the case of the Gaussian linear
dynamical system.

13.31 ( ) Starting from the result (13.103) and by substituting for

α(zn) using (13.84),

verify the result (13.104) for the covariance between zn and zn−1.

13.32 ( ) www Verify the results (13.110) and (13.111) for the M-step equations for

µ0 and V0 in the linear dynamical system.

13.33 ( ) Verify the results (13.113) and (13.114) for the M-step equations for A and Γ

in the linear dynamical system.

652

13. SEQUENTIAL DATA

13.34 ( ) Verify the results (13.115) and (13.116) for the M-step equations for C and Σ

in the linear dynamical system.

14

Combining

Models

In earlier chapters, we have explored a range of different models for solving classiﬁ-
cation and regression problems. It is often found that improved performance can be
obtained by combining multiple models together in some way, instead of just using
a single model in isolation. For instance, we might train L different models and then
make predictions using the average of the predictions made by each model. Such
combinations of models are sometimes called committees. In Section 14.2, we dis-
cuss ways to apply the committee concept in practice, and we also give some insight
into why it can sometimes be an effective procedure.

One important variant of the committee method, known as boosting, involves
training multiple models in sequence in which the error function used to train a par-
ticular model depends on the performance of the previous models. This can produce
substantial improvements in performance compared to the use of a single model and
is discussed in Section 14.3.

Instead of averaging the predictions of a set of models, an alternative form of

653




k=1

K

model combination is to select one of the models to make the prediction, in which
the choice of model is a function of the input variables. Thus different models be-
come responsible for making predictions in different regions of input space. One
widely used framework of this kind is known as a decision tree in which the selec-
tion process can be described as a sequence of binary selections corresponding to
the traversal of a tree structure and is discussed in Section 14.4. In this case, the
individual models are generally chosen to be very simple, and the overall ﬂexibility
of the model arises from the input-dependent selection process. Decision trees can
be applied to both classiﬁcation and regression problems.

One limitation of decision trees is that the division of input space is based on
hard splits in which only one model is responsible for making predictions for any
given value of the input variables. The decision process can be softened by moving
to a probabilistic framework for combining models, as discussed in Section 14.5. For
example, if we have a set of K models for a conditional distribution p(t|x, k) where
x is the input variable, t is the target variable, and k = 1, . . . , K indexes the model,
then we can form a probabilistic mixture of the form

p(t|x) =

πk(x)p(t|x, k)

(14.1)

in which πk(x) = p(k|x) represent the input-dependent mixing coefﬁcients. Such
models can be viewed as mixture distributions in which the component densities, as
well as the mixing coefﬁcients, are conditioned on the input variables and are known
as mixtures of experts. They are closely related to the mixture density network model
discussed in Section 5.6.

654

14. COMBINING MODELS

14.1. Bayesian Model Averaging

Section 9.2

It is important to distinguish between model combination methods and Bayesian
model averaging, as the two are often confused. To understand the difference, con-
sider the example of density estimation using a mixture of Gaussians in which several
Gaussian components are combined probabilistically. The model contains a binary
latent variable z that indicates which component of the mixture is responsible for
generating the corresponding data point. Thus the model is speciﬁed in terms of a
joint distribution

(14.2)
and the corresponding density over the observed variable x is obtained by marginal-
izing over the latent variable

p(x, z)

p(x) =

p(x, z).

z

(14.3)

 



N





N




k=1

K

H

In the case of our Gaussian mixture example, this leads to a distribution of the form

14.2. Committees

655

p(x) =

πkN (x|µk, Σk)

(14.4)

with the usual interpretation of the symbols. This is an example of model combi-
nation. For independent, identically distributed data, we can use (14.3) to write the
marginal probability of a data set X = {x1, . . . , xN} in the form

p(X) =

n=1

p(xn) =

p(xn, zn)

.

(14.5)

n=1

zn

Thus we see that each observed data point xn has a corresponding latent variable zn.
Now suppose we have several different models indexed by h = 1, . . . , H with
prior probabilities p(h). For instance one model might be a mixture of Gaussians and
another model might be a mixture of Cauchy distributions. The marginal distribution
over the data set is given by

p(X) =

p(X|h)p(h).

h=1

(14.6)

This is an example of Bayesian model averaging. The interpretation of this summa-
tion over h is that just one model is responsible for generating the whole data set,
and the probability distribution over h simply reﬂects our uncertainty as to which
model that is. As the size of the data set increases, this uncertainty reduces, and
the posterior probabilities p(h|X) become increasingly focussed on just one of the
models.
This highlights the key difference between Bayesian model averaging and model
combination, because in Bayesian model averaging the whole data set is generated
by a single model. By contrast, when we combine multiple models, as in (14.5), we
see that different data points within the data set can potentially be generated from
different values of the latent variable z and hence by different components.

Although we have considered the marginal probability p(X), the same consid-
erations apply for the predictive density p(x|X) or for conditional distributions such
as p(t|x, X, T).

Exercise 14.1

14.2. Committees

Section 3.2

The simplest way to construct a committee is to average the predictions of a set of
individual models. Such a procedure can be motivated from a frequentist perspective
by considering the trade-off between bias and variance, which decomposes the er-
ror due to a model into the bias component that arises from differences between the
model and the true function to be predicted, and the variance component that repre-
sents the sensitivity of the model to the individual data points. Recall from Figure 3.5




	





1
M

Ex

m=1

M

M

M

M

m=1

1
M

1
M

m=1

⎡⎣
⎡⎣

1
M

	


⎤⎦




	
⎤⎦

.

2

656

14. COMBINING MODELS

that when we trained multiple polynomials using the sinusoidal data, and then aver-
aged the resulting functions, the contribution arising from the variance term tended to
cancel, leading to improved predictions. When we averaged a set of low-bias mod-
els (corresponding to higher order polynomials), we obtained accurate predictions
for the underlying sinusoidal function from which the data were generated.

In practice, of course, we have only a single data set, and so we have to ﬁnd
a way to introduce variability between the different models within the committee.
One approach is to use bootstrap data sets, discussed in Section 1.2.3. Consider a
regression problem in which we are trying to predict the value of a single continuous
variable, and suppose we generate M bootstrap data sets and then use each to train
a separate copy ym(x) of a predictive model where m = 1, . . . , M. The committee
prediction is given by

yCOM(x) =

ym(x).

(14.7)

This procedure is known as bootstrap aggregation or bagging (Breiman, 1996).

Suppose the true regression function that we are trying to predict is given by
h(x), so that the output of each of the models can be written as the true value plus
an error in the form

ym(x) = h(x) + 	m(x).
The average sum-of-squares error then takes the form

Ex

(14.9)
where Ex[·] denotes a frequentist expectation with respect to the distribution of the
input vector x. The average error made by the models acting individually is therefore

{ym(x) − h(x)}2

	m(x)2

= Ex

EAV =

	m(x)2

Similarly, the expected error from the committee (14.7) is given by

ECOM = Ex

= Ex

2

ym(x) − h(x)

	m(x)

m=1

If we assume that the errors have zero mean and are uncorrelated, so that

Ex [	m(x)] = 0
Ex [	m(x)	l(x)] = 0,

m = l

(14.8)

(14.10)

(14.11)

(14.12)
(14.13)

Exercise 14.2

then we obtain

14.3. Boosting

657

ECOM =

1
M

EAV.

(14.14)

This apparently dramatic result suggests that the average error of a model can be
reduced by a factor of M simply by averaging M versions of the model. Unfortu-
nately, it depends on the key assumption that the errors due to the individual models
are uncorrelated. In practice, the errors are typically highly correlated, and the reduc-
tion in overall error is generally small. It can, however, be shown that the expected
committee error will not exceed the expected error of the constituent models, so that
ECOM � EAV. In order to achieve more signiﬁcant improvements, we turn to a more
sophisticated technique for building committees, known as boosting.

Exercise 14.3

14.3. Boosting

Boosting is a powerful technique for combining multiple ‘base’ classiﬁers to produce
a form of committee whose performance can be signiﬁcantly better than that of any
of the base classiﬁers. Here we describe the most widely used form of boosting
algorithm called AdaBoost, short for ‘adaptive boosting’, developed by Freund and
Schapire (1996). Boosting can give good results even if the base classiﬁers have a
performance that is only slightly better than random, and hence sometimes the base
classiﬁers are known as weak learners. Originally designed for solving classiﬁcation
problems, boosting can also be extended to regression (Friedman, 2001).

The principal difference between boosting and the committee methods such as
bagging discussed above, is that the base classiﬁers are trained in sequence, and
each base classiﬁer is trained using a weighted form of the data set in which the
weighting coefﬁcient associated with each data point depends on the performance
of the previous classiﬁers. In particular, points that are misclassiﬁed by one of the
base classiﬁers are given greater weight when used to train the next classiﬁer in
the sequence. Once all the classiﬁers have been trained, their predictions are then
combined through a weighted majority voting scheme, as illustrated schematically
in Figure 14.1.

Consider a two-class classiﬁcation problem, in which the training data comprises
input vectors x1, . . . , xN along with corresponding binary target variables t1, . . . , tN
where tn ∈ {−1, 1}. Each data point is given an associated weighting parameter
wn, which is initially set 1/N for all data points. We shall suppose that we have
a procedure available for training a base classiﬁer using weighted data to give a
function y(x) ∈ {−1, 1}. At each stage of the algorithm, AdaBoost trains a new
classiﬁer using a data set in which the weighting coefﬁcients are adjusted according
to the performance of the previously trained classiﬁer so as to give greater weight
to the misclassiﬁed data points. Finally, when the desired number of base classiﬁers
have been trained, they are combined to form a committee using coefﬁcients that
give different weight to different base classiﬁers. The precise form of the AdaBoost
algorithm is given below.



{w(M )

n

}

yM (x)

αmym(x)





M

m




n=1

N

N




n=1

N

658

14. COMBINING MODELS

Figure 14.1 Schematic illustration of

the
boosting framework.
Each
base classiﬁer ym(x) is trained
on a weighted form of the train-
ing set (blue arrows) in which
the weights w(m)
depend on
the performance of
the pre-
vious base classiﬁer ym−1(x)
(green arrows). Once all base
classiﬁers have been trained,
they are combined to give
the ﬁnal classiﬁer YM (x) (red
arrows).

n

{w(1)
n }

y1(x)

AdaBoost

{w(2)
n }

y2(x)

YM (x) = sign

1. Initialize the data weighting coefﬁcients {wn} by setting w(1)
2. For m = 1, . . . , M:

n = 1, . . . , N.

n = 1/N for

(a) Fit a classiﬁer ym(x) to the training data by minimizing the weighted

error function

Jm =

n I(ym(xn) = tn)
w(m)

(14.15)

where I(ym(xn) = tn) is the indicator function and equals 1 when
ym(xn) = tn and 0 otherwise.

(b) Evaluate the quantities

n I(ym(xn) = tn)
w(m)

	m =

n=1

w(m)

n

and then use these to evaluate

αm = ln

1 − 	m
	m

.

(c) Update the data weighting coefﬁcients

(14.16)

(14.17)

w(m+1)

n

= w(m)

n

exp{αmI(ym(xn) = tn)}

(14.18)





M




m



N

14.3. Boosting

659

3. Make predictions using the ﬁnal model, which is given by

YM (x) = sign

αmym(x)

.

(14.19)

m=1

n

We see that the ﬁrst base classiﬁer y1(x) is trained using weighting coefﬁ-
cients w(1)
n that are all equal, which therefore corresponds to the usual procedure
for training a single classiﬁer. From (14.18), we see that in subsequent iterations
the weighting coefﬁcients w(m)
are increased for data points that are misclassiﬁed
and decreased for data points that are correctly classiﬁed. Successive classiﬁers are
therefore forced to place greater emphasis on points that have been misclassiﬁed by
previous classiﬁers, and data points that continue to be misclassiﬁed by successive
classiﬁers receive ever greater weight. The quantities 	m represent weighted mea-
sures of the error rates of each of the base classiﬁers on the data set. We therefore
see that the weighting coefﬁcients αm deﬁned by (14.17) give greater weight to the
more accurate classiﬁers when computing the overall output given by (14.19).

The AdaBoost algorithm is illustrated in Figure 14.2, using a subset of 30 data
points taken from the toy classiﬁcation data set shown in Figure A.7. Here each base
learners consists of a threshold on one of the input variables. This simple classiﬁer
corresponds to a form of decision tree known as a ‘decision stumps’, i.e., a deci-
sion tree with a single node. Thus each base learner classiﬁes an input according to
whether one of the input features exceeds some threshold and therefore simply parti-
tions the space into two regions separated by a linear decision surface that is parallel
to one of the axes.

14.3.1 Minimizing exponential error
Boosting was originally motivated using statistical learning theory, leading to
upper bounds on the generalization error. However, these bounds turn out to be too
loose to have practical value, and the actual performance of boosting is much better
than the bounds alone would suggest. Friedman et al. (2000) gave a different and
very simple interpretation of boosting in terms of the sequential minimization of an
exponential error function.

Consider the exponential error function deﬁned by

E =

n=1

exp{−tnfm(xn)}

(14.20)

where fm(x) is a classiﬁer deﬁned in terms of a linear combination of base classiﬁers
yl(x) of the form

fm(x) =

1
2

αlyl(x)

l=1

(14.21)

and tn ∈ {−1, 1} are the training set target values. Our goal is to minimize E with
respect to both the weighting coefﬁcients αl and the parameters of the base classiﬁers
yl(x).

Section 14.4








n=1

N

N





660

14. COMBINING MODELS

m = 1

−1

0

1

2

m = 6

2

0

−2

2

0

−2

2

0

−2

2

0

−2

−1

−1

0

1

2

−1

m = 2

0

0

1

1

m = 10

2

2

2

0

−2

2

0

−2

−1

−1

0

0

m = 3

1

2

m = 150

1

2

Figure 14.2 Illustration of boosting in which the base learners consist of simple thresholds applied to one or
other of the axes. Each ﬁgure shows the number m of base learners trained so far, along with the decision
boundary of the most recent base learner (dashed black line) and the combined decision boundary of the en-
semble (solid green line). Each data point is depicted by a circle whose radius indicates the weight assigned to
that data point when training the most recently added base learner. Thus, for instance, we see that points that
are misclassiﬁed by the m = 1 base learner are given greater weight when training the m = 2 base learner.

Instead of doing a global error function minimization, however, we shall sup-
pose that the base classiﬁers y1(x), . . . , ym−1(x) are ﬁxed, as are their coefﬁcients
α1, . . . , αm−1, and so we are minimizing only with respect to αm and ym(x). Sep-
arating off the contribution from base classiﬁer ym(x), we can then write the error
function in the form

E =

exp

−tnfm−1(xn) −

1
2 tnαmym(xn)

=

w(m)

n

exp

n=1

1
2 tnαmym(xn)

−

(14.22)

where the coefﬁcients w(m)
n = exp{−tnfm−1(xn)} can be viewed as constants
because we are optimizing only αm and ym(x).
If we denote by Tm the set of
data points that are correctly classiﬁed by ym(x), and if we denote the remaining
misclassiﬁed points by Mm, then we can in turn rewrite the error function in the



form

E = e−αm/2

n∈Tm



N



w(m)

n




n=1

N

n










14.3. Boosting

661

w(m)

n + eαm/2

n∈Mm

= (eαm/2 − e−αm/2)

n I(ym(xn) = tn) + e−αm/2
w(m)

w(m)
n .

n=1

(14.23)

When we minimize this with respect to ym(x), we see that the second term is con-
stant, and so this is equivalent to minimizing (14.15) because the overall multiplica-
tive factor in front of the summation does not affect the location of the minimum.
Similarly, minimizing with respect to αm, we obtain (14.17) in which 	m is deﬁned
by (14.16).

From (14.22) we see that, having found αm and ym(x), the weights on the data

points are updated using

w(m+1)

n

Making use of the fact that

= w(m)

exp

1
2 tnαmym(xn)

−

.

(14.24)

(14.25)

tnym(xn) = 1 − 2I(ym(xn) = tn)

we see that the weights w(m)

n

are updated at the next iteration using
exp(−αm/2) exp{αmI(ym(xn) = tn)} .

n

n

= w(m)

w(m+1)

(14.26)
Because the term exp(−αm/2) is independent of n, we see that it weights all data
points by the same factor and so can be discarded. Thus we obtain (14.18).
Finally, once all the base classiﬁers are trained, new data points are classiﬁed by
evaluating the sign of the combined function deﬁned according to (14.21). Because
the factor of 1/2 does not affect the sign it can be omitted, giving (14.19).

14.3.2 Error functions for boosting
The exponential error function that is minimized by the AdaBoost algorithm
differs from those considered in previous chapters. To gain some insight into the
nature of the exponential error function, we ﬁrst consider the expected error given
by

Ex,t [exp{−ty(x)}] =

t

exp{−ty(x)}p(t|x)p(x) dx.

(14.27)

If we perform a variational minimization with respect to all possible functions y(x),
we obtain

y(x) =

ln

1
2

p(t = 1|x)
p(t = −1|x)

(14.28)

Exercise 14.6

Exercise 14.7

662

14. COMBINING MODELS

Figure 14.3 Plot of the exponential (green) and
rescaled cross-entropy (red) error
functions along with the hinge er-
ror
(blue) used in support vector
machines, and the misclassiﬁcation
for large
error (black). Note that
negative values of z = ty(x),
the
cross-entropy gives a linearly in-
creasing penalty, whereas the expo-
nential loss gives an exponentially in-
creasing penalty.

E(z)

−2

−1

0

1

z

2

which is half the log-odds. Thus the AdaBoost algorithm is seeking the best approx-
imation to the log odds ratio, within the space of functions represented by the linear
combination of base classiﬁers, subject to the constrained minimization resulting
from the sequential optimization strategy. This result motivates the use of the sign
function in (14.19) to arrive at the ﬁnal classiﬁcation decision.

We have already seen that the minimizer y(x) of the cross-entropy error (4.90)
for two-class classiﬁcation is given by the posterior class probability. In the case
of a target variable t ∈ {−1, 1}, we have seen that the error function is given by
ln(1 + exp(−yt)). This is compared with the exponential error function in Fig-
ure 14.3, where we have divided the cross-entropy error by a constant factor ln(2)
so that it passes through the point (0, 1) for ease of comparison. We see that both
can be seen as continuous approximations to the ideal misclassiﬁcation error func-
tion. An advantage of the exponential error is that its sequential minimization leads
to the simple AdaBoost scheme. One drawback, however, is that it penalizes large
negative values of ty(x) much more strongly than cross-entropy. In particular, we
see that for large negative values of ty, the cross-entropy grows linearly with |ty|,
whereas the exponential error function grows exponentially with |ty|. Thus the ex-
ponential error function will be much less robust to outliers or misclassiﬁed data
points. Another important difference between cross-entropy and the exponential er-
ror function is that the latter cannot be interpreted as the log likelihood function of
any well-deﬁned probabilistic model. Furthermore, the exponential error does not
generalize to classiﬁcation problems having K > 2 classes, again in contrast to the
cross-entropy for a probabilistic model, which is easily generalized to give (4.108).
The interpretation of boosting as the sequential optimization of an additive model
under an exponential error (Friedman et al., 2000) opens the door to a wide range
of boosting-like algorithms, including multiclass extensions, by altering the choice
of error function. It also motivates the extension to regression problems (Friedman,
2001). If we consider a sum-of-squares error function for regression, then sequential
minimization of an additive model of the form (14.21) simply involves ﬁtting each
new base classiﬁer to the residual errors tn−fm−1(xn) from the previous model. As
we have noted, however, the sum-of-squares error is not robust to outliers, and this

Section 7.1.2

Exercise 14.8

Section 4.3.4

Exercise 14.9

Figure 14.4 Comparison of

the squared error
(green) with the absolute error (red)
showing how the latter places much
less emphasis on large errors and
hence is more robust to outliers and
mislabelled data points.

14.4. Tree-based Models

663

E(z)

can be addressed by basing the boosting algorithm on the absolute deviation |y − t|
instead. These two error functions are compared in Figure 14.4.

−1

0

1

z

14.4. Tree-based Models

There are various simple, but widely used, models that work by partitioning the
input space into cuboid regions, whose edges are aligned with the axes, and then
assigning a simple model (for example, a constant) to each region. They can be
viewed as a model combination method in which only one model is responsible
for making predictions at any given point in input space. The process of selecting
a speciﬁc model, given a new input x, can be described by a sequential decision
making process corresponding to the traversal of a binary tree (one that splits into
two branches at each node). Here we focus on a particular tree-based framework
called classiﬁcation and regression trees, or CART (Breiman et al., 1984), although
there are many other variants going by such names as ID3 and C4.5 (Quinlan, 1986;
Quinlan, 1993).

Figure 14.5 shows an illustration of a recursive binary partitioning of the input
space, along with the corresponding tree structure. In this example, the ﬁrst step

Figure 14.5 Illustration of a two-dimensional

in-
put space that has been partitioned
into ﬁve regions using axis-aligned
boundaries.

x2

θ3

θ2

B

A

E

C

D

θ1

θ4

x1

664

14. COMBINING MODELS

Figure 14.6 Binary tree corresponding to the par-
input space shown in Fig-

titioning of
ure 14.5.

x1 > θ1

x2 � θ2

x2 > θ3

x1 � θ4

A

B

C

D

E

divides the whole of the input space into two regions according to whether x1 � θ1
or x1 > θ1 where θ1 is a parameter of the model. This creates two subregions, each
of which can then be subdivided independently. For instance, the region x1 � θ1
is further subdivided according to whether x2 � θ2 or x2 > θ2, giving rise to the
regions denoted A and B. The recursive subdivision can be described by the traversal
of the binary tree shown in Figure 14.6. For any new input x, we determine which
region it falls into by starting at the top of the tree at the root node and following
a path down to a speciﬁc leaf node according to the decision criteria at each node.
Note that such decision trees are not probabilistic graphical models.

Within each region, there is a separate model to predict the target variable. For
instance, in regression we might simply predict a constant over each region, or in
classiﬁcation we might assign each region to a speciﬁc class. A key property of tree-
based models, which makes them popular in ﬁelds such as medical diagnosis, for
example, is that they are readily interpretable by humans because they correspond
to a sequence of binary decisions applied to the individual input variables. For in-
stance, to predict a patient’s disease, we might ﬁrst ask “is their temperature greater
than some threshold?”. If the answer is yes, then we might next ask “is their blood
pressure less than some threshold?”. Each leaf of the tree is then associated with a
speciﬁc diagnosis.

In order to learn such a model from a training set, we have to determine the
structure of the tree, including which input variable is chosen at each node to form
the split criterion as well as the value of the threshold parameter θi for the split. We
also have to determine the values of the predictive variable within each region.

Consider ﬁrst a regression problem in which the goal is to predict a single target
variable t from a D-dimensional vector x = (x1, . . . , xD)T of input variables. The
training data consists of input vectors {x1, . . . , xN} along with the corresponding
continuous labels {t1, . . . , tN}. If the partitioning of the input space is given, and we
minimize the sum-of-squares error function, then the optimal value of the predictive
variable within any given region is just given by the average of the values of tn for
those data points that fall in that region.

Now consider how to determine the structure of the decision tree. Even for a
ﬁxed number of nodes in the tree, the problem of determining the optimal structure
(including choice of input variable for each split as well as the corresponding thresh-

Exercise 14.10






1
Nτ

|T|

τ =1

The pruning criterion is then given by

Qτ (T ) =

{tn − yτ}2 .

xn∈Rτ

C(T ) =

Qτ (T ) + λ|T|

The regularization parameter λ determines the trade-off between the overall residual
sum-of-squares error and the complexity of the model as measured by the number
|T| of leaf nodes, and its value is chosen by cross-validation.
For classiﬁcation problems, the process of growing and pruning the tree is sim-
ilar, except that the sum-of-squares error is replaced by a more appropriate measure

14.4. Tree-based Models

665

olds) to minimize the sum-of-squares error is usually computationally infeasible due
to the combinatorially large number of possible solutions. Instead, a greedy opti-
mization is generally done by starting with a single root node, corresponding to the
whole input space, and then growing the tree by adding nodes one at a time. At each
step there will be some number of candidate regions in input space that can be split,
corresponding to the addition of a pair of leaf nodes to the existing tree. For each
of these, there is a choice of which of the D input variables to split, as well as the
value of the threshold. The joint optimization of the choice of region to split, and the
choice of input variable and threshold, can be done efﬁciently by exhaustive search
noting that, for a given choice of split variable and threshold, the optimal choice of
predictive variable is given by the local average of the data, as noted earlier. This
is repeated for all possible choices of variable to be split, and the one that gives the
smallest residual sum-of-squares error is retained.

Given a greedy strategy for growing the tree, there remains the issue of when
to stop adding nodes. A simple approach would be to stop when the reduction in
residual error falls below some threshold. However, it is found empirically that often
none of the available splits produces a signiﬁcant reduction in error, and yet after
several more splits a substantial error reduction is found. For this reason, it is com-
mon practice to grow a large tree, using a stopping criterion based on the number
of data points associated with the leaf nodes, and then prune back the resulting tree.
The pruning is based on a criterion that balances residual error against a measure of
model complexity. If we denote the starting tree for pruning by T0, then we deﬁne
T ⊂ T0 to be a subtree of T0 if it can be obtained by pruning nodes from T0 (in
other words, by collapsing internal nodes by combining the corresponding regions).
Suppose the leaf nodes are indexed by τ = 1, . . . ,|T|, with leaf node τ representing
a region Rτ of input space having Nτ data points, and |T| denoting the total number
of leaf nodes. The optimal prediction for region Rτ is then given by

yτ =

tn

xn∈Rτ

and the corresponding contribution to the residual sum-of-squares is then

(14.29)

(14.30)

(14.31)




k=1

K

K

Qτ (T ) =

pτ k (1 − pτ k) .

k=1

(14.32)

(14.33)

Exercise 14.11

These both vanish for pτ k = 0 and pτ k = 1 and have a maximum at pτ k = 0.5. They
encourage the formation of regions in which a high proportion of the data points are
assigned to one class. The cross entropy and the Gini index are better measures than
the misclassiﬁcation rate for growing the tree because they are more sensitive to the
node probabilities. Also, unlike misclassiﬁcation rate, they are differentiable and
hence better suited to gradient based optimization methods. For subsequent pruning
of the tree, the misclassiﬁcation rate is generally used.

The human interpretability of a tree model such as CART is often seen as its
major strength. However, in practice it is found that the particular tree structure that
is learned is very sensitive to the details of the data set, so that a small change to the
training data can result in a very different set of splits (Hastie et al., 2001).

There are other problems with tree-based methods of the kind considered in
this section. One is that the splits are aligned with the axes of the feature space,
which may be very suboptimal. For instance, to separate two classes whose optimal
decision boundary runs at 45 degrees to the axes would need a large number of
axis-parallel splits of the input space as compared to a single non-axis-aligned split.
Furthermore, the splits in a decision tree are hard, so that each region of input space
is associated with one, and only one, leaf node model. The last issue is particularly
problematic in regression where we are typically aiming to model smooth functions,
and yet the tree model produces piecewise-constant predictions with discontinuities
at the split boundaries.

14.5. Conditional Mixture Models

We have seen that standard decision trees are restricted by hard, axis-aligned splits of
the input space. These constraints can be relaxed, at the expense of interpretability,
by allowing soft, probabilistic splits that can be functions of all of the input variables,
not just one of them at a time. If we also give the leaf models a probabilistic inter-
pretation, we arrive at a fully probabilistic tree-based model called the hierarchical
mixture of experts, which we consider in Section 14.5.3.

An alternative way to motivate the hierarchical mixture of experts model is to
start with a standard probabilistic mixtures of unconditional density models such as
Gaussians and replace the component densities with conditional distributions. Here
we consider mixtures of linear regression models (Section 14.5.1) and mixtures of

Chapter 9

666

14. COMBINING MODELS

of performance. If we deﬁne pτ k to be the proportion of data points in region Rτ
assigned to class k, where k = 1, . . . , K, then two commonly used choices are the
cross-entropy

Qτ (T ) =

pτ k ln pτ k

and the Gini index






k=1

k=1

K

K




n=1

N

K



N




14.5. Conditional Mixture Models

667

logistic regression models (Section 14.5.2). In the simplest case, the mixing coefﬁ-
cients are independent of the input variables. If we make a further generalization to
allow the mixing coefﬁcients also to depend on the inputs then we obtain a mixture
of experts model. Finally, if we allow each component in the mixture model to be
itself a mixture of experts model, then we obtain a hierarchical mixture of experts.

14.5.1 Mixtures of linear regression models
One of the many advantages of giving a probabilistic interpretation to the lin-
ear regression model is that it can then be used as a component in more complex
probabilistic models. This can be done, for instance, by viewing the conditional
distribution representing the linear regression model as a node in a directed prob-
abilistic graph. Here we consider a simple example corresponding to a mixture of
linear regression models, which represents a straightforward extension of the Gaus-
sian mixture model discussed in Section 9.2 to the case of conditional Gaussian
distributions.

We therefore consider K linear regression models, each governed by its own
weight parameter wk. In many applications, it will be appropriate to use a common
noise variance, governed by a precision parameter β, for all K components, and this
is the case we consider here. We will once again restrict attention to a single target
variable t, though the extension to multiple outputs is straightforward. If we denote
the mixing coefﬁcients by πk, then the mixture distribution can be written

p(t|θ) =

πkN (t|wT

k φ, β−1)

(14.34)

where θ denotes the set of all adaptive parameters in the model, namely W = {wk},
π = {πk}, and β. The log likelihood function for this model, given a data set of
observations {φn, tn}, then takes the form

ln p(t|θ) =

ln

πkN (tn|wT

k φn, β−1)

(14.35)

where t = (t1, . . . , tN )T denotes the vector of target variables.

In order to maximize this likelihood function, we can once again appeal to the
EM algorithm, which will turn out to be a simple extension of the EM algorithm for
unconditional Gaussian mixtures of Section 9.2. We can therefore build on our expe-
rience with the unconditional mixture and introduce a set Z = {zn} of binary latent
variables where znk ∈ {0, 1} in which, for each data point n, all of the elements
k = 1, . . . , K are zero except for a single value of 1 indicating which component
of the mixture was responsible for generating that data point. The joint distribution
over latent and observed variables can be represented by the graphical model shown
in Figure 14.7.

The complete-data log likelihood function then takes the form

ln p(t, Z|θ) =

n=1

k=1

znk ln

πkN (tn|wT

k φn, β−1)

.

(14.36)

Exercise 14.12

Exercise 14.13



zn

tn

φn

N









n=1

k=1

K

N





n=1

N

πk =

π

β

W







N

668

14. COMBINING MODELS

Figure 14.7 Probabilistic directed graph representing a mixture of

linear regression models, deﬁned by (14.35).

The EM algorithm begins by ﬁrst choosing an initial value θold for the model param-
eters. In the E step, these parameter values are then used to evaluate the posterior
probabilities, or responsibilities, of each component k for every data point n given
by

γnk = E[znk] = p(k|φn, θold) = πkN (tn|wT
j πjN (tn|wT

k φn, β−1)
j φn, β−1) .

(14.37)

The responsibilities are then used to determine the expectation, with respect to the
posterior distribution p(Z|t, θold), of the complete-data log likelihood, which takes
the form

Q(θ, θold) = EZ [ln p(t, Z|θ)] =

γnk

ln πk + lnN (tn|wT

k φn, β−1)

.

In the M step, we maximize the function Q(θ, θold) with respect to θ, keeping the
γnk ﬁxed. For the optimization with respect to the mixing coefﬁcients πk we need
k πk = 1, which can be done with the aid of a
to take account of the constraint
Lagrange multiplier, leading to an M-step re-estimation equation for πk in the form

Exercise 14.14

1
N

γnk.

(14.38)

Note that this has exactly the same form as the corresponding result for a simple
mixture of unconditional Gaussians given by (9.22).

Next consider the maximization with respect to the parameter vector wk of the
kth linear regression model. Substituting for the Gaussian distribution, we see that
the function Q(θ, θold), as a function of the parameter vector wk, takes the form

Q(θ, θold) =

γnk

n=1

β
2

−

tn − wT

k φn

2

+ const

(14.39)

where the constant term includes the contributions from other weight vectors wj for
j = k. Note that the quantity we are maximizing is similar to the (negative of the)
standard sum-of-squares error (3.12) for a single linear regression model, but with
the inclusion of the responsibilities γnk. This represents a weighted least squares





















γnk

γnk

1
2

n=1

N

N

K

n=1

k=1



N



K





14.5. Conditional Mixture Models

669

problem, in which the term corresponding to the nth data point carries a weighting
coefﬁcient given by βγnk, which could be interpreted as an effective precision for
each data point. We see that each component linear regression model in the mixture,
governed by its own parameter vector wk, is ﬁtted separately to the whole data set in
the M step, but with each data point n weighted by the responsibility γnk that model
k takes for that data point. Setting the derivative of (14.39) with respect to wk equal
to zero gives

0 =

tn − wT

k φn

φn

(14.40)

which we can write in matrix notation as

0 = ΦTRk(t − Φwk)

(14.41)
where Rk = diag(γnk) is a diagonal matrix of size N × N. Solving for wk, we
obtain
(14.42)
This represents a set of modiﬁed normal equations corresponding to the weighted
least squares problem, of the same form as (4.99) found in the context of logistic
regression. Note that after each E step, the matrix Rk will change and so we will
have to solve the normal equations afresh in the subsequent M step.

−1 ΦTRkt.

ΦTRkΦ

wk =

Finally, we maximize Q(θ, θold) with respect to β. Keeping only terms that

depend on β, the function Q(θ, θold) can be written

Q(θ, θold) =

n=1

k=1

ln β −

β
2

tn − wT

k φn

2

.

(14.43)

Setting the derivative with respect to β equal to zero, and rearranging, we obtain the
M-step equation for β in the form

1
β

=

1
N

γnk

tn − wT

k φn

2

.

(14.44)

In Figure 14.8, we illustrate this EM algorithm using the simple example of
ﬁtting a mixture of two straight lines to a data set having one input variable x and
one target variable t. The predictive density (14.34) is plotted in Figure 14.9 using
the converged parameter values obtained from the EM algorithm, corresponding to
the right-hand plot in Figure 14.8. Also shown in this ﬁgure is the result of ﬁtting
a single linear regression model, which gives a unimodal predictive density. We see
that the mixture model gives a much better representation of the data distribution,
and this is reﬂected in the higher likelihood value. However, the mixture model
also assigns signiﬁcant probability mass to regions where there is no data because its
predictive distribution is bimodal for all values of x. This problem can be resolved by
extending the model to allow the mixture coefﬁcients themselves to be functions of
x, leading to models such as the mixture density networks discussed in Section 5.6,
and hierarchical mixture of experts discussed in Section 14.5.3.

670

14. COMBINING MODELS

1.5
1
0.5
0
−0.5
−1
−1.5

1

0.8

0.6

0.4

0.2

1.5
1
0.5
0
−0.5
−1
−1.5

−1

−0.5

0

0.5

1

−1

−0.5

0

1

0.8

0.6

0.4

0.2

0

−1

−0.5

0

0.5

1

0

−1

−0.5

0

1.5
1
0.5
0
−0.5
−1
−1.5

1

0.8

0.6

0.4

0.2

−1

−0.5

0

0.5

1

0

−1

−0.5

0

0.5

1

0.5





0.5

k=1

K



1

1

Figure 14.8 Example of a synthetic data set, shown by the green points, having one input variable x and one
target variable t, together with a mixture of two linear regression models whose mean functions y(x, wk), where
k ∈ {1, 2}, are shown by the blue and red lines. The upper three plots show the initial conﬁguration (left), the
result of running 30 iterations of EM (centre), and the result after 50 iterations of EM (right). Here β was initialized
to the reciprocal of the true variance of the set of target values. The lower three plots show the corresponding
responsibilities plotted as a vertical line for each data point in which the length of the blue segment gives the
posterior probability of the blue line for that data point (and similarly for the red segment).

14.5.2 Mixtures of logistic models
Because the logistic regression model deﬁnes a conditional distribution for the
target variable, given the input vector, it is straightforward to use it as the component
distribution in a mixture model, thereby giving rise to a richer family of conditional
distributions compared to a single logistic regression model. This example involves
a straightforward combination of ideas encountered in earlier sections of the book
and will help consolidate these for the reader.

The conditional distribution of the target variable, for a probabilistic mixture of

K logistic regression models, is given by

p(t|φ, θ) =

πkyt

k [1 − yk]1−t

(14.45)

where φ is the feature vector, yk = σ
denotes the adjustable parameters namely {πk} and {wk}.

wT

k φ

Now suppose we are given a data set {φn, tn}. The corresponding likelihood

is the output of component k, and θ









k=1

K

K




N

N

n=1

k=1







n=1

k=1

14.5. Conditional Mixture Models

671

Figure 14.9 The left plot shows the predictive conditional density corresponding to the converged solution in
Figure 14.8. This gives a log likelihood value of −3.0. A vertical slice through one of these plots at a particular
value of x represents the corresponding conditional distribution p(t|x), which we see is bimodal. The plot on the
right shows the predictive density for a single linear regression model ﬁtted to the same data set using maximum
likelihood. This model has a smaller log likelihood of −27.6.

function is then given by

πkytn

p(t|θ) =
where ynk = σ(wT
k φn) and t = (t1, . . . , tN )T. We can maximize this likelihood
function iteratively by making use of the EM algorithm. This involves introducing
latent variables znk that correspond to a 1-of-K coded binary indicator variable for
each data point n. The complete-data likelihood function is then given by

nk [1 − ynk]1−tn

(14.46)

n=1

p(t, Z|θ) =

πkytn

nk [1 − ynk]1−tn

znk

(14.47)

where Z is the matrix of latent variables with elements znk. We initialize the EM
algorithm by choosing an initial value θold for the model parameters. In the E step,
we then use these parameter values to evaluate the posterior probabilities of the com-
ponents k for each data point n, which are given by

γnk = E[znk] = p(k|φn, θold) = πkytn
j πjytn

nk [1 − ynk]1−tn
nj [1 − ynj]1−tn

.

(14.48)

These responsibilities are then used to ﬁnd the expected complete-data log likelihood
as a function of θ, given by

Q(θ, θold) = EZ [ln p(t, Z|θ)]
=

K

N

γnk {ln πk + tn ln ynk + (1 − tn) ln (1 − ynk)} .

(14.49)





N





1
N

n=1

n=1

N

N

K

672

14. COMBINING MODELS

Section 4.3.3

Section 4.3.3

Exercise 14.16

The M step involves maximization of this function with respect to θ, keeping θold,
and hence γnk, ﬁxed. Maximization with respect to πk can be done in the usual way,
k πk = 1, giving
with a Lagrange multiplier to enforce the summation constraint
the familiar result

πk =

γnk.

(14.50)

To determine the {wk}, we note that the Q(θ, θold) function comprises a sum
over terms indexed by k each of which depends only on one of the vectors wk, so
that the different vectors are decoupled in the M step of the EM algorithm. In other
words, the different components interact only via the responsibilities, which are ﬁxed
during the M step. Note that the M step does not have a closed-form solution and
must be solved iteratively using, for instance, the iterative reweighted least squares
(IRLS) algorithm. The gradient and the Hessian for the vector wk are given by

∇kQ =

n=1

γnk(tn − ynk)φn

Hk = −∇k∇kQ =

γnkynk(1 − ynk)φnφT

n

(14.51)

(14.52)

where ∇k denotes the gradient with respect to wk. For ﬁxed γnk, these are indepen-
dent of {wj} for j = k and so we can solve for each wk separately using the IRLS
algorithm. Thus the M-step equations for component k correspond simply to ﬁtting
a single logistic regression model to a weighted data set in which data point n carries
a weight γnk. Figure 14.10 shows an example of the mixture of logistic regression
models applied to a simple classiﬁcation problem. The extension of this model to a
mixture of softmax models for more than two classes is straightforward.

14.5.3 Mixtures of experts
In Section 14.5.1, we considered a mixture of linear regression models, and in
Section 14.5.2 we discussed the analogous mixture of linear classiﬁers. Although
these simple mixtures extend the ﬂexibility of linear models to include more com-
plex (e.g., multimodal) predictive distributions, they are still very limited. We can
further increase the capability of such models by allowing the mixing coefﬁcients
themselves to be functions of the input variable, so that

p(t|x) =

k=1

πk(x)pk(t|x).

(14.53)

This is known as a mixture of experts model (Jacobs et al., 1991) in which the mix-
ing coefﬁcients πk(x) are known as gating functions and the individual component
densities pk(t|x) are called experts. The notion behind the terminology is that differ-
ent components can model the distribution in different regions of input space (they



are ‘experts’ at making predictions in their own regions), and the gating functions
determine which components are dominant in which region.

The gating functions πk(x) must satisfy the usual constraints for mixing co-
efﬁcients, namely 0 � πk(x) � 1 and
k πk(x) = 1. They can therefore be
represented, for example, by linear softmax models of the form (4.104) and (4.105).
If the experts are also linear (regression or classiﬁcation) models, then the whole
model can be ﬁtted efﬁciently using the EM algorithm, with iterative reweighted
least squares being employed in the M step (Jordan and Jacobs, 1994).

Exercise 14.17

Section 4.3.3

Such a model still has signiﬁcant limitations due to the use of linear models
for the gating and expert functions. A much more ﬂexible model is obtained by
using a multilevel gating function to give the hierarchical mixture of experts, or
HME model (Jordan and Jacobs, 1994). To understand the structure of this model,
imagine a mixture distribution in which each component in the mixture is itself a
mixture distribution. For simple unconditional mixtures, this hierarchical mixture is
trivially equivalent to a single ﬂat mixture distribution. However, when the mixing
coefﬁcients are input dependent, this hierarchical model becomes nontrivial. The
HME model can also be viewed as a probabilistic version of decision trees discussed
in Section 14.4 and can again be trained efﬁciently by maximum likelihood using an
EM algorithm with IRLS in the M step. A Bayesian treatment of the HME has been
given by Bishop and Svens´en (2003) based on variational inference.

We shall not discuss the HME in detail here. However, it is worth pointing out
the close connection with the mixture density network discussed in Section 5.6. The
principal advantage of the mixtures of experts model is that it can be optimized by
EM in which the M step for each mixture component and gating model involves
a convex optimization (although the overall optimization is nonconvex). By con-
trast, the advantage of the mixture density network approach is that the component

14.5. Conditional Mixture Models

673

Figure 14.10 Illustration of a mixture of logistic regression models. The left plot shows data points drawn
from two classes denoted red and blue, in which the background colour (which varies from pure red to pure blue)
denotes the true probability of the class label. The centre plot shows the result of ﬁtting a single logistic regression
model using maximum likelihood, in which the background colour denotes the corresponding probability of the
class label. Because the colour is a near-uniform purple, we see that the model assigns a probability of around
0.5 to each of the classes over most of input space. The right plot shows the result of ﬁtting a mixture of two
logistic regression models, which now gives much higher probability to the correct labels for many of the points
in the blue class.

674

14. COMBINING MODELS

Exercises



M



M

densities and the mixing coefﬁcients share the hidden units of the neural network.
Furthermore, in the mixture density network, the splits of the input space are further
relaxed compared to the hierarchical mixture of experts in that they are not only soft,
and not constrained to be axis aligned, but they can also be nonlinear.

14.1 ( ) www Consider a set models of the form p(t|x, zh, θh, h) in which x is the
input vector, t is the target vector, h indexes the different models, zh is a latent vari-
able for model h, and θh is the set of parameters for model h. Suppose the models
have prior probabilities p(h) and that we are given a training set X = {x1, . . . , xN}
and T = {t1, . . . , tN}. Write down the formulae needed to evaluate the predic-
tive distribution p(t|x, X, T) in which the latent variables and the model index are
marginalized out. Use these formulae to highlight the difference between Bayesian
averaging of different models and the use of latent variables within a single model.
14.2 () The expected sum-of-squares error EAV for a simple committee model can
be deﬁned by (14.10), and the expected error of the committee itself is given by
(14.11). Assuming that the individual errors satisfy (14.12) and (14.13), derive the
result (14.14).

14.3 () www By making use of Jensen’s inequality (1.115), for the special case of
the convex function f(x) = x2, show that the average expected sum-of-squares
error EAV of the members of a simple committee model, given by (14.10), and the
expected error ECOM of the committee itself, given by (14.11), satisfy

ECOM � EAV.

(14.54)

14.4 ( ) By making use of Jensen’s in equality (1.115), show that the result (14.54)
derived in the previous exercise hods for any error function E(y), not just sum-of-
squares, provided it is a convex function of y.

14.5 ( ) www Consider a committee in which we allow unequal weighting of the

constituent models, so that

yCOM(x) =

m=1

αmym(x).

(14.55)

In order to ensure that the predictions yCOM(x) remain within sensible limits, sup-
pose that we require that they be bounded at each value of x by the minimum and
maximum values given by any of the members of the committee, so that

ymin(x) � yCOM(x) � ymax(x).

(14.56)

Show that a necessary and sufﬁcient condition for this constraint is that the coefﬁ-
cients αm satisfy

αm � 0,

αm = 1.

m=1

(14.57)

Exercises

675

14.6 () www By differentiating the error function (14.23) with respect to αm, show
that the parameters αm in the AdaBoost algorithm are updated using (14.17) in
which 	m is deﬁned by (14.16).

14.7 () By making a variational minimization of the expected exponential error function
given by (14.27) with respect to all possible functions y(x), show that the minimizing
function is given by (14.28).

14.8 () Show that the exponential error function (14.20), which is minimized by the
AdaBoost algorithm, does not correspond to the log likelihood of any well-behaved
probabilistic model. This can be done by showing that the corresponding conditional
distribution p(t|x) cannot be correctly normalized.

14.9 () www Show that the sequential minimization of the sum-of-squares error func-
tion for an additive model of the form (14.21) in the style of boosting simply involves
ﬁtting each new base classiﬁer to the residual errors tn−fm−1(xn) from the previous
model.

14.10 () Verify that if we minimize the sum-of-squares error between a set of training
values {tn} and a single predictive value t, then the optimal solution for t is given
by the mean of the {tn}.

14.11 ( ) Consider a data set comprising 400 data points from class C1 and 400 data
points from class C2. Suppose that a tree model A splits these into (300, 100) at
the ﬁrst leaf node and (100, 300) at the second leaf node, where (n, m) denotes that
n points are assigned to C1 and m points are assigned to C2. Similarly, suppose
that a second tree model B splits them into (200, 400) and (200, 0). Evaluate the
misclassiﬁcation rates for the two trees and hence show that they are equal. Similarly,
evaluate the cross-entropy (14.32) and Gini index (14.33) for the two trees and show
that they are both lower for tree B than for tree A.

14.12 ( ) Extend the results of Section 14.5.1 for a mixture of linear regression models
to the case of multiple target values described by a vector t. To do this, make use of
the results of Section 3.1.5.

14.13 () www Verify that the complete-data log likelihood function for the mixture of

linear regression models is given by (14.36).

14.14 () Use the technique of Lagrange multipliers (Appendix E) to show that the M-step
re-estimation equation for the mixing coefﬁcients in the mixture of linear regression
models trained by maximum likelihood EM is given by (14.38).

14.15 () www We have already noted that if we use a squared loss function in a regres-
sion problem, the corresponding optimal prediction of the target variable for a new
input vector is given by the conditional mean of the predictive distribution. Show
that the conditional mean for the mixture of linear regression models discussed in
Section 14.5.1 is given by a linear combination of the means of each component dis-
tribution. Note that if the conditional distribution of the target data is multimodal,
the conditional mean can give poor predictions.



K

676

14. COMBINING MODELS

14.16 (  ) Extend the logistic regression mixture model of Section 14.5.2 to a mixture
of softmax classiﬁers representing C � 2 classes. Write down the EM algorithm for
determining the parameters of this model through maximum likelihood.

14.17 ( ) www Consider a mixture model for a conditional distribution p(t|x) of the

form

p(t|x) =

k=1

πkψk(t|x)

(14.58)

in which each mixture component ψk(t|x) is itself a mixture model. Show that this
two-level hierarchical mixture is equivalent to a conventional single-level mixture
model. Now suppose that the mixing coefﬁcients in both levels of such a hierar-
chical model are arbitrary functions of x. Again, show that this hierarchical model
is again equivalent to a single-level model with x-dependent mixing coefﬁcients.
Finally, consider the case in which the mixing coefﬁcients at both levels of the hi-
erarchical mixture are constrained to be linear classiﬁcation (logistic or softmax)
models. Show that the hierarchical mixture cannot in general be represented by a
single-level mixture having linear classiﬁcation models for the mixing coefﬁcients.
Hint: to do this it is sufﬁcient to construct a single counter-example, so consider a
mixture of two components in which one of those components is itself a mixture of
two components, with mixing coefﬁcients given by linear-logistic models. Show that
this cannot be represented by a single-level mixture of 3 components having mixing
coefﬁcients determined by a linear-softmax model.


