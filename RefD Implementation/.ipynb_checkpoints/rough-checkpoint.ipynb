{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.save_data import save_csv_data, save_evaluation_results, save_prereq_relation\n",
    "from library.data_reading import read_data, read_wiki_data\n",
    "from library.algorithm_evaluation import evaluate_prereq_estimation, check_opp_pairs\n",
    "from library.prereq_calculation import get_prereq_relations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_plotting(x_label, y_label):\n",
    "    plt.plot(x_label, y_label)\n",
    "    plt.xlabel('theta_values')\n",
    "    plt.ylabel('average_values')\n",
    "    plt.title('Accuracy measures')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = \"CS\"\n",
    "theta = 0.02\n",
    "method = \"refd\"\n",
    "w_type = \"equal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'RefD Implementation/output_data/CS_edge.csv' does not exist: b'RefD Implementation/output_data/CS_edge.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-46b5a6f9fe41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_wiki\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_wiki_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Personalise Learning Thesis/RefD Implementation/library/data_reading.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfile_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RefD Implementation/output_data/CS_edge.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mfile_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RefD Implementation/output_data/CS_edge_neg.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdf_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdf_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"MATH\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'RefD Implementation/output_data/CS_edge.csv' does not exist: b'RefD Implementation/output_data/CS_edge.csv'"
     ]
    }
   ],
   "source": [
    "df_pos, df_neg = read_data(subject)\n",
    "df_wiki = read_wiki_data(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(subject, theta, method, w_type):\n",
    "\n",
    "    # Data Reading\n",
    "    \n",
    "\n",
    "    # get prerequisite relations for the given parameters\n",
    "    df_estimated = get_prereq_relations(df_pos, df_neg, df_wiki,\n",
    "                                                theta, method, w_type, subject)\n",
    "    print(df_estimated.shape[0])\n",
    "    # save calculated prerequisite data\n",
    "    # save_prereq_relation(df_estimated, method, w_type, subject, theta)\n",
    "\n",
    "    # estimated result evaluation\n",
    "    estimated_results = evaluate_prereq_estimation(df_pos, df_neg, df_estimated)\n",
    "\n",
    "    # save evaluation results\n",
    "    # save_evaluation_results(estimated_results, method, w_type, subject, theta)\n",
    "\n",
    "    return estimated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = wikipedia.page(\"Neural Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network\n"
     ]
    }
   ],
   "source": [
    "print(wiki_data.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3Blue1Brown',\n",
       " 'A. K. Dewdney',\n",
       " 'ADALINE',\n",
       " 'Acetylcholine',\n",
       " 'Adaptive control',\n",
       " 'Adaptive resonance theory',\n",
       " 'Adaptive system',\n",
       " 'Alan Turing',\n",
       " 'Alexander Bain',\n",
       " 'Amplitude',\n",
       " 'Analog signal',\n",
       " 'Artificial intelligence',\n",
       " 'Artificial neural network',\n",
       " 'Artificial neuron',\n",
       " 'Autonomous robot',\n",
       " 'Axon',\n",
       " 'BCM theory',\n",
       " 'Back-propagation',\n",
       " 'Backpropagation',\n",
       " 'Bibcode',\n",
       " 'Biological cybernetics',\n",
       " 'Biological neural network',\n",
       " 'Biological neuron models',\n",
       " 'Biologically inspired computing',\n",
       " 'Biophysics',\n",
       " 'Blind signal separation',\n",
       " 'Boltzmann machine',\n",
       " 'CPU',\n",
       " 'Cerebellar model articulation controller',\n",
       " 'Charles Scott Sherrington',\n",
       " 'CiteSeerX',\n",
       " 'Cognitive architecture',\n",
       " 'Cognitive modeling',\n",
       " 'Cognitive science',\n",
       " 'Computation',\n",
       " 'Computational neuroscience',\n",
       " 'Computer simulation',\n",
       " 'Connectionism',\n",
       " 'Connectomics',\n",
       " 'Convolution',\n",
       " 'Cultured neuronal networks',\n",
       " 'Data compression',\n",
       " 'Data mining',\n",
       " 'Data modeling',\n",
       " 'Data processing',\n",
       " 'Database',\n",
       " 'David H. Hubel',\n",
       " 'Decision making',\n",
       " 'Deep learning',\n",
       " 'Dendrite',\n",
       " 'Dendrodendritic synapse',\n",
       " 'Digital data',\n",
       " 'Digital morphogenesis',\n",
       " 'Digital object identifier',\n",
       " 'Donald Hebb',\n",
       " 'Dopamine',\n",
       " 'E-mail spam',\n",
       " 'Exclusive-or',\n",
       " 'Exclusive or',\n",
       " 'Feedforward neural network',\n",
       " 'Frank Rosenblatt',\n",
       " 'Function approximation',\n",
       " 'GPU',\n",
       " 'Gene expression programming',\n",
       " 'Geoff Hinton',\n",
       " 'Geoffrey Hinton',\n",
       " 'Group method of data handling',\n",
       " 'Habituation',\n",
       " 'Hard drive',\n",
       " 'Hebbian learning',\n",
       " 'Hopfield network',\n",
       " 'IDSIA',\n",
       " 'Image analysis',\n",
       " 'In situ adaptive tabulation',\n",
       " 'Information processing',\n",
       " 'Information theory',\n",
       " 'International Standard Book Number',\n",
       " 'Jürgen Schmidhuber',\n",
       " 'Kunihiko Fukushima',\n",
       " 'Learning',\n",
       " 'Long short term memory',\n",
       " 'Long term potentiation',\n",
       " 'Machine learning',\n",
       " 'Marvin Minsky',\n",
       " 'Mathematical model',\n",
       " 'Memristor',\n",
       " 'Multilinear subspace learning',\n",
       " 'NYU',\n",
       " 'Nanodevice',\n",
       " 'Nature Nanotechnology',\n",
       " 'Nature Neuroscience',\n",
       " 'Neocognitron',\n",
       " 'Neural Computation (journal)',\n",
       " 'Neural backpropagation',\n",
       " 'Neural computing',\n",
       " 'Neural network (disambiguation)',\n",
       " 'Neural network software',\n",
       " 'Neural processing',\n",
       " 'Neuromodulators',\n",
       " 'Neuromorphic computing',\n",
       " 'Neuron',\n",
       " 'Neurotransmitter',\n",
       " 'Non-linear',\n",
       " 'Nonlinear system identification',\n",
       " 'Novelty detection',\n",
       " 'Parallel constraint satisfaction processes',\n",
       " 'Parallel distributed processing',\n",
       " 'Pattern recognition',\n",
       " 'Perceptron',\n",
       " 'Predictive analytics',\n",
       " 'Predictive modeling',\n",
       " 'Principal component',\n",
       " 'Programming language',\n",
       " 'PubMed Central',\n",
       " 'PubMed Identifier',\n",
       " 'Pyramidal neuron',\n",
       " 'Radial basis function network',\n",
       " 'Radial basis networks',\n",
       " 'Random-access memory',\n",
       " 'Recurrent neural network',\n",
       " 'Regression analysis',\n",
       " 'Roger Bridgman',\n",
       " 'Scientific American',\n",
       " 'Self-organizing map',\n",
       " 'Serotonin',\n",
       " 'Seymour Papert',\n",
       " 'Simulated reality',\n",
       " 'Software agents',\n",
       " 'Speech recognition',\n",
       " 'Statistical',\n",
       " 'Statistical classification',\n",
       " 'Support vector machine',\n",
       " 'Symbolic artificial intelligence',\n",
       " 'Synapse',\n",
       " 'Synaptic plasticity',\n",
       " 'Tensor product network',\n",
       " 'Threshold logic',\n",
       " 'Time delay neural network',\n",
       " 'Time series prediction',\n",
       " 'Tomaso Poggio',\n",
       " 'Torsten Wiesel',\n",
       " 'University of Chicago',\n",
       " 'University of Toronto',\n",
       " 'Unorganized machine',\n",
       " 'Unsupervised learning',\n",
       " 'Video game',\n",
       " 'Visual cortex',\n",
       " 'Von Neumann model',\n",
       " 'Walter Pitts',\n",
       " 'Warren McCulloch',\n",
       " 'Warren Sturgis McCulloch',\n",
       " 'William James',\n",
       " 'Yann LeCun',\n",
       " 'YouTube']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"mw-parser-output\"><div role=\"note\" class=\"hatnote navigation-not-searchable\">For other uses, see <a href=\"/wiki/Neural_network_(disambiguation)\" class=\"mw-disambig\" title=\"Neural network (disambiguation)\">Neural network (disambiguation)</a>.</div>\\n<div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">Structure in biology and artificial intelligence</div>\\n<p class=\"mw-empty-elt\">\\n</p>\\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:Neural_network_example.svg\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/220px-Neural_network_example.svg.png\" decoding=\"async\" width=\"220\" height=\"293\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/330px-Neural_network_example.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/440px-Neural_network_example.svg.png 2x\" data-file-width=\"330\" data-file-height=\"440\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Neural_network_example.svg\" class=\"internal\" title=\"Enlarge\"></a></div>Simplified view of a <a href=\"/wiki/Feedforward_neural_network\" title=\"Feedforward neural network\">feedforward</a> artificial neural network</div></div></div>\\n<p>A <b>neural network</b> is a network or circuit of <a href=\"/wiki/Neuron\" title=\"Neuron\">neurons</a>, or in a modern sense, an <a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">artificial neural network</a>, composed of <a href=\"/wiki/Artificial_neuron\" title=\"Artificial neuron\">artificial neurons</a> or nodes.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup> Thus a neural network is either a <a href=\"/wiki/Biological_neural_network\" class=\"mw-redirect\" title=\"Biological neural network\">biological neural network</a>, made up of real biological neurons, or an artificial neural network, for solving <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the <a href=\"/wiki/Amplitude\" title=\"Amplitude\">amplitude</a> of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.\\n</p><p>These artificial networks may be used for <a href=\"/wiki/Predictive_modeling\" class=\"mw-redirect\" title=\"Predictive modeling\">predictive modeling</a>, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup>\\n</p>\\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\\n<ul>\\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Overview\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Overview</span></a></li>\\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#History\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">History</span></a></li>\\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Artificial_intelligence\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Artificial intelligence</span></a></li>\\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Applications\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Applications</span></a></li>\\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Neuroscience\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Neuroscience</span></a>\\n<ul>\\n<li class=\"toclevel-2 tocsection-6\"><a href=\"#Types_of_models\"><span class=\"tocnumber\">5.1</span> <span class=\"toctext\">Types of models</span></a></li>\\n</ul>\\n</li>\\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#Criticism\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Criticism</span></a></li>\\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#Recent_improvements\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Recent improvements</span></a></li>\\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#See_also\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">See also</span></a></li>\\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#References\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">References</span></a></li>\\n<li class=\"toclevel-1 tocsection-11\"><a href=\"#External_links\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">External links</span></a></li>\\n</ul>\\n</div>\\n\\n<h2><span class=\"mw-headline\" id=\"Overview\">Overview</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=1\" title=\"Edit section: Overview\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>A <a href=\"/wiki/Biological_neural_network\" class=\"mw-redirect\" title=\"Biological neural network\">biological neural network</a> is composed of a group or groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called <a href=\"/wiki/Synapse\" title=\"Synapse\">synapses</a>, are usually formed from <a href=\"/wiki/Axon\" title=\"Axon\">axons</a> to <a href=\"/wiki/Dendrite\" title=\"Dendrite\">dendrites</a>, though <a href=\"/wiki/Dendrodendritic_synapse\" title=\"Dendrodendritic synapse\">dendrodendritic synapses</a><sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup> and other connections are possible. Apart from the electrical signaling, there are other forms of signaling that arise from <a href=\"/wiki/Neurotransmitter\" title=\"Neurotransmitter\">neurotransmitter</a> diffusion.\\n</p><p>Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">Artificial intelligence</a> and <a href=\"/wiki/Cognitive_modeling\" class=\"mw-redirect\" title=\"Cognitive modeling\">cognitive modeling</a> try to simulate some properties of biological neural networks. In the <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> field, artificial neural networks have been applied successfully to <a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">speech recognition</a>, <a href=\"/wiki/Image_analysis\" title=\"Image analysis\">image analysis</a> and <a href=\"/wiki/Adaptive_control\" title=\"Adaptive control\">adaptive control</a>, in order to construct <a href=\"/wiki/Software_agents\" class=\"mw-redirect\" title=\"Software agents\">software agents</a> (in <a href=\"/wiki/Video_game\" title=\"Video game\">computer and video games</a>) or <a href=\"/wiki/Autonomous_robot\" title=\"Autonomous robot\">autonomous robots</a>.\\n</p><p>Historically, digital computers evolved from the <a href=\"/wiki/Von_Neumann_model\" class=\"mw-redirect\" title=\"Von Neumann model\">von Neumann model</a>, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. Unlike the von Neumann model, neural network computing does not separate memory and processing.\\n</p><p>Neural network theory has served both to better identify how the neurons in the brain function and to provide the basis for efforts to create artificial intelligence.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"History\">History</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=2\" title=\"Edit section: History\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>The preliminary theoretical base for contemporary neural networks was independently proposed by <a href=\"/wiki/Alexander_Bain\" title=\"Alexander Bain\">Alexander Bain</a><sup id=\"cite_ref-Bain_1873_4-0\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> (1873) and <a href=\"/wiki/William_James\" title=\"William James\">William James</a><sup id=\"cite_ref-James_1890_5-0\" class=\"reference\"><a href=\"#cite_note-James_1890-5\">&#91;5&#93;</a></sup> (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.\\n</p>\\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/be/Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png/220px-Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\" decoding=\"async\" width=\"220\" height=\"220\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/be/Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png/330px-Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/be/Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png/440px-Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png 2x\" data-file-width=\"650\" data-file-height=\"650\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\" class=\"internal\" title=\"Enlarge\"></a></div><a href=\"/wiki/Computer_simulation\" title=\"Computer simulation\">Computer simulation</a> of the branching architecture of the <a href=\"/wiki/Dendrite\" title=\"Dendrite\">dendrites</a> of <a href=\"/wiki/Pyramidal_neuron\" class=\"mw-redirect\" title=\"Pyramidal neuron\">pyramidal neurons</a>.<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup></div></div></div>\\n<p>For Bain,<sup id=\"cite_ref-Bain_1873_4-1\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\\'s<sup id=\"cite_ref-Bain_1873_4-2\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs.\\n</p><p>James\\'s<sup id=\"cite_ref-James_1890_5-1\" class=\"reference\"><a href=\"#cite_note-James_1890-5\">&#91;5&#93;</a></sup> theory was similar to Bain\\'s,<sup id=\"cite_ref-Bain_1873_4-3\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.\\n</p><p><a href=\"/wiki/Charles_Scott_Sherrington\" title=\"Charles Scott Sherrington\">C. S. Sherrington</a><sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup> (1898) conducted experiments to test James\\'s theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of <a href=\"/wiki/Habituation\" title=\"Habituation\">habituation</a>. \\n</p><p><a href=\"/wiki/Warren_McCulloch\" class=\"mw-redirect\" title=\"Warren McCulloch\">McCulloch</a> and <a href=\"/wiki/Walter_Pitts\" title=\"Walter Pitts\">Pitts</a><sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup>  (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model <a href=\"/w/index.php?title=Threshold_logic&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Threshold logic (page does not exist)\">threshold logic</a>. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.\\n</p><p>In the late 1940s psychologist <a href=\"/wiki/Donald_Hebb\" class=\"mw-redirect\" title=\"Donald Hebb\">Donald Hebb</a><sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup>  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as <a href=\"/wiki/Hebbian_learning\" class=\"mw-redirect\" title=\"Hebbian learning\">Hebbian learning</a>. Hebbian learning is considered to be a \\'typical\\' <a href=\"/wiki/Unsupervised_learning\" title=\"Unsupervised learning\">unsupervised learning</a> rule and its later variants were early models for <a href=\"/wiki/Long_term_potentiation\" class=\"mw-redirect\" title=\"Long term potentiation\">long term potentiation</a>. These ideas started being applied to computational models in 1948 with <a href=\"/wiki/Unorganized_machine\" title=\"Unorganized machine\">Turing\\'s B-type machines</a>.\\n</p><p>Farley and Clark<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;10&#93;</a></sup> (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda<sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;11&#93;</a></sup> (1956).\\n</p><p><a href=\"/wiki/Frank_Rosenblatt\" title=\"Frank Rosenblatt\">Rosenblatt</a><sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup> (1958) created the <a href=\"/wiki/Perceptron\" title=\"Perceptron\">perceptron</a>, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the <a href=\"/wiki/Exclusive-or\" class=\"mw-redirect\" title=\"Exclusive-or\">exclusive-or</a> circuit, a circuit whose mathematical computation could not be processed until after the <a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">backpropagation</a> algorithm was created by Werbos<sup id=\"cite_ref-Werbos_1975_13-0\" class=\"reference\"><a href=\"#cite_note-Werbos_1975-13\">&#91;13&#93;</a></sup> (1975).\\n</p><p>Neural network research stagnated after the publication of machine learning research by <a href=\"/wiki/Marvin_Minsky\" title=\"Marvin Minsky\">Marvin Minsky</a> and <a href=\"/wiki/Seymour_Papert\" title=\"Seymour Papert\">Seymour Papert</a><sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;14&#93;</a></sup> (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key in later advances was the <a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">backpropagation</a> algorithm which effectively solved the exclusive-or problem (Werbos 1975).<sup id=\"cite_ref-Werbos_1975_13-1\" class=\"reference\"><a href=\"#cite_note-Werbos_1975-13\">&#91;13&#93;</a></sup>\\n</p><p>The <a href=\"/wiki/Connectionism\" title=\"Connectionism\">parallel distributed processing</a> of the mid-1980s became popular under the name <a href=\"/wiki/Connectionism\" title=\"Connectionism\">connectionism</a>. The text by Rumelhart and McClelland<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup> (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.\\n</p><p>Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of <a href=\"/w/index.php?title=Neural_processing&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Neural processing (page does not exist)\">neural processing</a> in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function.<sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\">&#91;16&#93;</a></sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Artificial_intelligence\">Artificial intelligence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=3\" title=\"Edit section: Artificial intelligence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">Artificial neural network</a></div>\\n<p>A <i>neural network</i> (NN), in the case of artificial neurons called <i>artificial neural network</i> (ANN) or <i>simulated neural network</i> (SNN), is an interconnected group of natural or <a href=\"/wiki/Artificial_neuron\" title=\"Artificial neuron\">artificial neurons</a> that uses a <a href=\"/wiki/Mathematical_model\" title=\"Mathematical model\">mathematical or computational model</a> for <a href=\"/wiki/Information_processing\" title=\"Information processing\">information processing</a> based on a <a href=\"/wiki/Connectionism\" title=\"Connectionism\">connectionistic</a> approach to <a href=\"/wiki/Computation\" title=\"Computation\">computation</a>. In most cases an ANN is an <a href=\"/wiki/Adaptive_system\" title=\"Adaptive system\">adaptive system</a> that changes its structure based on external or internal information that flows through the network.\\n</p><p>In more practical terms neural networks are <a href=\"/wiki/Non-linear\" class=\"mw-redirect\" title=\"Non-linear\">non-linear</a> <a href=\"/wiki/Statistical\" class=\"mw-redirect\" title=\"Statistical\">statistical</a> <a href=\"/wiki/Data_modeling\" title=\"Data modeling\">data modeling</a> or <a href=\"/wiki/Decision_making\" class=\"mw-redirect\" title=\"Decision making\">decision making</a> tools. They can be used to model complex relationships between inputs and outputs or to <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">find patterns</a> in data.\\n</p><p>An <a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">artificial neural network</a> involves a network of simple processing elements (<a href=\"/wiki/Artificial_neuron\" title=\"Artificial neuron\">artificial neurons</a>) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by <a href=\"/wiki/Warren_Sturgis_McCulloch\" title=\"Warren Sturgis McCulloch\">Warren McCulloch</a>, a neurophysiologist, and <a href=\"/wiki/Walter_Pitts\" title=\"Walter Pitts\">Walter Pitts</a>, a logician, who first collaborated at the <a href=\"/wiki/University_of_Chicago\" title=\"University of Chicago\">University of Chicago</a>.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup>\\n</p><p>One classical type of artificial neural network is the <a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">recurrent</a> <a href=\"/wiki/Hopfield_network\" title=\"Hopfield network\">Hopfield network</a>.\\n</p><p>The concept of a neural network appears to have first been proposed by <a href=\"/wiki/Alan_Turing\" title=\"Alan Turing\">Alan Turing</a> in his 1948 paper <i>Intelligent Machinery</i> in which called them \"B-type unorganised machines\".<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup>\\n</p><p>The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the <a href=\"/wiki/Boltzmann_machine\" title=\"Boltzmann machine\">Boltzmann machine</a> (1983), and more recently, <a href=\"/wiki/Deep_learning\" title=\"Deep learning\">deep learning</a> algorithms, which can implicitly learn the distribution function of the observed data. Learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Applications\">Applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=4\" title=\"Edit section: Applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>Neural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:\\n</p>\\n<ul><li><a href=\"/wiki/Function_approximation\" title=\"Function approximation\">Function approximation</a>, or <a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">regression analysis</a>, including <a href=\"/wiki/Time_series_prediction\" class=\"mw-redirect\" title=\"Time series prediction\">time series prediction</a> and modeling.</li>\\n<li><a href=\"/wiki/Statistical_classification\" title=\"Statistical classification\">Classification</a>, including <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern</a> and sequence recognition, <a href=\"/wiki/Novelty_detection\" title=\"Novelty detection\">novelty detection</a> and sequential decision making.</li>\\n<li><a href=\"/wiki/Data_processing\" title=\"Data processing\">Data processing</a>, including filtering, clustering, <a href=\"/wiki/Blind_signal_separation\" class=\"mw-redirect\" title=\"Blind signal separation\">blind signal separation</a> and <a href=\"/wiki/Data_compression\" title=\"Data compression\">compression</a>.</li></ul>\\n<p>Application areas of ANNs include <a href=\"/wiki/Nonlinear_system_identification\" title=\"Nonlinear system identification\">nonlinear system identification</a><sup id=\"cite_ref-SAB1_19-0\" class=\"reference\"><a href=\"#cite_note-SAB1-19\">&#91;19&#93;</a></sup> and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, <a href=\"/wiki/Data_mining\" title=\"Data mining\">data mining</a> (or knowledge discovery in databases, \"KDD\"), visualization and <a href=\"/wiki/E-mail_spam\" class=\"mw-redirect\" title=\"E-mail spam\">e-mail spam</a> filtering. For example, it is possible to create a semantic profile of user\\'s interests emerging from pictures trained for object recognition.<sup id=\"cite_ref-20\" class=\"reference\"><a href=\"#cite_note-20\">&#91;20&#93;</a></sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Neuroscience\">Neuroscience</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=5\" title=\"Edit section: Neuroscience\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>Theoretical and <a href=\"/wiki/Computational_neuroscience\" title=\"Computational neuroscience\">computational neuroscience</a> is the field concerned with the theoretical analysis and computational modeling of biological neural systems.\\nSince neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.\\n</p><p>The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (<a href=\"/wiki/Biological_neural_network\" class=\"mw-redirect\" title=\"Biological neural network\">biological neural network</a> models) and theory (statistical learning theory and <a href=\"/wiki/Information_theory\" title=\"Information theory\">information theory</a>).\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Types_of_models\">Types of models</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=6\" title=\"Edit section: Types of models\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>Many models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of <a href=\"/wiki/Biological_neuron_models\" class=\"mw-redirect\" title=\"Biological neuron models\">individual neurons</a>, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Criticism\">Criticism</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=7\" title=\"Edit section: Criticism\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper \"Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving,\" uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.\\n</p><p><a href=\"/wiki/A._K._Dewdney\" class=\"mw-redirect\" title=\"A. K. Dewdney\">A. K. Dewdney</a>, a former <i><a href=\"/wiki/Scientific_American\" title=\"Scientific American\">Scientific American</a></i> columnist, wrote in 1997, \"Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool\" (Dewdney, p.&#160;82).\\n</p><p>Arguments for Dewdney\\'s position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of <a href=\"/wiki/Database\" title=\"Database\">database</a> rows for its connections—which can consume vast amounts of computer <a href=\"/wiki/Random-access_memory\" title=\"Random-access memory\">memory</a> and <a href=\"/wiki/Hard_drive\" class=\"mw-redirect\" title=\"Hard drive\">hard disk</a> space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons—which must often be matched with incredible amounts of <a href=\"/wiki/CPU\" class=\"mw-redirect\" title=\"CPU\">CPU</a> processing power and time. While neural networks often yield <i>effective</i> programs, they too often do so at the cost of <i>efficiency</i> (they tend to consume considerable amounts of time and money).\\n</p><p>Arguments against Dewdney\\'s position are that neural nets have been successfully used to solve many complex and diverse tasks, such as autonomously flying aircraft.<sup id=\"cite_ref-21\" class=\"reference\"><a href=\"#cite_note-21\">&#91;21&#93;</a></sup>\\n</p><p>Technology writer <a href=\"/w/index.php?title=Roger_Bridgman&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Roger Bridgman (page does not exist)\">Roger Bridgman</a> commented on Dewdney\\'s statements about neural nets: \\n</p>\\n<blockquote><p>Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn\\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\\n</p><p>In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.<sup id=\"cite_ref-22\" class=\"reference\"><a href=\"#cite_note-22\">&#91;22&#93;</a></sup>\\n</p>\\n</blockquote>\\n<p>Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.<sup id=\"cite_ref-23\" class=\"reference\"><a href=\"#cite_note-23\">&#91;23&#93;</a></sup>\\n</p><p>Some other criticisms came from believers of hybrid models (combining neural networks and <a href=\"/wiki/Symbolic_artificial_intelligence\" title=\"Symbolic artificial intelligence\">symbolic</a> approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citing_sources#What_information_to_include\" title=\"Wikipedia:Citing sources\"><span title=\"A complete citation is needed (April 2019)\">full citation needed</span></a></i>&#93;</sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Recent_improvements\">Recent improvements</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=8\" title=\"Edit section: Recent improvements\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>While initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of <a href=\"/wiki/Neuromodulators\" class=\"mw-redirect\" title=\"Neuromodulators\">neuromodulators</a> such as <a href=\"/wiki/Dopamine\" title=\"Dopamine\">dopamine</a>, <a href=\"/wiki/Acetylcholine\" title=\"Acetylcholine\">acetylcholine</a>, and <a href=\"/wiki/Serotonin\" title=\"Serotonin\">serotonin</a> on behaviour and learning.\\n</p><p><a href=\"/wiki/Biophysics\" title=\"Biophysics\">Biophysical</a> models, such as <a href=\"/wiki/BCM_theory\" title=\"BCM theory\">BCM theory</a>, have been important in understanding mechanisms for <a href=\"/wiki/Synaptic_plasticity\" title=\"Synaptic plasticity\">synaptic plasticity</a>, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for <a href=\"/wiki/Radial_basis_networks\" class=\"mw-redirect\" title=\"Radial basis networks\">radial basis networks</a> and <a href=\"/wiki/Neural_backpropagation\" title=\"Neural backpropagation\">neural backpropagation</a> as mechanisms for processing data.\\n</p><p>Computational devices have been created in CMOS for both biophysical simulation and <a href=\"/wiki/Neuromorphic_computing\" class=\"mw-redirect\" title=\"Neuromorphic computing\">neuromorphic computing</a>. More recent efforts show promise for creating <a href=\"/wiki/Nanodevice\" class=\"mw-redirect\" title=\"Nanodevice\">nanodevices</a> for very large scale <a href=\"/wiki/Principal_component\" class=\"mw-redirect\" title=\"Principal component\">principal components</a> analyses and <a href=\"/wiki/Convolution\" title=\"Convolution\">convolution</a>.<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup> If successful, these efforts could usher in a new era of <a href=\"/wiki/Neural_computing\" class=\"mw-redirect\" title=\"Neural computing\">neural computing</a> that is a step beyond digital computing,<sup id=\"cite_ref-25\" class=\"reference\"><a href=\"#cite_note-25\">&#91;25&#93;</a></sup> because it depends on <a href=\"/wiki/Learning\" title=\"Learning\">learning</a> rather than <a href=\"/wiki/Programming_language\" title=\"Programming language\">programming</a> and because it is fundamentally <a href=\"/wiki/Analog_signal\" title=\"Analog signal\">analog</a> rather than <a href=\"/wiki/Digital_data\" title=\"Digital data\">digital</a> even though the first instantiations may in fact be with CMOS digital devices.\\n</p><p>Between 2009 and 2012, the <a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">recurrent neural networks</a> and deep <a href=\"/wiki/Feedforward_neural_network\" title=\"Feedforward neural network\">feedforward neural networks</a> developed in the research group of <a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">Jürgen Schmidhuber</a> at the <a href=\"/wiki/IDSIA\" class=\"mw-redirect\" title=\"IDSIA\">Swiss AI Lab IDSIA</a> have won eight international competitions in <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern recognition</a> and <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a>.<sup id=\"cite_ref-26\" class=\"reference\"><a href=\"#cite_note-26\">&#91;26&#93;</a></sup> For example, multi-dimensional <a href=\"/wiki/Long_short_term_memory\" class=\"mw-redirect\" title=\"Long short term memory\">long short term memory</a> (LSTM)<sup id=\"cite_ref-27\" class=\"reference\"><a href=\"#cite_note-27\">&#91;27&#93;</a></sup><sup id=\"cite_ref-28\" class=\"reference\"><a href=\"#cite_note-28\">&#91;28&#93;</a></sup> won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.\\n</p><p>Variants of the <a href=\"/wiki/Back-propagation\" class=\"mw-redirect\" title=\"Back-propagation\">back-propagation</a> algorithm as well as unsupervised methods by <a href=\"/wiki/Geoff_Hinton\" class=\"mw-redirect\" title=\"Geoff Hinton\">Geoff Hinton</a> and colleagues at the <a href=\"/wiki/University_of_Toronto\" title=\"University of Toronto\">University of Toronto</a> can be used to train deep, highly nonlinear neural architectures,<sup id=\"cite_ref-29\" class=\"reference\"><a href=\"#cite_note-29\">&#91;29&#93;</a></sup> similar to the 1980 <a href=\"/wiki/Neocognitron\" title=\"Neocognitron\">Neocognitron</a> by <a href=\"/wiki/Kunihiko_Fukushima\" title=\"Kunihiko Fukushima\">Kunihiko Fukushima</a>,<sup id=\"cite_ref-30\" class=\"reference\"><a href=\"#cite_note-30\">&#91;30&#93;</a></sup> and the \"standard architecture of vision\",<sup id=\"cite_ref-31\" class=\"reference\"><a href=\"#cite_note-31\">&#91;31&#93;</a></sup> inspired by the simple and complex cells identified by <a href=\"/wiki/David_H._Hubel\" title=\"David H. Hubel\">David H. Hubel</a> and <a href=\"/wiki/Torsten_Wiesel\" title=\"Torsten Wiesel\">Torsten Wiesel</a> in the primary <a href=\"/wiki/Visual_cortex\" title=\"Visual cortex\">visual cortex</a>.\\n</p><p>Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in <a href=\"/wiki/Nonlinear_system_identification\" title=\"Nonlinear system identification\">nonlinear system identification</a> and classification applications.<sup id=\"cite_ref-SAB1_19-1\" class=\"reference\"><a href=\"#cite_note-SAB1-19\">&#91;19&#93;</a></sup>\\n</p><p><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a> feedforward networks alternate <a href=\"/wiki/Convolution\" title=\"Convolution\">convolutional</a> layers and max-pooling layers, topped by several pure classification layers. Fast <a href=\"/wiki/GPU\" class=\"mw-redirect\" title=\"GPU\">GPU</a>-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition<sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\">&#91;32&#93;</a></sup> and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge.<sup id=\"cite_ref-33\" class=\"reference\"><a href=\"#cite_note-33\">&#91;33&#93;</a></sup> Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance<sup id=\"cite_ref-34\" class=\"reference\"><a href=\"#cite_note-34\">&#91;34&#93;</a></sup> on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of <a href=\"/wiki/Yann_LeCun\" title=\"Yann LeCun\">Yann LeCun</a> and colleagues at <a href=\"/wiki/NYU\" class=\"mw-redirect\" title=\"NYU\">NYU</a>.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=9\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div class=\"div-col columns column-width\" style=\"-moz-column-width: 22em; -webkit-column-width: 22em; column-width: 22em;\">\\n<ul><li><a href=\"/wiki/ADALINE\" title=\"ADALINE\">ADALINE</a></li>\\n<li><a href=\"/wiki/Adaptive_resonance_theory\" title=\"Adaptive resonance theory\">Adaptive resonance theory</a></li>\\n<li><a href=\"/wiki/Biological_cybernetics\" class=\"mw-redirect\" title=\"Biological cybernetics\">Biological cybernetics</a></li>\\n<li><a href=\"/wiki/Biologically_inspired_computing\" class=\"mw-redirect\" title=\"Biologically inspired computing\">Biologically inspired computing</a></li>\\n<li><a href=\"/wiki/Cerebellar_model_articulation_controller\" title=\"Cerebellar model articulation controller\">Cerebellar model articulation controller</a></li>\\n<li><a href=\"/wiki/Cognitive_architecture\" title=\"Cognitive architecture\">Cognitive architecture</a></li>\\n<li><a href=\"/wiki/Cognitive_science\" title=\"Cognitive science\">Cognitive science</a></li>\\n<li><a href=\"/wiki/Connectomics\" title=\"Connectomics\">Connectomics</a></li>\\n<li><a href=\"/wiki/Cultured_neuronal_networks\" class=\"mw-redirect\" title=\"Cultured neuronal networks\">Cultured neuronal networks</a></li>\\n<li><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></li>\\n<li><a href=\"/wiki/Digital_morphogenesis\" title=\"Digital morphogenesis\">Digital morphogenesis</a></li>\\n<li><a href=\"/wiki/Exclusive_or\" title=\"Exclusive or\">Exclusive or</a></li>\\n<li><a href=\"/wiki/Gene_expression_programming\" title=\"Gene expression programming\">Gene expression programming</a></li>\\n<li><a href=\"/wiki/Group_method_of_data_handling\" title=\"Group method of data handling\">Group method of data handling</a></li>\\n<li><a href=\"/wiki/Habituation\" title=\"Habituation\">Habituation</a></li>\\n<li><a href=\"/wiki/In_situ_adaptive_tabulation\" title=\"In situ adaptive tabulation\">In situ adaptive tabulation</a></li>\\n<li><a href=\"/wiki/Memristor\" title=\"Memristor\">Memristor</a></li>\\n<li><a href=\"/wiki/Multilinear_subspace_learning\" title=\"Multilinear subspace learning\">Multilinear subspace learning</a></li>\\n<li><a href=\"/wiki/Neural_network_software\" title=\"Neural network software\">Neural network software</a></li>\\n<li><a href=\"/wiki/Nonlinear_system_identification\" title=\"Nonlinear system identification\">Nonlinear system identification</a></li>\\n<li><a href=\"/wiki/Parallel_constraint_satisfaction_processes\" title=\"Parallel constraint satisfaction processes\">Parallel constraint satisfaction processes</a></li>\\n<li><a href=\"/wiki/Parallel_distributed_processing\" class=\"mw-redirect\" title=\"Parallel distributed processing\">Parallel distributed processing</a></li>\\n<li><a href=\"/wiki/Predictive_analytics\" title=\"Predictive analytics\">Predictive analytics</a></li>\\n<li><a href=\"/wiki/Radial_basis_function_network\" title=\"Radial basis function network\">Radial basis function network</a></li>\\n<li><a href=\"/wiki/Self-organizing_map\" title=\"Self-organizing map\">Self-organizing map</a></li>\\n<li><a href=\"/wiki/Simulated_reality\" title=\"Simulated reality\">Simulated reality</a></li>\\n<li><a href=\"/wiki/Support_vector_machine\" class=\"mw-redirect\" title=\"Support vector machine\">Support vector machine</a></li>\\n<li><a href=\"/wiki/Tensor_product_network\" title=\"Tensor product network\">Tensor product network</a></li>\\n<li><a href=\"/wiki/Time_delay_neural_network\" title=\"Time delay neural network\">Time delay neural network</a></li></ul>\\n</div>\\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=10\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\\n<ol class=\"references\">\\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hopfield, J. J. (1982). <a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC346238\">\"Neural networks and physical systems with emergent collective computational abilities\"</a>. <i>Proc. Natl. Acad. Sci. U.S.A</i>. <b>79</b> (8): 2554–2558. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/1982PNAS...79.2554H\">1982PNAS...79.2554H</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1073%2Fpnas.79.8.2554\">10.1073/pnas.79.8.2554</a>. <a href=\"/wiki/PubMed_Central\" title=\"PubMed Central\">PMC</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC346238\">346238</a></span>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/6953413\">6953413</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proc.+Natl.+Acad.+Sci.+U.S.A.&amp;rft.atitle=Neural+networks+and+physical+systems+with+emergent+collective+computational+abilities&amp;rft.volume=79&amp;rft.issue=8&amp;rft.pages=2554-2558&amp;rft.date=1982&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC346238&amp;rft_id=info%3Apmid%2F6953413&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.79.8.2554&amp;rft_id=info%3Abibcode%2F1982PNAS...79.2554H&amp;rft.aulast=Hopfield&amp;rft.aufirst=J.+J.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC346238&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><style data-mw-deduplicate=\"TemplateStyles:r935243608\">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>\\n</li>\\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.gartner.com/it-glossary/neural-net-or-neural-network\">\"Neural Net or Neural Network - Gartner IT Glossary\"</a>. <i>www.gartner.com</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.gartner.com&amp;rft.atitle=Neural+Net+or+Neural+Network+-+Gartner+IT+Glossary&amp;rft_id=https%3A%2F%2Fwww.gartner.com%2Fit-glossary%2Fneural-net-or-neural-network&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\">Arbib, p.666</span>\\n</li>\\n<li id=\"cite_note-Bain_1873-4\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Bain_1873_4-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Bain_1873_4-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Bain_1873_4-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Bain_1873_4-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Bain (1873). <i>Mind and Body: The Theories of Their Relation</i>. New York: D. Appleton and Company.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mind+and+Body%3A+The+Theories+of+Their+Relation&amp;rft.place=New+York&amp;rft.pub=D.+Appleton+and+Company&amp;rft.date=1873&amp;rft.au=Bain&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-James_1890-5\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-James_1890_5-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-James_1890_5-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">James (1890). <a rel=\"nofollow\" class=\"external text\" href=\"https://archive.org/details/principlespsych01jamegoog\"><i>The Principles of Psychology</i></a>. New York: H. Holt and Company.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Principles+of+Psychology&amp;rft.place=New+York&amp;rft.pub=H.+Holt+and+Company&amp;rft.date=1890&amp;rft.au=James&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fprinciplespsych01jamegoog&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Cuntz, Hermann (2010). \"PLoS Computational Biology Issue Image &#124; Vol. 6(8) August 2010\". <i>PLoS Computational Biology</i>. <b>6</b> (8): ev06.i08. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1371%2Fimage.pcbi.v06.i08\">10.1371/image.pcbi.v06.i08</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLoS+Computational+Biology&amp;rft.atitle=PLoS+Computational+Biology+Issue+Image+%26%23124%3B+Vol.+6%288%29+August+2010&amp;rft.volume=6&amp;rft.issue=8&amp;rft.pages=ev06.i08&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1371%2Fimage.pcbi.v06.i08&amp;rft.aulast=Cuntz&amp;rft.aufirst=Hermann&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Sherrington, C.S. (1898). \"Experiments in Examination of the Peripheral Distribution of the Fibers of the Posterior Roots of Some Spinal Nerves\". <i>Proceedings of the Royal Society of London</i>. <b>190</b>: 45–186. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1098%2Frstb.1898.0002\">10.1098/rstb.1898.0002</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Royal+Society+of+London&amp;rft.atitle=Experiments+in+Examination+of+the+Peripheral+Distribution+of+the+Fibers+of+the+Posterior+Roots+of+Some+Spinal+Nerves&amp;rft.volume=190&amp;rft.pages=45-186&amp;rft.date=1898&amp;rft_id=info%3Adoi%2F10.1098%2Frstb.1898.0002&amp;rft.aulast=Sherrington&amp;rft.aufirst=C.S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">McCulloch, Warren; Walter Pitts (1943). \"A Logical Calculus of Ideas Immanent in Nervous Activity\". <i>Bulletin of Mathematical Biophysics</i>. <b>5</b> (4): 115–133. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2FBF02478259\">10.1007/BF02478259</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+Mathematical+Biophysics&amp;rft.atitle=A+Logical+Calculus+of+Ideas+Immanent+in+Nervous+Activity&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=115-133&amp;rft.date=1943&amp;rft_id=info%3Adoi%2F10.1007%2FBF02478259&amp;rft.aulast=McCulloch&amp;rft.aufirst=Warren&amp;rft.au=Walter+Pitts&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Hebb, Donald (1949). <a rel=\"nofollow\" class=\"external text\" href=\"https://archive.org/details/in.ernet.dli.2015.226341\"><i>The Organization of Behavior</i></a>. New York: Wiley.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Organization+of+Behavior&amp;rft.place=New+York&amp;rft.pub=Wiley&amp;rft.date=1949&amp;rft.aulast=Hebb&amp;rft.aufirst=Donald&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fin.ernet.dli.2015.226341&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Farley, B.; W.A. Clark (1954). \"Simulation of Self-Organizing Systems by Digital Computer\". <i>IRE Transactions on Information Theory</i>. <b>4</b> (4): 76–84. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FTIT.1954.1057468\">10.1109/TIT.1954.1057468</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IRE+Transactions+on+Information+Theory&amp;rft.atitle=Simulation+of+Self-Organizing+Systems+by+Digital+Computer&amp;rft.volume=4&amp;rft.issue=4&amp;rft.pages=76-84&amp;rft.date=1954&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1954.1057468&amp;rft.aulast=Farley&amp;rft.aufirst=B.&amp;rft.au=W.A.+Clark&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Rochester, N.; J.H. Holland, L.H. Habit and W.L. Duda (1956). \"Tests on a cell assembly theory of the action of the brain, using a large digital computer\". <i>IRE Transactions on Information Theory</i>. <b>2</b> (3): 80–93. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FTIT.1956.1056810\">10.1109/TIT.1956.1056810</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IRE+Transactions+on+Information+Theory&amp;rft.atitle=Tests+on+a+cell+assembly+theory+of+the+action+of+the+brain%2C+using+a+large+digital+computer&amp;rft.volume=2&amp;rft.issue=3&amp;rft.pages=80-93&amp;rft.date=1956&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1956.1056810&amp;rft.aulast=Rochester&amp;rft.aufirst=N.&amp;rft.au=J.H.+Holland%2C+L.H.+Habit+and+W.L.+Duda&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Rosenblatt, F. (1958). \"The Perceptron: A Probalistic Model For Information Storage And Organization In The Brain\". <i>Psychological Review</i>. <b>65</b> (6): 386–408. <a href=\"/wiki/CiteSeerX\" title=\"CiteSeerX\">CiteSeerX</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.3775\">10.1.1.588.3775</a></span>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1037%2Fh0042519\">10.1037/h0042519</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/13602029\">13602029</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychological+Review&amp;rft.atitle=The+Perceptron%3A+A+Probalistic+Model+For+Information+Storage+And+Organization+In+The+Brain&amp;rft.volume=65&amp;rft.issue=6&amp;rft.pages=386-408&amp;rft.date=1958&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.588.3775&amp;rft_id=info%3Apmid%2F13602029&amp;rft_id=info%3Adoi%2F10.1037%2Fh0042519&amp;rft.aulast=Rosenblatt&amp;rft.aufirst=F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-Werbos_1975-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Werbos_1975_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Werbos_1975_13-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Werbos, P.J. (1975). <i>Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Beyond+Regression%3A+New+Tools+for+Prediction+and+Analysis+in+the+Behavioral+Sciences&amp;rft.date=1975&amp;rft.aulast=Werbos&amp;rft.aufirst=P.J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Minsky, M.; S. Papert (1969). <i>An Introduction to Computational Geometry</i>. MIT Press. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-0-262-63022-1\" title=\"Special:BookSources/978-0-262-63022-1\"><bdi>978-0-262-63022-1</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=An+Introduction+to+Computational+Geometry&amp;rft.pub=MIT+Press&amp;rft.date=1969&amp;rft.isbn=978-0-262-63022-1&amp;rft.aulast=Minsky&amp;rft.aufirst=M.&amp;rft.au=S.+Papert&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Rumelhart, D.E.; James McClelland (1986). <span class=\"cs1-lock-registration\" title=\"Free registration required\"><a rel=\"nofollow\" class=\"external text\" href=\"https://archive.org/details/paralleldistribu00rume\"><i>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</i></a></span>. Cambridge: MIT Press.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Parallel+Distributed+Processing%3A+Explorations+in+the+Microstructure+of+Cognition&amp;rft.place=Cambridge&amp;rft.pub=MIT+Press&amp;rft.date=1986&amp;rft.aulast=Rumelhart&amp;rft.aufirst=D.E.&amp;rft.au=James+McClelland&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fparalleldistribu00rume&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Russell, Ingrid. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20140529155320/http://uhaweb.hartford.edu/compsci/neural-networks-definition.html\">\"Neural Networks Module\"</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://uhaweb.hartford.edu/compsci/neural-networks-definition.html\">the original</a> on 29 May 2014<span class=\"reference-accessdate\">. Retrieved 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Neural+Networks+Module&amp;rft.aulast=Russell&amp;rft.aufirst=Ingrid&amp;rft_id=http%3A%2F%2Fuhaweb.hartford.edu%2Fcompsci%2Fneural-networks-definition.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span> <span class=\"cs1-visible-error error citation-comment\">Check date values in: <code class=\"cs1-code\">&#124;accessdate=</code> (<a href=\"/wiki/Help:CS1_errors#bad_date\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">McCulloch, Warren; Pitts, Walter (1943). \"A Logical Calculus of Ideas Immanent in Nervous Activity\". <i>Bulletin of Mathematical Biophysics</i>. <b>5</b> (4): 115–133. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2FBF02478259\">10.1007/BF02478259</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+Mathematical+Biophysics&amp;rft.atitle=A+Logical+Calculus+of+Ideas+Immanent+in+Nervous+Activity&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=115-133&amp;rft.date=1943&amp;rft_id=info%3Adoi%2F10.1007%2FBF02478259&amp;rft.aulast=McCulloch&amp;rft.aufirst=Warren&amp;rft.au=Pitts%2C+Walter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Copeland, B. Jack, ed. (2004). <i>The Essential Turing</i>. Oxford University Press. p.&#160;403. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-0-19-825080-7\" title=\"Special:BookSources/978-0-19-825080-7\"><bdi>978-0-19-825080-7</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Essential+Turing&amp;rft.pages=403&amp;rft.pub=Oxford+University+Press&amp;rft.date=2004&amp;rft.isbn=978-0-19-825080-7&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-SAB1-19\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-SAB1_19-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-SAB1_19-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Billings, S. A. (2013). <i>Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</i>. Wiley. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-1-119-94359-4\" title=\"Special:BookSources/978-1-119-94359-4\"><bdi>978-1-119-94359-4</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nonlinear+System+Identification%3A+NARMAX+Methods+in+the+Time%2C+Frequency%2C+and+Spatio-Temporal+Domains&amp;rft.pub=Wiley&amp;rft.date=2013&amp;rft.isbn=978-1-119-94359-4&amp;rft.aulast=Billings&amp;rft.aufirst=S.+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Wieczorek, Szymon; Filipiak, Dominik; Filipowska, Agata (2018). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/328964756\">\"Semantic Image-Based Profiling of Users\\' Interests with Neural Networks\"</a>. <i>Studies on the Semantic Web</i>. <b>36</b> (Emerging Topics in Semantic Technologies). <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.3233%2F978-1-61499-894-5-179\">10.3233/978-1-61499-894-5-179</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Studies+on+the+Semantic+Web&amp;rft.atitle=Semantic+Image-Based+Profiling+of+Users%27+Interests+with+Neural+Networks&amp;rft.volume=36&amp;rft.issue=Emerging+Topics+in+Semantic+Technologies&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.3233%2F978-1-61499-894-5-179&amp;rft.aulast=Wieczorek&amp;rft.aufirst=Szymon&amp;rft.au=Filipiak%2C+Dominik&amp;rft.au=Filipowska%2C+Agata&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F328964756&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Administrator, NASA (June 5, 2013). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.nasa.gov/centers/dryden/news/NewsReleases/2003/03-49.html\">\"Dryden Flight Research Center - News Room: News Releases: NASA NEURAL NETWORK PROJECT PASSES MILESTONE\"</a>. <i>NASA</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=NASA&amp;rft.atitle=Dryden+Flight+Research+Center+-+News+Room%3A+News+Releases%3A+NASA+NEURAL+NETWORK+PROJECT+PASSES+MILESTONE&amp;rft.date=2013-06-05&amp;rft.aulast=Administrator&amp;rft.aufirst=NASA&amp;rft_id=http%3A%2F%2Fwww.nasa.gov%2Fcenters%2Fdryden%2Fnews%2FNewsReleases%2F2003%2F03-49.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://members.fortunecity.com/templarseries/popper.html\">Roger Bridgman\\'s defence of neural networks</a></span>\\n</li>\\n<li id=\"cite_note-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-23\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/4\">\"Scaling Learning Algorithms towards {AI} - LISA - Publications - Aigaion 2.0\"</a>. <i>www.iro.umontreal.ca</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.iro.umontreal.ca&amp;rft.atitle=Scaling+Learning+Algorithms+towards+%7BAI%7D+-+LISA+-+Publications+-+Aigaion+2.0&amp;rft_id=http%3A%2F%2Fwww.iro.umontreal.ca%2F~lisa%2Fpublications2%2Findex.php%2Fpublications%2Fshow%2F4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Yang, J. J.;  et al. (2008). \"Memristive switching mechanism for metal/oxide/metal nanodevices\". <i><a href=\"/wiki/Nature_Nanotechnology\" title=\"Nature Nanotechnology\">Nat. Nanotechnol.</a></i> <b>3</b> (7): 429–433. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1038%2Fnnano.2008.160\">10.1038/nnano.2008.160</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/18654568\">18654568</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nat.+Nanotechnol.&amp;rft.atitle=Memristive+switching+mechanism+for+metal%2Foxide%2Fmetal+nanodevices&amp;rft.volume=3&amp;rft.issue=7&amp;rft.pages=429-433&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1038%2Fnnano.2008.160&amp;rft_id=info%3Apmid%2F18654568&amp;rft.aulast=Yang&amp;rft.aufirst=J.+J.&amp;rft.au=Pickett%2C+M.+D.&amp;rft.au=Li%2C+X.+M.&amp;rft.au=Ohlberg%2C+D.+A.+A.&amp;rft.au=Stewart%2C+D.+R.&amp;rft.au=Williams%2C+R.+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Strukov, D. B.;  et al. (2008). \"The missing memristor found\". <i>Nature</i>. <b>453</b> (7191): 80–83. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2008Natur.453...80S\">2008Natur.453...80S</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1038%2Fnature06932\">10.1038/nature06932</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/18451858\">18451858</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=The+missing+memristor+found&amp;rft.volume=453&amp;rft.issue=7191&amp;rft.pages=80-83&amp;rft.date=2008&amp;rft_id=info%3Apmid%2F18451858&amp;rft_id=info%3Adoi%2F10.1038%2Fnature06932&amp;rft_id=info%3Abibcode%2F2008Natur.453...80S&amp;rft.aulast=Strukov&amp;rft.aufirst=D.+B.&amp;rft.au=Snider%2C+G.+S.&amp;rft.au=Stewart%2C+D.+R.&amp;rft.au=Williams%2C+R.+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions\">\"2012 Kurzweil AI Interview with Jürgen Schmidhuber on the eight competitions won by his Deep Learning team 2009–2012\"</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=2012+Kurzweil+AI+Interview+with+J%C3%BCrgen+Schmidhuber+on+the+eight+competitions+won+by+his+Deep+Learning+team+2009%E2%80%932012&amp;rft_id=http%3A%2F%2Fwww.kurzweilai.net%2Fhow-bio-inspired-deep-learning-keeps-winning-competitions&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Graves, Alex; Schmidhuber, Jürgen (2008). <a rel=\"nofollow\" class=\"external text\" href=\"http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks\">\"Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks\"</a>.  In Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; Culotta, Aron (eds.). <i>Advances in Neural Information Processing Systems 21 (NIPS\\'21)</i>. Neural Information Processing Systems (NIPS) Foundation. pp.&#160;545–552.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Offline+Handwriting+Recognition+with+Multidimensional+Recurrent+Neural+Networks&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems+21+%28NIPS%2721%29&amp;rft.pages=545-552&amp;rft.pub=Neural+Information+Processing+Systems+%28NIPS%29+Foundation&amp;rft.date=2008&amp;rft.aulast=Graves&amp;rft.aufirst=Alex&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009). \"A Novel Connectionist System for Improved Unconstrained Handwriting Recognition\". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>31</b> (5): 855–868. <a href=\"/wiki/CiteSeerX\" title=\"CiteSeerX\">CiteSeerX</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502\">10.1.1.139.4502</a></span>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FTPAMI.2008.137\">10.1109/TPAMI.2008.137</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/19299860\">19299860</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=A+Novel+Connectionist+System+for+Improved+Unconstrained+Handwriting+Recognition&amp;rft.volume=31&amp;rft.issue=5&amp;rft.pages=855-868&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.139.4502&amp;rft_id=info%3Apmid%2F19299860&amp;rft_id=info%3Adoi%2F10.1109%2FTPAMI.2008.137&amp;rft.aulast=Graves&amp;rft.aufirst=A.&amp;rft.au=Liwicki%2C+M.&amp;rft.au=Fernandez%2C+S.&amp;rft.au=Bertolami%2C+R.&amp;rft.au=Bunke%2C+H.&amp;rft.au=Schmidhuber%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\"><a href=\"/wiki/Geoffrey_Hinton\" title=\"Geoffrey Hinton\">Hinton, G. E.</a>; Osindero, S.; Teh, Y. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\">\"A fast learning algorithm for deep belief nets\"</a> <span class=\"cs1-format\">(PDF)</span>. <i><a href=\"/wiki/Neural_Computation_(journal)\" title=\"Neural Computation (journal)\">Neural Computation</a></i>. <b>18</b> (7): 1527–1554. <a href=\"/wiki/CiteSeerX\" title=\"CiteSeerX\">CiteSeerX</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.76.1541\">10.1.1.76.1541</a></span>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1162%2Fneco.2006.18.7.1527\">10.1162/neco.2006.18.7.1527</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/16764513\">16764513</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=A+fast+learning+algorithm+for+deep+belief+nets&amp;rft.volume=18&amp;rft.issue=7&amp;rft.pages=1527-1554&amp;rft.date=2006&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.76.1541&amp;rft_id=info%3Apmid%2F16764513&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.7.1527&amp;rft.aulast=Hinton&amp;rft.aufirst=G.+E.&amp;rft.au=Osindero%2C+S.&amp;rft.au=Teh%2C+Y.&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Ffastnc.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-30\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-30\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Fukushima, K. (1980). \"Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position\". <i>Biological Cybernetics</i>. <b>36</b> (4): 93–202. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2FBF00344251\">10.1007/BF00344251</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/7370364\">7370364</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biological+Cybernetics&amp;rft.atitle=Neocognitron%3A+A+self-organizing+neural+network+model+for+a+mechanism+of+pattern+recognition+unaffected+by+shift+in+position&amp;rft.volume=36&amp;rft.issue=4&amp;rft.pages=93-202&amp;rft.date=1980&amp;rft_id=info%3Adoi%2F10.1007%2FBF00344251&amp;rft_id=info%3Apmid%2F7370364&amp;rft.aulast=Fukushima&amp;rft.aufirst=K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Riesenhuber, M.; <a href=\"/wiki/Tomaso_Poggio\" title=\"Tomaso Poggio\">Poggio, T.</a> (1999). \"Hierarchical models of object recognition in cortex\". <i><a href=\"/wiki/Nature_Neuroscience\" title=\"Nature Neuroscience\">Nature Neuroscience</a></i>. <b>2</b> (11): 1019–1025. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1038%2F14819\">10.1038/14819</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/10526343\">10526343</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Neuroscience&amp;rft.atitle=Hierarchical+models+of+object+recognition+in+cortex&amp;rft.volume=2&amp;rft.issue=11&amp;rft.pages=1019-1025&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1038%2F14819&amp;rft_id=info%3Apmid%2F10526343&amp;rft.aulast=Riesenhuber&amp;rft.aufirst=M.&amp;rft.au=Poggio%2C+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\">D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. <a rel=\"nofollow\" class=\"external text\" href=\"https://people.lu.usi.ch/mascij/data/papers/2012_nn_traffic.pdf\">Multi-Column Deep Neural Network for Traffic Sign Classification</a>. Neural Networks, 2012.</span>\\n</li>\\n<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\">D. Ciresan, A. Giusti, L. Gambardella, J. Schmidhuber. <a rel=\"nofollow\" class=\"external text\" href=\"https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf\">Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a>. In Advances in Neural Information Processing Systems (NIPS 2012), Lake Tahoe, 2012.</span>\\n</li>\\n<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\">D. C. Ciresan, U. Meier, <a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">J. Schmidhuber</a>. Multi-column Deep Neural Networks for Image Classification. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012.</span>\\n</li>\\n</ol></div>\\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=11\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div id=\"section_SpokenWikipedia\" class=\"infobox sisterproject plainlinks haudio\"><div style=\"text-align: center; white-space:nowrap\"><b>Listen to this article</b> (<a href=\"/wiki/File:En-Neural_network.ogg\" title=\"File:En-Neural network.ogg\">info/dl</a>)\\n<div class=\"center\"><div class=\"floatnone\"><div class=\"mediaContainer\" style=\"width:200px\"><audio id=\"mwe_player_0\" controls=\"\" preload=\"none\" style=\"width:200px\" class=\"kskin\" data-durationhint=\"1882.1398639456\" data-startoffset=\"0\" data-mwtitle=\"En-Neural_network.ogg\" data-mwprovider=\"wikimediacommons\"><source src=\"//upload.wikimedia.org/wikipedia/commons/a/a3/En-Neural_network.ogg\" type=\"audio/ogg; codecs=&quot;vorbis&quot;\" data-title=\"Original Ogg file (69 kbps)\" data-shorttitle=\"Ogg source\" data-width=\"0\" data-height=\"0\" data-bandwidth=\"68628\" /><source src=\"//upload.wikimedia.org/wikipedia/commons/transcoded/a/a3/En-Neural_network.ogg/En-Neural_network.ogg.mp3\" type=\"audio/mpeg\" data-title=\"MP3\" data-shorttitle=\"MP3\" data-transcodekey=\"mp3\" data-width=\"0\" data-height=\"0\" data-bandwidth=\"130904\" /></audio></div></div></div>\\n</div>\\n<p><br />\\n</p>\\n<div style=\"float: left; margin-left: 5px;\"><div class=\"floatnone\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/47/Sound-icon.svg/45px-Sound-icon.svg.png\" decoding=\"async\" title=\"Spoken Wikipedia\" width=\"45\" height=\"34\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/47/Sound-icon.svg/68px-Sound-icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/47/Sound-icon.svg/90px-Sound-icon.svg.png 2x\" data-file-width=\"128\" data-file-height=\"96\" /></div></div>\\n<div style=\"font-size: xx-small; line-height: 1.6em; margin-left: 60px;\">This audio file was created from a revision of the article \"<span class=\"fn\">Neural network</span>\" dated 2011-11-27, and does not reflect subsequent edits to the article. (<a href=\"/wiki/Wikipedia:Media_help\" class=\"mw-redirect\" title=\"Wikipedia:Media help\">Audio help</a>)</div>\\n<div style=\"text-align: center; clear: both\"><b><a href=\"/wiki/Wikipedia:Spoken_articles\" title=\"Wikipedia:Spoken articles\">More spoken articles</a></b></div>\\n</div>\\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.dkriesel.com/en/science/neural_networks\">A Brief Introduction to Neural Networks (D. Kriesel)</a> - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.</li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.html\">Review of Neural Networks in Materials Science</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.gc.ssr.upm.es/inves/neural/ann1/anntutorial.html\">Artificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html\">Another introduction to ANN</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.youtube.com/watch?v=AyzOUbkUf3M\">Next Generation of Neural Networks</a> - Google Tech Talks</li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.msm.cam.ac.uk/phase-trans/2009/performance.html\">Performance of Neural Networks</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.msm.cam.ac.uk/phase-trans/2009/review_Bhadeshia_SADM.pdf\">Neural Networks and Information</a></li>\\n<li><cite class=\"citation web\">Sanderson, Grant (October 5, 2017). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\">\"But what <i>is</i> a Neural Network?\"</a>. <i><a href=\"/wiki/3Blue1Brown\" title=\"3Blue1Brown\">3Blue1Brown</a></i> &#8211; via <a href=\"/wiki/YouTube\" title=\"YouTube\">YouTube</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=3Blue1Brown&amp;rft.atitle=But+what+is+a+Neural+Network%3F&amp;rft.date=2017-10-05&amp;rft.aulast=Sanderson&amp;rft.aufirst=Grant&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaircAruvnKk%26list%3DPLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></li></ul></div>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data.html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = wiki_data.html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = wiki_data.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_links = wiki_data.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1 1 3Blue1Brown\n",
      "2 1 1 A. K. Dewdney\n",
      "3 1 1 ADALINE\n",
      "2 1 0 Acetylcholine\n",
      "1 2 0 Adaptive control\n",
      "2 1 1 Adaptive resonance theory\n",
      "1 1 0 Adaptive system\n",
      "2 1 1 Alan Turing\n",
      "2 1 1 Alexander Bain\n",
      "2 1 0 Amplitude\n",
      "1 0 0 Analog signal\n",
      "8 10 4 Artificial intelligence\n",
      "4 15 1 Artificial neural network\n",
      "4 5 1 Artificial neuron\n",
      "1 2 0 Autonomous robot\n",
      "2 1 0 Axon\n",
      "2 1 1 BCM theory\n",
      "2 1 0 Back-propagation\n",
      "5 4 1 Backpropagation\n",
      "6 2 2 Bibcode\n",
      "2 2 1 Biological cybernetics\n",
      "3 5 0 Biological neural network\n",
      "1 0 0 Biological neuron models\n",
      "2 1 1 Biologically inspired computing\n",
      "6 2 2 Biophysics\n",
      "1 1 0 Blind signal separation\n",
      "2 1 1 Boltzmann machine\n",
      "3 1 1 CPU\n",
      "2 1 1 Cerebellar model articulation controller\n",
      "1 0 0 Charles Scott Sherrington\n",
      "9 3 3 CiteSeerX\n",
      "2 1 1 Cognitive architecture\n",
      "1 2 0 Cognitive modeling\n",
      "2 1 1 Cognitive science\n",
      "15 18 5 Computation\n",
      "1 1 0 Computational neuroscience\n",
      "2 1 1 Computer simulation\n",
      "6 2 0 Connectionism\n",
      "3 1 1 Connectomics\n",
      "4 2 0 Convolution\n",
      "2 1 1 Cultured neuronal networks\n",
      "1 0 0 Data compression\n",
      "1 1 0 Data mining\n",
      "1 1 0 Data modeling\n",
      "2 1 1 Data processing\n",
      "2 2 0 Database\n",
      "2 1 1 David H. Hubel\n",
      "1 3 0 Decision making\n",
      "5 4 2 Deep learning\n",
      "4 2 0 Dendrite\n",
      "1 1 0 Dendrodendritic synapse\n",
      "1 0 0 Digital data\n",
      "2 1 1 Digital morphogenesis\n",
      "15 0 0 Digital object identifier\n",
      "2 1 1 Donald Hebb\n",
      "2 1 0 Dopamine\n",
      "1 1 0 E-mail spam\n",
      "2 3 0 Exclusive-or\n",
      "2 1 1 Exclusive or\n",
      "2 1 0 Feedforward neural network\n",
      "1 0 0 Frank Rosenblatt\n",
      "2 1 1 Function approximation\n",
      "3 1 1 GPU\n",
      "2 1 1 Gene expression programming\n",
      "2 1 1 Geoff Hinton\n",
      "1 0 0 Geoffrey Hinton\n",
      "2 1 1 Group method of data handling\n",
      "5 2 1 Habituation\n",
      "1 0 0 Hard drive\n",
      "3 2 2 Hebbian learning\n",
      "2 2 1 Hopfield network\n",
      "3 1 1 IDSIA\n",
      "1 1 0 Image analysis\n",
      "2 1 1 In situ adaptive tabulation\n",
      "1 6 0 Information processing\n",
      "1 3 0 Information theory\n",
      "3 0 0 International Standard Book Number\n",
      "4 2 2 Jürgen Schmidhuber\n",
      "2 1 1 Kunihiko Fukushima\n",
      "7 25 3 Learning\n",
      "1 1 0 Long short term memory\n",
      "1 1 0 Long term potentiation\n",
      "1 2 0 Machine learning\n",
      "2 1 1 Marvin Minsky\n",
      "1 0 0 Mathematical model\n",
      "3 2 1 Memristor\n",
      "2 1 1 Multilinear subspace learning\n",
      "3 1 1 NYU\n",
      "2 2 0 Nanodevice\n",
      "1 0 0 Nature Nanotechnology\n",
      "2 1 1 Nature Neuroscience\n",
      "5 2 2 Neocognitron\n",
      "1 0 0 Neural Computation (journal)\n",
      "1 1 0 Neural backpropagation\n",
      "1 1 0 Neural computing\n",
      "2 1 1 Neural network (disambiguation)\n",
      "2 1 1 Neural network software\n",
      "1 2 0 Neural processing\n",
      "2 1 0 Neuromodulators\n",
      "1 1 0 Neuromorphic computing\n",
      "4 27 2 Neuron\n",
      "2 1 0 Neurotransmitter\n",
      "2 1 0 Non-linear\n",
      "4 4 1 Nonlinear system identification\n",
      "1 1 0 Novelty detection\n",
      "2 1 1 Parallel constraint satisfaction processes\n",
      "2 3 1 Parallel distributed processing\n",
      "3 6 0 Pattern recognition\n",
      "5 4 2 Perceptron\n",
      "2 1 1 Predictive analytics\n",
      "1 1 0 Predictive modeling\n",
      "1 1 0 Principal component\n",
      "1 0 0 Programming language\n",
      "1 0 0 PubMed Central\n",
      "8 0 0 PubMed Identifier\n",
      "1 1 0 Pyramidal neuron\n",
      "2 1 1 Radial basis function network\n",
      "1 1 0 Radial basis networks\n",
      "1 0 0 Random-access memory\n",
      "2 3 0 Recurrent neural network\n",
      "1 1 0 Regression analysis\n",
      "3 2 2 Roger Bridgman\n",
      "2 1 1 Scientific American\n",
      "2 1 1 Self-organizing map\n",
      "2 1 0 Serotonin\n",
      "2 1 1 Seymour Papert\n",
      "2 1 1 Simulated reality\n",
      "1 1 0 Software agents\n",
      "1 1 0 Speech recognition\n",
      "4 2 0 Statistical\n",
      "1 0 0 Statistical classification\n",
      "2 1 1 Support vector machine\n",
      "1 0 0 Symbolic artificial intelligence\n",
      "2 2 0 Synapse\n",
      "1 1 0 Synaptic plasticity\n",
      "2 1 1 Tensor product network\n",
      "1 1 0 Threshold logic\n",
      "2 1 1 Time delay neural network\n",
      "1 1 0 Time series prediction\n",
      "1 0 0 Tomaso Poggio\n",
      "2 1 1 Torsten Wiesel\n",
      "2 1 1 University of Chicago\n",
      "2 1 1 University of Toronto\n",
      "1 0 0 Unorganized machine\n",
      "1 1 0 Unsupervised learning\n",
      "1 1 0 Video game\n",
      "1 1 0 Visual cortex\n",
      "1 2 0 Von Neumann model\n",
      "4 2 2 Walter Pitts\n",
      "2 1 1 Warren McCulloch\n",
      "1 0 0 Warren Sturgis McCulloch\n",
      "2 1 1 William James\n",
      "2 1 1 Yann LeCun\n",
      "3 1 1 YouTube\n"
     ]
    }
   ],
   "source": [
    "for link in wiki_links:\n",
    "    count1 = html_text.count(link)\n",
    "    count2 = soup_text.lower().count(link.lower())\n",
    "    count3 = soup_text.count(link)\n",
    "    print(count1, count2, count3, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_text.count(\"wiki/CiteSeerX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_text, 'html.parser')\n",
    "soup_text = soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_network_(disambiguation)\n",
      "File:Neural_network_example.svg\n",
      "File:Neural_network_example.svg\n",
      "Feedforward_neural_network\n",
      "Neuron\n",
      "Artificial_neural_network\n",
      "Artificial_neuron\n",
      "Biological_neural_network\n",
      "Artificial_intelligence\n",
      "Amplitude\n",
      "Predictive_modeling\n",
      "Biological_neural_network\n",
      "Synapse\n",
      "Axon\n",
      "Dendrite\n",
      "Dendrodendritic_synapse\n",
      "Neurotransmitter\n",
      "Artificial_intelligence\n",
      "Cognitive_modeling\n",
      "Artificial_intelligence\n",
      "Speech_recognition\n",
      "Image_analysis\n",
      "Adaptive_control\n",
      "Software_agents\n",
      "Video_game\n",
      "Autonomous_robot\n",
      "Von_Neumann_model\n",
      "Alexander_Bain\n",
      "William_James\n",
      "File:Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\n",
      "File:Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\n",
      "Computer_simulation\n",
      "Dendrite\n",
      "Pyramidal_neuron\n",
      "Charles_Scott_Sherrington\n",
      "Habituation\n",
      "Warren_McCulloch\n",
      "Walter_Pitts\n",
      "Donald_Hebb\n",
      "Hebbian_learning\n",
      "Unsupervised_learning\n",
      "Long_term_potentiation\n",
      "Unorganized_machine\n",
      "Frank_Rosenblatt\n",
      "Perceptron\n",
      "Exclusive-or\n",
      "Backpropagation\n",
      "Marvin_Minsky\n",
      "Seymour_Papert\n",
      "Backpropagation\n",
      "Connectionism\n",
      "Connectionism\n",
      "Artificial_neural_network\n",
      "Artificial_neuron\n",
      "Mathematical_model\n",
      "Information_processing\n",
      "Connectionism\n",
      "Computation\n",
      "Adaptive_system\n",
      "Non-linear\n",
      "Statistical\n",
      "Data_modeling\n",
      "Decision_making\n",
      "Pattern_recognition\n",
      "Artificial_neural_network\n",
      "Artificial_neuron\n",
      "Warren_Sturgis_McCulloch\n",
      "Walter_Pitts\n",
      "University_of_Chicago\n",
      "Recurrent_neural_network\n",
      "Hopfield_network\n",
      "Alan_Turing\n",
      "Boltzmann_machine\n",
      "Deep_learning\n",
      "Function_approximation\n",
      "Regression_analysis\n",
      "Time_series_prediction\n",
      "Statistical_classification\n",
      "Pattern_recognition\n",
      "Novelty_detection\n",
      "Data_processing\n",
      "Blind_signal_separation\n",
      "Data_compression\n",
      "Nonlinear_system_identification\n",
      "Data_mining\n",
      "E-mail_spam\n",
      "Computational_neuroscience\n",
      "Biological_neural_network\n",
      "Information_theory\n",
      "Biological_neuron_models\n",
      "A._K._Dewdney\n",
      "Scientific_American\n",
      "Database\n",
      "Random-access_memory\n",
      "Hard_drive\n",
      "CPU\n",
      "Symbolic_artificial_intelligence\n",
      "Wikipedia:Citing_sources#What_information_to_include\n",
      "Neuromodulators\n",
      "Dopamine\n",
      "Acetylcholine\n",
      "Serotonin\n",
      "Biophysics\n",
      "BCM_theory\n",
      "Synaptic_plasticity\n",
      "Radial_basis_networks\n",
      "Neural_backpropagation\n",
      "Neuromorphic_computing\n",
      "Nanodevice\n",
      "Principal_component\n",
      "Convolution\n",
      "Neural_computing\n",
      "Learning\n",
      "Programming_language\n",
      "Analog_signal\n",
      "Digital_data\n",
      "Recurrent_neural_network\n",
      "Feedforward_neural_network\n",
      "J%C3%BCrgen_Schmidhuber\n",
      "IDSIA\n",
      "Pattern_recognition\n",
      "Machine_learning\n",
      "Long_short_term_memory\n",
      "Back-propagation\n",
      "Geoff_Hinton\n",
      "University_of_Toronto\n",
      "Neocognitron\n",
      "Kunihiko_Fukushima\n",
      "David_H._Hubel\n",
      "Torsten_Wiesel\n",
      "Visual_cortex\n",
      "Nonlinear_system_identification\n",
      "Deep_learning\n",
      "Convolution\n",
      "GPU\n",
      "Yann_LeCun\n",
      "NYU\n",
      "ADALINE\n",
      "Adaptive_resonance_theory\n",
      "Biological_cybernetics\n",
      "Biologically_inspired_computing\n",
      "Cerebellar_model_articulation_controller\n",
      "Cognitive_architecture\n",
      "Cognitive_science\n",
      "Connectomics\n",
      "Cultured_neuronal_networks\n",
      "Deep_learning\n",
      "Digital_morphogenesis\n",
      "Exclusive_or\n",
      "Gene_expression_programming\n",
      "Group_method_of_data_handling\n",
      "Habituation\n",
      "In_situ_adaptive_tabulation\n",
      "Memristor\n",
      "Multilinear_subspace_learning\n",
      "Neural_network_software\n",
      "Nonlinear_system_identification\n",
      "Parallel_constraint_satisfaction_processes\n",
      "Parallel_distributed_processing\n",
      "Predictive_analytics\n",
      "Radial_basis_function_network\n",
      "Self-organizing_map\n",
      "Simulated_reality\n",
      "Support_vector_machine\n",
      "Tensor_product_network\n",
      "Time_delay_neural_network\n",
      "Bibcode\n",
      "Digital_object_identifier\n",
      "PubMed_Central\n",
      "PubMed_Identifier\n",
      "Digital_object_identifier\n",
      "Digital_object_identifier\n",
      "Digital_object_identifier\n",
      "Digital_object_identifier\n",
      "Digital_object_identifier\n",
      "CiteSeerX\n",
      "Digital_object_identifier\n",
      "PubMed_Identifier\n",
      "International_Standard_Book_Number\n",
      "Special:BookSources\n",
      "Help:CS1_errors#bad_date\n",
      "Digital_object_identifier\n",
      "International_Standard_Book_Number\n",
      "Special:BookSources\n",
      "International_Standard_Book_Number\n",
      "Special:BookSources\n",
      "Digital_object_identifier\n",
      "Nature_Nanotechnology\n",
      "Digital_object_identifier\n",
      "PubMed_Identifier\n",
      "Bibcode\n",
      "Digital_object_identifier\n",
      "PubMed_Identifier\n",
      "CiteSeerX\n",
      "Digital_object_identifier\n",
      "PubMed_Identifier\n",
      "Geoffrey_Hinton\n",
      "Neural_Computation_(journal)\n",
      "CiteSeerX\n",
      "Digital_object_identifier\n",
      "PubMed_Identifier\n",
      "Digital_object_identifier\n",
      "PubMed_Identifier\n",
      "Tomaso_Poggio\n",
      "Nature_Neuroscience\n",
      "Digital_object_identifier\n",
      "PubMed_Identifier\n",
      "J%C3%BCrgen_Schmidhuber\n",
      "File:En-Neural_network.ogg\n",
      "Wikipedia:Media_help\n",
      "Wikipedia:Spoken_articles\n",
      "3Blue1Brown\n",
      "YouTube\n"
     ]
    }
   ],
   "source": [
    "all_links = []\n",
    "for link in soup.findAll('a'):\n",
    "    url_link = link.get('href')\n",
    "    if url_link[:5] == \"/wiki\":\n",
    "        keyword = url_link.split(\"/\")[2]\n",
    "        all_links.append(keyword)\n",
    "        print(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = wiki_data.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.count(\"Cognitive science\".lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data.html().count(\"A. K. Dewdney\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_link = wikipedia.page(\"Dendrodendritic_synapse\").url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dendrodendritic_synapse'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_link.split(\"/\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "if \"CiteSeerX\" in all_links:\n",
    "    print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28768207245178085"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.log(4/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4]\n",
    "if 1 in a: print(\"shivam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = wikipedia.page(\"Neural Netwrok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Neural network'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of real biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.\\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A neural network is a network or circuit of neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of real biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.\\nThese artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.\\n\\n\\n== Overview ==\\nA biological neural network is composed of a group or groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called synapses, are usually formed from axons to dendrites, though dendrodendritic synapses and other connections are possible. Apart from the electrical signaling, there are other forms of signaling that arise from neurotransmitter diffusion.\\nArtificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. Artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks. In the artificial intelligence field, artificial neural networks have been applied successfully to speech recognition, image analysis and adaptive control, in order to construct software agents (in computer and video games) or autonomous robots.\\nHistorically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. Unlike the von Neumann model, neural network computing does not separate memory and processing.\\nNeural network theory has served both to better identify how the neurons in the brain function and to provide the basis for efforts to create artificial intelligence.\\n\\n\\n== History ==\\nThe preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.\\n\\nFor Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\\'s theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs.\\nJames\\'s theory was similar to Bain\\'s, however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.\\nC. S. Sherrington (1898) conducted experiments to test James\\'s theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of habituation. \\nMcCulloch and Pitts  (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.\\nIn the late 1940s psychologist Donald Hebb  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a \\'typical\\' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing\\'s B-type machines.\\nFarley and Clark (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956).\\nRosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Werbos (1975).\\nNeural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key in later advances was the backpropagation algorithm which effectively solved the exclusive-or problem (Werbos 1975).The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.\\nNeural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of neural processing in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function.\\n\\n\\n== Artificial intelligence ==\\n\\nA neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.\\nIn more practical terms neural networks are non-linear statistical data modeling or decision making tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.\\nAn artificial neural network involves a network of simple processing elements (artificial neurons) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, who first collaborated at the University of Chicago.One classical type of artificial neural network is the recurrent Hopfield network.\\nThe concept of a neural network appears to have first been proposed by Alan Turing in his 1948 paper Intelligent Machinery in which called them \"B-type unorganised machines\".The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the Boltzmann machine (1983), and more recently, deep learning algorithms, which can implicitly learn the distribution function of the observed data. Learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical.\\n\\n\\n== Applications ==\\nNeural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:\\n\\nFunction approximation, or regression analysis, including time series prediction and modeling.\\nClassification, including pattern and sequence recognition, novelty detection and sequential decision making.\\nData processing, including filtering, clustering, blind signal separation and compression.Application areas of ANNs include nonlinear system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, data mining (or knowledge discovery in databases, \"KDD\"), visualization and e-mail spam filtering. For example, it is possible to create a semantic profile of user\\'s interests emerging from pictures trained for object recognition.\\n\\n\\n== Neuroscience ==\\nTheoretical and computational neuroscience is the field concerned with the theoretical analysis and computational modeling of biological neural systems.\\nSince neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.\\nThe aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).\\n\\n\\n=== Types of models ===\\nMany models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of individual neurons, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.\\n\\n\\n== Criticism ==\\nA common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper \"Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving,\" uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.\\nA. K. Dewdney, a former Scientific American columnist, wrote in 1997, \"Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool\" (Dewdney, p. 82).\\nArguments for Dewdney\\'s position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections—which can consume vast amounts of computer memory and hard disk space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons—which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money).\\nArguments against Dewdney\\'s position are that neural nets have been successfully used to solve many complex and diverse tasks, such as autonomously flying aircraft.Technology writer Roger Bridgman commented on Dewdney\\'s statements about neural nets: \\n\\nNeural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn\\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\\nIn spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.\\n\\nAlthough it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).\\n\\n\\n== Recent improvements ==\\nWhile initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of neuromodulators such as dopamine, acetylcholine, and serotonin on behaviour and learning.\\nBiophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data.\\nComputational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful, these efforts could usher in a new era of neural computing that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.\\nBetween 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.\\nVariants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto can be used to train deep, highly nonlinear neural architectures, similar to the 1980 Neocognitron by Kunihiko Fukushima, and the \"standard architecture of vision\", inspired by the simple and complex cells identified by David H. Hubel and Torsten Wiesel in the primary visual cortex.\\nRadial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.Deep learning feedforward networks alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge. Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun and colleagues at NYU.\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nA Brief Introduction to Neural Networks (D. Kriesel) - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.\\nReview of Neural Networks in Materials Science\\nArtificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)\\nAnother introduction to ANN\\nNext Generation of Neural Networks - Google Tech Talks\\nPerformance of Neural Networks\\nNeural Networks and Information\\nSanderson, Grant (October 5, 2017). \"But what is a Neural Network?\". 3Blue1Brown – via YouTube.'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<div class=\"mw-parser-output\"><div role=\"note\" class=\"hatnote navigation-not-searchable\">For other uses, see <a href=\"/wiki/Neural_network_(disambiguation)\" class=\"mw-disambig\" title=\"Neural network (disambiguation)\">Neural network (disambiguation)</a>.</div>\\n<div class=\"shortdescription nomobile noexcerpt noprint searchaux\" style=\"display:none\">Structure in biology and artificial intelligence</div>\\n<p class=\"mw-empty-elt\">\\n</p>\\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:Neural_network_example.svg\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/220px-Neural_network_example.svg.png\" decoding=\"async\" width=\"220\" height=\"293\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/330px-Neural_network_example.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/9/99/Neural_network_example.svg/440px-Neural_network_example.svg.png 2x\" data-file-width=\"330\" data-file-height=\"440\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Neural_network_example.svg\" class=\"internal\" title=\"Enlarge\"></a></div>Simplified view of a <a href=\"/wiki/Feedforward_neural_network\" title=\"Feedforward neural network\">feedforward</a> artificial neural network</div></div></div>\\n<p>A <b>neural network</b> is a network or circuit of <a href=\"/wiki/Neuron\" title=\"Neuron\">neurons</a>, or in a modern sense, an <a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">artificial neural network</a>, composed of <a href=\"/wiki/Artificial_neuron\" title=\"Artificial neuron\">artificial neurons</a> or nodes.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup> Thus a neural network is either a <a href=\"/wiki/Biological_neural_network\" class=\"mw-redirect\" title=\"Biological neural network\">biological neural network</a>, made up of real biological neurons, or an artificial neural network, for solving <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> (AI) problems. The connections of the biological neuron are modeled as weights. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred as a linear combination. Finally, an activation function controls the <a href=\"/wiki/Amplitude\" title=\"Amplitude\">amplitude</a> of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.\\n</p><p>These artificial networks may be used for <a href=\"/wiki/Predictive_modeling\" class=\"mw-redirect\" title=\"Predictive modeling\">predictive modeling</a>, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup>\\n</p>\\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\\n<ul>\\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Overview\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Overview</span></a></li>\\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#History\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">History</span></a></li>\\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Artificial_intelligence\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Artificial intelligence</span></a></li>\\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Applications\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Applications</span></a></li>\\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Neuroscience\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Neuroscience</span></a>\\n<ul>\\n<li class=\"toclevel-2 tocsection-6\"><a href=\"#Types_of_models\"><span class=\"tocnumber\">5.1</span> <span class=\"toctext\">Types of models</span></a></li>\\n</ul>\\n</li>\\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#Criticism\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Criticism</span></a></li>\\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#Recent_improvements\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Recent improvements</span></a></li>\\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#See_also\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">See also</span></a></li>\\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#References\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">References</span></a></li>\\n<li class=\"toclevel-1 tocsection-11\"><a href=\"#External_links\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">External links</span></a></li>\\n</ul>\\n</div>\\n\\n<h2><span class=\"mw-headline\" id=\"Overview\">Overview</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=1\" title=\"Edit section: Overview\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>A <a href=\"/wiki/Biological_neural_network\" class=\"mw-redirect\" title=\"Biological neural network\">biological neural network</a> is composed of a group or groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called <a href=\"/wiki/Synapse\" title=\"Synapse\">synapses</a>, are usually formed from <a href=\"/wiki/Axon\" title=\"Axon\">axons</a> to <a href=\"/wiki/Dendrite\" title=\"Dendrite\">dendrites</a>, though <a href=\"/wiki/Dendrodendritic_synapse\" title=\"Dendrodendritic synapse\">dendrodendritic synapses</a><sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup> and other connections are possible. Apart from the electrical signaling, there are other forms of signaling that arise from <a href=\"/wiki/Neurotransmitter\" title=\"Neurotransmitter\">neurotransmitter</a> diffusion.\\n</p><p>Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">Artificial intelligence</a> and <a href=\"/wiki/Cognitive_modeling\" class=\"mw-redirect\" title=\"Cognitive modeling\">cognitive modeling</a> try to simulate some properties of biological neural networks. In the <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> field, artificial neural networks have been applied successfully to <a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">speech recognition</a>, <a href=\"/wiki/Image_analysis\" title=\"Image analysis\">image analysis</a> and <a href=\"/wiki/Adaptive_control\" title=\"Adaptive control\">adaptive control</a>, in order to construct <a href=\"/wiki/Software_agents\" class=\"mw-redirect\" title=\"Software agents\">software agents</a> (in <a href=\"/wiki/Video_game\" title=\"Video game\">computer and video games</a>) or <a href=\"/wiki/Autonomous_robot\" title=\"Autonomous robot\">autonomous robots</a>.\\n</p><p>Historically, digital computers evolved from the <a href=\"/wiki/Von_Neumann_model\" class=\"mw-redirect\" title=\"Von Neumann model\">von Neumann model</a>, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. Unlike the von Neumann model, neural network computing does not separate memory and processing.\\n</p><p>Neural network theory has served both to better identify how the neurons in the brain function and to provide the basis for efforts to create artificial intelligence.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"History\">History</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=2\" title=\"Edit section: History\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>The preliminary theoretical base for contemporary neural networks was independently proposed by <a href=\"/wiki/Alexander_Bain\" title=\"Alexander Bain\">Alexander Bain</a><sup id=\"cite_ref-Bain_1873_4-0\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> (1873) and <a href=\"/wiki/William_James\" title=\"William James\">William James</a><sup id=\"cite_ref-James_1890_5-0\" class=\"reference\"><a href=\"#cite_note-James_1890-5\">&#91;5&#93;</a></sup> (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.\\n</p>\\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/be/Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png/220px-Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\" decoding=\"async\" width=\"220\" height=\"220\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/be/Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png/330px-Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/be/Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png/440px-Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png 2x\" data-file-width=\"650\" data-file-height=\"650\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Forest_of_synthetic_pyramidal_dendrites_grown_using_Cajal%27s_laws_of_neuronal_branching.png\" class=\"internal\" title=\"Enlarge\"></a></div><a href=\"/wiki/Computer_simulation\" title=\"Computer simulation\">Computer simulation</a> of the branching architecture of the <a href=\"/wiki/Dendrite\" title=\"Dendrite\">dendrites</a> of <a href=\"/wiki/Pyramidal_neuron\" class=\"mw-redirect\" title=\"Pyramidal neuron\">pyramidal neurons</a>.<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup></div></div></div>\\n<p>For Bain,<sup id=\"cite_ref-Bain_1873_4-1\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain\\'s<sup id=\"cite_ref-Bain_1873_4-2\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs.\\n</p><p>James\\'s<sup id=\"cite_ref-James_1890_5-1\" class=\"reference\"><a href=\"#cite_note-James_1890-5\">&#91;5&#93;</a></sup> theory was similar to Bain\\'s,<sup id=\"cite_ref-Bain_1873_4-3\" class=\"reference\"><a href=\"#cite_note-Bain_1873-4\">&#91;4&#93;</a></sup> however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.\\n</p><p><a href=\"/wiki/Charles_Scott_Sherrington\" title=\"Charles Scott Sherrington\">C. S. Sherrington</a><sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup> (1898) conducted experiments to test James\\'s theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of <a href=\"/wiki/Habituation\" title=\"Habituation\">habituation</a>. \\n</p><p><a href=\"/wiki/Warren_McCulloch\" class=\"mw-redirect\" title=\"Warren McCulloch\">McCulloch</a> and <a href=\"/wiki/Walter_Pitts\" title=\"Walter Pitts\">Pitts</a><sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup>  (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model <a href=\"/w/index.php?title=Threshold_logic&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Threshold logic (page does not exist)\">threshold logic</a>. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.\\n</p><p>In the late 1940s psychologist <a href=\"/wiki/Donald_Hebb\" class=\"mw-redirect\" title=\"Donald Hebb\">Donald Hebb</a><sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup>  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as <a href=\"/wiki/Hebbian_learning\" class=\"mw-redirect\" title=\"Hebbian learning\">Hebbian learning</a>. Hebbian learning is considered to be a \\'typical\\' <a href=\"/wiki/Unsupervised_learning\" title=\"Unsupervised learning\">unsupervised learning</a> rule and its later variants were early models for <a href=\"/wiki/Long_term_potentiation\" class=\"mw-redirect\" title=\"Long term potentiation\">long term potentiation</a>. These ideas started being applied to computational models in 1948 with <a href=\"/wiki/Unorganized_machine\" title=\"Unorganized machine\">Turing\\'s B-type machines</a>.\\n</p><p>Farley and Clark<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;10&#93;</a></sup> (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda<sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;11&#93;</a></sup> (1956).\\n</p><p><a href=\"/wiki/Frank_Rosenblatt\" title=\"Frank Rosenblatt\">Rosenblatt</a><sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup> (1958) created the <a href=\"/wiki/Perceptron\" title=\"Perceptron\">perceptron</a>, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the <a href=\"/wiki/Exclusive-or\" class=\"mw-redirect\" title=\"Exclusive-or\">exclusive-or</a> circuit, a circuit whose mathematical computation could not be processed until after the <a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">backpropagation</a> algorithm was created by Werbos<sup id=\"cite_ref-Werbos_1975_13-0\" class=\"reference\"><a href=\"#cite_note-Werbos_1975-13\">&#91;13&#93;</a></sup> (1975).\\n</p><p>Neural network research stagnated after the publication of machine learning research by <a href=\"/wiki/Marvin_Minsky\" title=\"Marvin Minsky\">Marvin Minsky</a> and <a href=\"/wiki/Seymour_Papert\" title=\"Seymour Papert\">Seymour Papert</a><sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;14&#93;</a></sup> (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key in later advances was the <a href=\"/wiki/Backpropagation\" title=\"Backpropagation\">backpropagation</a> algorithm which effectively solved the exclusive-or problem (Werbos 1975).<sup id=\"cite_ref-Werbos_1975_13-1\" class=\"reference\"><a href=\"#cite_note-Werbos_1975-13\">&#91;13&#93;</a></sup>\\n</p><p>The <a href=\"/wiki/Connectionism\" title=\"Connectionism\">parallel distributed processing</a> of the mid-1980s became popular under the name <a href=\"/wiki/Connectionism\" title=\"Connectionism\">connectionism</a>. The text by Rumelhart and McClelland<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup> (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.\\n</p><p>Neural networks, as used in artificial intelligence, have traditionally been viewed as simplified models of <a href=\"/w/index.php?title=Neural_processing&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Neural processing (page does not exist)\">neural processing</a> in the brain, even though the relation between this model and brain biological architecture is debated, as it is not clear to what degree artificial neural networks mirror brain function.<sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\">&#91;16&#93;</a></sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Artificial_intelligence\">Artificial intelligence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=3\" title=\"Edit section: Artificial intelligence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">Artificial neural network</a></div>\\n<p>A <i>neural network</i> (NN), in the case of artificial neurons called <i>artificial neural network</i> (ANN) or <i>simulated neural network</i> (SNN), is an interconnected group of natural or <a href=\"/wiki/Artificial_neuron\" title=\"Artificial neuron\">artificial neurons</a> that uses a <a href=\"/wiki/Mathematical_model\" title=\"Mathematical model\">mathematical or computational model</a> for <a href=\"/wiki/Information_processing\" title=\"Information processing\">information processing</a> based on a <a href=\"/wiki/Connectionism\" title=\"Connectionism\">connectionistic</a> approach to <a href=\"/wiki/Computation\" title=\"Computation\">computation</a>. In most cases an ANN is an <a href=\"/wiki/Adaptive_system\" title=\"Adaptive system\">adaptive system</a> that changes its structure based on external or internal information that flows through the network.\\n</p><p>In more practical terms neural networks are <a href=\"/wiki/Non-linear\" class=\"mw-redirect\" title=\"Non-linear\">non-linear</a> <a href=\"/wiki/Statistical\" class=\"mw-redirect\" title=\"Statistical\">statistical</a> <a href=\"/wiki/Data_modeling\" title=\"Data modeling\">data modeling</a> or <a href=\"/wiki/Decision_making\" class=\"mw-redirect\" title=\"Decision making\">decision making</a> tools. They can be used to model complex relationships between inputs and outputs or to <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">find patterns</a> in data.\\n</p><p>An <a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">artificial neural network</a> involves a network of simple processing elements (<a href=\"/wiki/Artificial_neuron\" title=\"Artificial neuron\">artificial neurons</a>) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by <a href=\"/wiki/Warren_Sturgis_McCulloch\" title=\"Warren Sturgis McCulloch\">Warren McCulloch</a>, a neurophysiologist, and <a href=\"/wiki/Walter_Pitts\" title=\"Walter Pitts\">Walter Pitts</a>, a logician, who first collaborated at the <a href=\"/wiki/University_of_Chicago\" title=\"University of Chicago\">University of Chicago</a>.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup>\\n</p><p>One classical type of artificial neural network is the <a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">recurrent</a> <a href=\"/wiki/Hopfield_network\" title=\"Hopfield network\">Hopfield network</a>.\\n</p><p>The concept of a neural network appears to have first been proposed by <a href=\"/wiki/Alan_Turing\" title=\"Alan Turing\">Alan Turing</a> in his 1948 paper <i>Intelligent Machinery</i> in which called them \"B-type unorganised machines\".<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup>\\n</p><p>The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the <a href=\"/wiki/Boltzmann_machine\" title=\"Boltzmann machine\">Boltzmann machine</a> (1983), and more recently, <a href=\"/wiki/Deep_learning\" title=\"Deep learning\">deep learning</a> algorithms, which can implicitly learn the distribution function of the observed data. Learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Applications\">Applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=4\" title=\"Edit section: Applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>Neural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:\\n</p>\\n<ul><li><a href=\"/wiki/Function_approximation\" title=\"Function approximation\">Function approximation</a>, or <a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">regression analysis</a>, including <a href=\"/wiki/Time_series_prediction\" class=\"mw-redirect\" title=\"Time series prediction\">time series prediction</a> and modeling.</li>\\n<li><a href=\"/wiki/Statistical_classification\" title=\"Statistical classification\">Classification</a>, including <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern</a> and sequence recognition, <a href=\"/wiki/Novelty_detection\" title=\"Novelty detection\">novelty detection</a> and sequential decision making.</li>\\n<li><a href=\"/wiki/Data_processing\" title=\"Data processing\">Data processing</a>, including filtering, clustering, <a href=\"/wiki/Blind_signal_separation\" class=\"mw-redirect\" title=\"Blind signal separation\">blind signal separation</a> and <a href=\"/wiki/Data_compression\" title=\"Data compression\">compression</a>.</li></ul>\\n<p>Application areas of ANNs include <a href=\"/wiki/Nonlinear_system_identification\" title=\"Nonlinear system identification\">nonlinear system identification</a><sup id=\"cite_ref-SAB1_19-0\" class=\"reference\"><a href=\"#cite_note-SAB1-19\">&#91;19&#93;</a></sup> and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, <a href=\"/wiki/Data_mining\" title=\"Data mining\">data mining</a> (or knowledge discovery in databases, \"KDD\"), visualization and <a href=\"/wiki/E-mail_spam\" class=\"mw-redirect\" title=\"E-mail spam\">e-mail spam</a> filtering. For example, it is possible to create a semantic profile of user\\'s interests emerging from pictures trained for object recognition.<sup id=\"cite_ref-20\" class=\"reference\"><a href=\"#cite_note-20\">&#91;20&#93;</a></sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Neuroscience\">Neuroscience</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=5\" title=\"Edit section: Neuroscience\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>Theoretical and <a href=\"/wiki/Computational_neuroscience\" title=\"Computational neuroscience\">computational neuroscience</a> is the field concerned with the theoretical analysis and computational modeling of biological neural systems.\\nSince neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.\\n</p><p>The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (<a href=\"/wiki/Biological_neural_network\" class=\"mw-redirect\" title=\"Biological neural network\">biological neural network</a> models) and theory (statistical learning theory and <a href=\"/wiki/Information_theory\" title=\"Information theory\">information theory</a>).\\n</p>\\n<h3><span class=\"mw-headline\" id=\"Types_of_models\">Types of models</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=6\" title=\"Edit section: Types of models\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\\n<p>Many models are used; defined at different levels of abstraction, and modeling different aspects of neural systems. They range from models of the short-term behaviour of <a href=\"/wiki/Biological_neuron_models\" class=\"mw-redirect\" title=\"Biological neuron models\">individual neurons</a>, through models of the dynamics of neural circuitry arising from interactions between individual neurons, to models of behaviour arising from abstract neural modules that represent complete subsystems. These include models of the long-term and short-term plasticity of neural systems and its relation to learning and memory, from the individual neuron to the system level.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Criticism\">Criticism</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=7\" title=\"Edit section: Criticism\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper \"Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving,\" uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.\\n</p><p><a href=\"/wiki/A._K._Dewdney\" class=\"mw-redirect\" title=\"A. K. Dewdney\">A. K. Dewdney</a>, a former <i><a href=\"/wiki/Scientific_American\" title=\"Scientific American\">Scientific American</a></i> columnist, wrote in 1997, \"Although neural nets do solve a few toy problems, their powers of computation are so limited that I am surprised anyone takes them seriously as a general problem-solving tool\" (Dewdney, p.&#160;82).\\n</p><p>Arguments for Dewdney\\'s position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of <a href=\"/wiki/Database\" title=\"Database\">database</a> rows for its connections—which can consume vast amounts of computer <a href=\"/wiki/Random-access_memory\" title=\"Random-access memory\">memory</a> and <a href=\"/wiki/Hard_drive\" class=\"mw-redirect\" title=\"Hard drive\">hard disk</a> space. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons—which must often be matched with incredible amounts of <a href=\"/wiki/CPU\" class=\"mw-redirect\" title=\"CPU\">CPU</a> processing power and time. While neural networks often yield <i>effective</i> programs, they too often do so at the cost of <i>efficiency</i> (they tend to consume considerable amounts of time and money).\\n</p><p>Arguments against Dewdney\\'s position are that neural nets have been successfully used to solve many complex and diverse tasks, such as autonomously flying aircraft.<sup id=\"cite_ref-21\" class=\"reference\"><a href=\"#cite_note-21\">&#91;21&#93;</a></sup>\\n</p><p>Technology writer <a href=\"/w/index.php?title=Roger_Bridgman&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Roger Bridgman (page does not exist)\">Roger Bridgman</a> commented on Dewdney\\'s statements about neural nets: \\n</p>\\n<blockquote><p>Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn\\'t?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\\n</p><p>In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.<sup id=\"cite_ref-22\" class=\"reference\"><a href=\"#cite_note-22\">&#91;22&#93;</a></sup>\\n</p>\\n</blockquote>\\n<p>Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles which allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.<sup id=\"cite_ref-23\" class=\"reference\"><a href=\"#cite_note-23\">&#91;23&#93;</a></sup>\\n</p><p>Some other criticisms came from believers of hybrid models (combining neural networks and <a href=\"/wiki/Symbolic_artificial_intelligence\" title=\"Symbolic artificial intelligence\">symbolic</a> approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citing_sources#What_information_to_include\" title=\"Wikipedia:Citing sources\"><span title=\"A complete citation is needed (April 2019)\">full citation needed</span></a></i>&#93;</sup>\\n</p>\\n<h2><span class=\"mw-headline\" id=\"Recent_improvements\">Recent improvements</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=8\" title=\"Edit section: Recent improvements\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<p>While initially research had been concerned mostly with the electrical characteristics of neurons, a particularly important part of the investigation in recent years has been the exploration of the role of <a href=\"/wiki/Neuromodulators\" class=\"mw-redirect\" title=\"Neuromodulators\">neuromodulators</a> such as <a href=\"/wiki/Dopamine\" title=\"Dopamine\">dopamine</a>, <a href=\"/wiki/Acetylcholine\" title=\"Acetylcholine\">acetylcholine</a>, and <a href=\"/wiki/Serotonin\" title=\"Serotonin\">serotonin</a> on behaviour and learning.\\n</p><p><a href=\"/wiki/Biophysics\" title=\"Biophysics\">Biophysical</a> models, such as <a href=\"/wiki/BCM_theory\" title=\"BCM theory\">BCM theory</a>, have been important in understanding mechanisms for <a href=\"/wiki/Synaptic_plasticity\" title=\"Synaptic plasticity\">synaptic plasticity</a>, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for <a href=\"/wiki/Radial_basis_networks\" class=\"mw-redirect\" title=\"Radial basis networks\">radial basis networks</a> and <a href=\"/wiki/Neural_backpropagation\" title=\"Neural backpropagation\">neural backpropagation</a> as mechanisms for processing data.\\n</p><p>Computational devices have been created in CMOS for both biophysical simulation and <a href=\"/wiki/Neuromorphic_computing\" class=\"mw-redirect\" title=\"Neuromorphic computing\">neuromorphic computing</a>. More recent efforts show promise for creating <a href=\"/wiki/Nanodevice\" class=\"mw-redirect\" title=\"Nanodevice\">nanodevices</a> for very large scale <a href=\"/wiki/Principal_component\" class=\"mw-redirect\" title=\"Principal component\">principal components</a> analyses and <a href=\"/wiki/Convolution\" title=\"Convolution\">convolution</a>.<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup> If successful, these efforts could usher in a new era of <a href=\"/wiki/Neural_computing\" class=\"mw-redirect\" title=\"Neural computing\">neural computing</a> that is a step beyond digital computing,<sup id=\"cite_ref-25\" class=\"reference\"><a href=\"#cite_note-25\">&#91;25&#93;</a></sup> because it depends on <a href=\"/wiki/Learning\" title=\"Learning\">learning</a> rather than <a href=\"/wiki/Programming_language\" title=\"Programming language\">programming</a> and because it is fundamentally <a href=\"/wiki/Analog_signal\" title=\"Analog signal\">analog</a> rather than <a href=\"/wiki/Digital_data\" title=\"Digital data\">digital</a> even though the first instantiations may in fact be with CMOS digital devices.\\n</p><p>Between 2009 and 2012, the <a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">recurrent neural networks</a> and deep <a href=\"/wiki/Feedforward_neural_network\" title=\"Feedforward neural network\">feedforward neural networks</a> developed in the research group of <a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">Jürgen Schmidhuber</a> at the <a href=\"/wiki/IDSIA\" class=\"mw-redirect\" title=\"IDSIA\">Swiss AI Lab IDSIA</a> have won eight international competitions in <a href=\"/wiki/Pattern_recognition\" title=\"Pattern recognition\">pattern recognition</a> and <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a>.<sup id=\"cite_ref-26\" class=\"reference\"><a href=\"#cite_note-26\">&#91;26&#93;</a></sup> For example, multi-dimensional <a href=\"/wiki/Long_short_term_memory\" class=\"mw-redirect\" title=\"Long short term memory\">long short term memory</a> (LSTM)<sup id=\"cite_ref-27\" class=\"reference\"><a href=\"#cite_note-27\">&#91;27&#93;</a></sup><sup id=\"cite_ref-28\" class=\"reference\"><a href=\"#cite_note-28\">&#91;28&#93;</a></sup> won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.\\n</p><p>Variants of the <a href=\"/wiki/Back-propagation\" class=\"mw-redirect\" title=\"Back-propagation\">back-propagation</a> algorithm as well as unsupervised methods by <a href=\"/wiki/Geoff_Hinton\" class=\"mw-redirect\" title=\"Geoff Hinton\">Geoff Hinton</a> and colleagues at the <a href=\"/wiki/University_of_Toronto\" title=\"University of Toronto\">University of Toronto</a> can be used to train deep, highly nonlinear neural architectures,<sup id=\"cite_ref-29\" class=\"reference\"><a href=\"#cite_note-29\">&#91;29&#93;</a></sup> similar to the 1980 <a href=\"/wiki/Neocognitron\" title=\"Neocognitron\">Neocognitron</a> by <a href=\"/wiki/Kunihiko_Fukushima\" title=\"Kunihiko Fukushima\">Kunihiko Fukushima</a>,<sup id=\"cite_ref-30\" class=\"reference\"><a href=\"#cite_note-30\">&#91;30&#93;</a></sup> and the \"standard architecture of vision\",<sup id=\"cite_ref-31\" class=\"reference\"><a href=\"#cite_note-31\">&#91;31&#93;</a></sup> inspired by the simple and complex cells identified by <a href=\"/wiki/David_H._Hubel\" title=\"David H. Hubel\">David H. Hubel</a> and <a href=\"/wiki/Torsten_Wiesel\" title=\"Torsten Wiesel\">Torsten Wiesel</a> in the primary <a href=\"/wiki/Visual_cortex\" title=\"Visual cortex\">visual cortex</a>.\\n</p><p>Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in <a href=\"/wiki/Nonlinear_system_identification\" title=\"Nonlinear system identification\">nonlinear system identification</a> and classification applications.<sup id=\"cite_ref-SAB1_19-1\" class=\"reference\"><a href=\"#cite_note-SAB1-19\">&#91;19&#93;</a></sup>\\n</p><p><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a> feedforward networks alternate <a href=\"/wiki/Convolution\" title=\"Convolution\">convolutional</a> layers and max-pooling layers, topped by several pure classification layers. Fast <a href=\"/wiki/GPU\" class=\"mw-redirect\" title=\"GPU\">GPU</a>-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition<sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\">&#91;32&#93;</a></sup> and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge.<sup id=\"cite_ref-33\" class=\"reference\"><a href=\"#cite_note-33\">&#91;33&#93;</a></sup> Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance<sup id=\"cite_ref-34\" class=\"reference\"><a href=\"#cite_note-34\">&#91;34&#93;</a></sup> on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of <a href=\"/wiki/Yann_LeCun\" title=\"Yann LeCun\">Yann LeCun</a> and colleagues at <a href=\"/wiki/NYU\" class=\"mw-redirect\" title=\"NYU\">NYU</a>.\\n</p>\\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=9\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div class=\"div-col columns column-width\" style=\"-moz-column-width: 22em; -webkit-column-width: 22em; column-width: 22em;\">\\n<ul><li><a href=\"/wiki/ADALINE\" title=\"ADALINE\">ADALINE</a></li>\\n<li><a href=\"/wiki/Adaptive_resonance_theory\" title=\"Adaptive resonance theory\">Adaptive resonance theory</a></li>\\n<li><a href=\"/wiki/Biological_cybernetics\" class=\"mw-redirect\" title=\"Biological cybernetics\">Biological cybernetics</a></li>\\n<li><a href=\"/wiki/Biologically_inspired_computing\" class=\"mw-redirect\" title=\"Biologically inspired computing\">Biologically inspired computing</a></li>\\n<li><a href=\"/wiki/Cerebellar_model_articulation_controller\" title=\"Cerebellar model articulation controller\">Cerebellar model articulation controller</a></li>\\n<li><a href=\"/wiki/Cognitive_architecture\" title=\"Cognitive architecture\">Cognitive architecture</a></li>\\n<li><a href=\"/wiki/Cognitive_science\" title=\"Cognitive science\">Cognitive science</a></li>\\n<li><a href=\"/wiki/Connectomics\" title=\"Connectomics\">Connectomics</a></li>\\n<li><a href=\"/wiki/Cultured_neuronal_networks\" class=\"mw-redirect\" title=\"Cultured neuronal networks\">Cultured neuronal networks</a></li>\\n<li><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></li>\\n<li><a href=\"/wiki/Digital_morphogenesis\" title=\"Digital morphogenesis\">Digital morphogenesis</a></li>\\n<li><a href=\"/wiki/Exclusive_or\" title=\"Exclusive or\">Exclusive or</a></li>\\n<li><a href=\"/wiki/Gene_expression_programming\" title=\"Gene expression programming\">Gene expression programming</a></li>\\n<li><a href=\"/wiki/Group_method_of_data_handling\" title=\"Group method of data handling\">Group method of data handling</a></li>\\n<li><a href=\"/wiki/Habituation\" title=\"Habituation\">Habituation</a></li>\\n<li><a href=\"/wiki/In_situ_adaptive_tabulation\" title=\"In situ adaptive tabulation\">In situ adaptive tabulation</a></li>\\n<li><a href=\"/wiki/Memristor\" title=\"Memristor\">Memristor</a></li>\\n<li><a href=\"/wiki/Multilinear_subspace_learning\" title=\"Multilinear subspace learning\">Multilinear subspace learning</a></li>\\n<li><a href=\"/wiki/Neural_network_software\" title=\"Neural network software\">Neural network software</a></li>\\n<li><a href=\"/wiki/Nonlinear_system_identification\" title=\"Nonlinear system identification\">Nonlinear system identification</a></li>\\n<li><a href=\"/wiki/Parallel_constraint_satisfaction_processes\" title=\"Parallel constraint satisfaction processes\">Parallel constraint satisfaction processes</a></li>\\n<li><a href=\"/wiki/Parallel_distributed_processing\" class=\"mw-redirect\" title=\"Parallel distributed processing\">Parallel distributed processing</a></li>\\n<li><a href=\"/wiki/Predictive_analytics\" title=\"Predictive analytics\">Predictive analytics</a></li>\\n<li><a href=\"/wiki/Radial_basis_function_network\" title=\"Radial basis function network\">Radial basis function network</a></li>\\n<li><a href=\"/wiki/Self-organizing_map\" title=\"Self-organizing map\">Self-organizing map</a></li>\\n<li><a href=\"/wiki/Simulated_reality\" title=\"Simulated reality\">Simulated reality</a></li>\\n<li><a href=\"/wiki/Support_vector_machine\" class=\"mw-redirect\" title=\"Support vector machine\">Support vector machine</a></li>\\n<li><a href=\"/wiki/Tensor_product_network\" title=\"Tensor product network\">Tensor product network</a></li>\\n<li><a href=\"/wiki/Time_delay_neural_network\" title=\"Time delay neural network\">Time delay neural network</a></li></ul>\\n</div>\\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=10\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\\n<ol class=\"references\">\\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hopfield, J. J. (1982). <a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC346238\">\"Neural networks and physical systems with emergent collective computational abilities\"</a>. <i>Proc. Natl. Acad. Sci. U.S.A</i>. <b>79</b> (8): 2554–2558. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/1982PNAS...79.2554H\">1982PNAS...79.2554H</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1073%2Fpnas.79.8.2554\">10.1073/pnas.79.8.2554</a>. <a href=\"/wiki/PubMed_Central\" title=\"PubMed Central\">PMC</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC346238\">346238</a></span>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/6953413\">6953413</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proc.+Natl.+Acad.+Sci.+U.S.A.&amp;rft.atitle=Neural+networks+and+physical+systems+with+emergent+collective+computational+abilities&amp;rft.volume=79&amp;rft.issue=8&amp;rft.pages=2554-2558&amp;rft.date=1982&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC346238&amp;rft_id=info%3Apmid%2F6953413&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.79.8.2554&amp;rft_id=info%3Abibcode%2F1982PNAS...79.2554H&amp;rft.aulast=Hopfield&amp;rft.aufirst=J.+J.&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC346238&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><style data-mw-deduplicate=\"TemplateStyles:r935243608\">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .id-lock-free a,.mw-parser-output .citation .cs1-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-limited a,.mw-parser-output .id-lock-registration a,.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .id-lock-subscription a,.mw-parser-output .citation .cs1-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png\")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>\\n</li>\\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.gartner.com/it-glossary/neural-net-or-neural-network\">\"Neural Net or Neural Network - Gartner IT Glossary\"</a>. <i>www.gartner.com</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.gartner.com&amp;rft.atitle=Neural+Net+or+Neural+Network+-+Gartner+IT+Glossary&amp;rft_id=https%3A%2F%2Fwww.gartner.com%2Fit-glossary%2Fneural-net-or-neural-network&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\">Arbib, p.666</span>\\n</li>\\n<li id=\"cite_note-Bain_1873-4\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Bain_1873_4-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Bain_1873_4-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Bain_1873_4-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Bain_1873_4-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Bain (1873). <i>Mind and Body: The Theories of Their Relation</i>. New York: D. Appleton and Company.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mind+and+Body%3A+The+Theories+of+Their+Relation&amp;rft.place=New+York&amp;rft.pub=D.+Appleton+and+Company&amp;rft.date=1873&amp;rft.au=Bain&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-James_1890-5\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-James_1890_5-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-James_1890_5-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">James (1890). <a rel=\"nofollow\" class=\"external text\" href=\"https://archive.org/details/principlespsych01jamegoog\"><i>The Principles of Psychology</i></a>. New York: H. Holt and Company.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Principles+of+Psychology&amp;rft.place=New+York&amp;rft.pub=H.+Holt+and+Company&amp;rft.date=1890&amp;rft.au=James&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fprinciplespsych01jamegoog&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Cuntz, Hermann (2010). \"PLoS Computational Biology Issue Image &#124; Vol. 6(8) August 2010\". <i>PLoS Computational Biology</i>. <b>6</b> (8): ev06.i08. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1371%2Fimage.pcbi.v06.i08\">10.1371/image.pcbi.v06.i08</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PLoS+Computational+Biology&amp;rft.atitle=PLoS+Computational+Biology+Issue+Image+%26%23124%3B+Vol.+6%288%29+August+2010&amp;rft.volume=6&amp;rft.issue=8&amp;rft.pages=ev06.i08&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1371%2Fimage.pcbi.v06.i08&amp;rft.aulast=Cuntz&amp;rft.aufirst=Hermann&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Sherrington, C.S. (1898). \"Experiments in Examination of the Peripheral Distribution of the Fibers of the Posterior Roots of Some Spinal Nerves\". <i>Proceedings of the Royal Society of London</i>. <b>190</b>: 45–186. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1098%2Frstb.1898.0002\">10.1098/rstb.1898.0002</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Royal+Society+of+London&amp;rft.atitle=Experiments+in+Examination+of+the+Peripheral+Distribution+of+the+Fibers+of+the+Posterior+Roots+of+Some+Spinal+Nerves&amp;rft.volume=190&amp;rft.pages=45-186&amp;rft.date=1898&amp;rft_id=info%3Adoi%2F10.1098%2Frstb.1898.0002&amp;rft.aulast=Sherrington&amp;rft.aufirst=C.S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">McCulloch, Warren; Walter Pitts (1943). \"A Logical Calculus of Ideas Immanent in Nervous Activity\". <i>Bulletin of Mathematical Biophysics</i>. <b>5</b> (4): 115–133. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2FBF02478259\">10.1007/BF02478259</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+Mathematical+Biophysics&amp;rft.atitle=A+Logical+Calculus+of+Ideas+Immanent+in+Nervous+Activity&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=115-133&amp;rft.date=1943&amp;rft_id=info%3Adoi%2F10.1007%2FBF02478259&amp;rft.aulast=McCulloch&amp;rft.aufirst=Warren&amp;rft.au=Walter+Pitts&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Hebb, Donald (1949). <a rel=\"nofollow\" class=\"external text\" href=\"https://archive.org/details/in.ernet.dli.2015.226341\"><i>The Organization of Behavior</i></a>. New York: Wiley.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Organization+of+Behavior&amp;rft.place=New+York&amp;rft.pub=Wiley&amp;rft.date=1949&amp;rft.aulast=Hebb&amp;rft.aufirst=Donald&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fin.ernet.dli.2015.226341&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Farley, B.; W.A. Clark (1954). \"Simulation of Self-Organizing Systems by Digital Computer\". <i>IRE Transactions on Information Theory</i>. <b>4</b> (4): 76–84. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FTIT.1954.1057468\">10.1109/TIT.1954.1057468</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IRE+Transactions+on+Information+Theory&amp;rft.atitle=Simulation+of+Self-Organizing+Systems+by+Digital+Computer&amp;rft.volume=4&amp;rft.issue=4&amp;rft.pages=76-84&amp;rft.date=1954&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1954.1057468&amp;rft.aulast=Farley&amp;rft.aufirst=B.&amp;rft.au=W.A.+Clark&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Rochester, N.; J.H. Holland, L.H. Habit and W.L. Duda (1956). \"Tests on a cell assembly theory of the action of the brain, using a large digital computer\". <i>IRE Transactions on Information Theory</i>. <b>2</b> (3): 80–93. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FTIT.1956.1056810\">10.1109/TIT.1956.1056810</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IRE+Transactions+on+Information+Theory&amp;rft.atitle=Tests+on+a+cell+assembly+theory+of+the+action+of+the+brain%2C+using+a+large+digital+computer&amp;rft.volume=2&amp;rft.issue=3&amp;rft.pages=80-93&amp;rft.date=1956&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1956.1056810&amp;rft.aulast=Rochester&amp;rft.aufirst=N.&amp;rft.au=J.H.+Holland%2C+L.H.+Habit+and+W.L.+Duda&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Rosenblatt, F. (1958). \"The Perceptron: A Probalistic Model For Information Storage And Organization In The Brain\". <i>Psychological Review</i>. <b>65</b> (6): 386–408. <a href=\"/wiki/CiteSeerX\" title=\"CiteSeerX\">CiteSeerX</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.588.3775\">10.1.1.588.3775</a></span>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1037%2Fh0042519\">10.1037/h0042519</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/13602029\">13602029</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychological+Review&amp;rft.atitle=The+Perceptron%3A+A+Probalistic+Model+For+Information+Storage+And+Organization+In+The+Brain&amp;rft.volume=65&amp;rft.issue=6&amp;rft.pages=386-408&amp;rft.date=1958&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.588.3775&amp;rft_id=info%3Apmid%2F13602029&amp;rft_id=info%3Adoi%2F10.1037%2Fh0042519&amp;rft.aulast=Rosenblatt&amp;rft.aufirst=F.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-Werbos_1975-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Werbos_1975_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Werbos_1975_13-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Werbos, P.J. (1975). <i>Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Beyond+Regression%3A+New+Tools+for+Prediction+and+Analysis+in+the+Behavioral+Sciences&amp;rft.date=1975&amp;rft.aulast=Werbos&amp;rft.aufirst=P.J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Minsky, M.; S. Papert (1969). <i>An Introduction to Computational Geometry</i>. MIT Press. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-0-262-63022-1\" title=\"Special:BookSources/978-0-262-63022-1\"><bdi>978-0-262-63022-1</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=An+Introduction+to+Computational+Geometry&amp;rft.pub=MIT+Press&amp;rft.date=1969&amp;rft.isbn=978-0-262-63022-1&amp;rft.aulast=Minsky&amp;rft.aufirst=M.&amp;rft.au=S.+Papert&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Rumelhart, D.E.; James McClelland (1986). <span class=\"cs1-lock-registration\" title=\"Free registration required\"><a rel=\"nofollow\" class=\"external text\" href=\"https://archive.org/details/paralleldistribu00rume\"><i>Parallel Distributed Processing: Explorations in the Microstructure of Cognition</i></a></span>. Cambridge: MIT Press.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Parallel+Distributed+Processing%3A+Explorations+in+the+Microstructure+of+Cognition&amp;rft.place=Cambridge&amp;rft.pub=MIT+Press&amp;rft.date=1986&amp;rft.aulast=Rumelhart&amp;rft.aufirst=D.E.&amp;rft.au=James+McClelland&amp;rft_id=https%3A%2F%2Farchive.org%2Fdetails%2Fparalleldistribu00rume&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Russell, Ingrid. <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20140529155320/http://uhaweb.hartford.edu/compsci/neural-networks-definition.html\">\"Neural Networks Module\"</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://uhaweb.hartford.edu/compsci/neural-networks-definition.html\">the original</a> on 29 May 2014<span class=\"reference-accessdate\">. Retrieved 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Neural+Networks+Module&amp;rft.aulast=Russell&amp;rft.aufirst=Ingrid&amp;rft_id=http%3A%2F%2Fuhaweb.hartford.edu%2Fcompsci%2Fneural-networks-definition.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span> <span class=\"cs1-visible-error error citation-comment\">Check date values in: <code class=\"cs1-code\">&#124;accessdate=</code> (<a href=\"/wiki/Help:CS1_errors#bad_date\" title=\"Help:CS1 errors\">help</a>)</span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">McCulloch, Warren; Pitts, Walter (1943). \"A Logical Calculus of Ideas Immanent in Nervous Activity\". <i>Bulletin of Mathematical Biophysics</i>. <b>5</b> (4): 115–133. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2FBF02478259\">10.1007/BF02478259</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Bulletin+of+Mathematical+Biophysics&amp;rft.atitle=A+Logical+Calculus+of+Ideas+Immanent+in+Nervous+Activity&amp;rft.volume=5&amp;rft.issue=4&amp;rft.pages=115-133&amp;rft.date=1943&amp;rft_id=info%3Adoi%2F10.1007%2FBF02478259&amp;rft.aulast=McCulloch&amp;rft.aufirst=Warren&amp;rft.au=Pitts%2C+Walter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Copeland, B. Jack, ed. (2004). <i>The Essential Turing</i>. Oxford University Press. p.&#160;403. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-0-19-825080-7\" title=\"Special:BookSources/978-0-19-825080-7\"><bdi>978-0-19-825080-7</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Essential+Turing&amp;rft.pages=403&amp;rft.pub=Oxford+University+Press&amp;rft.date=2004&amp;rft.isbn=978-0-19-825080-7&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-SAB1-19\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-SAB1_19-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-SAB1_19-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Billings, S. A. (2013). <i>Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains</i>. Wiley. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-1-119-94359-4\" title=\"Special:BookSources/978-1-119-94359-4\"><bdi>978-1-119-94359-4</bdi></a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Nonlinear+System+Identification%3A+NARMAX+Methods+in+the+Time%2C+Frequency%2C+and+Spatio-Temporal+Domains&amp;rft.pub=Wiley&amp;rft.date=2013&amp;rft.isbn=978-1-119-94359-4&amp;rft.aulast=Billings&amp;rft.aufirst=S.+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Wieczorek, Szymon; Filipiak, Dominik; Filipowska, Agata (2018). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/328964756\">\"Semantic Image-Based Profiling of Users\\' Interests with Neural Networks\"</a>. <i>Studies on the Semantic Web</i>. <b>36</b> (Emerging Topics in Semantic Technologies). <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.3233%2F978-1-61499-894-5-179\">10.3233/978-1-61499-894-5-179</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Studies+on+the+Semantic+Web&amp;rft.atitle=Semantic+Image-Based+Profiling+of+Users%27+Interests+with+Neural+Networks&amp;rft.volume=36&amp;rft.issue=Emerging+Topics+in+Semantic+Technologies&amp;rft.date=2018&amp;rft_id=info%3Adoi%2F10.3233%2F978-1-61499-894-5-179&amp;rft.aulast=Wieczorek&amp;rft.aufirst=Szymon&amp;rft.au=Filipiak%2C+Dominik&amp;rft.au=Filipowska%2C+Agata&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F328964756&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Administrator, NASA (June 5, 2013). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.nasa.gov/centers/dryden/news/NewsReleases/2003/03-49.html\">\"Dryden Flight Research Center - News Room: News Releases: NASA NEURAL NETWORK PROJECT PASSES MILESTONE\"</a>. <i>NASA</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=NASA&amp;rft.atitle=Dryden+Flight+Research+Center+-+News+Room%3A+News+Releases%3A+NASA+NEURAL+NETWORK+PROJECT+PASSES+MILESTONE&amp;rft.date=2013-06-05&amp;rft.aulast=Administrator&amp;rft.aufirst=NASA&amp;rft_id=http%3A%2F%2Fwww.nasa.gov%2Fcenters%2Fdryden%2Fnews%2FNewsReleases%2F2003%2F03-49.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://members.fortunecity.com/templarseries/popper.html\">Roger Bridgman\\'s defence of neural networks</a></span>\\n</li>\\n<li id=\"cite_note-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-23\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.iro.umontreal.ca/~lisa/publications2/index.php/publications/show/4\">\"Scaling Learning Algorithms towards {AI} - LISA - Publications - Aigaion 2.0\"</a>. <i>www.iro.umontreal.ca</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.iro.umontreal.ca&amp;rft.atitle=Scaling+Learning+Algorithms+towards+%7BAI%7D+-+LISA+-+Publications+-+Aigaion+2.0&amp;rft_id=http%3A%2F%2Fwww.iro.umontreal.ca%2F~lisa%2Fpublications2%2Findex.php%2Fpublications%2Fshow%2F4&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Yang, J. J.;  et al. (2008). \"Memristive switching mechanism for metal/oxide/metal nanodevices\". <i><a href=\"/wiki/Nature_Nanotechnology\" title=\"Nature Nanotechnology\">Nat. Nanotechnol.</a></i> <b>3</b> (7): 429–433. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1038%2Fnnano.2008.160\">10.1038/nnano.2008.160</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/18654568\">18654568</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nat.+Nanotechnol.&amp;rft.atitle=Memristive+switching+mechanism+for+metal%2Foxide%2Fmetal+nanodevices&amp;rft.volume=3&amp;rft.issue=7&amp;rft.pages=429-433&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1038%2Fnnano.2008.160&amp;rft_id=info%3Apmid%2F18654568&amp;rft.aulast=Yang&amp;rft.aufirst=J.+J.&amp;rft.au=Pickett%2C+M.+D.&amp;rft.au=Li%2C+X.+M.&amp;rft.au=Ohlberg%2C+D.+A.+A.&amp;rft.au=Stewart%2C+D.+R.&amp;rft.au=Williams%2C+R.+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Strukov, D. B.;  et al. (2008). \"The missing memristor found\". <i>Nature</i>. <b>453</b> (7191): 80–83. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://ui.adsabs.harvard.edu/abs/2008Natur.453...80S\">2008Natur.453...80S</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1038%2Fnature06932\">10.1038/nature06932</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/18451858\">18451858</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature&amp;rft.atitle=The+missing+memristor+found&amp;rft.volume=453&amp;rft.issue=7191&amp;rft.pages=80-83&amp;rft.date=2008&amp;rft_id=info%3Apmid%2F18451858&amp;rft_id=info%3Adoi%2F10.1038%2Fnature06932&amp;rft_id=info%3Abibcode%2F2008Natur.453...80S&amp;rft.aulast=Strukov&amp;rft.aufirst=D.+B.&amp;rft.au=Snider%2C+G.+S.&amp;rft.au=Stewart%2C+D.+R.&amp;rft.au=Williams%2C+R.+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.kurzweilai.net/how-bio-inspired-deep-learning-keeps-winning-competitions\">\"2012 Kurzweil AI Interview with Jürgen Schmidhuber on the eight competitions won by his Deep Learning team 2009–2012\"</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=2012+Kurzweil+AI+Interview+with+J%C3%BCrgen+Schmidhuber+on+the+eight+competitions+won+by+his+Deep+Learning+team+2009%E2%80%932012&amp;rft_id=http%3A%2F%2Fwww.kurzweilai.net%2Fhow-bio-inspired-deep-learning-keeps-winning-competitions&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Graves, Alex; Schmidhuber, Jürgen (2008). <a rel=\"nofollow\" class=\"external text\" href=\"http://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks\">\"Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks\"</a>.  In Bengio, Yoshua; Schuurmans, Dale; Lafferty, John; Williams, Chris K. I.; Culotta, Aron (eds.). <i>Advances in Neural Information Processing Systems 21 (NIPS\\'21)</i>. Neural Information Processing Systems (NIPS) Foundation. pp.&#160;545–552.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Offline+Handwriting+Recognition+with+Multidimensional+Recurrent+Neural+Networks&amp;rft.btitle=Advances+in+Neural+Information+Processing+Systems+21+%28NIPS%2721%29&amp;rft.pages=545-552&amp;rft.pub=Neural+Information+Processing+Systems+%28NIPS%29+Foundation&amp;rft.date=2008&amp;rft.aulast=Graves&amp;rft.aufirst=Alex&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Graves, A.; Liwicki, M.; Fernandez, S.; Bertolami, R.; Bunke, H.; Schmidhuber, J. (2009). \"A Novel Connectionist System for Improved Unconstrained Handwriting Recognition\". <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>31</b> (5): 855–868. <a href=\"/wiki/CiteSeerX\" title=\"CiteSeerX\">CiteSeerX</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.139.4502\">10.1.1.139.4502</a></span>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1109%2FTPAMI.2008.137\">10.1109/TPAMI.2008.137</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/19299860\">19299860</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=A+Novel+Connectionist+System+for+Improved+Unconstrained+Handwriting+Recognition&amp;rft.volume=31&amp;rft.issue=5&amp;rft.pages=855-868&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.139.4502&amp;rft_id=info%3Apmid%2F19299860&amp;rft_id=info%3Adoi%2F10.1109%2FTPAMI.2008.137&amp;rft.aulast=Graves&amp;rft.aufirst=A.&amp;rft.au=Liwicki%2C+M.&amp;rft.au=Fernandez%2C+S.&amp;rft.au=Bertolami%2C+R.&amp;rft.au=Bunke%2C+H.&amp;rft.au=Schmidhuber%2C+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\"><a href=\"/wiki/Geoffrey_Hinton\" title=\"Geoffrey Hinton\">Hinton, G. E.</a>; Osindero, S.; Teh, Y. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.toronto.edu/~hinton/absps/fastnc.pdf\">\"A fast learning algorithm for deep belief nets\"</a> <span class=\"cs1-format\">(PDF)</span>. <i><a href=\"/wiki/Neural_Computation_(journal)\" title=\"Neural Computation (journal)\">Neural Computation</a></i>. <b>18</b> (7): 1527–1554. <a href=\"/wiki/CiteSeerX\" title=\"CiteSeerX\">CiteSeerX</a>&#160;<span class=\"cs1-lock-free\" title=\"Freely accessible\"><a rel=\"nofollow\" class=\"external text\" href=\"//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.76.1541\">10.1.1.76.1541</a></span>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1162%2Fneco.2006.18.7.1527\">10.1162/neco.2006.18.7.1527</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/16764513\">16764513</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neural+Computation&amp;rft.atitle=A+fast+learning+algorithm+for+deep+belief+nets&amp;rft.volume=18&amp;rft.issue=7&amp;rft.pages=1527-1554&amp;rft.date=2006&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.76.1541&amp;rft_id=info%3Apmid%2F16764513&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.7.1527&amp;rft.aulast=Hinton&amp;rft.aufirst=G.+E.&amp;rft.au=Osindero%2C+S.&amp;rft.au=Teh%2C+Y.&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Ffastnc.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-30\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-30\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Fukushima, K. (1980). \"Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position\". <i>Biological Cybernetics</i>. <b>36</b> (4): 93–202. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1007%2FBF00344251\">10.1007/BF00344251</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/7370364\">7370364</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biological+Cybernetics&amp;rft.atitle=Neocognitron%3A+A+self-organizing+neural+network+model+for+a+mechanism+of+pattern+recognition+unaffected+by+shift+in+position&amp;rft.volume=36&amp;rft.issue=4&amp;rft.pages=93-202&amp;rft.date=1980&amp;rft_id=info%3Adoi%2F10.1007%2FBF00344251&amp;rft_id=info%3Apmid%2F7370364&amp;rft.aulast=Fukushima&amp;rft.aufirst=K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Riesenhuber, M.; <a href=\"/wiki/Tomaso_Poggio\" title=\"Tomaso Poggio\">Poggio, T.</a> (1999). \"Hierarchical models of object recognition in cortex\". <i><a href=\"/wiki/Nature_Neuroscience\" title=\"Nature Neuroscience\">Nature Neuroscience</a></i>. <b>2</b> (11): 1019–1025. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"https://doi.org/10.1038%2F14819\">10.1038/14819</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//pubmed.ncbi.nlm.nih.gov/10526343\">10526343</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nature+Neuroscience&amp;rft.atitle=Hierarchical+models+of+object+recognition+in+cortex&amp;rft.volume=2&amp;rft.issue=11&amp;rft.pages=1019-1025&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1038%2F14819&amp;rft_id=info%3Apmid%2F10526343&amp;rft.aulast=Riesenhuber&amp;rft.aufirst=M.&amp;rft.au=Poggio%2C+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></span>\\n</li>\\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\">D. C. Ciresan, U. Meier, J. Masci, J. Schmidhuber. <a rel=\"nofollow\" class=\"external text\" href=\"https://people.lu.usi.ch/mascij/data/papers/2012_nn_traffic.pdf\">Multi-Column Deep Neural Network for Traffic Sign Classification</a>. Neural Networks, 2012.</span>\\n</li>\\n<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\">D. Ciresan, A. Giusti, L. Gambardella, J. Schmidhuber. <a rel=\"nofollow\" class=\"external text\" href=\"https://papers.nips.cc/paper/4741-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images.pdf\">Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images</a>. In Advances in Neural Information Processing Systems (NIPS 2012), Lake Tahoe, 2012.</span>\\n</li>\\n<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\">D. C. Ciresan, U. Meier, <a href=\"/wiki/J%C3%BCrgen_Schmidhuber\" title=\"Jürgen Schmidhuber\">J. Schmidhuber</a>. Multi-column Deep Neural Networks for Image Classification. IEEE Conf. on Computer Vision and Pattern Recognition CVPR 2012.</span>\\n</li>\\n</ol></div>\\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Neural_network&amp;action=edit&amp;section=11\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\\n<div id=\"section_SpokenWikipedia\" class=\"infobox sisterproject plainlinks haudio\"><div style=\"text-align: center; white-space:nowrap\"><b>Listen to this article</b> (<a href=\"/wiki/File:En-Neural_network.ogg\" title=\"File:En-Neural network.ogg\">info/dl</a>)\\n<div class=\"center\"><div class=\"floatnone\"><div class=\"mediaContainer\" style=\"width:200px\"><audio id=\"mwe_player_0\" controls=\"\" preload=\"none\" style=\"width:200px\" class=\"kskin\" data-durationhint=\"1882.1398639456\" data-startoffset=\"0\" data-mwtitle=\"En-Neural_network.ogg\" data-mwprovider=\"wikimediacommons\"><source src=\"//upload.wikimedia.org/wikipedia/commons/a/a3/En-Neural_network.ogg\" type=\"audio/ogg; codecs=&quot;vorbis&quot;\" data-title=\"Original Ogg file (69 kbps)\" data-shorttitle=\"Ogg source\" data-width=\"0\" data-height=\"0\" data-bandwidth=\"68628\" /><source src=\"//upload.wikimedia.org/wikipedia/commons/transcoded/a/a3/En-Neural_network.ogg/En-Neural_network.ogg.mp3\" type=\"audio/mpeg\" data-title=\"MP3\" data-shorttitle=\"MP3\" data-transcodekey=\"mp3\" data-width=\"0\" data-height=\"0\" data-bandwidth=\"130904\" /></audio></div></div></div>\\n</div>\\n<p><br />\\n</p>\\n<div style=\"float: left; margin-left: 5px;\"><div class=\"floatnone\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/47/Sound-icon.svg/45px-Sound-icon.svg.png\" decoding=\"async\" title=\"Spoken Wikipedia\" width=\"45\" height=\"34\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/47/Sound-icon.svg/68px-Sound-icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/47/Sound-icon.svg/90px-Sound-icon.svg.png 2x\" data-file-width=\"128\" data-file-height=\"96\" /></div></div>\\n<div style=\"font-size: xx-small; line-height: 1.6em; margin-left: 60px;\">This audio file was created from a revision of the article \"<span class=\"fn\">Neural network</span>\" dated 2011-11-27, and does not reflect subsequent edits to the article. (<a href=\"/wiki/Wikipedia:Media_help\" class=\"mw-redirect\" title=\"Wikipedia:Media help\">Audio help</a>)</div>\\n<div style=\"text-align: center; clear: both\"><b><a href=\"/wiki/Wikipedia:Spoken_articles\" title=\"Wikipedia:Spoken articles\">More spoken articles</a></b></div>\\n</div>\\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.dkriesel.com/en/science/neural_networks\">A Brief Introduction to Neural Networks (D. Kriesel)</a> - Illustrated, bilingual manuscript about artificial neural networks; Topics so far: Perceptrons, Backpropagation, Radial Basis Functions, Recurrent Neural Networks, Self Organizing Maps, Hopfield Networks.</li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.msm.cam.ac.uk/phase-trans/abstracts/neural.review.html\">Review of Neural Networks in Materials Science</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.gc.ssr.upm.es/inves/neural/ann1/anntutorial.html\">Artificial Neural Networks Tutorial in three languages (Univ. Politécnica de Madrid)</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.doc.ic.ac.uk/~nd/surprise_96/journal/vol4/cs11/report.html\">Another introduction to ANN</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.youtube.com/watch?v=AyzOUbkUf3M\">Next Generation of Neural Networks</a> - Google Tech Talks</li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.msm.cam.ac.uk/phase-trans/2009/performance.html\">Performance of Neural Networks</a></li>\\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.msm.cam.ac.uk/phase-trans/2009/review_Bhadeshia_SADM.pdf\">Neural Networks and Information</a></li>\\n<li><cite class=\"citation web\">Sanderson, Grant (October 5, 2017). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi\">\"But what <i>is</i> a Neural Network?\"</a>. <i><a href=\"/wiki/3Blue1Brown\" title=\"3Blue1Brown\">3Blue1Brown</a></i> &#8211; via <a href=\"/wiki/YouTube\" title=\"YouTube\">YouTube</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=3Blue1Brown&amp;rft.atitle=But+what+is+a+Neural+Network%3F&amp;rft.date=2017-10-05&amp;rft.aulast=Sanderson&amp;rft.aufirst=Grant&amp;rft_id=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DaircAruvnKk%26list%3DPLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ANeural+network\" class=\"Z3988\"></span><link rel=\"mw-deduplicated-inline-style\" href=\"mw-data:TemplateStyles:r935243608\"/></li></ul></div>'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3Blue1Brown',\n",
       " 'A. K. Dewdney',\n",
       " 'ADALINE',\n",
       " 'Acetylcholine',\n",
       " 'Adaptive control',\n",
       " 'Adaptive resonance theory',\n",
       " 'Adaptive system',\n",
       " 'Alan Turing',\n",
       " 'Alexander Bain',\n",
       " 'Amplitude',\n",
       " 'Analog signal',\n",
       " 'Artificial intelligence',\n",
       " 'Artificial neural network',\n",
       " 'Artificial neuron',\n",
       " 'Autonomous robot',\n",
       " 'Axon',\n",
       " 'BCM theory',\n",
       " 'Back-propagation',\n",
       " 'Backpropagation',\n",
       " 'Bibcode',\n",
       " 'Biological cybernetics',\n",
       " 'Biological neural network',\n",
       " 'Biological neuron models',\n",
       " 'Biologically inspired computing',\n",
       " 'Biophysics',\n",
       " 'Blind signal separation',\n",
       " 'Boltzmann machine',\n",
       " 'CPU',\n",
       " 'Cerebellar model articulation controller',\n",
       " 'Charles Scott Sherrington',\n",
       " 'CiteSeerX',\n",
       " 'Cognitive architecture',\n",
       " 'Cognitive modeling',\n",
       " 'Cognitive science',\n",
       " 'Computation',\n",
       " 'Computational neuroscience',\n",
       " 'Computer simulation',\n",
       " 'Connectionism',\n",
       " 'Connectomics',\n",
       " 'Convolution',\n",
       " 'Cultured neuronal networks',\n",
       " 'Data compression',\n",
       " 'Data mining',\n",
       " 'Data modeling',\n",
       " 'Data processing',\n",
       " 'Database',\n",
       " 'David H. Hubel',\n",
       " 'Decision making',\n",
       " 'Deep learning',\n",
       " 'Dendrite',\n",
       " 'Dendrodendritic synapse',\n",
       " 'Digital data',\n",
       " 'Digital morphogenesis',\n",
       " 'Digital object identifier',\n",
       " 'Donald Hebb',\n",
       " 'Dopamine',\n",
       " 'E-mail spam',\n",
       " 'Exclusive-or',\n",
       " 'Exclusive or',\n",
       " 'Feedforward neural network',\n",
       " 'Frank Rosenblatt',\n",
       " 'Function approximation',\n",
       " 'GPU',\n",
       " 'Gene expression programming',\n",
       " 'Geoff Hinton',\n",
       " 'Geoffrey Hinton',\n",
       " 'Group method of data handling',\n",
       " 'Habituation',\n",
       " 'Hard drive',\n",
       " 'Hebbian learning',\n",
       " 'Hopfield network',\n",
       " 'IDSIA',\n",
       " 'Image analysis',\n",
       " 'In situ adaptive tabulation',\n",
       " 'Information processing',\n",
       " 'Information theory',\n",
       " 'International Standard Book Number',\n",
       " 'Jürgen Schmidhuber',\n",
       " 'Kunihiko Fukushima',\n",
       " 'Learning',\n",
       " 'Long short term memory',\n",
       " 'Long term potentiation',\n",
       " 'Machine learning',\n",
       " 'Marvin Minsky',\n",
       " 'Mathematical model',\n",
       " 'Memristor',\n",
       " 'Multilinear subspace learning',\n",
       " 'NYU',\n",
       " 'Nanodevice',\n",
       " 'Nature Nanotechnology',\n",
       " 'Nature Neuroscience',\n",
       " 'Neocognitron',\n",
       " 'Neural Computation (journal)',\n",
       " 'Neural backpropagation',\n",
       " 'Neural computing',\n",
       " 'Neural network (disambiguation)',\n",
       " 'Neural network software',\n",
       " 'Neural processing',\n",
       " 'Neuromodulators',\n",
       " 'Neuromorphic computing',\n",
       " 'Neuron',\n",
       " 'Neurotransmitter',\n",
       " 'Non-linear',\n",
       " 'Nonlinear system identification',\n",
       " 'Novelty detection',\n",
       " 'Parallel constraint satisfaction processes',\n",
       " 'Parallel distributed processing',\n",
       " 'Pattern recognition',\n",
       " 'Perceptron',\n",
       " 'Predictive analytics',\n",
       " 'Predictive modeling',\n",
       " 'Principal component',\n",
       " 'Programming language',\n",
       " 'PubMed Central',\n",
       " 'PubMed Identifier',\n",
       " 'Pyramidal neuron',\n",
       " 'Radial basis function network',\n",
       " 'Radial basis networks',\n",
       " 'Random-access memory',\n",
       " 'Recurrent neural network',\n",
       " 'Regression analysis',\n",
       " 'Roger Bridgman',\n",
       " 'Scientific American',\n",
       " 'Self-organizing map',\n",
       " 'Serotonin',\n",
       " 'Seymour Papert',\n",
       " 'Simulated reality',\n",
       " 'Software agents',\n",
       " 'Speech recognition',\n",
       " 'Statistical',\n",
       " 'Statistical classification',\n",
       " 'Support vector machine',\n",
       " 'Symbolic artificial intelligence',\n",
       " 'Synapse',\n",
       " 'Synaptic plasticity',\n",
       " 'Tensor product network',\n",
       " 'Threshold logic',\n",
       " 'Time delay neural network',\n",
       " 'Time series prediction',\n",
       " 'Tomaso Poggio',\n",
       " 'Torsten Wiesel',\n",
       " 'University of Chicago',\n",
       " 'University of Toronto',\n",
       " 'Unorganized machine',\n",
       " 'Unsupervised learning',\n",
       " 'Video game',\n",
       " 'Visual cortex',\n",
       " 'Von Neumann model',\n",
       " 'Walter Pitts',\n",
       " 'Warren McCulloch',\n",
       " 'Warren Sturgis McCulloch',\n",
       " 'William James',\n",
       " 'Yann LeCun',\n",
       " 'YouTube']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki.links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
