{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_section_combination(arr):\n",
    "    subsets = []\n",
    "    for l in range(0, len(arr) + 1):\n",
    "        for subset in itertools.combinations(arr, l):\n",
    "            subsets.append(subset)\n",
    "    subsets = [subsets[i] for i in range(1, len(subsets))]\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [31.1, 31.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(31.1,), (31.2,), (31.1, 31.2)]\n"
     ]
    }
   ],
   "source": [
    "print(get_section_combination(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_section(arr):\n",
    "    all_data = []\n",
    "    current_arr = [arr[0]]\n",
    "    i = 1\n",
    "    while i < len(arr):\n",
    "        if current_arr[0] in arr[i]:\n",
    "            current_arr.append(arr[i])\n",
    "        else:\n",
    "            all_data.append(current_arr)\n",
    "            current_arr = [arr[i]]\n",
    "        i += 1\n",
    "    all_data.append(current_arr)\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8', '8.2', '8.3', '8.3.1', '8.3.2', '8.3.3', '8.4', '8.4.1', '18.2', '18.3', '18.5']]\n"
     ]
    }
   ],
   "source": [
    "arr = \"8|8.2|8.3|8.3.1|8.3.2|8.3.3|8.4|8.4.1|18.2|18.3|18.5\".split(\"|\")\n",
    "print(merge_section(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3', '3.6', '3.6.2', '3.6.3', '3.9', '3.9.1', '3.12'], ['5.4.1'], ['5.4.2'], ['6.3'], ['12.3.2'], ['12.3.4'], ['18.5'], ['21', '21.2', '21.2.1', '21.2.2', '21.2.3']]\n"
     ]
    }
   ],
   "source": [
    "arr = \"3|3.6|3.6.2|3.6.3|3.9|3.9.1|3.12|5.4.1|5.4.2|6.3|12.3.2|12.3.4|18.5|21|21.2|21.2.1|21.2.2|21.2.3\".split(\"|\")\n",
    "print(merge_section(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8', '8.2', '8.3', '8.3.1', '8.3.2', '8.3.3', '8.4', '8.4.1'], ['18.2'], ['18.3'], ['18.5']]\n"
     ]
    }
   ],
   "source": [
    "def is_subsection(s1, s2):\n",
    "    x1 = s1.split(\".\")\n",
    "    x2 = s2.split(\".\")\n",
    "    if all(x in x2 for x in x1):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def merge_section(arr):\n",
    "    all_data = []\n",
    "    current_arr = [arr[0]]\n",
    "    i = 1\n",
    "    while i < len(arr):\n",
    "        if is_subsection(current_arr[0], arr[i]):\n",
    "            current_arr.append(arr[i])\n",
    "        else:\n",
    "            all_data.append(current_arr)\n",
    "            current_arr = [arr[i]]\n",
    "        i += 1\n",
    "    all_data.append(current_arr)\n",
    "    return all_data\n",
    "\n",
    "arr = \"8|8.2|8.3|8.3.1|8.3.2|8.3.3|8.4|8.4.1|18.2|18.3|18.5\".split(\"|\")\n",
    "print(merge_section(arr))\n",
    "\n",
    "# s1 = \"8.3.1\"\n",
    "# s2 = \"8.3.1.2\"\n",
    "\n",
    "# is_subsection(s1, s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/shivampal/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "\n",
    "def porter_stemming(text):\n",
    "    porter_stemmer  = PorterStemmer()\n",
    "    word_tokens = text.split(\" \")\n",
    "    words = [porter_stemmer.stem(word) for word in word_tokens]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def wordnet_lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = text.split(\" \")\n",
    "    words = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conduct'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Conduction\"\n",
    "porter_stemming(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Conduction'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet_lemmatization(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
