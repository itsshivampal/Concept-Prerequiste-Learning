{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from various files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_content_file = \"required_data/physics_book_content.csv\"\n",
    "labeled_pairs_file = \"required_data/physics_labeled_pairs.csv\"\n",
    "wikipedia_data_file = \"required_data/physics_correct_wikipedia_data.csv\"\n",
    "concepts_file = \"required_data/physics_concepts_ambiguity.csv\"\n",
    "\n",
    "def read_concepts_file():\n",
    "    df = pd.read_csv(concepts_file, encoding = \"utf-8\")\n",
    "    all_data = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        all_data[i] = {\n",
    "            \"concept\": df[[\"concept\"]].iloc[i].values[0],\n",
    "            \"key_terms\": df[[\"key_terms\"]].iloc[i].values[0]\n",
    "        }\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def read_book_data():\n",
    "    df = pd.read_csv(book_content_file, encoding = \"utf-8\")\n",
    "    all_data = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        all_data[i] = {\n",
    "            \"section\": df[[\"section\"]].iloc[i].values[0],\n",
    "            \"title\": df[[\"title\"]].iloc[i].values[0],\n",
    "            \"page_no\": df[[\"page_no\"]].iloc[i].values[0],\n",
    "            \"content\": df[[\"content\"]].iloc[i].values[0]\n",
    "        }\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def read_labeled_pairs():\n",
    "    df = pd.read_csv(labeled_pairs_file, encoding = \"utf-8\")\n",
    "    all_data = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        all_data[i] = {\n",
    "            \"topic_a\": df[[\"topic_a\"]].iloc[i].values[0],\n",
    "            \"topic_b\": df[[\"topic_b\"]].iloc[i].values[0],\n",
    "            \"relation\": df[[\"relation\"]].iloc[i].values[0],\n",
    "        }\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def read_wikipedia_data():\n",
    "    df = pd.read_csv(wikipedia_data_file, encoding = \"utf-8\")\n",
    "    all_data = {}\n",
    "    for i in range(df.shape[0]):\n",
    "        all_data[i] = {\n",
    "            'topic': df[[\"topic\"]].iloc[i].values[0],\n",
    "            'wiki_title': df[[\"wiki_title\"]].iloc[i].values[0],\n",
    "            'wiki_summary': df[[\"wiki_summary\"]].iloc[i].values[0],\n",
    "            'wiki_content': df[[\"wiki_content\"]].iloc[i].values[0],\n",
    "            'wiki_links': df[[\"wiki_links\"]].iloc[i].values[0],\n",
    "        }\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stemming(text):\n",
    "    porter_stemmer  = PorterStemmer()\n",
    "    word_tokens = text.split(\" \")\n",
    "    words = [porter_stemmer.stem(word) for word in word_tokens]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def wordnet_lemmatization(text):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    word_tokens = text.split(\" \")\n",
    "    words = [wordnet_lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "    new_text = \" \".join(words)\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    sentence = \" \".join(filtered_sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    new_text = \"\"\n",
    "    punctuations = \"!\\\"#$%&()*+-.,:;<=>?@[\\]^_'{|}~\"\n",
    "    for ch in text:\n",
    "        if ch not in punctuations:\n",
    "            new_text += ch\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def clean_text(content):\n",
    "    content = content.lower()\n",
    "    content = re.sub(r'\\d+', '', content)\n",
    "    content = remove_punctuations(content)\n",
    "    content = remove_stopwords(content)\n",
    "    content = porter_stemming(content)\n",
    "    # content = wordnet_lemmatization(content)\n",
    "    content = content.strip()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching concept title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def direct_matching(title, concept):\n",
    "    title = clean_text(title).split(\" \")\n",
    "    concept = clean_text(concept).split(\" \")\n",
    "    if set(concept).issubset(set(title)) and len(concept) == len(title):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def concept_in_title(title, concept):\n",
    "    title = clean_text(title).split(\" \")\n",
    "    concept = clean_text(concept).split(\" \")\n",
    "    if set(concept).issubset(set(title)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def title_in_concept(title, concept):\n",
    "    title = clean_text(title).split(\" \")\n",
    "    concept = clean_text(concept).split(\" \")\n",
    "    if set(title).issubset(set(concept)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def matching_function(title, concept, key_terms, func_type):\n",
    "    if func_type == 1:\n",
    "        if direct_matching(title, concept):\n",
    "            return True\n",
    "        else:\n",
    "            flag = 0\n",
    "            for key_term in key_terms:\n",
    "                if direct_matching(title, key_term):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag: return True\n",
    "            else: return False\n",
    "\n",
    "    elif func_type == 2:\n",
    "        if concept_in_title(title, concept):\n",
    "            return True\n",
    "        else:\n",
    "            flag = 0\n",
    "            for key_term in key_terms:\n",
    "                if concept_in_title(title, key_term):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag: return True\n",
    "            else: return False\n",
    "\n",
    "    elif func_type == 3:\n",
    "        if title_in_concept(title, concept):\n",
    "            return True\n",
    "        else:\n",
    "            flag = 0\n",
    "            for key_term in key_terms:\n",
    "                if title_in_concept(title, key_term):\n",
    "                    flag = 1\n",
    "                    break\n",
    "            if flag: return True\n",
    "            else: return False\n",
    "\n",
    "#--------------------------------------------------------------------#\n",
    "\n",
    "def match_title_concept():\n",
    "    book_data = read_book_data()\n",
    "    concept_data = read_concepts_file()\n",
    "    matching_data = {}\n",
    "    index = 0\n",
    "\n",
    "    for i in range(len(concept_data)):\n",
    "        concept = concept_data[i][\"concept\"]\n",
    "        key_terms = concept_data[i][\"key_terms\"].split(\"|\")\n",
    "        matched_index = []\n",
    "\n",
    "        for j in range(len(book_data)):\n",
    "            title = book_data[j][\"title\"]\n",
    "            if matching_function(title, concept, key_terms, func_type = 1):\n",
    "                matched_index.append(j)\n",
    "\n",
    "        if len(matched_index) == 0:\n",
    "            for j in range(len(book_data)):\n",
    "                title = book_data[j][\"title\"]\n",
    "                if matching_function(title, concept, key_terms, func_type = 2):\n",
    "                    matched_index.append(j)\n",
    "            if len(matched_index) == 0:\n",
    "                for j in range(len(book_data)):\n",
    "                    title = book_data[j][\"title\"]\n",
    "                    if matching_function(title, concept, key_terms, func_type = 3):\n",
    "                        matched_index.append(j)\n",
    "                if len(matched_index) == 0:\n",
    "                    concept_type = 0\n",
    "                else:\n",
    "                    concept_type = 3\n",
    "            else:\n",
    "                concept_type = 2\n",
    "        else:\n",
    "            concept_type = 1\n",
    "\n",
    "        matching_data[index] = {\n",
    "            \"concept\": concept,\n",
    "            \"index\": matched_index,\n",
    "            \"type\": concept_type\n",
    "        }\n",
    "        index += 1\n",
    "    return matching_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_data = match_title_concept()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(concept_type):\n",
    "    index = 0\n",
    "    for i in range(len(matching_data)):\n",
    "        concept = matching_data[i][\"concept\"]\n",
    "        conc_type = matching_data[i][\"type\"]\n",
    "        concept_len = len(matching_data[i][\"index\"])\n",
    "        if conc_type == concept_type:\n",
    "            print(concept, concept_len)\n",
    "            index += 1\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Angle of incidence 0\n",
      "Real image 0\n",
      "Crystallinity 0\n",
      "Hardness 0\n",
      "Virtual image 0\n",
      "Electrical polarity 0\n",
      "Hertz 0\n",
      "Stiffness 0\n",
      "Tangential and normal components 0\n",
      "Joule 0\n",
      "Planet 0\n",
      "Temperature 0\n",
      "Kilogram 0\n",
      "Friction 0\n",
      "Gravitational constant 0\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "get_stats(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorption spectroscopy 1\n",
      "Refraction 1\n",
      "Emission spectrum 1\n",
      "Potential energy 1\n",
      "Newton's laws of motion 1\n",
      "Relative velocity 1\n",
      "Electric potential 1\n",
      "Hooke's law 1\n",
      "Resonance (particle physics) 1\n",
      "Work (physics) 1\n",
      "Gravitational acceleration 1\n",
      "Diffraction 1\n",
      "Transverse wave 1\n",
      "Electroscope 1\n",
      "Torque 1\n",
      "Photoelectric effect 1\n",
      "Laser 1\n",
      "Sound intensity 1\n",
      "Dielectric 1\n",
      "Standing wave 1\n",
      "Refracting telescope 1\n",
      "Telescope 1\n",
      "Newton's law of universal gravitation 1\n",
      "Elastic collision 1\n",
      "Position (vector) 1\n",
      "Optical microscope 1\n",
      "Snell's law 1\n",
      "Insulator (electricity) 1\n",
      "Inelastic collision 1\n",
      "Electromotive force 1\n",
      "Huygens–Fresnel principle 1\n",
      "Magnetic field 1\n",
      "Plane mirror 1\n",
      "Gravitational field 1\n",
      "Electromagnetic radiation 1\n",
      "Capacitance 1\n",
      "Specular reflection 1\n",
      "Electromagnetic spectrum 1\n",
      "Direction (geometry) 1\n",
      "Energy 1\n",
      "Power (physics) 1\n",
      "Reflection (physics) 1\n",
      "Euclidean vector 1\n",
      "Free fall 1\n",
      "Interference (wave propagation) 1\n",
      "Acceleration 1\n",
      "Ohmmeter 1\n",
      "Musical tone 1\n",
      "Pitch (music) 1\n",
      "Capacitor 1\n",
      "Voltmeter 1\n",
      "Kinetic energy 1\n",
      "Physics 1\n",
      "Free body diagram 1\n",
      "Ammeter 1\n",
      "Magnification 1\n",
      "Scalar multiplication 1\n",
      "Total internal reflection 1\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "concept_type = 1\n",
    "index = 0\n",
    "for i in range(len(matching_data)):\n",
    "    concept = matching_data[i][\"concept\"]\n",
    "    conc_type = matching_data[i][\"type\"]\n",
    "    concept_len = len(matching_data[i][\"index\"])\n",
    "    if conc_type == concept_type and concept_len == 1:\n",
    "        print(concept, concept_len)\n",
    "        index += 1\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inertial frame of reference 3\n",
      "Normal force 2\n",
      "Contact force 2\n",
      "Non-inertial reference frame 4\n",
      "Mirror image 3\n",
      "Tension (physics) 3\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "concept_type = 2\n",
    "index = 0\n",
    "for i in range(len(matching_data)):\n",
    "    concept = matching_data[i][\"concept\"]\n",
    "    conc_type = matching_data[i][\"type\"]\n",
    "    concept_len = len(matching_data[i][\"index\"])\n",
    "    if conc_type == concept_type and concept_len > 1:\n",
    "        print(concept, concept_len)\n",
    "        index += 1\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absorption spectroscopy 1\n",
      "Refraction 1\n",
      "Emission spectrum 1\n",
      "Potential energy 1\n",
      "Newton's laws of motion 1\n",
      "Relative velocity 1\n",
      "Electric potential 1\n",
      "Hooke's law 1\n",
      "Resonance (particle physics) 1\n",
      "Work (physics) 1\n",
      "Gravitational acceleration 1\n",
      "Diffraction 1\n",
      "Transverse wave 1\n",
      "Electroscope 1\n",
      "Torque 1\n",
      "Photoelectric effect 1\n",
      "Laser 1\n",
      "Sound intensity 1\n",
      "Dielectric 1\n",
      "Standing wave 1\n",
      "Refracting telescope 1\n",
      "Telescope 1\n",
      "Newton's law of universal gravitation 1\n",
      "Elastic collision 1\n",
      "Position (vector) 1\n",
      "Optical microscope 1\n",
      "Snell's law 1\n",
      "Insulator (electricity) 1\n",
      "Inelastic collision 1\n",
      "Electromotive force 1\n",
      "Huygens–Fresnel principle 1\n",
      "Magnetic field 1\n",
      "Plane mirror 1\n",
      "Gravitational field 1\n",
      "Electromagnetic radiation 1\n",
      "Capacitance 1\n",
      "Specular reflection 1\n",
      "Electromagnetic spectrum 1\n",
      "Direction (geometry) 1\n",
      "Energy 1\n",
      "Power (physics) 1\n",
      "Reflection (physics) 1\n",
      "Euclidean vector 1\n",
      "Free fall 1\n",
      "Interference (wave propagation) 1\n",
      "Acceleration 1\n",
      "Ohmmeter 1\n",
      "Musical tone 1\n",
      "Pitch (music) 1\n",
      "Capacitor 1\n",
      "Voltmeter 1\n",
      "Kinetic energy 1\n",
      "Physics 1\n",
      "Free body diagram 1\n",
      "Ammeter 1\n",
      "Magnification 1\n",
      "Scalar multiplication 1\n",
      "Total internal reflection 1\n",
      "58\n"
     ]
    }
   ],
   "source": [
    "concept_type = 3\n",
    "index = 0\n",
    "for i in range(len(matching_data)):\n",
    "    concept = matching_data[i][\"concept\"]\n",
    "    conc_type = matching_data[i][\"type\"]\n",
    "    concept_len = len(matching_data[i][\"index\"])\n",
    "    if conc_type == concept_type and concept_len == 1:\n",
    "        print(concept, concept_len)\n",
    "        index += 1\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Matching code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "path = '/opt/datacourse/data/parts'\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        shakes = open(file_path, 'r')\n",
    "        text = shakes.read()\n",
    "        lowers = text.lower()\n",
    "        no_punctuation = lowers.translate(None, string.punctuation)\n",
    "        token_dict[file] = no_punctuation\n",
    "        \n",
    "#this can take some time\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36651513 1.         0.72875508 0.50699287]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7131300042306161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/anaconda3/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def process_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    result = []\n",
    "    for token in doc:\n",
    "        if token.text in nlp.Defaults.stop_words:\n",
    "            continue\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "        if token.lemma_ == '-PRON-':\n",
    "            continue\n",
    "        result.append(token.lemma_)\n",
    "    result = \" \".join(result)\n",
    "    result = nlp(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def match_docs(content1, content2):\n",
    "    content1 = process_text(content1)\n",
    "    content2 = process_text(content2)\n",
    "    score = content1.similarity(content2)\n",
    "    return score\n",
    "\n",
    "content1 = documents[0]\n",
    "content2 = documents[3]\n",
    "\n",
    "print(match_docs(content1, content2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
