12
Latent

Continuous

Variables

Appendix A

In Chapter 9, we discussed probabilistic models having discrete latent variables, such
as the mixture of Gaussians. We now explore models in which some, or all, of the
latent variables are continuous. An important motivation for such models is that
many data sets have the property that the data points all lie close to a manifold of
much lower dimensionality than that of the original data space. To see why this
might arise, consider an artiﬁcial data set constructed by taking one of the off-line
digits, represented by a 64 × 64 pixel grey-level image, and embedding it in a larger
image of size 100×100 by padding with pixels having the value zero (corresponding
to white pixels) in which the location and orientation of the digit is varied at random,
as illustrated in Figure 12.1. Each of the resulting images is represented by a point in
the 100× 100 = 10, 000-dimensional data space. However, across a data set of such
images, there are only three degrees of freedom of variability, corresponding to the
vertical and horizontal translations and the rotations. The data points will therefore
live on a subspace of the data space whose intrinsic dimensionality is three. Note

559

560

12. CONTINUOUS LATENT VARIABLES

Figure 12.1 A synthetic data set obtained by taking one of the off-line digit images and creating multi-
ple copies in each of which the digit has undergone a random displacement and rotation
within some larger image ﬁeld. The resulting images each have 100 × 100 = 10, 000
pixels.

that the manifold will be nonlinear because, for instance, if we translate the digit
past a particular pixel, that pixel value will go from zero (white) to one (black) and
back to zero again, which is clearly a nonlinear function of the digit position. In
this example, the translation and rotation parameters are latent variables because we
observe only the image vectors and are not told which values of the translation or
rotation variables were used to create them.

For real digit image data, there will be a further degree of freedom arising from
scaling. Moreover there will be multiple additional degrees of freedom associated
with more complex deformations due to the variability in an individual’s writing
as well as the differences in writing styles between individuals. Nevertheless, the
number of such degrees of freedom will be small compared to the dimensionality of
the data set.

Another example is provided by the oil ﬂow data set, in which (for a given ge-
ometrical conﬁguration of the gas, water, and oil phases) there are only two degrees
of freedom of variability corresponding to the fraction of oil in the pipe and the frac-
tion of water (the fraction of gas then being determined). Although the data space
comprises 12 measurements, a data set of points will lie close to a two-dimensional
manifold embedded within this space. In this case, the manifold comprises several
distinct segments corresponding to different ﬂow regimes, each such segment being
a (noisy) continuous two-dimensional manifold. If our goal is data compression, or
density modelling, then there can be beneﬁts in exploiting this manifold structure.

In practice, the data points will not be conﬁned precisely to a smooth low-
dimensional manifold, and we can interpret the departures of data points from the
manifold as ‘noise’. This leads naturally to a generative view of such models in
which we ﬁrst select a point within the manifold according to some latent variable
distribution and then generate an observed data point by adding noise, drawn from
some conditional distribution of the data variables given the latent variables.

The simplest continuous latent variable model assumes Gaussian distributions
for both the latent and observed variables and makes use of a linear-Gaussian de-
pendence of the observed variables on the state of the latent variables. This leads
to a probabilistic formulation of the well-known technique of principal component
analysis (PCA), as well as to a related model called factor analysis.

In this chapter w will begin with a standard, nonprobabilistic treatment of PCA,
and then we show how PCA arises naturally as the maximum likelihood solution to

Appendix A

Section 8.1.4

Section 12.1

12.1. Principal Component Analysis

Figure 12.2 Principal component analysis seeks a space
of lower dimensionality, known as the princi-
pal subspace and denoted by the magenta
line, such that
the orthogonal projection of
the data points (red dots) onto this subspace
maximizes the variance of the projected points
(green dots). An alternative deﬁnition of PCA
is based on minimizing the sum-of-squares
of the projection errors, indicated by the blue
lines.

x2

xn

(cid:4)xn

561

u1

x1

Section 12.2

Section 12.4

a particular form of linear-Gaussian latent variable model. This probabilistic refor-
mulation brings many advantages, such as the use of EM for parameter estimation,
principled extensions to mixtures of PCA models, and Bayesian formulations that
allow the number of principal components to be determined automatically from the
data. Finally, we discuss brieﬂy several generalizations of the latent variable concept
that go beyond the linear-Gaussian assumption including non-Gaussian latent vari-
ables, which leads to the framework of independent component analysis, as well as
models having a nonlinear relationship between latent and observed variables.

12.1. Principal Component Analysis

Principal component analysis, or PCA, is a technique that is widely used for appli-
cations such as dimensionality reduction, lossy data compression, feature extraction,
and data visualization (Jolliffe, 2002). It is also known as the Karhunen-Lo`eve trans-
form.

There are two commonly used deﬁnitions of PCA that give rise to the same
algorithm. PCA can be deﬁned as the orthogonal projection of the data onto a lower
dimensional linear space, known as the principal subspace, such that the variance of
the projected data is maximized (Hotelling, 1933). Equivalently, it can be deﬁned as
the linear projection that minimizes the average projection cost, deﬁned as the mean
squared distance between the data points and their projections (Pearson, 1901). The
process of orthogonal projection is illustrated in Figure 12.2. We consider each of
these deﬁnitions in turn.

12.1.1 Maximum variance formulation
Consider a data set of observations {xn} where n = 1, . . . , N, and xn is a
Euclidean variable with dimensionality D. Our goal is to project the data onto a
space having dimensionality M < D while maximizing the variance of the projected
data. For the moment, we shall assume that the value of M is given. Later in this

562

12. CONTINUOUS LATENT VARIABLES

chapter, we shall consider techniques to determine an appropriate value of M from
the data.

To begin with, consider the projection onto a one-dimensional space (M = 1).
We can deﬁne the direction of this space using a D-dimensional vector u1, which
for convenience (and without loss of generality) we shall choose to be a unit vector
1 u1 = 1 (note that we are only interested in the direction deﬁned by u1,
so that uT
not in the magnitude of u1 itself). Each data point xn is then projected onto a scalar
value uT
1 x where x is the sample set mean
given by

1 xn. The mean of the projected data is uT

N(cid:2)

n=1

xn

x =

1
N

1 xn − uT
uT
1 x
N(cid:2)

N(cid:2)

(cid:26)

n=1

1
N

(cid:27)2 = uT

1 Su1

(12.1)

(12.2)

(12.3)

and the variance of the projected data is given by

where S is the data covariance matrix deﬁned by

S =

1
N

n=1

(xn − x)(xn − x)T.

Appendix E

We now maximize the projected variance uT
1 Su1 with respect to u1. Clearly, this has
to be a constrained maximization to prevent (cid:5)u1(cid:5) → ∞. The appropriate constraint
1 u1 = 1. To enforce this constraint,
comes from the normalization condition uT
we introduce a Lagrange multiplier that we shall denote by λ1, and then make an
unconstrained maximization of

uT
1 Su1 + λ1

1 − uT

1 u1

.

(12.4)

(cid:10)

(cid:11)

By setting the derivative with respect to u1 equal to zero, we see that this quantity
will have a stationary point when

Su1 = λ1u1

(12.5)

which says that u1 must be an eigenvector of S. If we left-multiply by uT
use of uT

1 u1 = 1, we see that the variance is given by

1 and make

uT
1 Su1 = λ1

(12.6)

and so the variance will be a maximum when we set u1 equal to the eigenvector
having the largest eigenvalue λ1. This eigenvector is known as the ﬁrst principal
component.

We can deﬁne additional principal components in an incremental fashion by
choosing each new direction to be that which maximizes the projected variance

12.1. Principal Component Analysis

563

amongst all possible directions orthogonal to those already considered. If we con-
sider the general case of an M-dimensional projection space, the optimal linear pro-
jection for which the variance of the projected data is maximized is now deﬁned by
the M eigenvectors u1, . . . , uM of the data covariance matrix S corresponding to the
M largest eigenvalues λ1, . . . , λM . This is easily shown using proof by induction.

To summarize, principal component analysis involves evaluating the mean x
and the covariance matrix S of the data set and then ﬁnding the M eigenvectors of S
corresponding to the M largest eigenvalues. Algorithms for ﬁnding eigenvectors and
eigenvalues, as well as additional theorems related to eigenvector decomposition,
can be found in Golub and Van Loan (1996). Note that the computational cost of
computing the full eigenvector decomposition for a matrix of size D × D is O(D3).
If we plan to project our data onto the ﬁrst M principal components, then we only
need to ﬁnd the ﬁrst M eigenvalues and eigenvectors. This can be done with more
efﬁcient techniques, such as the power method (Golub and Van Loan, 1996), that
scale like O(M D2), or alternatively we can make use of the EM algorithm.

12.1.2 Minimum-error formulation
We now discuss an alternative formulation of PCA based on projection error
minimization. To do this, we introduce a complete orthonormal set of D-dimensional
basis vectors {ui} where i = 1, . . . , D that satisfy
uT
i uj = δij.

(12.7)

Exercise 12.1

Section 12.2.2

Appendix C

Because this basis is complete, each data point can be represented exactly by a linear
combination of the basis vectors

xn =

αniui

(12.8)

where the coefﬁcients αni will be different for different data points. This simply
corresponds to a rotation of the coordinate system to a new system deﬁned by the
{ui}, and the original D components {xn1, . . . , xnD} are replaced by an equivalent
set {αn1, . . . , αnD}. Taking the inner product with uj, and making use of the or-
thonormality property, we obtain αnj = xT
nuj, and so without loss of generality we
can write

xn =

xT
nui

ui.

(12.9)

Our goal, however, is to approximate this data point using a representation in-
volving a restricted number M < D of variables corresponding to a projection onto
a lower-dimensional subspace. The M-dimensional linear subspace can be repre-
sented, without loss of generality, by the ﬁrst M of the basis vectors, and so we
approximate each data point xn by

D(cid:2)

i=1

D(cid:2)

(cid:10)

i=1

(cid:11)

D(cid:2)

M(cid:2)

(cid:4)xn =

zniui +

biui

(12.10)

i=1

i=M +1

564

12. CONTINUOUS LATENT VARIABLES

N(cid:2)

1
N

(cid:5)xn −(cid:4)xn(cid:5)2.

where the {zni} depend on the particular data point, whereas the {bi} are constants
that are the same for all data points. We are free to choose the {ui}, the {zni}, and
the {bi} so as to minimize the distortion introduced by the reduction in dimensional-
ity. As our distortion measure, we shall use the squared distance between the original

data point xn and its approximation(cid:4)xn, averaged over the data set, so that our goal

is to minimize

J =

(12.11)
Consider ﬁrst of all the minimization with respect to the quantities {zni}. Sub-

stituting for(cid:4)xn, setting the derivative with respect to znj to zero, and making use of

n=1

the orthonormality conditions, we obtain

znj = xT

nuj

(12.12)

where j = 1, . . . , M. Similarly, setting the derivative of J with respect to bi to zero,
and again making use of the orthonormality relations, gives

bj = xTuj

(12.13)

where j = M +1, . . . , D. If we substitute for zni and bi, and make use of the general
expansion (12.9), we obtain

D(cid:2)

(cid:26)

xn −(cid:4)xn =

(cid:27)

(xn − x)Tui

ui

(12.14)

from which we see that the displacement vector from xn to (cid:4)xn lies in the space
projected points(cid:4)xn must lie within the principal subspace, but we can move them

orthogonal to the principal subspace, because it is a linear combination of {ui} for
i = M + 1, . . . , D, as illustrated in Figure 12.2. This is to be expected because the

i=M +1

We therefore obtain an expression for the distortion measure J as a function

freely within that subspace, and so the minimum error is given by the orthogonal
projection.
purely of the {ui} in the form
D(cid:2)

N(cid:2)

D(cid:2)

(cid:10)

(cid:11)2 =

nui − xTui
xT

uT

i Sui.

(12.15)

n=1

i=M +1

i=M +1

There remains the task of minimizing J with respect to the {ui}, which must
be a constrained minimization otherwise we will obtain the vacuous result ui = 0.
The constraints arise from the orthonormality conditions and, as we shall see, the
solution will be expressed in terms of the eigenvector expansion of the covariance
matrix. Before considering a formal solution, let us try to obtain some intuition about
the result by considering the case of a two-dimensional data space D = 2 and a one-
dimensional principal subspace M = 1. We have to choose a direction u2 so as to

J =

1
N

12.1. Principal Component Analysis

565

(cid:10)

1 − uT

2 u2

(cid:11)

(cid:4)J = uT

minimize J = uT
Lagrange multiplier λ2 to enforce the constraint, we consider the minimization of

2 Su2, subject to the normalization constraint uT

2 u2 = 1. Using a

.

2 Su2 + λ2

(12.16)
Setting the derivative with respect to u2 to zero, we obtain Su2 = λ2u2 so that u2
is an eigenvector of S with eigenvalue λ2. Thus any eigenvector will deﬁne a sta-
tionary point of the distortion measure. To ﬁnd the value of J at the minimum, we
back-substitute the solution for u2 into the distortion measure to give J = λ2. We
therefore obtain the minimum value of J by choosing u2 to be the eigenvector corre-
sponding to the smaller of the two eigenvalues. Thus we should choose the principal
subspace to be aligned with the eigenvector having the larger eigenvalue. This result
accords with our intuition that, in order to minimize the average squared projection
distance, we should choose the principal component subspace to pass through the
mean of the data points and to be aligned with the directions of maximum variance.
For the case when the eigenvalues are equal, any choice of principal direction will
give rise to the same value of J.
The general solution to the minimization of J for arbitrary D and arbitrary M <
D is obtained by choosing the {ui} to be eigenvectors of the covariance matrix given
by
(12.17)
where i = 1, . . . , D, and as usual the eigenvectors {ui} are chosen to be orthonor-
mal. The corresponding value of the distortion measure is then given by

Sui = λiui

D(cid:2)

J =

λi

i=M +1

(12.18)

which is simply the sum of the eigenvalues of those eigenvectors that are orthogonal
to the principal subspace. We therefore obtain the minimum value of J by selecting
these eigenvectors to be those having the D − M smallest eigenvalues, and hence
the eigenvectors deﬁning the principal subspace are those corresponding to the M
largest eigenvalues.

Although we have considered M < D, the PCA analysis still holds if M =
D, in which case there is no dimensionality reduction but simply a rotation of the
coordinate axes to align with principal components.

Finally, it is worth noting that there exists a closely related linear dimensionality
reduction technique called canonical correlation analysis, or CCA (Hotelling, 1936;
Bach and Jordan, 2002). Whereas PCA works with a single random variable, CCA
considers two (or more) variables and tries to ﬁnd a corresponding pair of linear
subspaces that have high cross-correlation, so that each component within one of the
subspaces is correlated with a single component from the other subspace. Its solution
can be expressed in terms of a generalized eigenvector problem.

12.1.3 Applications of PCA
We can illustrate the use of PCA for data compression by considering the off-
line digits data set. Because each eigenvector of the covariance matrix is a vector

Exercise 12.2

Appendix A

566

12. CONTINUOUS LATENT VARIABLES

Mean

λ1 = 3.4 · 105

λ2 = 2.8 · 105

λ3 = 2.4 · 105

λ4 = 1.6 · 105

Figure 12.3 The mean vector x along with the ﬁrst four PCA eigenvectors u1, . . . , u4 for the off-line

digits data set, together with the corresponding eigenvalues.

in the original D-dimensional space, we can represent the eigenvectors as images of
the same size as the data points. The ﬁrst ﬁve eigenvectors, along with the corre-
sponding eigenvalues, are shown in Figure 12.3. A plot of the complete spectrum of
eigenvalues, sorted into decreasing order, is shown in Figure 12.4(a). The distortion
measure J associated with choosing a particular value of M is given by the sum
of the eigenvalues from M + 1 up to D and is plotted for different values of M in
Figure 12.4(b).

If we substitute (12.12) and (12.13) into (12.10), we can write the PCA approx-

imation to a data vector xn in the form

D(cid:2)

M(cid:2)

(cid:4)xn =

(xT

M(cid:2)

(cid:10)

i=1

= x +

nui)ui +

(xTui)ui

(cid:11)

i=M +1

nui − xTui
xT

ui

(12.19)

(12.20)

i=1

x 106

3

J

2

1

200

400
(a)

600

i

0

0

200

400
(b)

600

M

x 105

3
λi

2

1

0

0

Figure 12.4 (a) Plot of the eigenvalue spectrum for the off-line digits data set.
(b) Plot of the sum of the
discarded eigenvalues, which represents the sum-of-squares distortion J introduced by projecting the data onto
a principal component subspace of dimensionality M.

12.1. Principal Component Analysis

567

Original

M = 1

M = 10

M = 50

M = 250

Figure 12.5 An original example from the off-line digits data set together with its PCA reconstructions
obtained by retaining M principal components for various values of M. As M increases
the reconstruction becomes more accurate and would become perfect when M = D =
28 × 28 = 784.

(cid:10)

(cid:11)

where we have made use of the relation

D(cid:2)

i=1

x =

(cid:10)

(cid:11)

ui

xTui

(12.21)

which follows from the completeness of the {ui}. This represents a compression
of the data set, because for each data point we have replaced the D-dimensional
vector xn with an M-dimensional vector having components
. The
smaller the value of M, the greater the degree of compression. Examples of PCA
reconstructions of data points for the digits data set are shown in Figure 12.5.

nui − xTui
xT

Another application of principal component analysis is to data pre-processing.
In this case, the goal is not dimensionality reduction but rather the transformation of
a data set in order to standardize certain of its properties. This can be important in
allowing subsequent pattern recognition algorithms to be applied successfully to the
data set. Typically, it is done when the original variables are measured in various dif-
ferent units or have signiﬁcantly different variability. For instance in the Old Faithful
data set, the time between eruptions is typically an order of magnitude greater than
the duration of an eruption. When we applied the K-means algorithm to this data
set, we ﬁrst made a separate linear re-scaling of the individual variables such that
each variable had zero mean and unit variance. This is known as standardizing the
data, and the covariance matrix for the standardized data has components

N(cid:2)

ρij =

1
N

(xni − xi)

(xnj − xj)

n=1

σi

σj

(12.22)

where σi is the variance of xi. This is known as the correlation matrix of the original
data and has the property that if two components xi and xj of the data are perfectly
correlated, then ρij = 1, and if they are uncorrelated, then ρij = 0.

However, using PCA we can make a more substantial normalization of the data
to give it zero mean and unit covariance, so that different variables become decorre-
lated. To do this, we ﬁrst write the eigenvector equation (12.17) in the form

SU = UL

(12.23)

Appendix A

Section 9.1

568

12. CONTINUOUS LATENT VARIABLES

100

90

80

70

60

50

40

2

0

−2

2

0

−2

2

4

6

−2

0

2

−2

0

2

Figure 12.6 Illustration of the effects of linear pre-processing applied to the Old Faithful data set. The plot on
the left shows the original data. The centre plot shows the result of standardizing the individual variables to zero
mean and unit variance. Also shown are the principal axes of this normalized data set, plotted over the range
±λ1/2

. The plot on the right shows the result of whitening of the data to give it zero mean and unit covariance.

i

where L is a D × D diagonal matrix with elements λi, and U is a D × D orthog-
onal matrix with columns given by ui. Then we deﬁne, for each data point xn, a
transformed value given by

yn = L−1/2UT(xn − x)

(12.24)
where x is the sample mean deﬁned by (12.1). Clearly, the set {yn} has zero mean,
and its covariance is given by the identity matrix because

N(cid:2)

n=1

N(cid:2)

n=1

1
N

ynyT

n =

1
N

L−1/2UT(xn − x)(xn − x)TUL−1/2

= L−1/2UTSUL−1/2 = L−1/2LL−1/2 = I.

(12.25)

Appendix A

Appendix A

This operation is known as whitening or sphereing the data and is illustrated for the
Old Faithful data set in Figure 12.6.

It is interesting to compare PCA with the Fisher linear discriminant which was
discussed in Section 4.1.4. Both methods can be viewed as techniques for linear
dimensionality reduction. However, PCA is unsupervised and depends only on the
values xn whereas Fisher linear discriminant also uses class-label information. This
difference is highlighted by the example in Figure 12.7.

Another common application of principal component analysis is to data visual-
ization. Here each data point is projected onto a two-dimensional (M = 2) principal
subspace, so that a data point xn is plotted at Cartesian coordinates given by xT
nu1
and xT
nu2, where u1 and u2 are the eigenvectors corresponding to the largest and
second largest eigenvalues. An example of such a plot, for the oil ﬂow data set, is
shown in Figure 12.8.

12.1. Principal Component Analysis

569

Figure 12.7 A comparison of principal compo-
nent analysis with Fisher’s linear
discriminant for linear dimension-
ality reduction. Here the data in
two dimensions, belonging to two
classes shown in red and blue, is
to be projected onto a single di-
mension. PCA chooses the direc-
tion of maximum variance, shown
by the magenta curve, which leads
to strong class overlap, whereas
the Fisher linear discriminant takes
account of
the class labels and
leads to a projection onto the green
curve giving much better class
separation.

1.5

1

0.5

0

−0.5

−1

−1.5

−2

−5

0

5

Figure 12.8 Visualization of the oil ﬂow data set obtained
by projecting the data onto the ﬁrst two prin-
cipal components. The red, blue, and green
points correspond to the ‘laminar’, ‘homo-
geneous’, and ‘annular’ ﬂow conﬁgurations
respectively.

12.1.4 PCA for high-dimensional data
In some applications of principal component analysis, the number of data points
is smaller than the dimensionality of the data space. For example, we might want to
apply PCA to a data set of a few hundred images, each of which corresponds to a
vector in a space of potentially several million dimensions (corresponding to three
colour values for each of the pixels in the image). Note that in a D-dimensional space
a set of N points, where N < D, deﬁnes a linear subspace whose dimensionality
is at most N − 1, and so there is little point in applying PCA for values of M
that are greater than N − 1. Indeed, if we perform PCA we will ﬁnd that at least
D − N + 1 of the eigenvalues are zero, corresponding to eigenvectors along whose
directions the data set has zero variance. Furthermore, typical algorithms for ﬁnding
the eigenvectors of a D×D matrix have a computational cost that scales like O(D3),
and so for applications such as the image example, a direct application of PCA will
be computationally infeasible.
We can resolve this problem as follows. First, let us deﬁne X to be the (N ×D)-

570

12. CONTINUOUS LATENT VARIABLES

dimensional centred data matrix, whose nth row is given by (xn − x)T. The covari-
−1XTX, and the corresponding
ance matrix (12.3) can then be written as S = N
eigenvector equation becomes

1
N

XTXui = λiui.

Now pre-multiply both sides by X to give

1
N

XXT(Xui) = λi(Xui).

If we now deﬁne vi = Xui, we obtain

(12.26)

(12.27)

XXTvi = λivi

(12.28)
which is an eigenvector equation for the N × N matrix N
−1XXT. We see that this
has the same N −1 eigenvalues as the original covariance matrix (which itself has an
additional D − N + 1 eigenvalues of value zero). Thus we can solve the eigenvector
problem in spaces of lower dimensionality with computational cost O(N 3) instead
of O(D3). In order to determine the eigenvectors, we multiply both sides of (12.28)
by XT to give

(cid:16)

XTX

(XTvi) = λi(XTvi)

(12.29)

1
N

(cid:15)

1
N

from which we see that (XTvi) is an eigenvector of S with eigenvalue λi. Note,
however, that these eigenvectors need not be normalized. To determine the appropri-
ate normalization, we re-scale ui ∝ XTvi by a constant such that (cid:5)ui(cid:5) = 1, which,
assuming vi has been normalized to unit length, gives

ui =

1

(N λi)1/2 XTvi.

(12.30)

In summary, to apply this approach we ﬁrst evaluate XXT and then ﬁnd its eigen-
vectors and eigenvalues and then compute the eigenvectors in the original data space
using (12.30).

12.2. Probabilistic PCA

The formulation of PCA discussed in the previous section was based on a linear
projection of the data onto a subspace of lower dimensionality than the original data
space. We now show that PCA can also be expressed as the maximum likelihood
solution of a probabilistic latent variable model. This reformulation of PCA, known
as probabilistic PCA, brings several advantages compared with conventional PCA:
• Probabilistic PCA represents a constrained form of the Gaussian distribution
in which the number of free parameters can be restricted while still allowing
the model to capture the dominant correlations in a data set.

Section 12.2.2

Section 12.2.3

Section 8.1.4

Section 8.2.2

12.2. Probabilistic PCA

571

• We can derive an EM algorithm for PCA that is computationally efﬁcient in
situations where only a few leading eigenvectors are required and that avoids
having to evaluate the data covariance matrix as an intermediate step.
• The combination of a probabilistic model and EM allows us to deal with miss-
ing values in the data set.
• Mixtures of probabilistic PCA models can be formulated in a principled way
and trained using the EM algorithm.
• Probabilistic PCA forms the basis for a Bayesian treatment of PCA in which
the dimensionality of the principal subspace can be found automatically from
the data.
• The existence of a likelihood function allows direct comparison with other
probabilistic density models. By contrast, conventional PCA will assign a low
reconstruction cost to data points that are close to the principal subspace even
if they lie arbitrarily far from the training data.
• Probabilistic PCA can be used to model class-conditional densities and hence
be applied to classiﬁcation problems.
• The probabilistic PCA model can be run generatively to provide samples from
the distribution.

This formulation of PCA as a probabilistic model was proposed independently by
Tipping and Bishop (1997, 1999b) and by Roweis (1998). As we shall see later, it is
closely related to factor analysis (Basilevsky, 1994).

Probabilistic PCA is a simple example of the linear-Gaussian framework, in
which all of the marginal and conditional distributions are Gaussian. We can formu-
late probabilistic PCA by ﬁrst introducing an explicit latent variable z corresponding
to the principal-component subspace. Next we deﬁne a Gaussian prior distribution
p(z) over the latent variable, together with a Gaussian conditional distribution p(x|z)
for the observed variable x conditioned on the value of the latent variable. Speciﬁ-
cally, the prior distribution over z is given by a zero-mean unit-covariance Gaussian

p(z) = N (z|0, I).

(12.31)

Similarly, the conditional distribution of the observed variable x, conditioned on the
value of the latent variable z, is again Gaussian, of the form

p(x|z) = N (x|Wz + µ, σ2I)

(12.32)
in which the mean of x is a general linear function of z governed by the D × M
matrix W and the D-dimensional vector µ. Note that this factorizes with respect to
the elements of x, in other words this is an example of the naive Bayes model. As
we shall see shortly, the columns of W span a linear subspace within the data space
that corresponds to the principal subspace. The other parameter in this model is the
scalar σ2 governing the variance of the conditional distribution. Note that there is no

572

12. CONTINUOUS LATENT VARIABLES

x2

w

x2

p(x|ˆz)

}

ˆz|w|

µ

p(z)

µ

p(x)

ˆz

z

x1

x1

Figure 12.9 An illustration of the generative view of the probabilistic PCA model for a two-dimensional data
space and a one-dimensional latent space. An observed data point x is generated by ﬁrst drawing a value bz
for the latent variable from its prior distribution p(z) and then drawing a value for x from an isotropic Gaussian
distribution (illustrated by the red circles) having mean wbz + µ and covariance σ2I. The green ellipses show the
density contours for the marginal distribution p(x).

Exercise 12.4

loss of generality in assuming a zero mean, unit covariance Gaussian for the latent
distribution p(z) because a more general Gaussian distribution would give rise to an
equivalent probabilistic model.

We can view the probabilistic PCA model from a generative viewpoint in which
a sampled value of the observed variable is obtained by ﬁrst choosing a value for
the latent variable and then sampling the observed variable conditioned on this la-
tent value. Speciﬁcally, the D-dimensional observed variable x is deﬁned by a lin-
ear transformation of the M-dimensional latent variable z plus additive Gaussian
‘noise’, so that

x = Wz + µ + 

(12.33)
where z is an M-dimensional Gaussian latent variable, and  is a D-dimensional
zero-mean Gaussian-distributed noise variable with covariance σ2I. This generative
process is illustrated in Figure 12.9. Note that this framework is based on a mapping
from latent space to data space, in contrast to the more conventional view of PCA
discussed above. The reverse mapping, from data space to the latent space, will be
obtained shortly using Bayes’ theorem.

Suppose we wish to determine the values of the parameters W, µ and σ2 using
maximum likelihood. To write down the likelihood function, we need an expression
for the marginal distribution p(x) of the observed variable. This is expressed, from
the sum and product rules of probability, in the form

(cid:6)

p(x) =

p(x|z)p(z) dz.

(12.34)

Exercise 12.7

Because this corresponds to a linear-Gaussian model, this marginal distribution is
again Gaussian, and is given by

p(x) = N (x|µ, C)

(12.35)

12.2. Probabilistic PCA

573

(cid:9)

(12.37)

where the D × D covariance matrix C is deﬁned by
C = WWT + σ2I.

(12.36)
This result can also be derived more directly by noting that the predictive distribution
will be Gaussian and then evaluating its mean and covariance using (12.33). This
gives

(cid:8)
(cid:8)

E[x] = E[Wz + µ + ] = µ

cov[x] = E
= E

(Wz + )(Wz + )T
WzzTWT

(cid:9)

(12.38)
where we have used the fact that z and  are independent random variables and hence
are uncorrelated.

+ E[T] = WWT + σ2I

Intuitively, we can think of the distribution p(x) as being deﬁned by taking an
isotropic Gaussian ‘spray can’ and moving it across the principal subspace spraying
Gaussian ink with density determined by σ2 and weighted by the prior distribution.
The accumulated ink density gives rise to a ‘pancake’ shaped distribution represent-
ing the marginal density p(x).

an orthogonal matrix. Using the orthogonality property RRT = I, we see that the

The predictive distribution p(x) is governed by the parameters µ, W, and σ2.
However, there is redundancy in this parameterization corresponding to rotations of

the latent space coordinates. To see this, consider a matrix,W = WR where R is
quantity,W,WT that appears in the covariance matrix C takes the form
and hence is independent of R. Thus there is a whole family of matrices,W all of

,W,WT = WRRTWT = WWT

(12.39)

which give rise to the same predictive distribution. This invariance can be understood
in terms of rotations within the latent space. We shall return to a discussion of the
number of independent parameters in this model later.
When we evaluate the predictive distribution, we require C−1, which involves
the inversion of a D× D matrix. The computation required to do this can be reduced
by making use of the matrix inversion identity (C.7) to give
−2WM−1WT

−1I − σ
where the M × M matrix M is deﬁned by

C−1 = σ

(12.40)

Exercise 12.8

M = WTW + σ2I.

(12.41)
Because we invert M rather than inverting C directly, the cost of evaluating C−1 is
reduced from O(D3) to O(M 3).
As well as the predictive distribution p(x), we will also require the posterior
distribution p(z|x), which can again be written down directly using the result (2.116)
for linear-Gaussian models to give

(12.42)
Note that the posterior mean depends on x, whereas the posterior covariance is in-
dependent of x.

.

z|M−1WT(x − µ), σ

−2M

p(z|x) = N(cid:10)

(cid:11)

574

12. CONTINUOUS LATENT VARIABLES

Figure 12.10 The probabilistic PCA model for a data set of N obser-
vations of x can be expressed as a directed graph in
which each observation xn is associated with a value
zn of the latent variable.

σ2

µ

zn

W

xn

N

12.2.1 Maximum likelihood PCA
We next consider the determination of the model parameters using maximum
likelihood. Given a data set X = {xn} of observed data points, the probabilistic
PCA model can be expressed as a directed graph, as shown in Figure 12.10. The
corresponding log likelihood function is given, from (12.35), by

N(cid:2)

ln p(X|µ, W, σ2) =

ln p(xn|W, µ, σ2)

n=1

= − N D
2

ln(2π) − N
2

ln|C| − 1
2

N(cid:2)

n=1

(xn − µ)TC−1(xn − µ). (12.43)

Setting the derivative of the log likelihood with respect to µ equal to zero gives the
expected result µ = x where x is the data mean deﬁned by (12.1). Back-substituting
we can then write the log likelihood function in the form
D ln(2π) + ln|C| + Tr

ln p(X|W, µ, σ2) = − N
2

C−1S

(cid:11)(cid:27)

(12.44)

(cid:26)

(cid:10)

where S is the data covariance matrix deﬁned by (12.3). Because the log likelihood
is a quadratic function of µ, this solution represents the unique maximum, as can be
conﬁrmed by computing second derivatives.

Maximization with respect to W and σ2 is more complex but nonetheless has
an exact closed-form solution. It was shown by Tipping and Bishop (1999b) that all
of the stationary points of the log likelihood function can be written as

WML = UM (LM − σ2I)1/2R

(12.45)
where UM is a D × M matrix whose columns are given by any subset (of size M)
of the eigenvectors of the data covariance matrix S, the M × M diagonal matrix
LM has elements given by the corresponding eigenvalues λi, and R is an arbitrary
M × M orthogonal matrix.

Furthermore, Tipping and Bishop (1999b) showed that the maximum of the like-
lihood function is obtained when the M eigenvectors are chosen to be those whose
eigenvalues are the M largest (all other solutions being saddle points). A similar re-
sult was conjectured independently by Roweis (1998), although no proof was given.

12.2. Probabilistic PCA

575

Again, we shall assume that the eigenvectors have been arranged in order of decreas-
ing values of the corresponding eigenvalues, so that the M principal eigenvectors are
u1, . . . , uM . In this case, the columns of W deﬁne the principal subspace of stan-
dard PCA. The corresponding maximum likelihood solution for σ2 is then given by

D(cid:2)

σ2
ML =

1

D − M

λi

i=M +1

(12.46)

so that σ2

ML is the average variance associated with the discarded dimensions.

Because R is orthogonal, it can be interpreted as a rotation matrix in the M ×M
latent space. If we substitute the solution for W into the expression for C, and make
use of the orthogonality property RRT = I, we see that C is independent of R.
This simply says that the predictive density is unchanged by rotations in the latent
space as discussed earlier. For the particular case of R = I, we see that the columns
of W are the principal component eigenvectors scaled by the variance parameters
λi − σ2. The interpretation of these scaling factors is clear once we recognize that
for a convolution of independent Gaussian distributions (in this case the latent space
distribution and the noise model) the variances are additive. Thus the variance λi
in the direction of an eigenvector ui is composed of the sum of a contribution λi −
σ2 from the projection of the unit-variance latent space distribution into data space
through the corresponding column of W, plus an isotropic contribution of variance
σ2 which is added in all directions by the noise model.

It is worth taking a moment to study the form of the covariance matrix given
by (12.36). Consider the variance of the predictive distribution along some direction
speciﬁed by the unit vector v, where vTv = 1, which is given by vTCv. First
suppose that v is orthogonal to the principal subspace, in other words it is given by
some linear combination of the discarded eigenvectors. Then vTU = 0 and hence
vTCv = σ2. Thus the model predicts a noise variance orthogonal to the principal
subspace, which, from (12.46), is just the average of the discarded eigenvalues. Now
suppose that v = ui where ui is one of the retained eigenvectors deﬁning the prin-
cipal subspace. Then vTCv = (λi − σ2) + σ2 = λi. In other words, this model
correctly captures the variance of the data along the principal axes, and approximates
the variance in all remaining directions with a single average value σ2.

One way to construct the maximum likelihood density model would simply be
to ﬁnd the eigenvectors and eigenvalues of the data covariance matrix and then to
evaluate W and σ2 using the results given above. In this case, we would choose
R = I for convenience. However, if the maximum likelihood solution is found by
numerical optimization of the likelihood function, for instance using an algorithm
such as conjugate gradients (Fletcher, 1987; Nocedal and Wright, 1999; Bishop and
Nabney, 2008) or through the EM algorithm, then the resulting value of R is es-
sentially arbitrary. This implies that the columns of W need not be orthogonal. If
an orthogonal basis is required, the matrix W can be post-processed appropriately
(Golub and Van Loan, 1996). Alternatively, the EM algorithm can be modiﬁed in
such a way as to yield orthonormal principal directions, sorted in descending order
of the corresponding eigenvalues, directly (Ahn and Oh, 2003).

Section 12.2.2

576

12. CONTINUOUS LATENT VARIABLES

The rotational invariance in latent space represents a form of statistical noniden-
tiﬁability, analogous to that encountered for mixture models in the case of discrete
latent variables. Here there is a continuum of parameters all of which lead to the
same predictive density, in contrast to the discrete nonidentiﬁability associated with
component re-labelling in the mixture setting.

If we consider the case of M = D, so that there is no reduction of dimension-
ality, then UM = U and LM = L. Making use of the orthogonality properties
UUT = I and RRT = I, we see that the covariance C of the marginal distribution
for x becomes

C = U(L − σ2I)1/2RRT(L − σ2I)1/2UT + σ2I = ULUT = S

(12.47)

and so we obtain the standard maximum likelihood solution for an unconstrained
Gaussian distribution in which the covariance matrix is given by the sample covari-
ance.

Conventional PCA is generally formulated as a projection of points from the D-
dimensional data space onto an M-dimensional linear subspace. Probabilistic PCA,
however, is most naturally expressed as a mapping from the latent space into the data
space via (12.33). For applications such as visualization and data compression, we
can reverse this mapping using Bayes’ theorem. Any point x in data space can then
be summarized by its posterior mean and covariance in latent space. From (12.42)
the mean is given by

E[z|x] = M−1WT

ML(x − x)

where M is given by (12.41). This projects to a point in data space given by

WE[z|x] + µ.

(12.48)

(12.49)

Section 3.3.1

Note that this takes the same form as the equations for regularized linear regression
and is a consequence of maximizing the likelihood function for a linear Gaussian
model. Similarly, the posterior covariance is given from (12.42) by σ2M−1 and is
independent of x.

If we take the limit σ2 → 0, then the posterior mean reduces to

MLWML)−1WT

ML(x − x)

(WT

(12.50)

Exercise 12.11

Exercise 12.12

Section 2.3

which represents an orthogonal projection of the data point onto the latent space,
and so we recover the standard PCA model. The posterior covariance in this limit is
zero, however, and the density becomes singular. For σ2 > 0, the latent projection
is shifted towards the origin, relative to the orthogonal projection.

Finally, we note that an important role for the probabilistic PCA model is in
deﬁning a multivariate Gaussian distribution in which the number of degrees of free-
dom, in other words the number of independent parameters, can be controlled whilst
still allowing the model to capture the dominant correlations in the data. Recall
that a general Gaussian distribution has D(D + 1)/2 independent parameters in its
covariance matrix (plus another D parameters in its mean). Thus the number of
parameters scales quadratically with D and can become excessive in spaces of high

12.2. Probabilistic PCA

577

dimensionality. If we restrict the covariance matrix to be diagonal, then it has only D
independent parameters, and so the number of parameters now grows linearly with
dimensionality. However, it now treats the variables as if they were independent and
hence can no longer express any correlations between them. Probabilistic PCA pro-
vides an elegant compromise in which the M most signiﬁcant correlations can be
captured while still ensuring that the total number of parameters grows only linearly
with D. We can see this by evaluating the number of degrees of freedom in the
PPCA model as follows. The covariance matrix C depends on the parameters W,
which has size D× M, and σ2, giving a total parameter count of DM +1. However,
we have seen that there is some redundancy in this parameterization associated with
rotations of the coordinate system in the latent space. The orthogonal matrix R that
expresses these rotations has size M × M. In the ﬁrst column of this matrix there are
M − 1 independent parameters, because the column vector must be normalized to
unit length. In the second column there are M − 2 independent parameters, because
the column must be normalized and also must be orthogonal to the previous column,
and so on. Summing this arithmetic series, we see that R has a total of M(M −1)/2
independent parameters. Thus the number of degrees of freedom in the covariance
matrix C is given by

(12.51)
The number of independent parameters in this model therefore only grows linearly
with D, for ﬁxed M. If we take M = D − 1, then we recover the standard result
for a full covariance Gaussian. In this case, the variance along D − 1 linearly in-
dependent directions is controlled by the columns of W, and the variance along the
remaining direction is given by σ2. If M = 0, the model is equivalent to the isotropic
covariance case.

DM + 1 − M(M − 1)/2.

12.2.2 EM algorithm for PCA
As we have seen, the probabilistic PCA model can be expressed in terms of a
marginalization over a continuous latent space z in which for each data point xn,
there is a corresponding latent variable zn. We can therefore make use of the EM
algorithm to ﬁnd maximum likelihood estimates of the model parameters. This may
seem rather pointless because we have already obtained an exact closed-form so-
lution for the maximum likelihood parameter values. However, in spaces of high
dimensionality, there may be computational advantages in using an iterative EM
procedure rather than working directly with the sample covariance matrix. This EM
procedure can also be extended to the factor analysis model, for which there is no
closed-form solution. Finally, it allows missing data to be handled in a principled
way.

We can derive the EM algorithm for probabilistic PCA by following the general
framework for EM. Thus we write down the complete-data log likelihood and take
its expectation with respect to the posterior distribution of the latent distribution
evaluated using ‘old’ parameter values. Maximization of this expected complete-
data log likelihood then yields the ‘new’ parameter values. Because the data points

Exercise 12.14

Section 12.2.4

Section 9.4

578

12. CONTINUOUS LATENT VARIABLES

are assumed independent, the complete-data log likelihood function takes the form

ln p

X, Z|µ, W, σ2

=

{ln p(xn|zn) + ln p(zn)}

(12.52)

(cid:10)

(cid:11)

N(cid:2)

n=1

where the nth row of the matrix Z is given by zn. We already know that the exact
maximum likelihood solution for µ is given by the sample mean x deﬁned by (12.1),
and it is convenient to substitute for µ at this stage. Making use of the expressions
(12.31) and (12.32) for the latent and conditional distributions, respectively, and tak-
ing the expectation with respect to the posterior distribution over the latent variables,
we obtain

(cid:10)

(cid:11)

D
2

ln(2πσ2) +

1
2 Tr

E[znzT
n]

E[ln p

X, Z|µ, W, σ2

(cid:10)

(cid:10)

+

+

1
2σ2
1
2σ2 Tr

(cid:12)

(cid:11)

] = − N(cid:2)
(cid:11)(cid:13)

n=1

(cid:5)xn − µ(cid:5)2 − 1
σ2

E[zn]TWT(xn − µ)

E[znzT

n]WTW

.

(12.53)

Exercise 12.15

Note that this depends on the posterior distribution only through the sufﬁcient statis-
tics of the Gaussian. Thus in the E step, we use the old parameter values to evaluate

E[zn] = M−1WT(xn − x)

n] = σ2M−1 + E[zn]E[zn]T

E[znzT

(12.54)
(12.55)

which follow directly from the posterior distribution (12.42) together with the stan-
dard result E[znzT

n] = cov[zn] + E[zn]E[zn]T. Here M is deﬁned by (12.41).

In the M step, we maximize with respect to W and σ2, keeping the posterior
statistics ﬁxed. Maximization with respect to σ2 is straightforward. For the maxi-
mization with respect to W we make use of (C.24), and obtain the M-step equations

(xn − x)E[zn]T

 (cid:31)

N(cid:2)

 −1
(cid:26)(cid:5)xn − x(cid:5)2 − 2E[zn]TWT

E[znzT
n]

n=1

(cid:11)(cid:27)

(cid:31)

N(cid:2)

n=1

1
N D

+Tr

N(cid:2)
(cid:10)

n=1
E[znzT

new(xn − x)

n]WT

newWnew

.

(12.57)

(12.56)

Wnew =

σ2
new =

The EM algorithm for probabilistic PCA proceeds by initializing the parameters
and then alternately computing the sufﬁcient statistics of the latent space posterior
distribution using (12.54) and (12.55) in the E step and revising the parameter values
using (12.56) and (12.57) in the M step.

One of the beneﬁts of the EM algorithm for PCA is computational efﬁciency
for large-scale applications (Roweis, 1998). Unlike conventional PCA based on an

12.2. Probabilistic PCA

579

eigenvector decomposition of the sample covariance matrix, the EM approach is
iterative and so might appear to be less attractive. However, each cycle of the EM
algorithm can be computationally much more efﬁcient than conventional PCA in
spaces of high dimensionality. To see this, we note that the eigendecomposition of
the covariance matrix requires O(D3) computation. Often we are interested only
in the ﬁrst M eigenvectors and their corresponding eigenvalues, in which case we
can use algorithms that are O(M D2). However, the evaluation of the covariance
matrix itself takes O(N D2) computations, where N is the number of data points.
Algorithms such as the snapshot method (Sirovich, 1987), which assume that the
eigenvectors are linear combinations of the data vectors, avoid direct evaluation of
the covariance matrix but are O(N 3) and hence unsuited to large data sets. The EM
algorithm described here also does not construct the covariance matrix explicitly.
Instead, the most computationally demanding steps are those involving sums over
the data set that are O(N DM). For large D, and M (cid:13) D, this can be a signiﬁcant
saving compared to O(N D2) and can offset the iterative nature of the EM algorithm.
Note that this EM algorithm can be implemented in an on-line form in which
each D-dimensional data point is read in and processed and then discarded before
the next data point is considered. To see this, note that the quantities evaluated in
the E step (an M-dimensional vector and an M × M matrix) can be computed for
each data point separately, and in the M step we need to accumulate sums over data
points, which we can do incrementally. This approach can be advantageous if both
N and D are large.

Because we now have a fully probabilistic model for PCA, we can deal with
missing data, provided that it is missing at random, by marginalizing over the dis-
tribution of the unobserved variables. Again these missing values can be treated
using the EM algorithm. We give an example of the use of this approach for data
visualization in Figure 12.11.
Another elegant feature of the EM approach is that we can take the limit σ2 → 0,
corresponding to standard PCA, and still obtain a valid EM-like algorithm (Roweis,
1998). From (12.55), we see that the only quantity we need to compute in the E step
is E[zn]. Furthermore, the M step is simpliﬁed because M = WTW. To emphasize
nth row is given by the vector xn − x and similarly deﬁne Ω to be a matrix of size
D × M whose nth row is given by the vector E[zn]. The E step (12.54) of the EM
algorithm for PCA then becomes

the simplicity of the algorithm, let us deﬁne (cid:4)X to be a matrix of size N × D whose

and the M step (12.56) takes the form

old

Ω = (WT

oldWold)−1WT

(cid:4)X
Wnew = (cid:4)XTΩT(ΩΩT)−1.

(12.58)

(12.59)

Again these can be implemented in an on-line form. These equations have a simple
interpretation as follows. From our earlier discussion, we see that the E step involves
an orthogonal projection of the data points onto the current estimate for the principal
subspace. Correspondingly, the M step represents a re-estimation of the principal

580

12. CONTINUOUS LATENT VARIABLES

Figure 12.11 Probabilistic PCA visualization of a portion of the oil ﬂow data set for the ﬁrst 100 data points. The
left-hand plot shows the posterior mean projections of the data points on the principal subspace. The right-hand
plot is obtained by ﬁrst randomly omitting 30% of the variable values and then using EM to handle the missing
values. Note that each data point then has at least one missing measurement but that the plot is very similar to
the one obtained without missing values.

Exercise 12.17

subspace to minimize the squared reconstruction error in which the projections are
ﬁxed.

We can give a simple physical analogy for this EM algorithm, which is easily
visualized for D = 2 and M = 1. Consider a collection of data points in two
dimensions, and let the one-dimensional principal subspace be represented by a solid
rod. Now attach each data point to the rod via a spring obeying Hooke’s law (stored
energy is proportional to the square of the spring’s length). In the E step, we keep
the rod ﬁxed and allow the attachment points to slide up and down the rod so as to
minimize the energy. This causes each attachment point (independently) to position
itself at the orthogonal projection of the corresponding data point onto the rod. In
the M step, we keep the attachment points ﬁxed and then release the rod and allow it
to move to the minimum energy position. The E and M steps are then repeated until
a suitable convergence criterion is satisﬁed, as is illustrated in Figure 12.12.

12.2.3 Bayesian PCA
So far in our discussion of PCA, we have assumed that the value M for the
dimensionality of the principal subspace is given. In practice, we must choose a
suitable value according to the application. For visualization, we generally choose
M = 2, whereas for other applications the appropriate choice for M may be less
clear. One approach is to plot the eigenvalue spectrum for the data set, analogous
to the example in Figure 12.4 for the off-line digits data set, and look to see if the
eigenvalues naturally form two groups comprising a set of small values separated by
a signiﬁcant gap from a set of relatively large values, indicating a natural choice for
M. In practice, such a gap is often not seen.

12.2. Probabilistic PCA

581

(a)

−2

(d)

2

0

−2

2

0

−2

0

2

(b)

−2

(e)

2

0

−2

2

0

−2

0

2

(c)

−2

(f)

2

0

−2

2

0

−2

0

2

−2

0

2

−2

0

2

−2

0

2

Figure 12.12 Synthetic data illustrating the EM algorithm for PCA deﬁned by (12.58) and (12.59). (a) A data
set X with the data points shown in green, together with the true principal components (shown as eigenvectors
scaled by the square roots of the eigenvalues). (b) Initial conﬁguration of the principal subspace deﬁned by W,
shown in red, together with the projections of the latent points Z into the data space, given by ZWT, shown in
cyan. (c) After one M step, the latent space has been updated with Z held ﬁxed. (d) After the successive E step,
the values of Z have been updated, giving orthogonal projections, with W held ﬁxed. (e) After the second M
step. (f) After the second E step.

Section 1.3

Because the probabilistic PCA model has a well-deﬁned likelihood function, we
could employ cross-validation to determine the value of dimensionality by selecting
the largest log likelihood on a validation data set. Such an approach, however, can
become computationally costly, particularly if we consider a probabilistic mixture
of PCA models (Tipping and Bishop, 1999a) in which we seek to determine the
appropriate dimensionality separately for each component in the mixture.

Given that we have a probabilistic formulation of PCA, it seems natural to seek
a Bayesian approach to model selection. To do this, we need to marginalize out
the model parameters µ, W, and σ2 with respect to appropriate prior distributions.
This can be done by using a variational framework to approximate the analytically
intractable marginalizations (Bishop, 1999b). The marginal likelihood values, given
by the variational lower bound, can then be compared for a range of different values
of M and the value giving the largest marginal likelihood selected.

Here we consider a simpler approach introduced by based on the evidence ap-

582

12. CONTINUOUS LATENT VARIABLES

Figure 12.13 Probabilistic graphical model for Bayesian PCA in
which the distribution over the parameter matrix W
is governed by a vector α of hyperparameters.

σ2

zn

α

µ

xn

N

W

proximation, which is appropriate when the number of data points is relatively large
and the corresponding posterior distribution is tightly peaked (Bishop, 1999a). It
involves a speciﬁc choice of prior over W that allows surplus dimensions in the
principal subspace to be pruned out of the model. This corresponds to an example of
automatic relevance determination, or ARD, discussed in Section 7.2.2. Speciﬁcally,
we deﬁne an independent Gaussian prior over each column of W, which represent
the vectors deﬁning the principal subspace. Each such Gaussian has an independent
variance governed by a precision hyperparameter αi so that

(cid:18)D/2

(cid:12)

M(cid:14)

(cid:17)

i=1

αi
2π

(cid:13)

p(W|α) =

exp

−1
2 αiwT

i wi

(12.60)

where wi is the ith column of W. The resulting model can be represented using the
directed graph shown in Figure 12.13.

The values for αi will be found iteratively by maximizing the marginal likeli-
hood function in which W has been integrated out. As a result of this optimization,
some of the αi may be driven to inﬁnity, with the corresponding parameters vec-
tor wi being driven to zero (the posterior distribution becomes a delta function at
the origin) giving a sparse solution. The effective dimensionality of the principal
subspace is then determined by the number of ﬁnite αi values, and the correspond-
ing vectors wi can be thought of as ‘relevant’ for modelling the data distribution.
In this way, the Bayesian approach is automatically making the trade-off between
improving the ﬁt to the data, by using a larger number of vectors wi with their cor-
responding eigenvalues λi each tuned to the data, and reducing the complexity of
the model by suppressing some of the wi vectors. The origins of this sparsity were
discussed earlier in the context of relevance vector machines.

The values of αi are re-estimated during training by maximizing the log marginal

likelihood given by

p(X|α, µ, σ2) =

p(X|W, µ, σ2)p(W|α) dW

(12.61)
where the log of p(X|W, µ, σ2) is given by (12.43). Note that for simplicity we also
treat µ and σ2 as parameters to be estimated, rather than deﬁning priors over these
parameters.

(cid:6)

Section 7.2

12.2. Probabilistic PCA

583

Section 4.4

Section 3.5.3

Because this integration is intractable, we make use of the Laplace approxima-
tion. If we assume that the posterior distribution is sharply peaked, as will occur for
sufﬁciently large data sets, then the re-estimation equations obtained by maximizing
the marginal likelihood with respect to αi take the simple form

i = D
αnew
i wi
wT

(12.62)

which follows from (3.98), noting that the dimensionality of wi is D. These re-
estimations are interleaved with the EM algorithm updates for determining W and
σ2. The E-step equations are again given by (12.54) and (12.55). Similarly, the M-
step equation for σ2 is again given by (12.57). The only change is to the M-step
equation for W, which is modiﬁed to give

(cid:31)

N(cid:2)

 (cid:31)

N(cid:2)

 −1

Wnew =

(xn − x)E[zn]T

E[znzT

n] + σ2A

(12.63)

n=1

n=1

where A = diag(αi). The value of µ is given by the sample mean, as before.

If we choose M = D − 1 then, if all αi values are ﬁnite, the model represents
a full-covariance Gaussian, while if all the αi go to inﬁnity the model is equivalent
to an isotropic Gaussian, and so the model can encompass all permissible values for
the effective dimensionality of the principal subspace. It is also possible to consider
smaller values of M, which will save on computational cost but which will limit
the maximum dimensionality of the subspace. A comparison of the results of this
algorithm with standard probabilistic PCA is shown in Figure 12.14.

Bayesian PCA provides an opportunity to illustrate the Gibbs sampling algo-
rithm discussed in Section 11.3. Figure 12.15 shows an example of the samples
from the hyperparameters ln αi for a data set in D = 4 dimensions in which the di-
mensionality of the latent space is M = 3 but in which the data set is generated from
a probabilistic PCA model having one direction of high variance, with the remaining
directions comprising low variance noise. This result shows clearly the presence of
three distinct modes in the posterior distribution. At each step of the iteration, one of
the hyperparameters has a small value and the remaining two have large values, so
that two of the three latent variables are suppressed. During the course of the Gibbs
sampling, the solution makes sharp transitions between the three modes.

The model described here involves a prior only over the matrix W. A fully
Bayesian treatment of PCA, including priors over µ, σ2, and α, and solved us-
ing variational methods, is described in Bishop (1999b). For a discussion of vari-
ous Bayesian approaches to determining the appropriate dimensionality for a PCA
model, see Minka (2001c).

12.2.4 Factor analysis
Factor analysis is a linear-Gaussian latent variable model that is closely related
to probabilistic PCA. Its deﬁnition differs from that of probabilistic PCA only in that
the conditional distribution of the observed variable x given the latent variable z is

584

12. CONTINUOUS LATENT VARIABLES

Figure 12.14 ‘Hinton’ diagrams of the matrix W in which each element of the matrix is depicted as
a square (white for positive and black for negative values) whose area is proportional
to the magnitude of that element. The synthetic data set comprises 300 data points in
D = 10 dimensions sampled from a Gaussian distribution having standard deviation 1.0
in 3 directions and standard deviation 0.5 in the remaining 7 directions for a data set in
D = 10 dimensions having M = 3 directions with larger variance than the remaining 7
directions. The left-hand plot shows the result from maximum likelihood probabilistic PCA,
and the left-hand plot shows the corresponding result from Bayesian PCA. We see how
the Bayesian model is able to discover the appropriate dimensionality by suppressing the
6 surplus degrees of freedom.

taken to have a diagonal rather than an isotropic covariance so that

p(x|z) = N (x|Wz + µ, Ψ)

(12.64)
where Ψ is a D×D diagonal matrix. Note that the factor analysis model, in common
with probabilistic PCA, assumes that the observed variables x1, . . . , xD are indepen-
dent, given the latent variable z. In essence, the factor analysis model is explaining
the observed covariance structure of the data by representing the independent vari-
ance associated with each coordinate in the matrix Ψ and capturing the covariance
between variables in the matrix W. In the factor analysis literature, the columns
of W, which capture the correlations between observed variables, are called factor
loadings, and the diagonal elements of Ψ, which represent the independent noise
variances for each of the variables, are called uniquenesses.

The origins of factor analysis are as old as those of PCA, and discussions of
factor analysis can be found in the books by Everitt (1984), Bartholomew (1987),
and Basilevsky (1994). Links between factor analysis and PCA were investigated
by Lawley (1953) and Anderson (1963) who showed that at stationary points of
the likelihood function, for a factor analysis model with Ψ = σ2I, the columns of
W are scaled eigenvectors of the sample covariance matrix, and σ2 is the average
of the discarded eigenvalues. Later, Tipping and Bishop (1999b) showed that the
maximum of the log likelihood function occurs when the eigenvectors comprising
W are chosen to be the principal eigenvectors.

Making use of (2.115), we see that the marginal distribution for the observed

12.2. Probabilistic PCA

585

Figure 12.15 Gibbs sampling for Bayesian
ln αi
PCA showing plots of
for
versus iteration number
three α values,
showing
transitions
the
three modes of the posterior
distribution.

between

10

5

0
10

5

0
10

5

0

Exercise 12.19

Section 12.4

Exercise 12.21

variable is given by p(x) = N (x|µ, C) where now
C = WWT + Ψ.

(12.65)

As with probabilistic PCA, this model is invariant to rotations in the latent space.

Historically, factor analysis has been the subject of controversy when attempts
have been made to place an interpretation on the individual factors (the coordinates
in z-space), which has proven problematic due to the nonidentiﬁability of factor
analysis associated with rotations in this space. From our perspective, however, we
shall view factor analysis as a form of latent variable density model, in which the
form of the latent space is of interest but not the particular choice of coordinates
used to describe it. If we wish to remove the degeneracy associated with latent space
rotations, we must consider non-Gaussian latent variable distributions, giving rise to
independent component analysis (ICA) models.

We can determine the parameters µ, W, and Ψ in the factor analysis model by
maximum likelihood. The solution for µ is again given by the sample mean. How-
ever, unlike probabilistic PCA, there is no longer a closed-form maximum likelihood
solution for W, which must therefore be found iteratively. Because factor analysis is
a latent variable model, this can be done using an EM algorithm (Rubin and Thayer,
1982) that is analogous to the one used for probabilistic PCA. Speciﬁcally, the E-step
equations are given by

E[zn] = GWTΨ

−1(xn − x)

E[znzT

n] = G + E[zn]E[zn]T

(12.66)
(12.67)

where we have deﬁned
(12.68)
Note that this is expressed in a form that involves inversion of matrices of size M×M
rather than D × D (except for the D × D diagonal matrix Ψ whose inverse is trivial

G = (I + WTΨ

−1W)−1.

586

12. CONTINUOUS LATENT VARIABLES

Exercise 12.22

Exercise 12.25

to compute in O(D) steps), which is convenient because often M (cid:13) D. Similarly,
the M-step equations take the form

 −1

Wnew =

(xn − x)E[zn]T

Ψnew = diag

S − Wnew

1
N

E[znzT
n]

n=1

E[zn](xn − x)T

(cid:25)

(12.69)

(12.70)

(cid:31)

N(cid:2)
(cid:24)

n=1

N(cid:2)

 (cid:31)
N(cid:2)

n=1

where the ‘diag’ operator sets all of the nondiagonal elements of a matrix to zero. A
Bayesian treatment of the factor analysis model can be obtained by a straightforward
application of the techniques discussed in this book.

Another difference between probabilistic PCA and factor analysis concerns their
different behaviour under transformations of the data set. For PCA and probabilis-
tic PCA, if we rotate the coordinate system in data space, then we obtain exactly
the same ﬁt to the data but with the W matrix transformed by the corresponding
rotation matrix. However, for factor analysis, the analogous property is that if we
make a component-wise re-scaling of the data vectors, then this is absorbed into a
corresponding re-scaling of the elements of Ψ.

12.3. Kernel PCA

In Chapter 6, we saw how the technique of kernel substitution allows us to take an
algorithm expressed in terms of scalar products of the form xTx(cid:4)
and generalize
that algorithm by replacing the scalar products with a nonlinear kernel. Here we
apply this technique of kernel substitution to principal component analysis, thereby
obtaining a nonlinear generalization called kernel PCA (Sch¨olkopf et al., 1998).
Consider a data set {xn} of observations, where n = 1, . . . , N, in a space of
(cid:5)
dimensionality D. In order to keep the notation uncluttered, we shall assume that
we have already subtracted the sample mean from each of the vectors xn, so that
n xn = 0. The ﬁrst step is to express conventional PCA in such a form that the
data vectors {xn} appear only in the form of the scalar products xT
nxm. Recall that
the principal components are deﬁned by the eigenvectors ui of the covariance matrix

where i = 1, . . . , D. Here the D × D sample covariance matrix S is deﬁned by

Sui = λiui

(12.71)

N(cid:2)

n=1

S =

1
N

xnxT
n,

(12.72)

and the eigenvectors are normalized such that uT

i ui = 1.

Now consider a nonlinear transformation φ(x) into an M-dimensional feature
space, so that each data point xn is thereby projected onto a point φ(xn). We can

x2

12.3. Kernel PCA

587

φ1

v1

x1

φ2

Figure 12.16 Schematic illustration of kernel PCA. A data set in the original data space (left-hand plot) is
projected by a nonlinear transformation φ(x) into a feature space (right-hand plot). By performing PCA in the
feature space, we obtain the principal components, of which the ﬁrst is shown in blue and is denoted by the
vector v1. The green lines in feature space indicate the linear projections onto the ﬁrst principal component,
which correspond to nonlinear projections in the original data space. Note that in general it is not possible to
represent the nonlinear principal component by a vector in x space.

(cid:5)
now perform standard PCA in the feature space, which implicitly deﬁnes a nonlinear
principal component model in the original data space, as illustrated in Figure 12.16.
For the moment, let us assume that the projected data set also has zero mean,
n φ(xn) = 0. We shall return to this point shortly. The M × M sample

so that
covariance matrix in feature space is given by

φ(xn)φ(xn)T

(12.73)

N(cid:2)

n=1

C =

1
N

and its eigenvector expansion is deﬁned by

(12.74)
i = 1, . . . , M. Our goal is to solve this eigenvalue problem without having to work
explicitly in the feature space. From the deﬁnition of C, the eigenvector equations
tells us that vi satisﬁes

Cvi = λivi

φ(xn)

φ(xn)Tvi

= λivi

(12.75)

and so we see that (provided λi > 0) the vector vi is given by a linear combination
of the φ(xn) and so can be written in the form

N(cid:2)

n=1

1
N

(cid:27)

(cid:26)

N(cid:2)

n=1

vi =

ainφ(xn).

(12.76)

588

12. CONTINUOUS LATENT VARIABLES

Exercise 12.26

Substituting this expansion back into the eigenvector equation, we obtain

φ(xn)φ(xn)T

aimφ(xm) = λi

ainφ(xn).

(12.77)

n=1

m=1

n=1

The key step is now to express this in terms of the kernel function k(xn, xm) =

φ(xn)Tφ(xm), which we do by multiplying both sides by φ(xl)T to give

N(cid:2)

N(cid:2)

1
N

1
N

N(cid:2)

m(cid:2)

N(cid:2)

N(cid:2)

k(xl, xn)

aimk(xn, xm) = λi

aink(xl, xn).

(12.78)

n=1

m=1

n=1

This can be written in matrix notation as

K2ai = λiNKai

(12.79)

where ai is an N-dimensional column vector with elements ani for n = 1, . . . , N.
We can ﬁnd solutions for ai by solving the following eigenvalue problem

Kai = λiNai

(12.80)

in which we have removed a factor of K from both sides of (12.79). Note that
the solutions of (12.79) and (12.80) differ only by eigenvectors of K having zero
eigenvalues that do not affect the principal components projection.

The normalization condition for the coefﬁcients ai is obtained by requiring that
the eigenvectors in feature space be normalized. Using (12.76) and (12.80), we have

1 = vT

i vi =

ainaimφ(xn)Tφ(xm) = aT

i Kai = λiNaT

i ai.

(12.81)

N(cid:2)

N(cid:2)

n=1

m=1

Having solved the eigenvector problem, the resulting principal component pro-
jections can then also be cast in terms of the kernel function so that, using (12.76),
the projection of a point x onto eigenvector i is given by

N(cid:2)

N(cid:2)

yi(x) = φ(x)Tvi =

ainφ(x)Tφ(xn) =

aink(x, xn)

(12.82)

n=1

n=1

and so again is expressed in terms of the kernel function.

In the original D-dimensional x space there are D orthogonal eigenvectors and
hence we can ﬁnd at most D linear principal components. The dimensionality M
of the feature space, however, can be much larger than D (even inﬁnite), and thus
we can ﬁnd a number of nonlinear principal components that can exceed D. Note,
however, that the number of nonzero eigenvalues cannot exceed the number N of
data points, because (even if M > N) the covariance matrix in feature space has
rank at most equal to N. This is reﬂected in the fact that kernel PCA involves the
eigenvector expansion of the N × N matrix K.

12.3. Kernel PCA

589

So far we have assumed that the projected data set given by φ(xn) has zero
mean, which in general will not be the case. We cannot simply compute and then
subtract off the mean, since we wish to avoid working directly in feature space, and
so again, we formulate the algorithm purely in terms of the kernel function. The

projected data points after centralizing, denoted(cid:4)φ(xn), are given by

φ(xl)

(12.83)

and the corresponding elements of the Gram matrix are given by

(cid:4)φ(xn) = φ(xn) − 1

N(cid:2)

N

l=1

(cid:4)Knm = (cid:4)φ(xn)T(cid:4)φ(xm)

= φ(xn)Tφ(xm) − 1
N

− 1
N

N(cid:2)

l=1

N(cid:2)

l=1

= k(xn, xm) − 1
N

N(cid:2)

l=1

φ(xn)Tφ(xl)

N(cid:2)

N(cid:2)

1
N 2

j=1

l=1

N(cid:2)

l=1

k(xl, xm)

N(cid:2)

N(cid:2)

φ(xl)Tφ(xm) +

φ(xj)Tφ(xl)

This can be expressed in matrix notation as

− 1
N

k(xn, xl) +

1
N 2

j=1

l=1

(cid:4)K = K − 1N K − K1N + 1N K1N

k(xj, xl).

(12.84)

(12.85)
where 1N denotes the N × N matrix in which every element takes the value 1/N.

Thus we can evaluate (cid:4)K using only the kernel function and then use (cid:4)K to determine

Exercise 12.27

the eigenvalues and eigenvectors. Note that the standard PCA algorithm is recovered
as a special case if we use a linear kernel k(x, x(cid:4)) = xTx(cid:4)
. Figure 12.17 shows an
example of kernel PCA applied to a synthetic data set (Sch¨olkopf et al., 1998). Here
a ‘Gaussian’ kernel of the form

k(x, x(cid:4)) = exp(−(cid:5)x − x(cid:4)(cid:5)2/0.1)

(12.86)

is applied to a synthetic data set. The lines correspond to contours along which the
projection onto the corresponding principal component, deﬁned by

N(cid:2)

φ(x)Tvi =

aink(x, xn)

(12.87)

is constant.

n=1

590

12. CONTINUOUS LATENT VARIABLES

Figure 12.17 Example of kernel PCA, with a Gaussian kernel applied to a synthetic data set in two dimensions,
showing the ﬁrst eight eigenfunctions along with their eigenvalues. The contours are lines along which the
projection onto the corresponding principal component is constant. Note how the ﬁrst two eigenvectors separate
the three clusters, the next three eigenvectors split each of the cluster into halves, and the following three
eigenvectors again split the clusters into halves along directions orthogonal to the previous splits.

One obvious disadvantage of kernel PCA is that it involves ﬁnding the eigenvec-

tors of the N × N matrix (cid:4)K rather than the D × D matrix S of conventional linear
(cid:1)xn onto the L-dimensional principal subspace, deﬁned by

Finally, we note that in standard linear PCA, we often retain some reduced num-
ber L < D of eigenvectors and then approximate a data vector xn by its projection

PCA, and so in practice for large data sets approximations are often used.

xT
nui

ui.

(12.88)

i=1

In kernel PCA, this will in general not be possible. To see this, note that the map-
ping φ(x) maps the D-dimensional x space into a D-dimensional manifold in the
M-dimensional feature space φ. The vector x is known as the pre-image of the
corresponding point φ(x). However, the projection of points in feature space onto
the linear PCA subspace in that space will typically not lie on the nonlinear D-
dimensional manifold and so will not have a corresponding pre-image in data space.
Techniques have therefore been proposed for ﬁnding approximate pre-images (Bakir
et al., 2004).

L(cid:2)

(cid:10)

(cid:1)xn =

(cid:11)

12.4. Nonlinear Latent Variable Models

591

12.4. Nonlinear Latent Variable Models

In this chapter, we have focussed on the simplest class of models having continuous
latent variables, namely those based on linear-Gaussian distributions. As well as
having great practical importance, these models are relatively easy to analyse and
to ﬁt to data and can also be used as components in more complex models. Here
we consider brieﬂy some generalizations of this framework to models that are either
nonlinear or non-Gaussian, or both.

In fact, the issues of nonlinearity and non-Gaussianity are related because a
general probability density can be obtained from a simple ﬁxed reference density,
such as a Gaussian, by making a nonlinear change of variables. This idea forms the
basis of several practical latent variable models as we shall see shortly.

Exercise 12.28

12.4.1 Independent component analysis
We begin by considering models in which the observed variables are related
linearly to the latent variables, but for which the latent distribution is non-Gaussian.
An important class of such models, known as independent component analysis, or
ICA, arises when we consider a distribution over the latent variables that factorizes,
so that

M(cid:14)

p(z) =

p(zj).

(12.89)

j=1

To understand the role of such models, consider a situation in which two people
are talking at the same time, and we record their voices using two microphones.
If we ignore effects such as time delay and echoes, then the signals received by
the microphones at any point in time will be given by linear combinations of the
amplitudes of the two voices. The coefﬁcients of this linear combination will be
constant, and if we can infer their values from sample data, then we can invert the
mixing process (assuming it is nonsingular) and thereby obtain two clean signals
each of which contains the voice of just one person. This is an example of a problem
called blind source separation in which ‘blind’ refers to the fact that we are given
only the mixed data, and neither the original sources nor the mixing coefﬁcients are
observed (Cardoso, 1998).

This type of problem is sometimes addressed using the following approach
(MacKay, 2003) in which we ignore the temporal nature of the signals and treat the
successive samples as i.i.d. We consider a generative model in which there are two
latent variables corresponding to the unobserved speech signal amplitudes, and there
are two observed variables given by the signal values at the microphones. The latent
variables have a joint distribution that factorizes as above, and the observed variables
are given by a linear combination of the latent variables. There is no need to include
a noise distribution because the number of latent variables equals the number of ob-
served variables, and therefore the marginal distribution of the observed variables
will not in general be singular, so the observed variables are simply deterministic
linear combinations of the latent variables. Given a data set of observations, the

592

12. CONTINUOUS LATENT VARIABLES

Exercise 12.29

likelihood function for this model is a function of the coefﬁcients in the linear com-
bination. The log likelihood can be maximized using gradient-based optimization
giving rise to a particular version of independent component analysis.

The success of this approach requires that the latent variables have non-Gaussian
distributions. To see this, recall that in probabilistic PCA (and in factor analysis) the
latent-space distribution is given by a zero-mean isotropic Gaussian. The model
therefore cannot distinguish between two different choices for the latent variables
where these differ simply by a rotation in latent space. This can be veriﬁed directly
by noting that the marginal density (12.35), and hence the likelihood function, is
unchanged if we make the transformation W → WR where R is an orthogonal
matrix satisfying RRT = I, because the matrix C given by (12.36) is itself invariant.
Extending the model to allow more general Gaussian latent distributions does not
change this conclusion because, as we have seen, such a model is equivalent to the
zero-mean isotropic Gaussian latent variable model.

Another way to see why a Gaussian latent variable distribution in a linear model
is insufﬁcient to ﬁnd independent components is to note that the principal compo-
nents represent a rotation of the coordinate system in data space such as to diagonal-
ize the covariance matrix, so that the data distribution in the new coordinates is then
uncorrelated. Although zero correlation is a necessary condition for independence
it is not, however, sufﬁcient. In practice, a common choice for the latent-variable
distribution is given by

p(zj) =

1

π cosh(zj)

=

1

π(ezj + e−zj )

(12.90)

which has heavy tails compared to a Gaussian, reﬂecting the observation that many
real-world distributions also exhibit this property.

The original ICA model (Bell and Sejnowski, 1995) was based on the optimiza-
tion of an objective function deﬁned by information maximization. One advantage
of a probabilistic latent variable formulation is that it helps to motivate and formu-
late generalizations of basic ICA. For instance, independent factor analysis (Attias,
1999a) considers a model in which the number of latent and observed variables can
differ, the observed variables are noisy, and the individual latent variables have ﬂex-
ible distributions modelled by mixtures of Gaussians. The log likelihood for this
model is maximized using EM, and the reconstruction of the latent variables is ap-
proximated using a variational approach. Many other types of model have been
considered, and there is now a huge literature on ICA and its applications (Jutten
and Herault, 1991; Comon et al., 1991; Amari et al., 1996; Pearlmutter and Parra,
1997; Hyv¨arinen and Oja, 1997; Hinton et al., 2001; Miskin and MacKay, 2001;
Hojen-Sorensen et al., 2002; Choudrey and Roberts, 2003; Chan et al., 2003; Stone,
2004).

12.4.2 Autoassociative neural networks
In Chapter 5 we considered neural networks in the context of supervised learn-
ing, where the role of the network is to predict the output variables given values

12.4. Nonlinear Latent Variable Models

593

Figure 12.18 An autoassociative multilayer perceptron having
two layers of weights. Such a network is trained to
map input vectors onto themselves by minimiza-
tion of a sum-of-squares error. Even with non-
linear units in the hidden layer, such a network
is equivalent to linear principal component anal-
ysis. Links representing bias parameters have
been omitted for clarity.

xD

zM

xD

inputs

outputs

x1

z1

x1

for the input variables. However, neural networks have also been applied to un-
supervised learning where they have been used for dimensionality reduction. This
is achieved by using a network having the same number of outputs as inputs, and
optimizing the weights so as to minimize some measure of the reconstruction error
between inputs and outputs with respect to a set of training data.

Consider ﬁrst a multilayer perceptron of the form shown in Figure 12.18, hav-
ing D inputs, D output units and M hidden units, with M < D. The targets used
to train the network are simply the input vectors themselves, so that the network
is attempting to map each input vector onto itself. Such a network is said to form
an autoassociative mapping. Since the number of hidden units is smaller than the
number of inputs, a perfect reconstruction of all input vectors is not in general pos-
sible. We therefore determine the network parameters w by minimizing an error
function which captures the degree of mismatch between the input vectors and their
reconstructions. In particular, we shall choose a sum-of-squares error of the form

N(cid:2)

n=1

E(w) =

1
2

(cid:5)y(xn, w) − xn(cid:5)2.

(12.91)

If the hidden units have linear activations functions, then it can be shown that the
error function has a unique global minimum, and that at this minimum the network
performs a projection onto the M-dimensional subspace which is spanned by the ﬁrst
M principal components of the data (Bourlard and Kamp, 1988; Baldi and Hornik,
1989). Thus, the vectors of weights which lead into the hidden units in Figure 12.18
form a basis set which spans the principal subspace. Note, however, that these vec-
tors need not be orthogonal or normalized. This result is unsurprising, since both
principal component analysis and the neural network are using linear dimensionality
reduction and are minimizing the same sum-of-squares error function.

It might be thought that the limitations of a linear dimensionality reduction could
be overcome by using nonlinear (sigmoidal) activation functions for the hidden units
in the network in Figure 12.18. However, even with nonlinear hidden units, the min-
imum error solution is again given by the projection onto the principal component
subspace (Bourlard and Kamp, 1988). There is therefore no advantage in using two-
layer neural networks to perform dimensionality reduction. Standard techniques for
principal component analysis (based on singular value decomposition) are guaran-
teed to give the correct solution in ﬁnite time, and they also generate an ordered set
of eigenvalues with corresponding orthonormal eigenvectors.

594

12. CONTINUOUS LATENT VARIABLES

Figure 12.19 Addition of extra hidden lay-
ers of nonlinear units gives an
autoassociative network which
can perform a nonlinear dimen-
sionality reduction.

xD

inputs

x1

F1

F2

non-linear

xD

outputs

x1

The situation is different, however, if additional hidden layers are permitted in
the network. Consider the four-layer autoassociative network shown in Figure 12.19.
Again the output units are linear, and the M units in the second hidden layer can also
be linear, however, the ﬁrst and third hidden layers have sigmoidal nonlinear activa-
tion functions. The network is again trained by minimization of the error function
(12.91). We can view this network as two successive functional mappings F1 and
F2, as indicated in Figure 12.19. The ﬁrst mapping F1 projects the original D-
dimensional data onto an M-dimensional subspace S deﬁned by the activations of
the units in the second hidden layer. Because of the presence of the ﬁrst hidden layer
of nonlinear units, this mapping is very general, and in particular is not restricted to
being linear. Similarly, the second half of the network deﬁnes an arbitrary functional
mapping from the M-dimensional space back into the original D-dimensional input
space. This has a simple geometrical interpretation, as indicated for the case D = 3
and M = 2 in Figure 12.20.

Such a network effectively performs a nonlinear principal component analysis.

x3

z2

F1

x3

F2

S

x1

z1

x1

x2

x2

Figure 12.20 Geometrical interpretation of the mappings performed by the network in Figure 12.19 for the case
of D = 3 inputs and M = 2 units in the middle hidden layer. The function F2 maps from an M-dimensional
space S into a D-dimensional space and therefore deﬁnes the way in which the space S is embedded within the
original x-space. Since the mapping F2 can be nonlinear, the embedding of S can be nonplanar, as indicated
in the ﬁgure. The mapping F1 then deﬁnes a projection of points in the original D-dimensional space into the
M-dimensional subspace S.

12.4. Nonlinear Latent Variable Models

595

It has the advantage of not being limited to linear transformations, although it con-
tains standard principal component analysis as a special case. However, training
the network now involves a nonlinear optimization problem, since the error function
(12.91) is no longer a quadratic function of the network parameters. Computation-
ally intensive nonlinear optimization techniques must be used, and there is the risk of
ﬁnding a suboptimal local minimum of the error function. Also, the dimensionality
of the subspace must be speciﬁed before training the network.

12.4.3 Modelling nonlinear manifolds
As we have already noted, many natural sources of data correspond to low-
dimensional, possibly noisy, nonlinear manifolds embedded within the higher di-
mensional observed data space. Capturing this property explicitly can lead to im-
proved density modelling compared with more general methods. Here we consider
brieﬂy a range of techniques that attempt to do this.

One way to model the nonlinear structure is through a combination of linear
models, so that we make a piece-wise linear approximation to the manifold. This can
be obtained, for instance, by using a clustering technique such as K-means based on
Euclidean distance to partition the data set into local groups with standard PCA ap-
plied to each group. A better approach is to use the reconstruction error for cluster
assignment (Kambhatla and Leen, 1997; Hinton et al., 1997) as then a common cost
function is being optimized in each stage. However, these approaches still suffer
from limitations due to the absence of an overall density model. By using prob-
abilistic PCA it is straightforward to deﬁne a fully probabilistic model simply by
considering a mixture distribution in which the components are probabilistic PCA
models (Tipping and Bishop, 1999a). Such a model has both discrete latent vari-
ables, corresponding to the discrete mixture, as well as continuous latent variables,
and the likelihood function can be maximized using the EM algorithm. A fully
Bayesian treatment, based on variational inference (Bishop and Winn, 2000), allows
the number of components in the mixture, as well as the effective dimensionalities
of the individual models, to be inferred from the data. There are many variants of
this model in which parameters such as the W matrix or the noise variances are tied
across components in the mixture, or in which the isotropic noise distributions are
replaced by diagonal ones, giving rise to a mixture of factor analysers (Ghahramani
and Hinton, 1996a; Ghahramani and Beal, 2000). The mixture of probabilistic PCA
models can also be extended hierarchically to produce an interactive data visualiza-
tion algorithm (Bishop and Tipping, 1998).

An alternative to considering a mixture of linear models is to consider a single
nonlinear model. Recall that conventional PCA ﬁnds a linear subspace that passes
close to the data in a least-squares sense. This concept can be extended to one-
dimensional nonlinear surfaces in the form of principal curves (Hastie and Stuetzle,
1989). We can describe a curve in a D-dimensional data space using a vector-valued
function f(λ), which is a vector each of whose elements is a function of the scalar λ.
There are many possible ways to parameterize the curve, of which a natural choice

is the arc length along the curve. For any given point(cid:1)x in data space, we can ﬁnd

the point on the curve that is closest in Euclidean distance. We denote this point by

596

12. CONTINUOUS LATENT VARIABLES

λ = gf (x) because it depends on the particular curve f(λ). For a continuous data
density p(x), a principal curve is deﬁned as one for which every point on the curve
is the mean of all those points in data space that project to it, so that

E [x|gf (x) = λ] = f(λ).

(12.92)

For a given continuous density, there can be many principal curves. In practice, we
are interested in ﬁnite data sets, and we also wish to restrict attention to smooth
curves. Hastie and Stuetzle (1989) propose a two-stage iterative procedure for ﬁnd-
ing such principal curves, somewhat reminiscent of the EM algorithm for PCA. The
curve is initialized using the ﬁrst principal component, and then the algorithm alter-
nates between a data projection step and curve re-estimation step. In the projection
step, each data point is assigned to a value of λ corresponding to the closest point
on the curve. Then in the re-estimation step, each point on the curve is given by
a weighted average of those points that project to nearby points on the curve, with
points closest on the curve given the greatest weight. In the case where the subspace
is constrained to be linear, the procedure converges to the ﬁrst principal component
and is equivalent to the power method for ﬁnding the largest eigenvector of the co-
variance matrix. Principal curves can be generalized to multidimensional manifolds
called principal surfaces although these have found limited use due to the difﬁculty
of data smoothing in higher dimensions even for two-dimensional manifolds.

PCA is often used to project a data set onto a lower-dimensional space, for ex-
ample two dimensional, for the purposes of visualization. Another linear technique
with a similar aim is multidimensional scaling, or MDS (Cox and Cox, 2000). It ﬁnds
a low-dimensional projection of the data such as to preserve, as closely as possible,
the pairwise distances between data points, and involves ﬁnding the eigenvectors of
the distance matrix. In the case where the distances are Euclidean, it gives equivalent
results to PCA. The MDS concept can be extended to a wide variety of data types
speciﬁed in terms of a similarity matrix, giving nonmetric MDS.

Two other nonprobabilistic methods for dimensionality reduction and data vi-
sualization are worthy of mention. Locally linear embedding, or LLE (Roweis and
Saul, 2000) ﬁrst computes the set of coefﬁcients that best reconstructs each data
point from its neighbours. These coefﬁcients are arranged to be invariant to rota-
tions, translations, and scalings of that data point and its neighbours, and hence they
characterize the local geometrical properties of the neighbourhood. LLE then maps
the high-dimensional data points down to a lower dimensional space while preserv-
ing these neighbourhood coefﬁcients.
If the local neighbourhood for a particular
data point can be considered linear, then the transformation can be achieved using
a combination of translation, rotation, and scaling, such as to preserve the angles
formed between the data points and their neighbours. Because the weights are in-
variant to these transformations, we expect the same weight values to reconstruct the
data points in the low-dimensional space as in the high-dimensional data space. In
spite of the nonlinearity, the optimization for LLE does not exhibit local minima.

In isometric feature mapping, or isomap (Tenenbaum et al., 2000), the goal is
to project the data to a lower-dimensional space using MDS, but where the dissim-
ilarities are deﬁned in terms of the geodesic distances measured along the mani-

12.4. Nonlinear Latent Variable Models

597

fold. For instance, if two points lie on a circle, then the geodesic is the arc-length
distance measured around the circumference of the circle not the straight line dis-
tance measured along the chord connecting them. The algorithm ﬁrst deﬁnes the
neighbourhood for each data point, either by ﬁnding the K nearest neighbours or by
ﬁnding all points within a sphere of radius . A graph is then constructed by link-
ing all neighbouring points and labelling them with their Euclidean distance. The
geodesic distance between any pair of points is then approximated by the sum of
the arc lengths along the shortest path connecting them (which itself is found using
standard algorithms). Finally, metric MDS is applied to the geodesic distance matrix
to ﬁnd the low-dimensional projection.

Our focus in this chapter has been on models for which the observed vari-
ables are continuous. We can also consider models having continuous latent vari-
ables together with discrete observed variables, giving rise to latent trait models
(Bartholomew, 1987). In this case, the marginalization over the continuous latent
variables, even for a linear relationship between latent and observed variables, can-
not be performed analytically, and so more sophisticated techniques are required.
Tipping (1999) uses variational inference in a model with a two-dimensional latent
space, allowing a binary data set to be visualized analogously to the use of PCA to
visualize continuous data. Note that this model is the dual of the Bayesian logistic
regression problem discussed in Section 4.5. In the case of logistic regression we
have N observations of the feature vector φn which are parameterized by a single
parameter vector w, whereas in the latent space visualization model there is a single
latent space variable x (analogous to φ) and N copies of the latent variable wn. A
generalization of probabilistic latent variable models to general exponential family
distributions is described in Collins et al. (2002).

We have already noted that an arbitrary distribution can be formed by taking a
Gaussian random variable and transforming it through a suitable nonlinearity. This
is exploited in a general latent variable model called a density network (MacKay,
1995; MacKay and Gibbs, 1999) in which the nonlinear function is governed by a
multilayered neural network. If the network has enough hidden units, it can approx-
imate a given nonlinear function to any desired accuracy. The downside of having
such a ﬂexible model is that the marginalization over the latent variables, required in
order to obtain the likelihood function, is no longer analytically tractable. Instead,
the likelihood is approximated using Monte Carlo techniques by drawing samples
from the Gaussian prior. The marginalization over the latent variables then becomes
a simple sum with one term for each sample. However, because a large number
of sample points may be required in order to give an accurate representation of the
marginal, this procedure can be computationally costly.

If we consider more restricted forms for the nonlinear function, and make an ap-
propriate choice of the latent variable distribution, then we can construct a latent vari-
able model that is both nonlinear and efﬁcient to train. The generative topographic
mapping, or GTM (Bishop et al., 1996; Bishop et al., 1997a; Bishop et al., 1998b)
uses a latent distribution that is deﬁned by a ﬁnite regular grid of delta functions over
the (typically two-dimensional) latent space. Marginalization over the latent space
then simply involves summing over the contributions from each of the grid locations.

Chapter 5

Chapter 11

598

12. CONTINUOUS LATENT VARIABLES

Figure 12.21 Plot of the oil ﬂow data set visualized using PCA on the left and GTM on the right. For the GTM
model, each data point is plotted at the mean of its posterior distribution in latent space. The nonlinearity of the
GTM model allows the separation between the groups of data points to be seen more clearly.

Chapter 3

Section 1.4

The nonlinear mapping is given by a linear regression model that allows for general
nonlinearity while being a linear function of the adaptive parameters. Note that the
usual limitation of linear regression models arising from the curse of dimensionality
does not arise in the context of the GTM since the manifold generally has two dimen-
sions irrespective of the dimensionality of the data space. A consequence of these
two choices is that the likelihood function can be expressed analytically in closed
form and can be optimized efﬁciently using the EM algorithm. The resulting GTM
model ﬁts a two-dimensional nonlinear manifold to the data set, and by evaluating
the posterior distribution over latent space for the data points, they can be projected
back to the latent space for visualization purposes. Figure 12.21 shows a comparison
of the oil data set visualized with linear PCA and with the nonlinear GTM.

The GTM can be seen as a probabilistic version of an earlier model called the self
organizing map, or SOM (Kohonen, 1982; Kohonen, 1995), which also represents
a two-dimensional nonlinear manifold as a regular array of discrete points. The
SOM is somewhat reminiscent of the K-means algorithm in that data points are
assigned to nearby prototype vectors that are then subsequently updated. Initially,
the prototypes are distributed at random, and during the training process they ‘self
organize’ so as to approximate a smooth manifold. Unlike K-means, however, the
SOM is not optimizing any well-deﬁned cost function (Erwin et al., 1992) making it
difﬁcult to set the parameters of the model and to assess convergence. There is also
no guarantee that the ‘self-organization’ will take place as this is dependent on the
choice of appropriate parameter values for any particular data set.

By contrast, GTM optimizes the log likelihood function, and the resulting model
deﬁnes a probability density in data space. In fact, it corresponds to a constrained
mixture of Gaussians in which the components share a common variance, and the
means are constrained to lie on a smooth two-dimensional manifold. This proba-

Section 6.4

Exercises

Appendix E

Exercises

599

bilistic foundation also makes it very straightforward to deﬁne generalizations of
GTM (Bishop et al., 1998a) such as a Bayesian treatment, dealing with missing val-
ues, a principled extension to discrete variables, the use of Gaussian processes to
deﬁne the manifold, or a hierarchical GTM model (Tino and Nabney, 2002).

Because the manifold in GTM is deﬁned as a continuous surface, not just at the
prototype vectors as in the SOM, it is possible to compute the magniﬁcation factors
corresponding to the local expansions and compressions of the manifold needed to
ﬁt the data set (Bishop et al., 1997b) as well as the directional curvatures of the
manifold (Tino et al., 2001). These can be visualized along with the projected data
and provide additional insight into the model.

12.1 ((cid:12) (cid:12)) www In this exercise, we use proof by induction to show that the linear
projection onto an M-dimensional subspace that maximizes the variance of the pro-
jected data is deﬁned by the M eigenvectors of the data covariance matrix S, given
by (12.3), corresponding to the M largest eigenvalues. In Section 12.1, this result
was proven for the case of M = 1. Now suppose the result holds for some general
value of M and show that it consequently holds for dimensionality M + 1. To do
this, ﬁrst set the derivative of the variance of the projected data with respect to a
vector uM +1 deﬁning the new direction in data space equal to zero. This should
be done subject to the constraints that uM +1 be orthogonal to the existing vectors
u1, . . . , uM , and also that it be normalized to unit length. Use Lagrange multipli-
ers to enforce these constraints. Then make use of the orthonormality properties of
the vectors u1, . . . , uM to show that the new vector uM +1 is an eigenvector of S.
Finally, show that the variance is maximized if the eigenvector is chosen to be the
one corresponding to eigenvector λM +1 where the eigenvalues have been ordered in
decreasing value.

(cid:4)J = Tr

(cid:19)(cid:1)UTS(cid:1)U
(cid:20)

(cid:19)
(cid:20)
H(I −(cid:1)UT(cid:1)U)

+ Tr

12.2 ((cid:12) (cid:12))

Show that the minimum value of the PCA distortion measure J given by
(12.15) with respect to the ui, subject to the orthonormality constraints (12.7), is
obtained when the ui are eigenvectors of the data covariance matrix S. To do this,
introduce a matrix H of Lagrange multipliers, one for each constraint, so that the
modiﬁed distortion measure, in matrix notation reads

where (cid:1)U is a matrix of dimension D × (D − M) whose columns are given by ui.
Now minimize(cid:4)J with respect to (cid:1)U and show that the solution satisﬁes S(cid:1)U = (cid:1)UH.
Clearly, one possible solution is that the columns of (cid:1)U are eigenvectors of S, in
and by using its eigenvector expansion show that the general solution to S(cid:1)U = (cid:1)UH
gives the same value for(cid:4)J as the speciﬁc solution in which the columns of (cid:1)U are

which case H is a diagonal matrix containing the corresponding eigenvalues. To
obtain the general solution, show that H can be assumed to be a symmetric matrix,

(12.93)

600

12. CONTINUOUS LATENT VARIABLES

the eigenvectors of S. Because these solutions are all equivalent, it is convenient to
choose the eigenvector solution.

12.3 ((cid:12)) Verify that the eigenvectors deﬁned by (12.30) are normalized to unit length,

assuming that the eigenvectors vi have unit length.

12.4 ((cid:12)) www Suppose we replace the zero-mean, unit-covariance latent space distri-
bution (12.31) in the probabilistic PCA model by a general Gaussian distribution of
the form N (z|m, Σ). By redeﬁning the parameters of the model, show that this leads
to an identical model for the marginal distribution p(x) over the observed variables
for any valid choice of m and Σ.

12.5 ((cid:12) (cid:12)) Let x be a D-dimensional random variable having a Gaussian distribution
given by N (x|µ, Σ), and consider the M-dimensional random variable given by
y = Ax + b where A is an M × D matrix. Show that y also has a Gaussian
distribution, and ﬁnd expressions for its mean and covariance. Discuss the form of
this Gaussian distribution for M < D, for M = D, and for M > D.

12.6 ((cid:12)) www Draw a directed probabilistic graph for the probabilistic PCA model
described in Section 12.2 in which the components of the observed variable x are
shown explicitly as separate nodes. Hence verify that the probabilistic PCA model
has the same independence structure as the naive Bayes model discussed in Sec-
tion 8.2.2.

12.7 ((cid:12) (cid:12)) By making use of the results (2.270) and (2.271) for the mean and covariance
of a general distribution, derive the result (12.35) for the marginal distribution p(x)
in the probabilistic PCA model.

12.8 ((cid:12) (cid:12)) www By making use of the result (2.116), show that the posterior distribution

p(z|x) for the probabilistic PCA model is given by (12.42).

12.9 ((cid:12)) Verify that maximizing the log likelihood (12.43) for the probabilistic PCA
model with respect to the parameter µ gives the result µML = x where x is the
mean of the data vectors.

12.10 ((cid:12) (cid:12)) By evaluating the second derivatives of the log likelihood function (12.43) for
the probabilistic PCA model with respect to the parameter µ, show that the stationary
point µML = x represents the unique maximum.

12.11 ((cid:12) (cid:12)) www Show that in the limit σ2 → 0, the posterior mean for the probabilistic
PCA model becomes an orthogonal projection onto the principal subspace, as in
conventional PCA.

12.12 ((cid:12) (cid:12)) For σ2 > 0 show that the posterior mean in the probabilistic PCA model is

shifted towards the origin relative to the orthogonal projection.

12.13 ((cid:12) (cid:12)) Show that the optimal reconstruction of a data point under probabilistic PCA,

according to the least squares projection cost of conventional PCA, is given by

(cid:4)x = WML(WT

MLWML)−1ME[z|x].

(12.94)

Exercises

601

12.14 ((cid:12)) The number of independent parameters in the covariance matrix for the proba-
bilistic PCA model with an M-dimensional latent space and a D-dimensional data
space is given by (12.51). Verify that in the case of M = D − 1, the number of
independent parameters is the same as in a general covariance Gaussian, whereas for
M = 0 it is the same as for a Gaussian with an isotropic covariance.

12.15 ((cid:12) (cid:12)) www Derive the M-step equations (12.56) and (12.57) for the probabilistic
PCA model by maximization of the expected complete-data log likelihood function
given by (12.53).

12.16 ((cid:12) (cid:12) (cid:12))

In Figure 12.11, we showed an application of probabilistic PCA to a data set
in which some of the data values were missing at random. Derive the EM algorithm
for maximizing the likelihood function for the probabilistic PCA model in this situ-
ation. Note that the {zn}, as well as the missing data values that are components of
the vectors {xn}, are now latent variables. Show that in the special case in which all
of the data values are observed, this reduces to the EM algorithm for probabilistic
PCA derived in Section 12.2.2.

12.17 ((cid:12) (cid:12)) www Let W be a D × M matrix whose columns deﬁne a linear subspace
of dimensionality M embedded within a data space of dimensionality D, and let µ
be a D-dimensional vector. Given a data set {xn} where n = 1, . . . , N, we can
approximate the data points using a linear mapping from a set of M-dimensional
vectors {zn}, so that xn is approximated by Wzn + µ. The associated sum-of-
squares reconstruction cost is given by

N(cid:2)

J =

(cid:5)xn − µ − Wzn(cid:5)2.

(12.95)

n=1

First show that minimizing J with respect to µ leads to an analogous expression with
xn and zn replaced by zero-mean variables xn−x and zn−z, respectively, where x
and z denote sample means. Then show that minimizing J with respect to zn, where
W is kept ﬁxed, gives rise to the PCA E step (12.58), and that minimizing J with
respect to W, where {zn} is kept ﬁxed, gives rise to the PCA M step (12.59).

12.18 ((cid:12)) Derive an expression for the number of independent parameters in the factor

analysis model described in Section 12.2.4.

12.19 ((cid:12) (cid:12)) www Show that the factor analysis model described in Section 12.2.4 is

invariant under rotations of the latent space coordinates.

12.20 ((cid:12) (cid:12)) By considering second derivatives, show that the only stationary point of
the log likelihood function for the factor analysis model discussed in Section 12.2.4
with respect to the parameter µ is given by the sample mean deﬁned by (12.1).
Furthermore, show that this stationary point is a maximum.

12.21 ((cid:12) (cid:12)) Derive the formulae (12.66) and (12.67) for the E step of the EM algorithm
for factor analysis. Note that from the result of Exercise 12.20, the parameter µ can
be replaced by the sample mean x.

602

12. CONTINUOUS LATENT VARIABLES

12.22 ((cid:12) (cid:12)) Write down an expression for the expected complete-data log likelihood func-
tion for the factor analysis model, and hence derive the corresponding M step equa-
tions (12.69) and (12.70).

12.23 ((cid:12)) www Draw a directed probabilistic graphical model representing a discrete
mixture of probabilistic PCA models in which each PCA model has its own values
of W, µ, and σ2. Now draw a modiﬁed graph in which these parameter values are
shared between the components of the mixture.

12.24 ((cid:12) (cid:12) (cid:12)) We saw in Section 2.3.7 that Student’s t-distribution can be viewed as an
inﬁnite mixture of Gaussians in which we marginalize with respect to a continu-
ous latent variable. By exploiting this representation, formulate an EM algorithm
for maximizing the log likelihood function for a multivariate Student’s t-distribution
given an observed set of data points, and derive the forms of the E and M step equa-
tions.

12.25 ((cid:12) (cid:12)) www Consider a linear-Gaussian latent-variable model having a latent space
distribution p(z) = N (x|0, I) and a conditional distribution for the observed vari-
able p(x|z) = N (x|Wz + µ, Φ) where Φ is an arbitrary symmetric, positive-
deﬁnite noise covariance matrix. Now suppose that we make a nonsingular linear
transformation of the data variables x → Ax, where A is a D × D matrix.
If
µML, WML and ΦML represent the maximum likelihood solution corresponding to
the original untransformed data, show that AµML, AWML, and AΦMLAT will rep-
resent the corresponding maximum likelihood solution for the transformed data set.
Finally, show that the form of the model is preserved in two cases: (i) A is a diagonal
matrix and Φ is a diagonal matrix. This corresponds to the case of factor analysis.
The transformed Φ remains diagonal, and hence factor analysis is covariant under
component-wise re-scaling of the data variables; (ii) A is orthogonal and Φ is pro-
portional to the unit matrix so that Φ = σ2I. This corresponds to probabilistic PCA.
The transformed Φ matrix remains proportional to the unit matrix, and hence proba-
bilistic PCA is covariant under a rotation of the axes of data space, as is the case for
conventional PCA.

12.26 ((cid:12) (cid:12)) Show that any vector ai that satisﬁes (12.80) will also satisfy (12.79). Also,
show that for any solution of (12.80) having eigenvalue λ, we can add any multiple
of an eigenvector of K having zero eigenvalue, and obtain a solution to (12.79)
that also has eigenvalue λ. Finally, show that such modiﬁcations do not affect the
principal-component projection given by (12.82).

12.27 ((cid:12) (cid:12)) Show that the conventional linear PCA algorithm is recovered as a special case

of kernel PCA if we choose the linear kernel function given by k(x, x(cid:4)) = xTx(cid:4)

.

12.28 ((cid:12) (cid:12)) www Use the transformation property (1.27) of a probability density under
a change of variable to show that any density p(y) can be obtained from a ﬁxed
density q(x) that is everywhere nonzero by making a nonlinear change of variable
(cid:4)(x) < ∞. Write
y = f(x) in which f(x) is a monotonic function so that 0 (cid:1) f
down the differential equation satisﬁed by f(x) and draw a diagram illustrating the
transformation of the density.

Exercises

603

12.29 ((cid:12) (cid:12)) www Suppose that two variables z1 and z2 are independent so that p(z1, z2) =
p(z1)p(z2). Show that the covariance matrix between these variables is diagonal.
This shows that independence is a sufﬁcient condition for two variables to be un-
correlated. Now consider two variables y1 and y2 in which −1 (cid:1) y1 (cid:1) 1 and
2. Write down the conditional distribution p(y2|y1) and observe that this is
y2 = y2
dependent on y1, showing that the two variables are not independent. Now show
that the covariance matrix between these two variables is again diagonal. To do this,
use the relation p(y1, y2) = p(y1)p(y2|y1) to show that the off-diagonal terms are
zero. This counter-example shows that zero correlation is not a sufﬁcient condition
for independence.

